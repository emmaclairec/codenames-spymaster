{"id": "15012", "url": "https://en.wikipedia.org/wiki?curid=15012", "title": "Islamism", "text": "Islamism\n\nIslamism is a concept whose meaning has been debated in both public and academic contexts. The term can refer to diverse forms of social and political activism advocating that public and political life should be guided by Islamic principles or more specifically to movements which call for full implementation of \"sharia\". It is commonly used interchangeably with the terms political Islam or Islamic fundamentalism. In academic usage, the term \"Islamism\" does not specify what vision of \"Islamic order\" or sharia are being advocated, or how their advocates intend to bring them about. In Western mass media it tends to refer to groups whose aim is to establish a sharia-based Islamic state, often with implication of violent tactics and human rights violations, and has acquired connotations of political extremism. In the Muslim world, the term has positive connotations among its proponents.\n\nDifferent currents of Islamist thought include advocating a \"revolutionary\" strategy of Islamizing society through exercise of state power, and alternately a \"reformist\" strategy to re-Islamizing society through grass-roots social and political activism. Islamists may emphasize the implementation of sharia (Islamic law); pan-Islamic political unity, including an Islamic state; or selective removal of non-Muslim, particularly Western military, economic, political, social, or cultural influences in the Muslim world that they believe to be incompatible with Islam.\n\nGraham Fuller has argued for a broader notion of Islamism as a form of identity politics, involving \"support for [Muslim] identity, authenticity, broader regionalism, revivalism, [and] revitalization of the community.\" Some authors hold the term \"Islamic activism\" to be synonymous and preferable to \"Islamism\", and Rached Ghannouchi writes that Islamists prefer to use the term \"Islamic movement\" themselves.\n\nCentral and prominent figures in twentieth-century Islamism include Hasan al-Banna, Sayyid Qutb, Abul Ala Maududi, and Ruhollah Khomeini. Most Islamist thinkers emphasize peaceful political processes, which are supported by the majority of contemporary Islamists. Others, Sayyid Qutb in particular, called for violence, and his followers are generally considered Islamic extremists, although Qutb denounced the killing of innocents.\nAccording to Robin Wright, Islamist movements have \"arguably altered the Middle East more than any trend since the modern states gained independence\", redefining \"politics and even borders\". Following the Arab Spring, some Islamist currents became heavily involved in democratic politics, while others spawned \"the most aggressive and ambitious Islamist militia\" to date, ISIS.\n\nThe term \"Islamism\", which originally denoted the religion of Islam, first appeared in the English language as \"Islamismus\" in 1696, and as \"Islamism\" in 1712. The term appears in the U.S. Supreme Court decision in \"In Re Ross\" (1891). By the turn of the twentieth century the shorter and purely Arabic term \"Islam\" had begun to displace it, and by 1938, when Orientalist scholars completed \"The Encyclopaedia of Islam\", \"Islamism\" seems to have virtually disappeared from English usage.\n\nThe term \"Islamism\" acquired its contemporary connotations in French academia in the late 1970s and early 1980s. From French, it began to migrate to the English language in the mid-1980s, and in recent years has largely displaced the term Islamic fundamentalism in academic circles.\n\nThe new use of the term \"Islamism\" at first functioned as \"a marker for scholars more likely to sympathize\" with new Islamic movements; however, as the term gained popularity it became more specifically associated with political groups such as the Taliban or the Algerian Armed Islamic Group, as well as with highly publicized acts of violence.\n\n\"Islamists\" who have spoken out against the use of the term, insisting they are merely \"Muslims\", include Ayatollah Mohammad Hussein Fadlallah (1935-2010), the spiritual mentor of Hezbollah, and Abbassi Madani (1931- ), leader of the Algerian Islamic Salvation Front.\n\nA 2003 article in the \"Middle East Quarterly\" states:\nIn summation, the term Islamism enjoyed its first run, lasting from Voltaire to the First World War, as a synonym for Islam. Enlightened scholars and writers generally preferred it to Mohammedanism. Eventually both terms yielded to Islam, the Arabic name of the faith, and a word free of either pejorative or comparative associations. There was no need for any other term, until the rise of an ideological and political interpretation of Islam challenged scholars and commentators to come up with an alternative, to distinguish Islam as modern ideology from Islam as a faith... To all intents and purposes, Islamic fundamentalism and Islamism have become synonyms in contemporary American usage.\n\nThe Council on American–Islamic Relations complained in 2013 that the Associated Press's definition of \"Islamist\"—a \"supporter of government in accord with the laws of Islam [and] who view the Quran as a political model\"—had become a pejorative shorthand for \"Muslims we don't like\". Mansoor Moaddel, a sociologist at Eastern Michigan University, criticized it as \"not a good term\" because \"the use of the term Islamist does not capture the phenomena that is quite heterogeneous.\"\n\nThe AP Stylebook entry for \"Islamist\" reads as follows: \"An advocate or supporter of a political movement that favors reordering government and society in accordance with laws prescribed by Islam. Do not use as a synonym for Islamic fighters, militants, extremists or radicals, who may or may not be Islamists. Where possible, be specific and use the name of militant affiliations: al-Qaida-linked, Hezbollah, Taliban, etc. Those who view the Quran as a political model encompass a wide range of Muslims, from mainstream politicians to militants known as jihadi.\"\n\nIslamism has been defined as:\nBULLET::::- \"the belief that Islam should guide social and political as well as personal life\",\nBULLET::::- a form of \"religionized politics\" and an instance of religious fundamentalism\nBULLET::::- \"political movement that favors reordering government and society in accordance with laws prescribed by Islam\" (from Associated Press's definition of \"Islamist\")\nBULLET::::- \"[the term 'Islamist' has become shorthand for] 'Muslims we don't like.'\" (from Council on American–Islamic Relations's complaint about AP's earlier definition of Islamist)\nBULLET::::- \"a theocratic ideology that seeks to impose any version of Islam over society \"by law\"\". (Maajid Nawaz, a former Islamist turned critic). Subsequently, clarified to be \"the desire to impose any given interpretation of Islam on society\".\nBULLET::::- \"the [Islamic] ideology that guides society as a whole and that [teaches] law must be in conformity with the Islamic sharia\",\nBULLET::::- a term \"used by outsiders to denote a strand of activity which they think justifies their misconception of Islam as something rigid and immobile, a mere tribal affiliation.\"\nBULLET::::- a movement so broad and flexible it reaches out to \"everything to everyone\" in Islam, making it \"unsustainable\".\nBULLET::::- an alternative social provider to the poor masses;\nBULLET::::- an angry platform for the disillusioned young;\nBULLET::::- a loud trumpet-call announcing \"a return to the pure religion\" to those seeking an identity;\nBULLET::::- a \"progressive, moderate religious platform\" for the affluent and liberal;\nBULLET::::- ... and at the extremes, a violent vehicle for rejectionists and radicals.\nBULLET::::- an Islamic \"movement that seeks cultural differentiation from the West and reconnection with the pre-colonial symbolic universe\",\nBULLET::::- \"the organised political trend [...] that seeks to solve modern political problems by reference to Muslim texts [...] the whole body of thought which seeks to invest society with Islam which may be integrationist, but may also be traditionalist, reform-minded or even revolutionary\"\nBULLET::::- \"the active assertion and promotion of beliefs, prescriptions, laws or policies that are held to be Islamic in character,\"\nBULLET::::- a movement of \"Muslims who draw upon the belief, symbols, and language of Islam to inspire, shape, and animate political activity;\" which may contain moderate, tolerant, peaceful activists or those who \"preach intolerance and espouse violence.\"\nBULLET::::- \"All who seek to Islamize their environment, whether in relation to their lives in society, their family circumstances, or the workplace, may be described as Islamists.\"\n\nIslamism takes different forms and spans a wide range of strategies and tactics towards the powers in place—\"destruction, opposition, collaboration, indifference\" that have varied as \"circumstances have changed\"—and thus is not a united movement.\n\nModerate and reformist Islamists who accept and work within the democratic process include parties like the Tunisian Ennahda Movement. Jamaat-e-Islami of Pakistan is basically a socio-political and democratic Vanguard party but has also gained political influence through military coup d'états in the past. Other Islamist groups like Hezbollah in Lebanon and Hamas in Palestine participate in the democratic and political process as well as armed attacks. Jihadist organizations like al-Qaeda and the Egyptian Islamic Jihad, and groups such as the Taliban, entirely reject democracy, often declaring as \"kuffar\" those Muslims who support it (see \"takfirism\"), as well as calling for violent/offensive jihad or urging and conducting attacks on a religious basis.\n\nAnother major division within Islamism is between what Graham E. Fuller has described as the fundamentalist \"guardians of the tradition\" (Salafis, such as those in the Wahhabi movement) and the \"vanguard of change and Islamic reform\" centered around the Muslim Brotherhood. Olivier Roy argues that \"Sunni pan-Islamism underwent a remarkable shift in the second half of the 20th century\" when the Muslim Brotherhood movement and its focus on Islamisation of pan-Arabism was eclipsed by the Salafi movement with its emphasis on \"sharia rather than the building of Islamic institutions,\" and rejection of Shia Islam. Following the Arab Spring, Roy has described Islamism as \"increasingly interdependent\" with democracy in much of the Arab Muslim world, such that \"neither can now survive without the other.\" While Islamist political culture itself may not be democratic, Islamists need democratic elections to maintain their legitimacy. At the same time, their popularity is such that no government can call itself democratic that excludes mainstream Islamist groups.\n\nThe relationship between the notions of Islam and Islamism has been subject to disagreement.\n\nHayri Abaza argues that the failure to distinguish between Islam and Islamism leads many in the West to support illiberal Islamic regimes, to the detriment of progressive moderates who seek to separate religion from politics.\nIn contrast, Abid Ullah Jan, writes \"If Islam is a way of life, how can we say that those who want to live by its principles in legal, social, political, economic, and political spheres of life are not Muslims, but Islamists and believe in Islamism, not [just] Islam.\" A writer for the International Crisis Group maintains that \"the conception of 'political Islam'\" is a creation of Americans to explain the Iranian Islamic Revolution and apolitical Islam was a historical fluke of the \"short-lived era of the heyday of secular Arab nationalism between 1945 and 1970\", and it is quietist/non-political Islam, not Islamism, that requires explanation.\n\nAnother source distinguishes Islamist from Islamic \"by the fact that the latter refers to a religion and culture in existence over a millennium, whereas the first is a political/religious phenomenon linked to the great events of the 20th century\". Islamists have, at least at times, defined themselves as \"Islamiyyoun/Islamists\" to differentiate themselves from \"Muslimun/Muslims\". Daniel Pipes describes Islamism as a modern ideology that owes more to European utopian political ideologies and \"isms\" than to the traditional Islamic religion.\n\nFew observers contest the influence of Islamism within the Muslim world. Following the collapse of the Soviet Union, political movements based on the liberal ideology of free expression and democratic rule have led the opposition in other parts of the world such as Latin America, Eastern Europe and many parts of Asia; however \"the simple fact is that political Islam currently reigns as the most powerful ideological force across the Muslim world today\".\n\nPeople see the unchanging socioeconomic condition in the Muslim world as a major factor. Olivier Roy believes \"the socioeconomic realities that sustained the Islamist wave are still here and are not going to change: poverty, uprootedness, crises in values and identities, the decay of the educational systems, the North-South opposition, and the problem of immigrant integration into the host societies\".\n\nThe strength of Islamism also draws from the strength of religiosity in general in the Muslim world. Compared to Western societies, \"[w]hat is striking about the Islamic world is that ... it seems to have been the least penetrated by irreligion\". Where other peoples may look to the physical or social sciences for answers in areas which their ancestors regarded as best left to scripture, in the Muslim world, religion has become more encompassing, not less, as \"in the last few decades, it has been the fundamentalists who have increasingly represented the cutting edge\" of Muslim culture.\n\nEven before the Arab Spring, Islamists in Egypt and other Muslim countries had been described as \"extremely influential. ... They determine how one dresses, what one eats. In these areas, they are incredibly successful. ... Even if the Islamists never come to power, they have transformed their countries.\" Democratic, peaceful and political Islamists are now dominating the spectrum of Islamist ideology as well as the political system of the Muslim world. Moderate strains of Islamism have been described as \"competing in the democratic public square in places like Turkey, Tunisia, Malaysia and Indonesia\".\n\nModerate Islamism is the emerging Islamist discourses and movements which considered deviated from the traditional Islamist discourses of the mid-20th century. Moderate Islamism is characterized by pragmatic participation within the existing constitutional and political framework, in the most cases democratic institution. Moderate Islamists make up the majority of the contemporary Islamist movements. From the philosophical perspective, their discourses are represented by reformation or reinterpretation of modern socio-political institutions and values imported from the West including democracy. This had led to the conception of Islamic form of such institutions, and Islamic interpretations are often attempted within this conception. In the example of democracy, Islamic democracy as an Islamized form of the system has been intellectually developed. In Islamic democracy, the concept of \"shura\", the tradition of consultation which considered as Sunnah of the prophet Muhammad, is invoked to Islamically reinterpret and legitimatize the institution of democracy.\n\nPerformance, goal, strategy, and outcome of moderate Islamist movements vary considerably depending on the country and its socio-political and historical context. In terms of performance, most of the Islamist political parties are oppositions. However, there are few examples they govern or obtain the substantial amount of the popular votes. This includes National Congress of Sudan, National Iraqi Alliance of Iraq and Justice and Development Party (PJD) of Morocco. Their goal also ranges widely. The Ennahda Movement of Tunisia and Prosperous Justice Party (PKS) of Indonesia formally resigned their vision of implementing sharia. In Morocco, PJD supported King Muhammad VI's \"Mudawana\", a \"startlingly progressive family law\" which grants women the right to a divorce, raises the minimum age for marriage to 18, and, in the event of separation, stipulates equal distribution of property. To the contrary, National Congress of Sudan has implemented the strict interpretation of sharia with the foreign support from the conservative states. Movements of the former category are also termed as Post-Islamism (see below). Their political outcome is interdependent with their goal and strategy, in which what analysts call \"inclusion-moderation theory\" is in effect. Inclusion-moderation theory assumes that the more lenient the Islamists become, the less likely their survival will be threatened. Similarly, the more accommodating the government be, the less extreme Islamists become.\n\nModerate Islamism within the democratic institution is a relatively recent phenomenon. Throughout the 80s and 90s, major moderate Islamist movements such as the Muslim Brotherhood and the Ennahda were excluded from democratic political participation. Islamist movements operated within the state framework were markedly scrutinized during the Algerian Civil War (1991-2002) and after the increase of terrorism in Egypt in the 90s. Reflecting on these failures, Islamists turned increasingly into revisionist and receptive to democratic procedures in the 21st century. The possibility of accommodating this new wave of modernist Islamism has been explored among the Western intellectuals, with the concept such as Turkish model was proposed. The concept was inspired by the perceived success of Turkish Justice and Development Party (AKP) led by Recep Tayyip Erdogan in harmonizing the Islamist principles within the secular state framework. Turkish model, however, has been considered came \"unstuck\" after recent purge and violations of democratic principles by the Erdogan regime. Critics of the concept hold that Islamist aspirations are fundamentally incompatible with the democratic principles, thus even moderate Islamists are totalitarian in nature. As such, it requires strong constitutional checks and the effort of the mainstream Islam to detach political Islam from the public discourses.\n\nPost-Islamism is a term proposed by Iranian political sociologist Asef Bayat, referring to the Islamist movements which marked by the critical departure from the traditional Islamist discourses of the mid-20th century. Bayat explained it as \"a condition where, following a phase of experimentation, the appeal, energy, symbols and sources of legitimacy of Islamism get exhausted, even among its once-ardent supporters. As such, post-Islamism is not anti-Islamic, but rather reflects a tendency to resecularize religion.\" It originally pertained only to Iran, where \"post-Islamism is expressed in the idea of fusion between Islam (as a personalized faith) and individual freedom and choice; and post-Islamism is associated with the values of democracy and aspects of modernity\". A 2008 Lowy Institute for International Policy paper suggests that PKS of Indonesia and AKP of Turkey are post-Islamist. The characterization can be applied to Malaysian Islamic Party (PAS), and used to describe the \"ideological evolution\" within the Ennahda of Tunisia.\n\nThe contemporary Salafi movement encompasses a broad range of ultraconservative Islamist doctrines which share the reformist mission of Ibn Taymiyyah. From the perspective of political Islam, the Salafi movement can be broadly categorized into three groups; the quietist (or the purist), the activist (or \"haraki\") and the jihadist (Salafi jihadism, see below). The quietist school advocates for societal reform through religious education and proselytizing rather than political activism. The activist school, to the contrary, encourages political participation within the constitutional and political framework. The jihadist school is inspired by the ideology of Sayyid Qutb (Qutbism, see below), and rejects the legitimacy of secular institutions and promotes the revolution in order to pave the way for the establishment of a new Caliphate.\n\nThe quietist Salafi movement is stemming from the teaching of Nasiruddin Albani, who challenged the notion of \"taqlid\" (imitation, conformity to the legal precedent) as a blind adherence. As such, they alarm the political participation as potentially leading to the division of the Muslim community. This school is exemplified by Madkhalism which based on the writings of Rabee al-Madkhali. Madkhalism was originated in the 90s Saudi Arabia, as a reaction against the rise of the Salafi activism and the threat of Salafi Jihadism. It rejects any kind of opposition against the secular governance, thus endorsed by the authoritarian governments of Egypt and Saudi Arabia during the 90s. The influence of the quietist school has waned significantly in the Middle East recently, as the governments began incorporating Islamist factions emanating from the popular demand.\n\nThe politically active Salafi movement, Salafi activism or \"harakis\", is based on the religious belief that endorses non-violent political activism in order to protect God's Divine governance. This means that politics is a field which requires Salafi principles to be applied as well, in the same manner with other aspects of society and life. Salafi activism was originated in the 50s to 60s Saudi Arabia, where many Muslim Brothers had taken refuge from the prosecution by the Nasser regime. There, Muslim Brothers' Islamism had synthesized with Salafism, and led to the creation of the Salafi activist trend exemplified by the Sahwa movement in the 80s, promulgated by Safar Al-Hawali and Salman al-Ouda. Today, the school makes up the majority of Salafism. There are many active Salafist political parties throughout the Muslim world, including Al Nour Party of Egypt, Al Islah of Yemen and Al Asalah of Bahrain.\n\nThe antecedent of the contemporary Salafi movement is Wahhabism, which traces back to the 18th-century reform movement in Najd by Muhammad ibn Abd al-Wahhab. Although having different roots, Wahhabism and Salafism are considered more or less merged in the 60s Saudi Arabia. In the process, Salafism had been greatly influenced by Wahhabism, and today they share the similar religious outlook. Wahhabism is also described as a Saudi brand of Salafism. From the political perspective, Wahhabism is marked in its teaching of \"bay'ah\" (oath to allegiance), which requires Muslims to present an allegiance to the ruler of the society. Wahhabis have traditionally given their allegiance to the House of Saud, and this has made them apolitical in Saudi Arabia. However, there are small numbers of other strains including Salafi Jihadist offshoot which decline to present an allegiance to the House of Saud. Wahhabism is also characterized by its disinterest in social justice, anticolonialism, or economic equality, expounded upon by the mainstream Islamists. Historically, Wahhabism was state-sponsored and internationally propagated by Saudi Arabia with the help of funding from mainly Saudi petroleum exports, leading to the \"explosive growth\" of its influence (and subsequently, the influence of Salafism) from the 70s (a phenomenon often dubbed as Petro-Islam). Today, both Wahhabism and Salafism exert their influence worldwide, and they have been indirectly contributing to the upsurge of Salafi Jihadism as well.\n\nQutbism is an ideology formulated by Sayyid Qutb, an influential figure of the Muslim Brotherhood during the 50s and 60s, which justifies the use of violence in order to push the Islamist goals. Qutbism is marked by the two distinct methodological concepts; one is \"takfirism\", which in the context of Qutbism, indicates the excommunication of fellow Muslims who are deemed equivalent to apostate, and another is \"offensive Jihad\", a concept which promotes violence in the name of Islam against the perceived \"kuffar\" (infidels). Based on the two concepts, Qutbism promotes engagement against the state apparatus in order to topple down its regime. Fusion of Qutbism and Salafi Movement had resulted in the development of Salafi jihadism (see below).\n\nQutbism is considered a product of the extreme repression experienced by Qutb and his fellow Muslim Brothers under the Nasser regime, which was resulted from the 1954 Muslim Brothers plot to assassinate Nasser. During the repression, thousands of Muslim Brothers were imprisoned, many of them, including Qutb, tortured and held in concentration camps. Under this condition, Qutb had cultivated his Islamist ideology in his seminal work \"Ma'alim fi-l-Tariq (Milestones)\", in which he equated the Muslims within the Nasser regime with secularism and the West, and described them as regression back to \"jahiliyyah\" (period of time before the advent of Islam). In this context, he allowed the \"tafkir\" (which was an unusual practice before the rejuvenation by Qutb) of said Muslims. Although Qutb was executed before the completion of his ideology, his idea was disseminated and continuously expanded by the later generations, among them Abdullah Yusuf Azzam and Ayman Al-Zawahiri, who was a student of Qutb's brother Muhammad Qutb and later became a mentor of Osama bin Laden. Al-Zawahiri was considered \"the purity of Qutb's character and the torment he had endured in prison,\" and had played an extensive role in the normalization of offensive Jihad within the Qutbist discourse. Both al-Zawahiri and bin Laden had become the core of Jihadist movements which exponentially developed in the backdrop of the late 20th-century geopolitical crisis throughout the Muslim world.\n\nSalafi jihadism is a term coined by Gilles Kepel in 2002, referring to the ideology which actively promotes and conducts violence and terrorism in order to pursue the establishment of an Islamic state or a new Caliphate. Today, the term is often simplified to \"Jihadism\" or \"Jihadist movement\" in popular usage according to Martin Kramer. It is a hybrid ideology between Qutbism, Salafism, Wahhabism and other minor Islamist strains. Qutbism taught by scholars like Abdullah Azzam provided the political intellectual underpinnings with the concepts like takfirism, and Salafism and Wahhabism provided the religious intellectual input. Salafi Jihadism makes up a tiny minority of the contemporary Islamist movements.\n\nDistinct characteristics of Salafi Jihadism noted by Robin Wright include the formal process of taking \"bay'ah\" (oath of allegiance) to the leader, which is inspired by the Wahhabi teaching. Another characteristic is its flexibility to cut ties with the less-popular movements when its strategically or financially convenient, exemplified by the relations between al-Qaeda and al-Nusra Front. Other marked developments of Salafi Jihadism include the concepts of \"near enemy\" and \"far enemy\". \"Near enemy\" connotes the despotic regime occupying the Muslim society, and the term was coined by Mohammed Abdul-Salam Farag in order to justify the assassination of Anwar al-Sadat by the Salafi Jihadi organization Egyptian Islamic Jihad (EIJ) in 1981. Later, the concept of \"far enemy\" which connotes the West was introduced and formally declared by al-Qaeda in 1996.\n\nSalafi Jihadism emerged out during the 80s when the Soviet invaded Afghanistan. Local mujahideen had extracted financial, logistical and military support from Saudi Arabia, Pakistan and the United States. Later, Osama bin Laden established al-Qaeda as a transnational Salafi Jihadi organization in 1988 to capitalize this financial, logistical and military network and to expand their operation. The ideology had seen its rise during the 90s when the Muslim world experienced numerous geopolitical crisis, notably the Algerian Civil War (1991–2002), Bosnian War (1992–1995), and the First Chechen War (1994–1996). Within these conflicts, political Islam often acted as a mobilizing factor for the local belligerents, who demanded financial, logistical and military support from al-Qaeda, in the exchange for active proliferation of the ideology. After the 1998 bombings of US embassies, September 11 attacks (2001), the US-led invasion of Afghanistan (2001) and Iraq (2003), Salafi Jihadism had seen its momentum. However, it got devastated by the US counterterrorism operations, culminated in bin Laden's death in 2011. After the Arab Spring (2011) and subsequent Syrian Civil War (2011–present), the remnants of al-Qaeda franchise in Iraq had restored their capacity, which rapidly developed into the Islamic State of Iraq and the Levant, spreading its influence throughout the conflict zones of MENA region and the globe.\n\nSome Islamic revivalist movements and leaders pre-dating Islamism include:\nBULLET::::- Ahmad Sirhindi (~1564–1624) was part of a reassertion of orthodoxy within Islamic Mysticism (Taṣawwuf) and was known to his followers as the 'renovator of the second millennium'. It has been said of Sirhindi that he 'gave to Indian Islam the rigid and conservative stamp it bears today.'\nBULLET::::- Ibn Taymiyyah, a Syrian Islamic jurist during the 13th and 14th centuries who is often quoted by contemporary Islamists. Ibn Taymiyya argued against the shirking of Sharia law, was against practices such as the celebration of Muhammad's birthday, and \"he believed that those who ask assistance from the grave of the Prophet or saints, are mushrikin (polytheists), someone who is engaged in shirk.\"\nBULLET::::- Shah Waliullah of India and Muhammad ibn Abd-al-Wahhab of Arabia were contemporaries who met each other while studying in Mecca. Muhammad ibn Abd-al-Wahhab advocated doing away with the later accretions like grave worship and getting back to the letter and the spirit of Islam as preached and practiced by Muhammad. He went on to found Wahhabism. Shah Waliullah was a forerunner of reformist Islamists like Muhammad Abduh, Muhammad Iqbal and Muhammad Asad in his belief that there was \"a constant need for new ijtihad as the Muslim community progressed and expanded and new generations had to cope with new problems\" and his interest in the social and economic problems of the poor.\nBULLET::::- Sayyid Ahmad Barelvi was a disciple and successor of Shah Waliullah's son who emphasized the 'purification' of Islam from un-Islamic beliefs and practices. He anticipated modern militant Islamists by leading an extremist, jihadist movement and attempted to create an Islamic state based on the enforcement of Islamic law. While he battled Sikh fundamentalist rule in Muslim-majority North-Western India, his followers fought against British colonialism after his death and allied themselves with the Indian Mutiny.\nBULLET::::- After the failure of the Indian Mutiny, some of Shah Waliullah's followers turned to more peaceful methods for preserving India's Islamic heritage and founded the Dar al-Ulum seminary in 1867 in the town of Deoband. From the school developed the Deobandi movement which became the largest philosophical movement of traditional Islamic thought on the subcontinent and led to the establishment of thousands of madrasahs throughout modern-day India, Pakistan and Bangladesh.\n\nThe end of the 19th century saw the dismemberment of most of the Muslim Ottoman Empire by non-Muslim European colonial powers. The empire spent massive sums on Western civilian and military technology to try to modernize and compete with the encroaching European powers, and in the process went deep into debt to these powers.\n\nIn this context, the publications of Jamal ad-din al-Afghani (1837–97), Muhammad Abduh (1849–1905) and Rashid Rida (1865–1935) preached Islamic alternatives to the political, economic, and cultural decline of the empire. Muhammad Abduh and Rashid Rida formed the beginning of the Islamist movement, as well as the reformist Islamist movement.\n\nTheir ideas included the creation of a truly Islamic society under sharia law, and the rejection of taqlid, the blind imitation of earlier authorities, which they believed deviated from the true messages of Islam. Unlike some later Islamists, Early Salafiyya strongly emphasized the restoration of the Caliphate.\n\nMuhammad Iqbal was a philosopher, poet and politician in British India who is widely regarded as having inspired the Islamic Nationalism and Pakistan Movement in British India. Iqbal is admired as a prominent classical poet by Pakistani, Iranian, Indian and other international scholars of literature. Though Iqbal is best known as an eminent poet, he is also a highly acclaimed \"Islamic philosophical thinker of modern times\".\n\nWhile studying law and philosophy in England and Germany, Iqbal became a member of the London branch of the All India Muslim League. He came back to Lahore in 1908. While dividing his time between law practice and philosophical poetry, Iqbal had remained active in the Muslim League. He did not support Indian involvement in World War I and remained in close touch with Muslim political leaders such as Muhammad Ali Johar and Muhammad Ali Jinnah. He was a critic of the mainstream Indian nationalist and secularist Indian National Congress. Iqbal's seven English lectures were published by Oxford University press in 1934 in a book titled The Reconstruction of Religious Thought in Islam. These lectures dwell on the role of Islam as a religion as well as a political and legal philosophy in the modern age.\n\nIqbal expressed fears that not only would secularism and secular nationalism weaken the spiritual foundations of Islam and Muslim society, but that India's Hindu-majority population would crowd out Muslim heritage, culture and political influence. In his travels to Egypt, Afghanistan, Palestine and Syria, he promoted ideas of greater Islamic political co-operation and unity, calling for the shedding of nationalist differences. Sir Mummad Iqbal was elected president of the Muslim League in 1930 at its session in Allahabad as well as for the session in Lahore in 1932. In his Allahabad Address on 29 December 1930, Iqbal outlined a vision of an independent state for Muslim-majority provinces in northwestern India. This address later inspired the Pakistan movement.\n\nThe thoughts and vision of Iqbal later influenced many reformist Islamists, e.g., Muhammad Asad, Sayyid Abul Ala Maududi and Ali Shariati.\n\nSayyid Abul Ala Maududi was an important early twentieth-century figure in the Islamic revival in India, and then after independence from Britain, in Pakistan. Trained as a lawyer he chose the profession of journalism, and wrote about contemporary issues and most importantly about Islam and Islamic law. Maududi founded the Jamaat-e-Islami party in 1941 and remained its leader until 1972. However, Maududi had much more impact through his writing than through his political organising. His extremely influential books (translated into many languages) placed Islam in a modern context, and influenced not only conservative ulema but liberal modernizer Islamists such as al-Faruqi, whose \"Islamization of Knowledge\" carried forward some of Maududi's key principles.\n\nMaududi believed that Islam was all-encompassing: \"Everything in the universe is 'Muslim' for it obeys God by submission to His laws... The man who denies God is called Kafir (concealer) because he conceals by his disbelief what is inherent in his nature and embalmed in his own soul.\"\n\nMaududi also believed that Muslim society could not be Islamic without Sharia, and Islam required the establishment of an Islamic state. This state should be a \"theo-democracy,\" based on the principles of: \"tawhid\" (unity of God), \"risala\" (prophethood) and \"khilafa\" (caliphate).\nAlthough Maududi talked about Islamic revolution, by \"revolution\" he meant not the violence or populist policies of the Iranian Revolution, but the gradual changing the hearts and minds of individuals from the top of society downward through an educational process or \"da'wah\".\n\nRoughly contemporaneous with Maududi was the founding of the Muslim Brotherhood in Ismailiyah, Egypt in 1928 by Hassan al Banna. His was arguably the first, largest and most influential modern Islamic political/religious organization. Under the motto \"the Qur'an is our constitution,\"\nit sought Islamic revival through preaching and also by providing basic community services including schools, mosques, and workshops. Like Maududi, Al Banna believed in the necessity of government rule based on Shariah law implemented gradually and by persuasion, and of eliminating all imperialist influence in the Muslim world.\n\nSome elements of the Brotherhood, though perhaps against orders, did engage in violence against the government, and its founder Al-Banna was assassinated in 1949 in retaliation for the assassination of Egypt's premier Mahmud Fami Naqrashi three months earlier. The Brotherhood has suffered periodic repression in Egypt and has been banned several times, in 1948 and several years later following confrontations with Egyptian president Gamal Abdul Nasser, who jailed thousands of members for several years.\n\nDespite periodic repression, the Brotherhood has become one of the most influential movements in the Islamic world, particularly in the Arab world. For many years it was\ndescribed as \"semi-legal\" and was the only opposition group in Egypt able to field candidates during elections. In the Egyptian parliamentary election, 2011–2012, the political parties identified as \"Islamist\" (the Brotherhood's Freedom and Justice Party, Salafi Al-Nour Party and liberal Islamist Al-Wasat Party) won 75% of the total seats. Mohamed Morsi, an Islamist of Muslim Brotherhood, was the first democratically elected president of Egypt. He was deposed during the 2013 Egyptian coup d'état.\n\nMaududi's political ideas influenced Sayyid Qutb a leading member of the Muslim Brotherhood movement, and one of the key philosophers of Islamism and highly influential thinkers of Islamic universalism. Qutb believed things had reached such a state that the Muslim community had literally ceased to exist. It \"has been extinct for a few centuries,\" having reverted to Godless ignorance (Jahiliyya).\n\nTo eliminate jahiliyya, Qutb argued Sharia, or Islamic law, must be established. Sharia law was not only accessible to humans and essential to the existence of Islam, but also all-encompassing, precluding \"evil and corrupt\" non-Islamic ideologies like communism, nationalism, or secular democracy.\n\nQutb preached that Muslims must engage in a two-pronged attack of converting individuals through preaching Islam peacefully and also waging what he called militant jihad so as to forcibly eliminate the \"power structures\" of Jahiliyya—not only from the Islamic homeland but from the face of the earth.\n\nQutb was both a member of the brotherhood and enormously influential in the Muslim world at large. Qutb is considered by some (Fawaz A. Gerges) to be \"the founding father and leading theoretician\" of modern jihadists, such as Osama bin Laden. However, the Muslim Brotherhood in Egypt and in Europe has not embraced his vision of undemocratic Islamic state and armed jihad, something for which they have been denounced by radical Islamists.\n\nThe quick and decisive defeat of the Arab troops during the Six-Day War by Israeli troops constituted a pivotal event in the Arab Muslim world. The defeat along with economic stagnation in the defeated countries, was blamed on the secular Arab nationalism of the ruling regimes. A steep and steady decline in the popularity and credibility of secular, socialist and nationalist politics ensued. Ba'athism, Arab socialism, and Arab nationalism suffered, and different democratic and anti-democratic Islamist movements inspired by Maududi and Sayyid Qutb gained ground.\n\nThe first modern \"Islamist state\" (with the possible exception of Zia's Pakistan) was established among the Shia of Iran. In a major shock to the rest of the world, Ayatollah Ruhollah Khomeini led the Iranian Revolution of 1979 in order to overthrow the oil-rich, well-armed, Westernized and pro-American secular monarchy ruled by Shah Muhammad Reza Pahlavi.\n\nThe views of Ali Shariati, the ideologue of the Iranian Revolution, resembled those of Mohammad Iqbal, the ideological father of the State of Pakistan, but Khomeini's beliefs are perceived to be placed somewhere between the beliefs of Shia Islam and the beliefs of Sunni Islamic thinkers like Mawdudi and Qutb. He believed that complete imitation of the Prophet Mohammad and his successors such as Ali for the restoration of Sharia law was essential to Islam, that many secular, Westernizing Muslims were actually agents of the West and therefore serving Western interests, and that acts such as the \"plundering\" of Muslim lands was part of a long-term conspiracy against Islam by Western governments.\n\nHis views differed from those of Sunni scholars in:\nBULLET::::- As a Shia, Khomeini looked to Ali ibn Abī Tālib and Husayn ibn Ali Imam, but not Caliphs Abu Bakr, Omar or Uthman.\nBULLET::::- Khomeini talked not about restoring the Caliphate or Sunni Islamic democracy, but about establishing a state where the guardianship of the democratic or the dictatorial political system was performed by Shia jurists (\"ulama\") as the successors of Shia Imams until the Mahdi returns from occultation. His concept of \"velayat-e-faqih\" (\"guardianship of the [Islamic] jurist\"), held that the leading Shia Muslim cleric in society—which Khomeini's mass of followers believed and chose to be himself—should serve as the supervisor of the state in order to protect or \"guard\" Islam and \"Sharia\" law from \"innovation\" and \"anti-Islamic laws\" passed by dictators or democratic parliaments.\n\nThe revolution was influenced by Marxism through Islamist thought and also by writings that sought either to counter Marxism (Muhammad Baqir al-Sadr's work) or to integrate socialism and Islamism (Ali Shariati's work). A strong wing of the revolutionary leadership was made up of leftists or \"radical populists\", such as Ali Akbar Mohtashami-Pur.\n\nWhile initial enthusiasm for the Iranian revolution in the Muslim world was intense, it has waned as critics hold and campaign that \"purges, executions, and atrocities tarnished its image\".\n\nThe Islamic Republic has also maintained its hold on power in Iran in spite of US economic sanctions, and has created or assisted like-minded Shia terrorist groups in Iraq, Egypt, Syria, Jordan (SCIRI) and Lebanon (Hezbollah) (two Muslim countries that also have large Shiite populations).\nDuring the 2006 Israel-Lebanon conflict, the Iranian government enjoyed something of a resurgence in popularity amongst the predominantly Sunni \"Arab street,\" due to its support for Hezbollah and to President Mahmoud Ahmadinejad's vehement opposition to the United States and his call that Israel shall vanish.\n\nThe strength of the Islamist movement was manifest in an event which might have seemed sure to turn Muslim public opinion against fundamentalism, but did just the opposite. In 1979 the Grand Mosque in Mecca Saudi Arabia was seized by an armed fundamentalist group and held for over a week. Scores were killed, including many pilgrim bystanders in a gross violation of one of the most holy sites in Islam (and one where arms and violence are strictly forbidden).\n\nInstead of prompting a backlash against the movement from which the attackers originated, however, Saudi Arabia, already very conservative, responded by shoring up its fundamentalist credentials with even more Islamic restrictions. Crackdowns followed on everything from shopkeepers who did not close for prayer and newspapers that published pictures of women, to the selling of dolls, teddy bears (images of animate objects are considered haraam), and dog food (dogs are considered unclean).\n\nIn other Muslim countries, blame for and wrath against the seizure was directed not against fundamentalists, but against Islamic fundamentalism's foremost geopolitical enemy—the United States. Ayatollah Khomeini sparked attacks on American embassies when he announced:\nIt is not beyond guessing that this is the work of criminal American imperialism and international Zionism despite the fact that the object of the fundamentalists' revolt was the Kingdom of Saudi Arabia, America's major ally in the region. Anti-American demonstrations followed in the Philippines, Turkey, Bangladesh, India, the UAE, Pakistan, and Kuwait. The US Embassy in Libya was burned by protesters chanting pro-Khomeini slogans and the embassy in Islamabad, Pakistan was burned to the ground.\n\nIn 1979, the Soviet Union deployed its 40th Army into Afghanistan, attempting to suppress an Islamic rebellion against an allied Marxist regime in the Afghan Civil War. The conflict, pitting indigenous impoverished Muslims (mujahideen) against an anti-religious superpower, galvanized thousands of Muslims around the world to send aid and sometimes to go themselves to fight for their faith. Leading this pan-Islamic effort was Palestinian sheikh Abdullah Yusuf Azzam. While the military effectiveness of these \"Afghan Arabs\" was marginal, an estimated 16,000 to 35,000 Muslim volunteers came from around the world to fight in Afghanistan.\n\nWhen the Soviet Union abandoned the Marxist Najibullah regime and withdrew from Afghanistan in 1989 (the regime finally fell in 1992), the victory was seen by many Muslims as the triumph of Islamic faith over superior military power and technology that could be duplicated elsewhere.\n\nThe jihadists gained legitimacy and prestige from their triumph both within the militant community and among ordinary Muslims, as well as the confidence to carry their jihad to other countries where they believed Muslims required assistance.\n\nThe \"veterans of the guerrilla campaign\" returning home to Algeria, Egypt, and other countries \"with their experience, ideology, and weapons,\" were often eager to continue armed jihad.\n\nThe collapse of the Soviet Union itself, in 1991, was seen by many Islamists, including Bin Laden, as the defeat of a superpower at the hands of Islam. Concerning the $6 billion in aid given by the US and Pakistan's military training and intelligence support to the mujahideen, bin Laden wrote: \"[T]he US has no mentionable role\" in \"the collapse of the Soviet Union ... rather the credit goes to God and the mujahidin\" of Afghanistan.\n\nAnother factor in the early 1990s that worked to radicalize the Islamist movement was the Gulf War, which brought several hundred thousand US and allied non-Muslim military personnel to Saudi Arabian soil to put an end to Saddam Hussein's occupation of Kuwait. Prior to 1990 Saudi Arabia played an important role in restraining the many Islamist groups that received its aid. But when Saddam, secularist and Ba'athist dictator of neighboring Iraq, attacked Kuwait (his enemy in the war), western troops came to protect the Saudi monarchy. Islamists accused the Saudi regime of being a puppet of the west.\n\nThese attacks resonated with conservative Muslims and the problem did not go away with Saddam's defeat either, since American troops remained stationed in the kingdom, and a de facto cooperation with the Palestinian-Israeli peace process developed. Saudi Arabia attempted to compensate for its loss of prestige among these groups by repressing those domestic Islamists who attacked it (bin Laden being a prime example), and increasing aid to Islamic groups (Islamist madrassas around the world and even aiding some violent Islamist groups) that did not, but its pre-war influence on behalf of moderation was greatly reduced. One result of this was a campaign of attacks on government officials and tourists in Egypt, a bloody civil war in Algeria and Osama bin Laden's terror attacks climaxing in the 9/11 attack.\n\nIn Afghanistan, the mujahideen's victory against the Soviet Union in the 1980s did not lead to justice and prosperity, due to a vicious and destructive civil war between political and tribal warlords, making Afghanistan one of the poorest countries on earth. In 1992, the Democratic Republic of Afghanistan ruled by communist forces collapsed, and democratic Islamist elements of mujahdeen founded the Islamic State of Afghanistan. In 1996, a more conservative and anti-democratic Islamist movement known as the Taliban rose to power, defeated most of the warlords and took over roughly 80% of Afghanistan.\n\nThe Taliban were spawned by the thousands of madrasahs the Deobandi movement established for impoverished Afghan refugees and supported by governmental and religious groups in neighboring Pakistan. The Taliban differed from other Islamist movements to the point where they might be more properly described as Islamic fundamentalist or neofundamentalist, interested in spreading \"an idealized and systematized version of conservative tribal village customs\" under the label of Sharia to an entire country. Their ideology was also described as being influenced by Wahhabism, and the extremist jihadism of their guest Osama bin Laden.\n\nThe Taliban considered \"politics\" to be against Sharia and thus did not hold elections. They were led by Mullah Mohammed Omar who was given the title \"Amir al-Mu'minin\" or Commander of the Faithful, and a pledge of loyalty by several hundred Taliban-selected Pashtun clergy in April 1996. Taliban were overwhelmingly Pashtun and were accused of not sharing power with the approximately 60% of Afghans who belonged to other ethnic groups. (see: Taliban#Ideology)\n\nThe Taliban's hosting of Osama bin Laden led to an American-organized attack which drove them from power following the 9/11 attacks.\nTaliban are still very much alive and fighting a vigorous insurgency with suicide bombings and armed attacks being launched against NATO and Afghan government targets.\n\nAn Islamist movement influenced by Salafism and the jihad in Afghanistan, as well as the Muslim Brotherhood, was the FIS or Front Islamique de Salut (the Islamic Salvation Front) in Algeria. Founded as a broad Islamist coalition in 1989 it was led by Abbassi Madani, and a charismatic Islamist young preacher, Ali Belhadj. Taking advantage of economic failure and unpopular social liberalization and secularization by the ruling leftist-nationalist FLN government, it used its preaching to advocate the establishment of a legal system following Sharia law, economic liberalization and development program, education in Arabic rather than French, and gender segregation, with women staying home to alleviate the high rate of unemployment among young Algerian men. The FIS won sweeping victories in local elections and it was going to win national elections in 1991 when voting was canceled by a military coup d'état.\n\nAs Islamists took up arms to overthrow the government, the FIS's leaders were arrested and it became overshadowed by Islamist guerrilla groups, particularly the Islamic Salvation Army, MIA and Armed Islamic Group (or GIA). A bloody and devastating civil war ensued in which between 150,000 and 200,000 people were killed over the next decade.\n\nThe civil war was not a victory for Islamists. By 2002 the main guerrilla groups had either been destroyed or had surrendered. The popularity of Islamist parties has declined to the point that \"the Islamist candidate, Abdallah Jaballah, came a distant third with 5% of the vote\" in the 2004 presidential election.\n\nJamaat-e-Islami Bangladesh is the largest Islamist party in the country and supports the implementation of Sharia law and promotes the country's main right-wing politics. Since 2000, the main political opposition Bangladesh Nationalist Party (BNP) has been allied with it and another Islamic party, Islami Oikya Jote. Some of their leaders and supporters, including former ministers and MPs, have been hanged for alleged war crimes during Bangladesh's struggle for independence and speaking against the ruling Bangladesh Awami League.\n\nIn the 2012, the party named \"Islam\" had four candidates and they were elected in Molenbeek and Anderlecht. In 2018, they ran candidates in 28 municipalities. Its policies include schools must offer halal food and women must be able to wear a headscarf anywhere. Another of the Islam Party's goals is to separate men and women on public transportation. The party's president argues this policy will help protect women from sexual harassment.\n\nWhile Qutb's ideas became increasingly radical during his imprisonment prior to his execution in 1966, the leadership of the Brotherhood, led by Hasan al-Hudaybi, remained moderate and interested in political negotiation and activism. Fringe or splinter movements inspired by the final writings of Qutb in the mid-1960s (particularly the manifesto \"Milestones\", a.k.a. \"Ma'alim fi-l-Tariq\") did, however, develop and they pursued a more radical direction. By the 1970s, the Brotherhood had renounced violence as a means of achieving its goals.\n\nThe path of violence and military struggle was then taken up by the Egyptian Islamic Jihad organization responsible for the assassination of Anwar Sadat in 1981. Unlike earlier anti-colonial movements the extremist group directed its attacks against what it believed were \"apostate\" leaders of Muslim states, leaders who held secular leanings or who had introduced or promoted Western/foreign ideas and practices into Islamic societies. Its views were outlined in a pamphlet written by Muhammad Abd al-Salaam Farag, in which he states:\n...there is no doubt that the first battlefield for jihad is the extermination of these infidel leaders and to replace them by a complete Islamic Order...\n\nAnother of the Egyptian groups which employed violence in their struggle for Islamic order was al-Gama'a al-Islamiyya (Islamic Group). Victims of their campaign against the Egyptian state in the 1990s included the head of the counter-terrorism police (Major General Raouf Khayrat), a parliamentary speaker (Rifaat al-Mahgoub), dozens of European tourists and Egyptian bystanders, and over 100 Egyptian police. Ultimately the campaign to overthrow the government was unsuccessful, and the major jihadi group, Jamaa Islamiya (or al-Gama'a al-Islamiyya), renounced violence in 2003. Other lesser known groups include the Islamic Liberation Party, Salvation from Hell and Takfir wal-Hijra, and these groups have variously been involved in activities such as attempted assassinations of political figures, arson of video shops and attempted takeovers of government buildings.\n\nThe Democratic Union of Muslims, a party founded in 2012, planned to take part in 2019 municipal elections. They presented candidate lists for 50 different cities. The Democratic Union of Muslims also fielded candidates for European Parliament elections. The rise of the party can be attributed to French Muslim dissatisfaction with mainstream political parties. Ultimately, it represents an alternative to the Islamophobia in France.\n\nHamas is a Palestinian Sunni Islamist organization that governs the Gaza Strip where it has moved to establish sharia law in matters such as separation of the genders, using the lash for punishment, and Islamic dress code.<ref name=\"islamist/islamic\">* \"This is particularly the case in view of the scholarly debate on the compatibility of Islam and democracy but even more so in view of Hamas's self-definition as an Islamic national liberation movement.\" \"The Palestinian Hamas: vision, violence, and coexistence\", by Shaul Mishal & Avraham Sela, 2006, p. xxviii ;\nBULLET::::- In this way the PA has been able to control the economic activities of its political adversaries, including the Hamas and other Islamic opposition groups. \"Investment in peace: politics of economic cooperation between Israel, Jordan, and the Palestinian Authority\", by Shaul Mishal, Ranan D. Kuperman, David Boas, 2001, p. 85 ;\nBULLET::::- \"Hamas is a radical Islamic fundamentalist organization that has stated that its highest priority is a Jihad (holy war) for the liberation of Palestine ...\" \"Peace and war: the Arab-Israeli military balance enters the 21st century\", by Anthony H. Cordesman, 2002, p. 243. ;\nBULLET::::- \"One of the secrets behind the success of Hamas is that it is an Islamic and national movement at one and the same time ...\" 'Hamas: Palestinian Identity, Islam, and National Sovereignty', by Meir Litvak, in \"Challenges to the cohesion of the Arabic State\", by Asher Susser, 2008, p. 153. ;\nBULLET::::- \"Hamas is an Islamic fundamentalist movement founded in 1987...\" Understanding Terrorism: Challenges, Perspectives, and Issues, by Gus Martin, 2009, p. 153. ;\nBULLET::::- \"Hamas is an Islamic jihadist organization...\" \"Why Israel Can't Wait: The Coming War Between Israel and Iran\", by Jerome R. Corsi, 2009, p. 39. ;\nBULLET::::- \"The Islamic Resistance Movement (Harakat al-Muqawama al-Islam- iyya), known by its acronym Hamas, is an Islamic fundamentalist organization which defines itself as the military wing of the Muslim Brethren.\" \"Anti-semitic motifs in the ideology of Hizballah and Hamas\", by Esther Webman, 1994, p. 17.\nBULLET::::- \"Understanding Islamism\", Crisis Group Middle East/North Africa Report N°37, 2 March 2005\nBULLET::::- The New Hamas: Between Resistance and Participation. Middle East Report. Graham Usher, August 21, 2005 </ref>\nHamas also has a military resistance wing, the Izz ad-Din al-Qassam Brigades.\n\nFor some decades prior to the First Palestine Intifada in 1987, the Muslim Brotherhood in Palestine took a \"quiescent\" stance towards Israel, focusing on preaching, education and social services, and benefiting from Israel's \"indulgence\" to build up a network of mosques and charitable organizations. As the First Intifada gathered momentum and Palestinian shopkeepers closed their shops in support of the uprising, the Brotherhood announced the formation of HAMAS (\"zeal\"), devoted to Jihad against Israel. Rather than being more moderate than the PLO, the 1988 Hamas charter took a more uncompromising stand, calling for the destruction of Israel and the establishment of an Islamic state in Palestine. It was soon competing with and then overtaking the PLO for control of the intifada. The Brotherhood's base of devout middle class found common cause with the impoverished youth of the intifada in their cultural conservatism and antipathy for activities of the secular middle class such as drinking alcohol and going about without hijab.\n\nHamas has continued to be a major player in Palestine. From 2000 to 2007 it killed 542 people in 140 suicide bombing or \"martyrdom operations\". In the January 2006 legislative election—its first foray into the political process—it won the majority of the seats, and in 2007 it drove the PLO out of Gaza. Hamas has been praised by Muslims for driving Israel out of the Gaza Strip, but criticized for failure to achieve its demands in the 2008–09 and 2014 Gaza Wars despite heavy destruction and significant loss of life.\n\nEarly in the history of the state of Pakistan (12 March 1949), a parliamentary resolution (the Objectives Resolution) was adopted in accordance with the vision of founding fathers of Pakistan (Muhammad Iqbal, Muhammad Ali Jinnah, Liaquat Ali Khan). proclaiming:\n\nThis resolution later became a key source of inspiration for writers of the Constitution of Pakistan, and is included in the constitution as preamble.\n\nIn July 1977, General Zia-ul-Haq overthrew Prime Minister Zulfiqar Ali Bhutto's regime in Pakistan. Ali Bhutto, a leftist in democratic competition with Islamists, had announced banning alcohol and nightclubs within six months, shortly before he was overthrown. Zia-ul-Haq was much more committed to Islamism, and \"Islamization\" or implementation of Islamic law, became a cornerstone of his eleven-year military dictatorship and Islamism became his \"official state ideology\". Zia ul Haq was an admirer of Mawdudi and Mawdudi's party Jamaat-e-Islami became the \"regime's ideological and political arm\". In Pakistan this Islamization from above was \"probably\" more complete \"than under any other regime except those in Iran and Sudan,\" but Zia-ul-Haq was also criticized by many Islamists for imposing \"symbols\" rather than substance, and using Islamization to legitimize his means of seizing power. Unlike neighboring Iran, Zia-ul-Haq's policies were intended to \"avoid revolutionary excess\", and not to strain relations with his American and Persian Gulf state allies. Zia-ul-Haq was killed in 1988 but Islamization remains an important element in Pakistani society.\n\nFor many years, Sudan had an Islamist regime under the leadership of Hassan al-Turabi. His National Islamic Front first gained influence when strongman General Gaafar al-Nimeiry invited members to serve in his government in 1979. Turabi built a powerful economic base with money from foreign Islamist banking systems, especially those linked with Saudi Arabia. He also recruited and built a cadre of influential loyalists by placing sympathetic students in the university and military academy while serving as minister of education.\n\nAfter al-Nimeiry was overthrown in 1985 the party did poorly in national elections, but in 1989 it was able to overthrow the elected post-al-Nimeiry government with the help of the military. Turabi was noted for proclaiming his support for the democratic process and a liberal government before coming to power, but strict application of sharia law, torture and mass imprisonment of the opposition, and an intensification of the long-running war in southern Sudan, once in power. The NIF regime also harbored Osama bin Laden for a time (before 9/11), and worked to unify Islamist opposition to the American attack on Iraq in the 1991 Gulf War.\n\nAfter Sudanese intelligence services were implicated in an assassination attempt on the President of Egypt, UN economic sanctions were imposed on Sudan, a poor country, and Turabi fell from favor. He was imprisoned for a time in 2004–05. Some of the NIF policies, such as the war with the non-Muslim south, have been reversed, though the National Islamic Front still holds considerable power in the government of Omar al-Bashir and National Congress Party, another Islamist party in country.\n\nSwitzerland is not normally seen as a center of Islamism, especially when compared to countries such as Belgium or France. However, from 2012 to 2018, the majority of the country's jihadist and would-be jihadist population were radicalized in Switzerland.\n\nTurkey had a number of Islamist parties, often changing names as they were banned by the constitutional court for anti-secular activities. Necmettin Erbakan (1926–2011) was the leader of several of the parties, the National Order Party (\"Milli Nizam Partisi\", 1970–1971), the National Salvation Party (\"Milli Selamet Partisi\", 1972–1981), and the Welfare Party (\"Refah Partisi\", 1983-1998); he also became a member of the Felicity Party (\"Saadet Partisi\", 2003–2011). Current Turkish President Recep Tayyip Erdogan has long been considered a champion of political Islam.\n\nThe Justice and Development Party (AKP), which has dominated Turkish politics since 2002, is sometimes described as Islamist, but rejects such classification.\n\nBULLET::::- Various Islamist political groups are dominant forces in the political systems of Afghanistan, Iran and Iraq.\nBULLET::::- The Green Algeria Alliance is an Islamist coalition of political parties, created for the legislative election of 2012 in Algeria. It includes the Movement of Society for Peace (Hamas), Islamic Renaissance Movement (Ennahda) and the Movement for National Reform (Islah). The alliance is led by Bouguerra Soltani of Hamas. However, the incumbent coalition, comprising the FLN of President Abdelaziz Bouteflika and the RND of Prime Minister Ahmed Ouyahia, held on to power after winning a majority of seats, and the Islamist parties of the Green Algeria Alliance lost seats in the legislative election of 2012.\nBULLET::::- Shia Islamist Al Wefaq, Salafi Islamist Al Asalah and Sunni Islamist Al-Menbar Islamic Society are dominant democratic forces in Bahrain.\nBULLET::::- In Indonesia, Prosperous Justice Party is the major Islamist political party in the country's democratic process.\nBULLET::::- Islamic Action Front is Jordan's Islamist political party and largest democratic political force in the country. The IAF's survival in Jordan is primarily due to its flexibility and less radical approach to politics.\nBULLET::::- Hadas or \"Islamic Constitutional Movement\" is Kuwait's Sunni Islamist party.\nBULLET::::- Islamic Group (Lebanon) is a Sunni Islamist political party in Lebanon. Hezbollah is a Shia Islamist political party in Lebanon.\nBULLET::::- The Justice and Construction Party is the Muslim Brotherhood's political arm in Libya and the second largest political force in the country. The National Forces Alliance, the largest political group in country, does not believe the country should be run entirely by Sharia law or secular]] law, but does hold that Sharia should be \"the main inspiration for legislation.\" Party leader Jibril has said the NFA is a moderate Islamic moveme that recognises the importance of Islam in political life and favours Sharia as the basis of the law.\nBULLET::::- The Pan-Malaysian Islamic Party is a major opposition party in Malaysia which espouses Islamism.\nBULLET::::- The Justice and Development Party (Morocco) is the ruling party in Morocco since 29 November 2011, advocating Islamism and Islamic democracy.\nBULLET::::- The Muslim Brotherhood of Syria is a Sunni Islamist force in Syria and very loosely affiliated to the Egyptian Muslim Brotherhood. It has also been called the \"dominant group\" or \"dominant force\" in the Arab Spring uprising in Syria. The group's stated political positions are moderate and in its most recent April 2012 manifesto it \"pledges to respect individual rights\", to promote pluralism and democracy.\nBULLET::::- The Islamic Renaissance Party of Tajikistan is Tajikistan's Islamist party and main opposition and democratic force in the country.\nBULLET::::- The Ennahda Movement, also known as Renaissance Party or simply Ennahda, is a moderate Islamist political party in Tunisia. On 1 March 2011, after the government of Zine El Abidine Ben Ali collapsed in the wake of the 2011 Tunisian revolution, Tunisia's interim government granted the group permission to form a political party. Since then it has become the biggest and most well-organized party in Tunisia, so far outdistancing its more secular competitors. In the Tunisian Constituent Assembly election of 2011, the first honest election in the country's history with a turnout of 51% of all eligible voters, the party won 37% of the popular vote and 89 (41%) of the 217 assembly seats, far more than any other party.\nBULLET::::- Eastern Africa has become a hotbed of violent Islamic extremism since the late 1990s, one of the relevant movements being al-Shabaab, active in Somalia and Kenya, which emerged in response to the 2006–09 Ethiopian intervention in Somalia.\nBULLET::::- West Africa has seen the rise of influential Islamic extremist organizations, notably Boko Haram in Northern Nigeria and al-Qaeda in the Islamic Maghreb in Mali.\n\nHizb ut-Tahrir is an influential international Islamist movement, founded in 1953 by an Islamic Qadi \"(judge)\" Taqiuddin al-Nabhani. HT is unique from most other Islamist movements in that the party focuses not on implementation of Sharia on local level or on providing social services, but on unifying the Muslim world under its vision of a new Islamic caliphate spanning from North Africa and the Middle East to much of central and South Asia.\n\nTo this end it has drawn up and published a 186-article constitution for its proposed caliphate-state specifying specific policies such as sharia law, a \"unitary ruling system\" headed by a caliph elected by Muslims, an economy based on the gold standard, public ownership of utilities, public transport, and energy resources, death for apostates and Arabic as the \"sole language of the State.\"\n\nIn its focus on the Caliphate, the party takes a different view of Muslim history than some other Islamists such as Muhammad Qutb. HT sees Islam's pivotal turning point as occurring not with the death of Ali, or one of the other four rightly guided Caliphs in the 7th century, but with the abolition of the Ottoman Caliphate in 1924. This is believed to have ended the true Islamic system, something for which it blames \"the disbelieving (Kafir) colonial powers\" working through Turkish modernist Mustafa Kemal Atatürk.\n\nHT does not engage in armed jihad or work for a democratic system, but works to take power through \"ideological struggle\" to change Muslim public opinion, and in particular through elites who will \"facilitate\" a \"change of the government,\" i.e., launch a \"bloodless\" coup. It allegedly attempted and failed such coups in 1968 and 1969 in Jordan, and in 1974 in Egypt, and is now banned in both countries.\n\nThe party is sometimes described as \"Leninist\" and \"rigidly controlled by its central leadership,\" with its estimated one million members required to spend \"at least two years studying party literature under the guidance of mentors \"(Murshid)\"\" before taking \"the party oath.\" HT is particularly active in the ex-soviet republics of Central Asia and in Europe.\n\nIn the UK its rallies have drawn thousands of Muslims, and the party has been described by two observers (Robert S. Leiken and Steven Brooke) to have outpaced the Muslim Brotherhood in both membership and radicalism.\n\nOne observer (Quinn Mecham) notes four trends in Islamism rising from the Arab Spring of 2010-11:\nBULLET::::- The repression of the Muslim Brotherhood. Primarily by the Egyptian military and courts following the forcible removal of Morsi from office in 2013; but also by Saudi Arabia and a number of Gulf countries (not Qatar).\nBULLET::::- Rise of Islamist \"state-building\" where \"state failure\" has taken place—most prominently in Syria, Iraq, Libya and Yemen. Islamists have found it easier than competing non-Islamists trying to fill the void of state failure, by securing external funding, weaponry and fighters—\"many of which have come from abroad and have rallied around a pan-Islamic identity\". The norms of governance in these Islamist areas are militia-based, and the population submit to their authority out of fear, loyalty, other reasons, or some combination. The \"most expansive\" of these new \"models\" is the Islamic State.\nBULLET::::- Increasing sectarianism at least in part from Proxy Wars. Fighters are proxies primarily for Saudi Arabia and the Gulf states and for Iran. Islamists are fighting Islamists across sectarian lines in Lebanon (Sunni militants targeting Hezbollah positions), Yemen (between mainstream Sunni Islamists of Islah and the Shiite Zaydi Houthi movement), in Iraq (Islamic State and Iraqi Shiite militias)\nBULLET::::- Increased caution and political learning in countries such as Algeria and Jordan where Islamist have chosen not to lead a major challenge against their governments. In Yemen Islah \"has sought to frame its ideology in a way that will avoid charges of militancy\".\nAnother observer (Tarek Osman) notes with concern that\nBULLET::::- the failure to take power during the Arab Spring has led not to \"soul-searching\" in major Islamist groups about what went wrong, but instead to \"antagonism and fiery anger\" and a thirst for revenge. Partisans of political Islam (although this does not include some prominent leaders such as Rached Ghannouchi but is particularly true in Egypt) see themselves as victims of an injustice whose perpetrators are not just \"individual conspirators but entire social groups\".\n\n\"The Islamic State\", formerly known as the \"Islamic State of Iraq and the Levant\" and before that as the \"Islamic State of Iraq\", (also called by the Arabic acronym \"Daesh\"), is a Wahhabi/Salafi jihadist extremist militant group which is led by and mainly composed of Sunni Arabs from Syria and Iraq. In 2014, the group proclaimed itself a caliphate, with religious, political and military authority over all Muslims worldwide.\n, it had control over territory occupied by ten million people in Syria and Iraq, and has nominal control over small areas of Libya, Nigeria, and Afghanistan. (While a self-described state, it lacks international recognition.) ISIL also operates or has affiliates in other parts of the world, including North Africa and South Asia\n\nOriginating as the \"Jama'at al-Tawhid wal-Jihad\" in 1999, ISIL pledged allegiance to al-Qaeda in 2004, participated in the Iraqi insurgency that followed the invasion of Iraq by Western coalition forces in 2003, joined the fight in the Syrian Civil War beginning in 2011, and was expelled from al-Qaeda in early 2014, (which complained of its failure to consult and \"notorious intransigence\"). ISIL gained prominence after it drove Iraqi government forces out of key cities in western Iraq in an offensive in June that same year. The group is adept at social media, posting Internet videos of beheadings of soldiers, civilians, journalists and aid workers, and is known for its destruction of cultural heritage sites.\nThe United Nations (UN) has held ISIL responsible for human rights abuses and war crimes, and Amnesty International has reported ethnic cleansing by the group on a \"historic scale\". The group has been designated a terrorist organisation by the UN, the European Union (EU) and member states, the United States, India, Indonesia, Turkey, Saudi Arabia, Syria and other countries.\n\nIslamist movements such as the Muslim Brotherhood, \"are well known for providing shelters, educational assistance, free or low cost medical clinics, housing assistance to students from out of town, student advisory groups, facilitation of inexpensive mass marriage ceremonies to avoid prohibitively costly dowry demands, legal assistance, sports facilities, and women's groups.\" All this compares very favourably against incompetent, inefficient, or neglectful governments whose commitment to social justice is limited to rhetoric.\n\nThe Arab world—the original heart of the Muslim world—has been afflicted with economic stagnation. For example, it has been estimated that in the mid 1990s the exports of Finland, a country of five million, exceeded those of the entire Arab world of 260 million, excluding oil revenue. This economic stagnation is argued to have commenced with the demise of the Ottoman Caliphate in 1924, with trade networks being disrupted and societies torn apart with the creation of new nation states; prior to this, the Middle East had a diverse and growing economy and more general prosperity.\nStrong population growth combined with economic stagnation has created urban agglomerations in Cairo, Istanbul, Tehran, Karachi, Dhaka, and Jakarta each with well over 12 million citizens, millions of them young and unemployed or underemployed. Such a demographic, alienated from the westernized ways of the urban elite, but uprooted from the comforts and more passive traditions of the villages they came from, is understandably favourably disposed to an Islamic system promising a better world—an ideology providing an \"emotionally familiar basis for group identity, solidarity, and exclusion; an acceptable basis for legitimacy and authority; an immediately intelligible formulation of principles for both a critique of the present and a program for the future.\"\n\nIslamism can also be described as part of identity politics, specifically the religiously-oriented nationalism that emerged in the Third World in the 1970s: \"resurgent Hinduism in India, Religious Zionism in Israel, militant Buddhism in Sri Lanka, resurgent Sikh nationalism in the Punjab, 'Liberation Theology' of Catholicism in Latin America, and Islamism in the Muslim world.\" These all challenged Westernized ruling elites on behalf of 'authenticity' and tradition.\n\nThe modern revival of Islamic devotion and the attraction to things Islamic can be traced to several events.\n\nBy the end of World War I, most Muslim states were seen to be dominated by the Christian-leaning Western states. It is argued that either the claims of Islam were false and the Christian or post-Christian West had finally come up with another system that was superior, or Islam had failed through not being true to itself. Thus, a redoubling of faith and devotion by Muslims was called for to reverse this tide.\n\nThe connection between the lack of an Islamic spirit and the lack of victory was underscored by the disastrous defeat of Arab nationalist-led armies fighting under the slogan \"Land, Sea and Air\" in the 1967 Six-Day War, compared to the (perceived) near-victory of the Yom Kippur War six years later. In that war the military's slogan was \"God is Great\".\n\nAlong with the Yom Kippur War came the Arab oil embargo where the (Muslim) Persian Gulf oil-producing states' dramatic decision to cut back on production and quadruple the price of oil, made the terms oil, Arabs and Islam synonymous—with power—in the world, and especially in the Muslim world's public imagination. Many Muslims believe as Saudi Prince Saud al Faisal did that the hundreds of billions of dollars in wealth obtained from the Persian Gulf's huge oil deposits were nothing less than a gift from God to the Islamic faithful.\n\nAs the Islamic revival gained momentum, governments such as Egypt's, which had previously repressed (and was still continuing to repress) Islamists, joined the bandwagon. They banned alcohol and flooded the airwaves with religious programming, giving the movement even more exposure.\n\nStarting in the mid-1970s the Islamic resurgence was funded by an abundance of money from Saudi Arabian oil exports. The tens of billions of dollars in \"petro-Islam\" largesse obtained from the recently heightened price of oil funded an estimated \"90% of the expenses of the entire faith.\"\n\nThroughout the Muslim world, religious institutions for people both young and old, from children's maddrassas to high-level scholarships received Saudi funding,\n\"books, scholarships, fellowships, and mosques\" (for example, \"more than 1500 mosques were built and paid for with money obtained from public Saudi funds over the last 50 years\"), along with training in the Kingdom for the preachers and teachers who went on to teach and work at these universities, schools, mosques, etc.\n\nThe funding was also used to reward journalists and academics who followed the Saudis' strict interpretation of Islam; and satellite campuses were built around Egypt for Al-Azhar University, the world's oldest and most influential Islamic university.\n\nThe interpretation of Islam promoted by this funding was the strict, conservative Saudi-based Wahhabism or Salafism. In its harshest form it preached that Muslims should not only \"always oppose\" infidels \"in every way,\" but \"hate them for their religion ... for Allah's sake,\" that democracy \"is responsible for all the horrible wars of the 20th century,\" that Shia and other non-Wahhabi Muslims were infidels, etc. While this effort has by no means converted all, or even most Muslims to the Wahhabist interpretation of Islam, it has done much to overwhelm more moderate local interpretations, and has set the Saudi-interpretation of Islam as the \"gold standard\" of religion in minds of some or many Muslims.\n\nQatar stands out among state sponsors of Islamism as well. Over the past two decades, the country has exerted a semi-formal patronage for the international movement of the Muslim Brotherhood. Former Qatari Sheikh Hamad bin Khalifa al-Thani in particular has distinguished himself as one of the most dedicated supporter of the Muslim Brotherhood and of Islamist movements in general both in the Middle Eastern region and across the globe.\n\nIn 1999 the Muslim Brotherhood was disbanded in Qatar. The country's longstanding support for the group has been often explained as determined by a strategic calculus that limited the role played by religion in Qatar. As the director of the Center for International and Regional Studies at the Doha-based branch of Georgetown University, Mehran Kamrava, posited, Qatar presenting itself as the state patron of the Muslim Brotherhood has caused religion in Qatar to not \"play any role in articulating or forming oppositional sentiments.\"\n\nQatar's patronage has been primarily expressed through the ruling family's endorsement of Muslim Brotherhood's most representative figures, especially Yusuf al-Qaradawi. Qaradawi is a prominent, yet controversial Sunni preacher and theologian who continues to serve as the spiritual leader of the Muslim Brotherhood. An Egyptian citizen, Qaradawi fled Egypt for Qatar in 1961 after being imprisoned under President Gamal Abdul Nasser. In 1962 he chaired the Qatari Secondary Institute of Religious Studies, and in 1977 he founded and directed the Shariah and Islamic Studies department at the University of Qatar. He left Qatar to return to Egypt shortly before the 2011 Egyptian Revolution.\n\nFor twenty years, Qaradawi has hosted a popular show titled Shariah and Life on the Qatari-based media channel Al-Jazeera, a government sponsored channel notoriously supportive of the Muslim Brotherhood and Islamism and often designated as a propaganda outlet for the Qatari government. From that platform, he has promoted his Islamist—and often radical views—on life, politics, and culture.\n\nHis positions, as well as his controversial ties to extremist and terrorist individuals and organizations, made him persona non grata to the U.S., UK and French governments respectively in 1999, 2008, and 2012.\n\nBeyond the visibility and political protection granted to Yussuf al-Qaradawi, Qatar has historically hosted several Muslim Brothers especially after Egyptian President Mohammed Morsi, a Muslim Brotherhood representative, was overthrown in July 2013. Before 2013, however, Qatar had made a substantial investment on Morsi's leadership and had devolved about $10 million to Egypt since Morsi was elected, allegedly also to \"buy political advantage\" in the country.\n\nQatar's political and financial support for Islamist movements and factions was not limited to the Egyptian case. Qatar is known to have backed Islamist factions in Libya, Syria and Yemen.\n\nIn Libya in particular, Qatar has supported the Islamist government established in Tripoli. During the 2011 revolution that ousted President Muammar Gaddafi, Qatar provided \"tens of millions of dollars in aid, military training and more than 20,000 tons of weapons\" to anti-Gaddafi rebels and Islamist militias in particular. The flow of weapons was not suspended after Gaddafi's government was removed. Qatar maintained its influence through key facilitators on the field, including cleric Ali al-Sallabi, the leader of the Islamist militia \"February 17 Katiba\" Ismail al-Sallabi, and the Tripoli Military Council leader Abdel Hakim Belhaj.\n\nHamas, as well, has been among the primary beneficiaries of Qatar's financial support. Not only does the Gulf emirate host Hamas' politburo continuously since 2012; Hamas leader Khaled Meshaal has often met with international delegations on Qatari territory.\n\nMore recently, Qatar has channeled material support to Hamas' terrorist operations by exploiting its official commitment to finance Gaza reconstruction. Mostly through \"truckloads of construction material being shipped into Gaza\", Qatar has funneled dual-use substances that could be employed to produce explosives into Gaza.\n\nIn a 2003 interview with Al-Hayat Hamas politburo declared that most of Qatar's support was collected through charities and popular committees. Qatar's largest NGO, Qatar Charity, in particular has played a great role in Qatar's mission to support Islamist worldwide.\n\nOfficially through its \"Ghaith\" initiative but also through conspicuous donations that preceded the \"Ghaith\" program, Qatar Charity has financed the building or reconstruction of mosques and cultural institutes across the globe. Just like Saudi Arabia, Qatar has devolved considerable energies to spreading Salafism and to \"win areas of influence\" in the countries that beneficiated from its support. In France in particular Qatar has heavily invested in the Union des Organisations Islamiques des France (UOIF), an umbrella organization informally acting as the representative of the Muslim Brotherhood in the country through which Qatar Charity has channeled funds for the Assalam mosque in Nantes (€4.4 million) and the mosque in Mulhouse (€2 million).\n\nDuring the 1970s and sometimes later, Western and pro-Western governments often supported sometimes fledgling Islamists and Islamist groups that later came to be seen as dangerous enemies. Islamists were considered by Western governments bulwarks against—what were thought to be at the time—more dangerous leftist/communist/nationalist insurgents/opposition, which Islamists were correctly seen as opposing. The US spent billions of dollars to aid the mujahideen Muslim Afghanistan enemies of the Soviet Union, and non-Afghan veterans of the war returned home with their prestige, \"experience, ideology, and weapons\", and had considerable impact.\n\nAlthough it is a strong opponent of Israel's existence, Hamas, officially created in 1987, traces back its origins to institutions and clerics supported by Israel in the 1970s and 1980s. Israel tolerated and supported Islamist movements in Gaza, with figures like Ahmed Yassin, as Israel perceived them preferable to the secular and then more powerful al-Fatah with the PLO.\n\nEgyptian President Anwar Sadatwhose policies included opening Egypt to Western investment (\"infitah\"); transferring Egypt's allegiance from the Soviet Union to the United States; and making peace with Israel—released Islamists from prison and welcomed home exiles in tacit exchange for political support in his struggle against leftists. His \"encouraging of the emergence of the Islamist movement\" was said to have been \"imitated by many other Muslim leaders in the years that followed.\" This \"gentlemen's agreement\" between Sadat and Islamists broke down in 1975 but not before Islamists came to completely dominate university student unions. Sadat was later assassinated and a formidable insurgency was formed in Egypt in the 1990s. The French government has also been reported to have promoted Islamist preachers \"in the hope of channeling Muslim energies into zones of piety and charity.\"\n\nMuslim alienation from Western ways, including its political ways.\nBULLET::::- The memory in Muslim societies of the many centuries of \"cultural and institutional success\" of Islamic civilization that have created an \"intense resistance to an alternative 'civilizational order'\", such as Western civilization,\nBULLET::::- The proximity of the core of the Muslim world to Europe and Christendom where it first conquered and then was conquered. Iberia in the eighth century, the Crusades which began in the eleventh century, then for centuries the Ottoman Empire, were all fields of war between Europe and Islam.\n\nBULLET::::- The end of the Cold War and Soviet occupation of Afghanistan has eliminated the common atheist Communist enemy uniting some religious Muslims and the capitalist west.\n\nIslamism, or elements of Islamism, have been criticized for: repression of free expression and individual rights, rigidity, hypocrisy, lack of true understanding of Islam, misinterpreting the Quran and Sunnah, antisemitism, and for innovations to Islam (bid'ah), notwithstanding proclaimed opposition to any such innovation by Islamists.\n\nThe U.S. government has engaged in efforts to counter militant Islamism (Jihadism), since 2001. These efforts were centred in the U.S. around public diplomacy programmes conducted by the State Department. There have been calls to create an independent agency in the U.S. with a specific mission of undermining Jihadism. Christian Whiton, an official in the George W. Bush administration, called for a new agency focused on the nonviolent practice of \"political warfare\" aimed at undermining the ideology. U.S. Defense Secretary Robert Gates called for establishing something similar to the defunct U.S. Information Agency, which was charged with undermining the communist ideology during the Cold War.\n\nBULLET::::- Clash of Civilizations\nBULLET::::- Dominionism\nBULLET::::- Social Gospel\nBULLET::::- Islamicism (disambiguation)\n\n\n"}
{"id": "15014", "url": "https://en.wikipedia.org/wiki?curid=15014", "title": "Instructional theory", "text": "Instructional theory\n\nAn instructional theory is \"a theory that offers explicit guidance on how to better help people learn and develop.\" It provides insights about what is likely to happen and why with respect to different kinds of teaching and learning activities while helping indicate approaches for their evaluation. Instructional designers focus on how to best structure material and instructional behavior to facilitate learning. \n\nOriginating in the United States in the late 1970s, \"instructional theory\" is influenced by three basic theories in educational thought: behaviorism, the theory that helps us understand how people conform to predetermined standards; cognitivism, the theory that learning occurs through mental associations; and constructivism, the theory explores the value of human activity as a critical function of gaining knowledge. Instructional theory is heavily influenced by the 1956 work of Benjamin Bloom, a University of Chicago professor, and the results of his Taxonomy of Education Objectives—one of the first modern codifications of the learning process. One of the first instructional theorists was Robert M. Gagne, who in 1965 published \"Conditions of Learning\" for the Florida State University's Department of Educational Research.\n\nInstructional theory is different than learning theory. A learning theory \"describes\" how learning takes place, and an instructional theory \"prescribes\" how to better help people learn. Learning theories often inform instructional theory, and three general theoretical stances take part in this influence: behaviorism (learning as response acquisition), cognitivism (learning as knowledge acquisition), and constructivism (learning as knowledge construction). Instructional theory helps us create conditions that increases the probability of learning. Its goal is understanding the instructional system and to improve the process of instruction.\n\nInstructional theories identify what instruction or teaching should be like. It outlines strategies that an educator may adopt to achieve the learning objectives. Instructional theories are adapted based on the educational content and more importantly the learning style of the students. They are used as teaching guidelines/tools by teachers/trainers to facilitate learning. Instructional theories encompass different instructional methods, models and strategies.\n\nDavid Merrill's First Principles of Instruction discusses universal methods of instruction, situational methods and core ideas of the post-industrial paradigm of instruction.\n\nUniversal Methods of Instruction:\n\nBULLET::::- Task-Centered Principle - instruction should use a progression of increasingly complex whole tasks.\nBULLET::::- Demonstration Principle - instruction should guide learners through a skill and engage peer discussion/demonstration.\nBULLET::::- Application Principle - instruction should provide intrinsic or corrective feedback and engage peer-collaboration.\nBULLET::::- Activation Principle - instruction should build upon prior knowledge and encourage learners to acquire a structure for organizing new knowledge.\nBULLET::::- Integration Principle - instruction should engage learners in peer-critiques and synthesizing newly acquired knowledge.\nSituational Methods:\n\nbased on different approaches to instruction\nBULLET::::- Role play\nBULLET::::- Synectics\nBULLET::::- Mastery learning\nBULLET::::- Direct instruction\nBULLET::::- Discussion\nBULLET::::- Conflict resolution\nBULLET::::- Peer learning\nBULLET::::- Experiential learning\nBULLET::::- Problem-based learning\nBULLET::::- Simulation-based learning\nbased on different learning outcomes:\nBULLET::::- Knowledge\nBULLET::::- Comprehension\nBULLET::::- Application\nBULLET::::- Analysis\nBULLET::::- Synthesis\nBULLET::::- Evaluation\nBULLET::::- Affective development\nBULLET::::- Integrated learning\nCore ideas for the Post-industrial Paradigm of Instruction:\nBULLET::::- Learner centered vs. teacher centered instruction – with respect to the focus, instruction can be based on the capability and style of the learner or the teacher.\nBULLET::::- Learning by doing vs. teacher presenting – Students often learn more by doing rather than simply listening to instructions given by the teacher.\nBULLET::::- Attainment based vs. time based progress – The instruction can either be based on the focus on the mastery of the concept or the time spent on learning the concept.\nBULLET::::- Customized vs. standardized instruction – The instruction can be different for different learners or the instruction can be given in general to the entire classroom\nBULLET::::- Criterion referenced vs. norm referenced instruction – Instruction related to different types of evaluations.\nBULLET::::- Collaborative vs. individual instruction – Instruction can be for a team of students or individual students.\nBULLET::::- Enjoyable vs. unpleasant instructions – Instructions can create a pleasant learning experience or a negative one (often to enforce discipline). Teachers must take care to ensure positive experiences.\nFour tasks of Instructional theory:\nBULLET::::- Knowledge selection\nBULLET::::- Knowledge sequence\nBULLET::::- Interaction management\nBULLET::::- Setting of interaction environment\n\nPaulo Freire's work appears to critique instructional approaches that adhere to the knowledge acquisition stance, and his work \"Pedagogy of the Oppressed\" has had a broad influence over a generation of American educators with his critique of various \"banking\" models of education and analysis of the teacher-student relationship.\n\nFreire explains, \"Narration (with the teacher as narrator) leads the students to memorize mechanically the narrated content. Worse yet, it turns them into \"containers\", into \"receptacles\" to be \"filled\" by the teacher. The more completely she fills the receptacles, the better a teacher she is. The more meekly the receptacles permit themselves to be filled, the better students they are.\" In this way he explains educator creates an act of depositing knowledge in a student. The student thus becomes a repository of knowledge. Freire explains that this system that diminishes creativity and knowledge suffers. Knowledge, according to Freire, comes about only through the learner by inquiry and pursuing the subjects in the world and through interpersonal interaction.\n\nFreire further states, \"In the banking concept of education, knowledge is a gift bestowed by those who consider themselves knowledgeable upon those whom they consider to know nothing. Projecting an absolute ignorance onto others, a characteristic of the ideology of oppression, negates education and knowledge as processes of inquiry. The teacher presents himself to his students as their necessary opposite; by considering their ignorance absolute, he justifies his own existence. The students, alienated like the slave in the Hegelian dialectic, accept their ignorance as justifying the teacher's existence—but, unlike the slave, they never discover that they educate the teacher.\"\nFreire then offered an alternative stance and wrote, \"The raison d'etre of libertarian education, on the other hand, lies in its drive towards reconciliation. Education must begin with the solution of the teacher-student contradiction, by reconciling the poles of the contradiction so that both are simultaneously teachers and students.\"\n\nIn the article, \"A process for the critical analysis of instructional theory\", the authors use an ontology-building process to review and analyze concepts across different instructional theories. Here are their findings:\nBULLET::::- Concepts exist in theoretical writing that theorists do not address directly.\nBULLET::::- These tacit concepts, which supply the ontological categories, enable a more detailed comparison of theories beyond specific terminologies.\nBULLET::::- Divergences between theories can be concealed behind common terms used by different theorists.\nBULLET::::- A false sense of understanding often arises from a cursory, uncritical reading of the theories.\nBULLET::::- Discontinuities and gaps are revealed within the theoretical literature when the tacit concepts are elicited.\n\nBULLET::::- (the use of electronic educational technology is also called e-learning)\nBULLET::::- was developed during WWII and is still in use around the world\n\nLinking Premise to Practice: An Instructional Theory-Strategy Model Approach By: Bowden, Randall. Journal of College Teaching & Learning, v5 n3 p69-76 Mar 2008\nBULLET::::- Paulo Freire, \"Pedagogy of the Oppressed\". .\n"}
{"id": "15018", "url": "https://en.wikipedia.org/wiki?curid=15018", "title": "Infusoria", "text": "Infusoria\n\nInfusoria is a collective term for minute aquatic creatures such as ciliates, euglenoids, protozoa, unicellular algae and small invertebrates that exist in freshwater ponds. Some authors (e.g., Bütschli) used the term as a synonym for Ciliophora. In modern formal classifications, the term is considered obsolete; the microorganisms previously included in the Infusoria are mostly assigned to the kingdom Protista. Researchers have proposed that infusoria reproductive rates periodically increase and decrease over periods of time.\n\nInfusoria are used by owners of aquariums to feed fish fry; newly hatched fry of many common aquarium species can be successfully raised on this food during early development due to its size and nutritional content. Many home aquaria are unable to naturally supply sufficient infusoria for fish-rearing, so hobbyists may create and maintain their own supply cultures or use one of the many commercial cultures available. Infusoria can be cultured by soaking any decomposing matter of organic or vegetative origin, such as papaya skin, in a jar of aged water. The culture starts to proliferate in two to three days, depending on temperature and light received. The water first turns cloudy, but clears up once the infusoria eat the bacteria that caused the cloudiness. At this point, the infusoria are ready, and usually are visible to the naked eye as small, white specks swimming in the container.\n\nBULLET::::- Animalcules\n\nBULLET::::- Ratcliff, Marc J. (2009). The Emergence of the Systematics of Infusoria. In: \"The Quest for the Invisible: Microscopy in the Enlightenment\". Aldershot: Ashgate.\n\nBULLET::::- Types of Protozoans and video\nBULLET::::- Pond Life Identification Kit\n"}
{"id": "15019", "url": "https://en.wikipedia.org/wiki?curid=15019", "title": "ISO/IEC 8859-1", "text": "ISO/IEC 8859-1\n\nISO/IEC 8859-1:1998, \"Information technology — 8-bit single-byte coded graphic character sets — Part 1: Latin alphabet No. 1\", is part of the ISO/IEC 8859 series of ASCII-based standard character encodings, first edition published in 1987. ISO 8859-1 encodes what it refers to as \"Latin alphabet no. 1\", consisting of 191 characters from the Latin script. This character-encoding scheme is used throughout the Americas, Western Europe, Oceania, and much of Africa. It is also commonly used in most standard romanizations of East-Asian languages. It is the basis for most popular 8-bit character sets and the first block of characters in Unicode.\n\nISO-8859-1 was (according to the standards at least) the default encoding of documents delivered via HTTP with a MIME type beginning with \"text/\" (HTML5 changed this to Windows-1252). , 2.6% of all (and 0.8% of the top-1000) web sites claim to use ISO 8859-1. However, this includes an unknown number of pages actually using Windows-1252 and/or UTF-8, both of which are commonly recognized by browsers despite the character set tag.\n\nIt is the default encoding of the values of certain descriptive HTTP headers, and defines the repertoire of characters allowed in HTML 3.2 documents (HTML 4.0 uses Unicode, i.e. UTF-8), and is specified by many other standards. This and similar sets are often assumed to be the encoding of 8-bit text on Unix and Microsoft Windows if there is no byte order mark (BOM), this is only gradually being changed to UTF-8.\n\nISO-8859-1 is the IANA preferred name for this standard when supplemented with the C0 and C1 control codes from ISO/IEC 6429. The following other aliases are registered: iso-ir-100, csISOLatin1, latin1, l1, IBM819. Code page 28591 a.k.a. Windows-28591 is used for it in Windows. IBM calls it code page 819 or CP819. Oracle calls it WE8ISO8859P1.\n\nEach character is encoded as a single eight-bit code value. These code values can be used in almost any data interchange system to communicate in the following languages:\n\nBULLET::::- Notes\nISO-8859-1 was commonly used for certain languages, even though it lacks characters used by these languages. In most cases, only a few letters are missing or they are rarely used, and they can be replaced with characters that are in ISO-8859-1 using some form of typographic approximation. The following table lists such languages.\n! Language !! Missing characters !! Typical workaround !! Supported by\n\nThe letter \"ÿ\", which appears in French only very rarely, mainly in city names such as L'Haÿ-les-Roses and never at the beginning of words, is included only in lowercase form. The slot corresponding to its uppercase form is occupied by the lowercase letter \"ß\" from the German language, which did not have an uppercase form at the time when the standard was created.\n\nFor some languages listed above, the correct typographical quotation marks are missing, as only , , and are included. Also, this scheme does not provide for oriented (6- or 9-shaped) single or double quotation marks. Some fonts will display the spacing grave accent (0x60) and the apostrophe (0x27) as a matching pair of oriented single quotation marks, but this is not considered part of the modern standard.\n\nISO 8859-1 was based on the Multinational Character Set used by Digital Equipment Corporation (DEC) in the popular VT220 terminal in 1983. It was developed within ECMA, the European Computer Manufacturers Association, and published in March 1985 as ECMA-94, by which name it is still sometimes known. The second edition of ECMA-94 (June 1986) also included ISO 8859-2, ISO 8859-3, and ISO 8859-4 as part of the specification.\n\nThe original draft placed French \"Œ\" and \"œ\" at code points 215 (0xD7) and 247 (0xF7). However, the French delegate, being neither a linguist nor a typographer, falsely stated that these are not independent French letters on their own, but mere ligatures (like \"ﬁ\" or \"ﬂ\"). These code points were soon filled with × and ÷ under the suggestion of the German delegation. Then things went even worse for the French language, when it was again falsely stated that the letter \"ÿ\" is \"not French\", resulting in the absence of the capital \"Ÿ\". In fact the letter \"ÿ\" is found in a number of French proper names, and the capital letter has been used in dictionaries and encyclopedias. These characters were added to . BraSCII matches the original draft.\n\nIn 1985, Commodore adopted ECMA-94 for its new AmigaOS operating system. The Seikosha MP-1300AI impact dot-matrix printer, used with the Amiga 1000, included this encoding.\n\nIn 1990, the very first version of Unicode used the code points of ISO-8859-1 as the first 256 Unicode code points.\n\nIn 1992, the IANA registered the character map ISO_8859-1:1987, more commonly known by its preferred MIME name of ISO-8859-1 (note the extra hyphen over ISO 8859-1), a superset of ISO 8859-1, for use on the Internet. This map assigns the C0 and C1 control characters to the unassigned code values thus provides for 256 characters via every possible 8-bit value.\n\n!\n\n!\n\n!\n\n!\n\n!\n\n!\n\n!\n\n!\n\n!\n\n!\n\n!\n\n!\n\n!\n\n!\n\n!\n\n!\n\nISO/IEC 8859-15 was developed in 1999, as an update of ISO/IEC 8859-1. It provides some characters for French and Finnish text and the euro sign, which are missing from ISO/IEC 8859-1. This required the removal of some infrequently used characters from ISO/IEC 8859-1, including fraction symbols and letter-free diacritics: , , , , , , , and . Ironically, three of the newly added characters (, , and ) had already been present in DEC's 1983 Multinational Character Set (MCS), the predecessor to ISO/IEC 8859-1 (1987). Since their original code points were now reused for other purposes, the characters had to be reintroduced under different, less logical code points.\n\nISO-IR-204, a more minor modification, had been registered in 1998, altering ISO-8859-1 by replacing the universal currency sign (¤) with the euro sign (the same substitution made by ISO-8859-15).\n\nThe popular Windows-1252 character set adds all the missing characters provided by ISO/IEC 8859-15, plus a number of typographic symbols, by replacing the rarely used C1 controls in the range 128 to 159 (hex 80 to 9F). It is very common to mislabel Windows-1252 text as being in ISO-8859-1. A common result was that all the quotes and apostrophes (produced by \"smart quotes\" in word-processing software) were replaced with question marks or boxes on non-Windows operating systems, making text difficult to read. Many web browsers and e-mail clients will interpret ISO-8859-1 control codes as Windows-1252 characters, and that behavior was later standardized in HTML5.\n\nThe Apple Macintosh computer introduced a character encoding called Mac Roman in 1984. It was meant to be suitable for Western European desktop publishing. It is a superset of ASCII, and has most of the characters that are in ISO-8859-1 and all the extra characters from Windows-1252 but in a totally different arrangement. The few printable characters that are in ISO 8859-1 but not in this set are often a source of trouble when editing text on websites using older Macintosh browsers (including the last version of Internet Explorer for Mac).\n\nDOS had code page 850, which had all printable characters that ISO-8859-1 had (albeit in a totally different arrangement) plus the most widely used graphic characters from code page 437.\n\nBetween 1989 and 2015, Hewlett-Packard used another superset of ISO-8859-1 on many of their calculators. This proprietary character set was sometimes referred to simply as \"ECMA-94\" as well.\n\nBULLET::::- Latin script in Unicode\nBULLET::::- Unicode\nBULLET::::- Universal Character Set\nBULLET::::- UTF-8\nBULLET::::- Windows code pages\nBULLET::::- ISO/IEC JTC 1/SC 2\n\nBULLET::::- ISO/IEC 8859-1:1998\nBULLET::::- ISO/IEC 8859-1:1998 8-bit single-byte coded graphic character sets, Part 1: Latin alphabet No. 1 \"(draft dated February 12, 1998, published April 15, 1998)\"\nBULLET::::- Standard ECMA-94: 8-Bit Single Byte Coded Graphic Character Sets Latin Alphabets No. 1 to No. 4 \"2nd edition (June 1986)\"\nBULLET::::- ISO-IR 100 Right-Hand Part of Latin Alphabet No.1 \"(February 1, 1986)\"\nBULLET::::- The Letter Database\n"}
{"id": "15020", "url": "https://en.wikipedia.org/wiki?curid=15020", "title": "ISO/IEC 8859", "text": "ISO/IEC 8859\n\nISO/IEC 8859 is a joint ISO and IEC series of standards for 8-bit character encodings. The series of standards consists of numbered parts, such as ISO/IEC 8859-1, ISO/IEC 8859-2, etc. There are 15 parts, excluding the abandoned ISO/IEC 8859-12. The ISO working group maintaining this series of standards has been disbanded.\n\nISO/IEC 8859 parts 1, 2, 3, and 4 were originally Ecma International standard ECMA-94.\n\nWhile the bit patterns of the 95 printable ASCII characters are sufficient to exchange information in modern English, most other languages that use Latin alphabets need additional symbols not covered by ASCII. ISO/IEC 8859 sought to remedy this problem by utilizing the eighth bit in an 8-bit byte to allow positions for another 96 printable characters. Early encodings were limited to 7 bits because of restrictions of some data transmission protocols, and partially for historical reasons. However, more characters were needed than could fit in a single 8-bit character encoding, so several mappings were developed, including at least ten suitable for various Latin alphabets.\n\nThe ISO/IEC 8859-\"n\" encodings only contain printable characters, and were designed to be used in conjunction with control characters mapped to the unassigned bytes. To this end a series of encodings registered with the IANA add the C0 control set (control characters mapped to bytes 0 to 31) from ISO 646 and the C1 control set (control characters mapped to bytes 128 to 159) from ISO 6429, resulting in full 8-bit character maps with most, if not all, bytes assigned. These sets have ISO-8859-\"n\" as their preferred MIME name or, in cases where a preferred MIME name is not specified, their canonical name. Many people use the terms ISO/IEC 8859-\"n\" and ISO-8859-\"n\" interchangeably. ISO/IEC 8859-11 did not get such a charset assigned, presumably because it was almost identical to TIS 620.\n\nThe ISO/IEC 8859 standard is designed for reliable information exchange, not typography; the standard omits symbols needed for high-quality typography, such as optional ligatures, curly quotation marks, dashes, etc. As a result, high-quality typesetting systems often use proprietary or idiosyncratic extensions on top of the ASCII and ISO/IEC 8859 standards, or use Unicode instead.\n\nAs a rule of thumb, if a character or symbol was not already part of a widely used data-processing character set and was also not usually provided on typewriter keyboards for a national language, it did not get in. Hence the directional double quotation marks \"«\" and \"»\" used for some European languages were included, but not the directional double quotation marks \"“\" and \"”\" used for English and some other languages.\n\nFrench did not get its \"œ\" and \"Œ\" ligatures because they could be typed as 'oe'. Likewise, \"Ÿ\", needed for all-caps text, was dropped as well. Albeit under different codepoints, these three characters were later reintroduced with ISO/IEC 8859-15 in 1999, which also introduced the new euro sign character €. Likewise Dutch did not get the \"ĳ\" and \"Ĳ\" letters, because Dutch speakers had become used to typing these as two letters instead.\n\nRomanian did not initially get its \"Ș\"/\"ș\" and \"Ț\"/\"ț\" (with comma) letters, because these letters were initially unified with \"Ş\"/\"ş\" and \"Ţ\"/\"ţ\" (with cedilla) by the Unicode Consortium, considering the shapes with comma beneath to be glyph variants of the shapes with cedilla. However, the letters with explicit comma below were later added to the Unicode standard and are also in ISO/IEC 8859-16.\n\nMost of the ISO/IEC 8859 encodings provide diacritic marks required for various European languages using the Latin script. Others provide non-Latin alphabets: Greek, Cyrillic, Hebrew, Arabic and Thai. Most of the encodings contain only spacing characters, although the Thai, Hebrew, and Arabic ones do also contain combining characters.\n\nThe standard makes no provision for the scripts of East Asian languages (\"CJK\"), as their ideographic writing systems require many thousands of code points. Although it uses Latin based characters, Vietnamese does not fit into 96 positions (without using combining diacritics such as in Windows-1258) either. Each Japanese syllabic alphabet (hiragana or katakana, see Kana) would fit, as in JIS X 0201, but like several other alphabets of the world they are not encoded in the ISO/IEC 8859 system.\n\nISO/IEC 8859 is divided into the following parts:\n\n! Part\n! Name\n! Revisions\n! Other standards\n! Description\n! Part 1\n! Part 2\n! Part 3\n! Part 4\n! Part 5\n! Part 6\n! Part 7\n! Part 8\n! Part 9\n! Part 10\n! Part 11\n! Part 12\n! Part 13\n! Part 14\n! Part 15\n! Part 16\n\nEach part of ISO/IEC 8859 is designed to support languages that often borrow from each other, so the characters needed by each language are usually accommodated by a single part. However, there are some characters and language combinations that are not accommodated without transcriptions. Efforts were made to make conversions as smooth as possible. For example, German has all of its seven special characters at the same positions in all Latin variants (1–4, 9, 10, 13–16), and in many positions the characters only differ in the diacritics between the sets. In particular, variants 1–4 were designed jointly, and have the property that every encoded character appears either at a given position or not at all.\n\n+Comparison of the various parts (1–16) of ISO/IEC 8859\n!Binary!!Oct!!Dec!!Hex\n!1!!2!!3!!4!!5!!6!!7!!8!!9!!10!!11!!13!!14!!15!!16\n!1010 0000!!240!!160!!A0\ncolspan=\"16\" Non-breaking space (NBSP)\n!1010 0001!!241!!161!!A1\n¡ĄĦĄЁstyle=\"background-color:#ccffcc;\" ‘style=\"background-color:#ccffcc;\" ¡Ąก”Ḃ¡Ą\n!1010 0010!!242!!162!!A2\n¢colspan=\"2\"˘ĸЂstyle=\"background-color:#ccffcc;\" ’¢¢Ēข¢ḃ¢ą\n!1010 0011!!243!!163!!A3\n£Ł£ŖЃstyle=\"background-color:#ccffcc;\" colspan=\"3\"£Ģฃcolspan=\"3\"£Ł\n!1010 0100 !!244!!164!!A4\ncolspan=\"4\"¤Є¤style=\"background-color:#ffff99;\"€colspan=\"2\"¤Īค¤Ċcolspan=\"2\"€\n!1010 0101!!245!!165!!A5\n¥Ľstyle=\"background-color:#ccffcc;\" ĨЅstyle=\"background-color:#ccffcc;\" style=\"background-color:#ffff99;\"₯colspan=\"2\"¥Ĩฅ„ċ¥„\n!1010 0110!!246!!166!!A6\n¦ŚĤĻІstyle=\"background-color:#ccffcc;\" colspan=\"3\"¦Ķฆ¦Ḋcolspan=\"2\"Š\n!1010 0111!!247!!167!!A7\ncolspan=\"4\"§Їstyle=\"background-color:#ccffcc;\" colspan=\"4\"§งcolspan=\"4\"§\n!1010 1000!!250!!168!!A8\ncolspan=\"4\"¨Јstyle=\"background-color:#ccffcc;\" colspan=\"3\"¨ĻจØẀcolspan=\"2\"š\n!1010 1001!!251!!169!!A9\n©ŠİŠЉstyle=\"background-color:#ccffcc;\" colspan=\"3\"©Đฉcolspan=\"4\"©\n!1010 1010!!252!!170!!AA\nªcolspan=\"2\"ŞĒЊstyle=\"background-color:#ccffcc;\" style=\"background-color:#ffff99;\"ͺ×ªŠชŖẂªȘ\n!1010 1011!!253!!171!!AB\n«ŤĞĢЋstyle=\"background-color:#ccffcc;\" colspan=\"3\"«Ŧซ«ḋcolspan=\"2\"«\n!1010 1100!!254!!172!!AC\n¬ŹĴŦЌ،colspan=\"3\"¬Žฌ¬Ỳ¬Ź\n!1010 1101!!255!!173!!AD\ncolspan=10 align=centerSoft hyphen (SHY)ญcolspan=4 align=centerSHY\n!1010 1110!!256!!174!!AE\n®Žstyle=\"background-color:#ccffcc;\" ŽЎstyle=\"background-color:#ccffcc;\" style=\"background-color:#ccffcc;\" colspan=\"2\"®Ūฎcolspan=\"3\"®ź\n!1010 1111!!257!!175!!AF\n¯colspan=\"2\"Ż¯Џstyle=\"background-color:#ccffcc;\" ―colspan=\"2\"¯ŊฏÆŸ¯Ż\n!1011 0000!!260!!176!!B0\ncolspan=\"4\"°Аstyle=\"background-color:#ccffcc;\" colspan=\"4\"°ฐ°Ḟcolspan=\"2\"°\n!1011 0001!!261!!177!!B1\n±ąħąБstyle=\"background-color:#ccffcc;\" colspan=\"3\"±ąฑ±ḟcolspan=\"2\"±\n!1011 0010!!262!!178!!B2\n²˛²˛Вstyle=\"background-color:#ccffcc;\" colspan=\"3\"²ēฒ²Ġ²Č\n!1011 0011!!263!!179!!B3\n³ł³ŗГstyle=\"background-color:#ccffcc;\" colspan=\"3\"³ģณ³ġ³ł\n!1011 0100!!264!!180!!B4\ncolspan=\"4\"´Дstyle=\"background-color:#ccffcc;\" ΄colspan=\"2\"´īด“Ṁcolspan=\"2\"Ž\n!1011 0101!!265!!181!!B5\nµľµĩЕstyle=\"background-color:#ccffcc;\" ΅colspan=\"2\"µĩตµṁµ”\n!1011 0110!!266!!182!!B6\n¶śĥļЖstyle=\"background-color:#ccffcc;\" Άcolspan=\"2\"¶ķถcolspan=\"4\"¶\n!1011 0111!!267!!183!!B7\n·ˇ·ˇЗstyle=\"background-color:#ccffcc;\" colspan=\"4\"·ท·Ṗcolspan=\"2\"·\n!1011 1000!!270!!184!!B8\ncolspan=\"4\"¸Иstyle=\"background-color:#ccffcc;\" Έcolspan=\"2\"¸ļธøẁcolspan=\"2\"ž\n!1011 1001!!271!!185!!B9\n¹šıšЙstyle=\"background-color:#ccffcc;\" Ήcolspan=\"2\"¹đน¹ṗ¹č\n!1011 1010!!272!!186!!BA\nºcolspan=\"2\"şēКstyle=\"background-color:#ccffcc;\" Ί÷ºšบŗẃºș\n!1011 1011!!273!!187!!BB\n»ťğģЛ؛colspan=\"3\"»ŧป»Ṡcolspan=\"2\"»\n!1011 1100!!274!!188!!BC\n¼źĵŧМstyle=\"background-color:#ccffcc;\" Όcolspan=\"2\"¼žผ¼ỳcolspan=\"2\"Œ\n!1011 1101!!275!!189!!BD\n½˝½ŊНstyle=\"background-color:#ccffcc;\" colspan=\"3\"½―ฝ½Ẅcolspan=\"2\"œ\n!1011 1110!!276!!190!!BE\n¾žstyle=\"background-color:#ccffcc;\" žОstyle=\"background-color:#ccffcc;\" Ύcolspan=\"2\"¾ūพ¾ẅcolspan=\"2\"Ÿ\n!1011 1111!!277!!191!!BF\n¿colspan=\"2\"żŋП؟Ώstyle=\"background-color:#ccffcc;\" ¿ŋฟæṡ¿ż\n!1100 0000!!300!!192!!C0\nÀŔÀĀРstyle=\"background-color:#ccffcc;\" ΐstyle=\"background-color:#ccffcc;\" ÀĀภĄcolspan=\"3\"À\n!1100 0001!!301!!193!!C1\ncolspan=\"4\"ÁСءΑstyle=\"background-color:#ccffcc;\" colspan=\"2\"ÁมĮcolspan=\"3\"Á\n!1100 0010!!302!!194!!C2\ncolspan=\"4\"ÂТآΒstyle=\"background-color:#ccffcc;\" colspan=\"2\"ÂยĀcolspan=\"3\"Â\n!1100 0011!!303!!195!!C3\nÃĂstyle=\"background-color:#ccffcc;\" ÃУأΓstyle=\"background-color:#ccffcc;\" colspan=\"2\"ÃรĆcolspan=\"2\"ÃĂ\n!1100 0100!!304!!196!!C4\ncolspan=\"4\"ÄФؤΔstyle=\"background-color:#ccffcc;\" colspan=\"2\"Äฤcolspan=\"4\"Ä\n!1100 0101!!305!!197!!C5\nÅĹĊÅХإΕstyle=\"background-color:#ccffcc;\" colspan=\"2\"Åลcolspan=\"3\"ÅĆ\n!1100 0110!!306!!198!!C6\nÆĆĈÆЦئΖstyle=\"background-color:#ccffcc;\" colspan=\"2\"ÆฦĘcolspan=\"3\"Æ\n!1100 0111!!307!!199!!C7\ncolspan=\"3\"ÇĮЧاΗstyle=\"background-color:#ccffcc;\" ÇĮวĒcolspan=\"3\"Ç\n!1100 1000!!310!!200!!C8\nÈČÈČШبΘstyle=\"background-color:#ccffcc;\" ÈČศČcolspan=\"3\"È\n!1100 1001!!311!!201!!C9\ncolspan=\"4\"ÉЩةΙstyle=\"background-color:#ccffcc;\" colspan=\"2\"Éษcolspan=\"4\"É\n!1100 1010!!312!!202!!CA\nÊĘÊĘЪتΚstyle=\"background-color:#ccffcc;\" ÊĘสŹcolspan=\"3\"Ê\n!1100 1011!!313!!203!!CB\ncolspan=\"4\"ËЫثΛstyle=\"background-color:#ccffcc;\" colspan=\"2\"ËหĖcolspan=\"3\"Ë\n!1100 1100!!314!!204!!CC\nÌĚÌĖЬجΜstyle=\"background-color:#ccffcc;\" ÌĖฬĢcolspan=\"3\"Ì\n!1100 1101!!315!!205!!CD\ncolspan=\"4\"ÍЭحΝstyle=\"background-color:#ccffcc;\" colspan=\"2\"ÍอĶcolspan=\"3\"Í\n!1100 1110!!316!!206!!CE\ncolspan=\"4\"ÎЮخΞstyle=\"background-color:#ccffcc;\" colspan=\"2\"ÎฮĪcolspan=\"3\"Î\n!1100 1111!!317!!207!!CF\nÏĎÏĪЯدΟstyle=\"background-color:#ccffcc;\" colspan=\"2\"ÏฯĻcolspan=\"3\"Ï\n!Binary!!Oct!!Dec!!Hex\n!1!!2!!3!!4!!5!!6!!7!!8!!9!!10!!11!!13!!14!!15!!16\n!1101 0000!!320!!208!!D0\nÐĐstyle=\"background-color:#ccffcc;\" ĐаذΠstyle=\"background-color:#ccffcc;\" ĞÐะŠŴcolspan=\"2\"Ð\n!1101 0001!!321!!209!!D1\nÑŃÑŅбرΡstyle=\"background-color:#ccffcc;\" ÑŅัŃcolspan=\"2\"ÑŃ\n!1101 0010!!322!!210!!D2\nÒŇÒŌвزstyle=\"background-color:#ccffcc;\" style=\"background-color:#ccffcc;\" ÒŌาŅcolspan=\"3\"Ò\n!1101 0011!!323!!211!!D3\ncolspan=\"3\"ÓĶгسΣstyle=\"background-color:#ccffcc;\" colspan=\"2\"Óำcolspan=\"4\"Ó\n!1101 0100!!324!!212!!D4\ncolspan=\"4\"ÔдشΤstyle=\"background-color:#ccffcc;\" colspan=\"2\"ÔิŌcolspan=\"3\"Ô\n!1101 0101!!325!!213!!D5\nÕŐĠÕеصΥstyle=\"background-color:#ccffcc;\" colspan=\"2\"Õีcolspan=\"4\"Ő\n!1101 0110!!326!!214!!D6\ncolspan=\"4\"ÖжضΦstyle=\"background-color:#ccffcc;\" colspan=\"2\"Öึcolspan=\"4\"Ö\n!1101 0111!!327!!215!!D7\ncolspan=\"4\"×зطΧstyle=\"background-color:#ccffcc;\" ×Ũื×Ṫ×Ś\n!1101 1000!!330!!216!!D8\nØŘĜØиظΨstyle=\"background-color:#ccffcc;\" colspan=\"2\"ØุŲcolspan=\"2\"ØŰ\n!1101 1001!!331!!217!!D9\nÙŮÙŲйعΩstyle=\"background-color:#ccffcc;\" ÙŲูŁcolspan=\"3\"Ù\n!1101 1010!!332!!218!!DA\ncolspan=\"4\"ÚкغΪstyle=\"background-color:#ccffcc;\" colspan=\"2\"ÚฺŚcolspan=\"3\"Ú\n!1101 1011!!333!!219!!DB\nÛŰcolspan=\"2\"Ûлstyle=\"background-color:#ccffcc;\" Ϋstyle=\"background-color:#ccffcc;\" colspan=\"2\"Ûstyle=\"background-color:#ccffcc;\" Ūcolspan=\"3\"Û\n!1101 1100!!334!!220!!DC\ncolspan=\"4\"Üмstyle=\"background-color:#ccffcc;\" άstyle=\"background-color:#ccffcc;\" colspan=\"2\"Üstyle=\"background-color:#ccffcc;\" colspan=\"4\"Ü\n!1101 1101!!335!!221!!DD\ncolspan=\"2\"ÝŬŨнstyle=\"background-color:#ccffcc;\" έstyle=\"background-color:#ccffcc;\" İÝstyle=\"background-color:#ccffcc;\" Żcolspan=\"2\"ÝĘ\n!1101 1110!!336!!222!!DE\nÞŢŜŪоstyle=\"background-color:#ccffcc;\" ήstyle=\"background-color:#ccffcc;\" ŞÞstyle=\"background-color:#ccffcc;\" ŽŶÞȚ\n!1101 1111!!337!!223!!DF\ncolspan=\"4\"ßпstyle=\"background-color:#ccffcc;\" ί‗colspan=\"2\"ß฿colspan=\"4\"ß\n!1110 0000!!340!!224!!E0\nàŕàāрـΰאàāเącolspan=\"3\"à\n!1110 0001!!341!!225!!E1\ncolspan=\"4\"áсفαבcolspan=\"2\"áแįcolspan=\"3\"á\n!1110 0010!!342!!226!!E2\ncolspan=\"4\"âтقβגcolspan=\"2\"âโācolspan=\"3\"â\n!1110 0011!!343!!227!!E3\nãăstyle=\"background-color:#ccffcc;\" ãуكγדcolspan=\"2\"ãใćcolspan=\"2\"ãă\n!1110 0100!!344!!228!!E4\ncolspan=\"4\"äфلδהcolspan=\"2\"äไcolspan=\"4\"ä\n!1110 0101!!345!!229!!E5\nåĺċåхمεוcolspan=\"2\"åๅcolspan=\"3\"åć\n!1110 0110!!346!!230!!E6\næćĉæцنζזcolspan=\"2\"æๆęcolspan=\"3\"æ\n!1110 0111!!347!!231!!E7\ncolspan=\"3\"çįчهηחçį็ēcolspan=\"3\"ç\n!1110 1000!!350!!232!!E8\nèčèčшوθטèč่čcolspan=\"3\"è\n!1110 1001!!351!!233!!E9\ncolspan=\"4\"éщىιיcolspan=\"2\"é้colspan=\"4\"é\n!1110 1010!!352!!234!!EA\nêęêęъيκךêę๊źcolspan=\"3\"ê\n!1110 1011!!353!!235!!EB\ncolspan=\"4\"ëыًλכcolspan=\"2\"ë๋ėcolspan=\"3\"ë\n!1110 1100!!354!!236!!EC\nìěìėьٌμלìė์ģcolspan=\"3\"ì\n!1110 1101!!355!!237!!ED\ncolspan=\"4\"íэٍνםcolspan=\"2\"íํķcolspan=\"3\"í\n!1110 1110!!356!!238!!EE\ncolspan=\"4\"îюَξמcolspan=\"2\"î๎īcolspan=\"3\"î\n!1110 1111!!357!!239!!EF\nïďïīяُοןcolspan=\"2\"ï๏ļcolspan=\"3\"ï\n!1111 0000!!360!!240!!F0\nðđstyle=\"background-color:#ccffcc;\" đِπנğð๐šŵðđ\n!1111 0001!!361!!241!!F1\nñńñņёّρסñņ๑ńcolspan=\"2\"ñń\n!1111 0010!!362!!242!!F2\nòňòōђْςעòō๒ņcolspan=\"3\"ò\n!1111 0011!!363!!243!!F3\ncolspan=\"3\"óķѓstyle=\"background-color:#ccffcc;\" σףcolspan=\"2\"ó๓colspan=\"4\"ó\n!1111 0100!!364!!244!!F4\ncolspan=\"4\"ôєstyle=\"background-color:#ccffcc;\" τפcolspan=\"2\"ô๔ōcolspan=\"3\"ô\n!1111 0101!!365!!245!!F5\nõőġõѕstyle=\"background-color:#ccffcc;\" υץcolspan=\"2\"õ๕colspan=\"4\"ő\n!1111 0110!!366!!246!!F6\ncolspan=\"4\"öіstyle=\"background-color:#ccffcc;\" φצcolspan=\"2\"ö๖colspan=\"4\"ö\n!1111 0111!!367!!247!!F7\ncolspan=\"4\"÷їstyle=\"background-color:#ccffcc;\" χק÷ũ๗÷ṫ÷ś\n!1111 1000!!370!!248!!F8\nøřĝøјstyle=\"background-color:#ccffcc;\" ψרcolspan=\"2\"ø๘ųcolspan=\"2\"øű\n!1111 1001!!371!!249!!F9\nùůùųљstyle=\"background-color:#ccffcc;\" ωשùų๙łcolspan=\"3\"ù\n!1111 1010!!372!!250!!FA\ncolspan=\"4\"úњstyle=\"background-color:#ccffcc;\" ϊתcolspan=\"2\"ú๚ścolspan=\"3\"ú\n!1111 1011!!373!!251!!FB\nûűcolspan=\"2\"ûћstyle=\"background-color:#ccffcc;\" ϋstyle=\"background-color:#ccffcc;\" colspan=\"2\"û๛ūcolspan=\"3\"û\n!1111 1100!!374!!252!!FC\ncolspan=\"4\"üќstyle=\"background-color:#ccffcc;\" όstyle=\"background-color:#ccffcc;\" colspan=\"2\"üstyle=\"background-color:#ccffcc;\" colspan=\"4\"ü\n!1111 1101!!375!!253!!FD\ncolspan=\"2\"ýŭũ§style=\"background-color:#ccffcc;\" ύstyle=\"background-color:#ffff99; font-family: monospace;\"ıýstyle=\"background-color:#ccffcc;\" żcolspan=\"2\"ýę\n!1111 1110!!376!!254!!FE\nþţŝūўstyle=\"background-color:#ccffcc;\" ώstyle=\"background-color:#ffff99; font-family: monospace;\"şþstyle=\"background-color:#ccffcc;\" žŷþț\n!1111 1111!!377!!255!!FF\nÿcolspan=\"3\"˙џstyle=\"background-color:#ccffcc;\" style=\"background-color:#ccffcc;\" style=\"background-color:#ccffcc;\" ÿĸstyle=\"background-color:#ccffcc;\" ’colspan=\"3\"ÿ\n!Binary!!Oct!!Dec!!Hex\n!1!!2!!3!!4!!5!!6!!7!!8!!9!!10!!11!!13!!14!!15!!16\n\nAt position 0xA0 there's always the non breaking space and 0xAD is mostly the soft hyphen, which only shows at line breaks. Other empty fields are either unassigned or the system used is not able to display them.\n\nThere are new additions as and versions. LRM stands for left-to-right mark (U+200E) and RLM stands for right-to-left mark (U+200F).\n\nSince 1991, the Unicode Consortium has been working with ISO and IEC to develop the Unicode Standard and ISO/IEC 10646: the Universal Character Set (UCS) in tandem. Newer editions of ISO/IEC 8859 express characters in terms of their Unicode/UCS names and the \"U+nnnn\" notation, effectively causing each part of ISO/IEC 8859 to be a Unicode/UCS character encoding scheme that maps a very small subset of the UCS to single 8-bit bytes. The first 256 characters in Unicode and the UCS are identical to those in ISO/IEC-8859-1 (Latin-1).\n\nSingle-byte character sets including the parts of ISO/IEC 8859 and derivatives of them were favoured throughout the 1990s, having the advantages of being well-established and more easily implemented in software: the equation of one byte to one character is simple and adequate for most single-language applications, and there are no combining characters or variant forms. As Unicode-enabled operating systems became more widespread, ISO/IEC 8859 and other legacy encodings became less popular. While remnants of ISO 8859 and single-byte character models remain entrenched in many operating systems, programming languages, data storage systems, networking applications, display hardware, and end-user application software, most modern computing applications use Unicode internally, and rely on conversion tables to map to and from other encodings, when necessary.\n\nThe ISO/IEC 8859 standard was maintained by ISO/IEC Joint Technical Committee 1, Subcommittee 2, Working Group 3 (ISO/IEC JTC 1/SC 2/WG 3). In June 2004, WG 3 disbanded, and maintenance duties were transferred to SC 2. The standard is not currently being updated, as the Subcommittee's only remaining working group, WG 2, is concentrating on development of Unicode's Universal Coded Character Set.\n\nBULLET::::- List of computer character sets\nBULLET::::- RPL character set (An ISO 8859-1 superset on HP calculators, referred to as \"ECMA-94\" as well)\nBULLET::::- DEC Multinational Character Set (MCS)\nBULLET::::- DEC National Replacement Character Set (NRCS)\n\nBULLET::::- Published versions of each part of ISO/IEC 8859 are available, for a fee, from the ISO catalogue site and from the IEC Webstore.\nBULLET::::- PDF versions of the final drafts of some parts of ISO/IEC 8859 as submitted to the ISO/IEC JTC 1/SC 2/WG 3 for review & publication are available at the WG 3 web site:\nBULLET::::- ISO/IEC 8859-1:1998 - 8-bit single-byte coded graphic character sets, Part 1: Latin alphabet No. 1 \"(draft dated February 12, 1998, published April 15, 1998)\"\nBULLET::::- ISO/IEC 8859-4:1998 - 8-bit single-byte coded graphic character sets, Part 4: Latin alphabet No. 4 \"(draft dated February 12, 1998, published July 1, 1998)\"\nBULLET::::- ISO/IEC 8859-7:1999 - 8-bit single-byte coded graphic character sets, Part 7: Latin/Greek alphabet \"(draft dated June 10, 1999; superseded by ISO/IEC 8859-7:2003, published October 10, 2003)\"\nBULLET::::- ISO/IEC 8859-10:1998 - 8-bit single-byte coded graphic character sets, Part 10: Latin alphabet No. 6 \"(draft dated February 12, 1998, published July 15, 1998)\"\nBULLET::::- ISO/IEC 8859-11:1999 - 8-bit single-byte coded graphic character sets, Part 11: Latin/Thai character set \"(draft dated June 22, 1999; superseded by ISO/IEC 8859-11:2001, published 15 December 2001)\"\nBULLET::::- ISO/IEC 8859-13:1998 - 8-bit single-byte coded graphic character sets, Part 13: Latin alphabet No. 7 \"(draft dated April 15, 1998, published October 15, 1998)\"\nBULLET::::- ISO/IEC 8859-15:1998 - 8-bit single-byte coded graphic character sets, Part 15: Latin alphabet No. 9 \"(draft dated August 1, 1997; superseded by ISO/IEC 8859-15:1999, published March 15, 1999)\"\nBULLET::::- ISO/IEC 8859-16:2000 - 8-bit single-byte coded graphic character sets, Part 16: Latin alphabet No. 10 \"(draft dated November 15, 1999; superseded by ISO/IEC 8859-16:2001, published July 15, 2001)\"\nBULLET::::- ECMA standards, which in intent correspond exactly to the ISO/IEC 8859 character set standards, can be found at:\nBULLET::::- Standard ECMA-94: 8-Bit Single Byte Coded Graphic Character Sets - Latin Alphabets No. 1 to No. 4 \"2nd edition (June 1986)\"\nBULLET::::- Standard ECMA-113: 8-Bit Single-Byte Coded Graphic Character Sets - Latin/Cyrillic Alphabet \"3rd edition (December 1999)\"\nBULLET::::- Standard ECMA-114: 8-Bit Single-Byte Coded Graphic Character Sets - Latin/Arabic Alphabet \"2nd edition (December 2000)\"\nBULLET::::- Standard ECMA-118: 8-Bit Single-Byte Coded Graphic Character Sets - Latin/Greek Alphabet \"(December 1986)\"\nBULLET::::- Standard ECMA-121: 8-Bit Single-Byte Coded Graphic Character Sets - Latin/Hebrew Alphabet \"2nd edition (December 2000)\"\nBULLET::::- Standard ECMA-128: 8-Bit Single-Byte Coded Graphic Character Sets - Latin Alphabet No. 5 \"2nd edition (December 1999)\"\nBULLET::::- Standard ECMA-144: 8-Bit Single-Byte Coded Character Sets - Latin Alphabet No. 6 \"3rd edition (December 2000)\"\nBULLET::::- ISO/IEC 8859-1 to Unicode mapping tables as plain text files are at the Unicode FTP site.\nBULLET::::- Informal descriptions and code charts for most ISO/IEC 8859 standards are available in ISO/IEC 8859 Alphabet Soup (Mirror)\n"}
{"id": "15022", "url": "https://en.wikipedia.org/wiki?curid=15022", "title": "Infrared", "text": "Infrared\n\nInfrared radiation (IR), sometimes called infrared light, is electromagnetic radiation (EMR) with wavelengths longer than those of visible light. It is therefore generally invisible to the human eye, although IR at wavelengths up to 1050 nanometers (nm)s from specially pulsed lasers can be seen by humans under certain conditions. IR wavelengths extend from the nominal red edge of the visible spectrum at 700 nanometers (frequency 430 THz), to 1 millimeter (300 GHz). Most of the thermal radiation emitted by objects near room temperature is infrared. As with all EMR, IR carries radiant energy and behaves both like a wave and like its quantum particle, the photon.\n\nInfrared radiation was discovered in 1800 by astronomer Sir William Herschel, who discovered a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the total energy from the Sun was eventually found to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has a critical effect on Earth's climate.\n\nInfrared radiation is emitted or absorbed by molecules when they change their rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range.\n\nInfrared radiation is used in industrial, scientific, military, \nlaw enforcement, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, and to detect overheating of electrical apparatus.\n\nExtensive uses for military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 μm (micrometers). Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting.\n\nInfrared radiation extends from the nominal red edge of the visible spectrum at 700 nanometers (nm) to 1 millimeter (mm). This range of wavelengths corresponds to a frequency range of approximately 430 THz down to 300 GHz. Below infrared is the microwave portion of the electromagnetic spectrum.\n! Name  Wavelength  Frequency (Hz)  Photon energy (eV)\n\nSunlight, at an effective temperature of 5780 kelvins (5510 °C, 9940 °F), is composed of near-thermal-spectrum radiation that is slightly more than half infrared. At zenith, sunlight provides an irradiance of just over 1 kilowatt per square meter at sea level. Of this energy, 527 watts is infrared radiation, 445 watts is visible light, and 32 watts is ultraviolet radiation. Nearly all the infrared radiation in sunlight is near infrared, shorter than 4 micrometers.\n\nOn the surface of Earth, at far lower temperatures than the surface of the Sun, some thermal radiation consists of infrared in the mid-infrared region, much longer than in sunlight. However, black-body, or thermal, radiation is continuous: it gives off radiation at all wavelengths. Of these natural thermal radiation processes, only lightning and natural fires are hot enough to produce much visible energy, and fires produce far more infrared than visible-light energy.\n\nIn general, objects emit infrared radiation across a spectrum of wavelengths, but sometimes only a limited region of the spectrum is of interest because sensors usually collect radiation only within a specific bandwidth. Thermal infrared radiation also has a maximum emission wavelength, which is inversely proportional to the absolute temperature of object, in accordance with Wien's displacement law.\n\nTherefore, the infrared band is often subdivided into smaller sections.\n\nA commonly used sub-division scheme is:\n\n! Division name\n! Abbreviation\n! Wavelength\n! Frequency\n! Photon energy\n! Temperature\n! Characteristics\n! Near-infrared\nNIR, IR-A \"DIN\"\n! Short-wavelength infrared\n! Mid-wavelength infrared\nMWIR, IR-C \"DIN\"; MidIR. Also called intermediate infrared (IIR)\n! Long-wavelength infrared\n! Far infrared\n\nNIR and SWIR is sometimes called \"reflected infrared\", whereas MWIR and LWIR is sometimes referred to as \"thermal infrared\". Due to the nature of the blackbody radiation curves, typical \"hot\" objects, such as exhaust pipes, often appear brighter in the MW compared to the same object viewed in the LW.\n\nThe International Commission on Illumination (CIE) recommended the division of infrared radiation into the following three bands:\n!Abbreviation\n!Wavelength\n!Frequency\n\nISO 20473 specifies the following scheme:\n\n! style=\"width:100pt; text-align:left;\" Designation\n! style=\"width:100pt; text-align:center;\" Abbreviation\n! style=\"width:150pt; text-align:center;\" Wavelength\nalign=\"left\" Near-Infrared\nalign=\"left\" Mid-Infrared\nalign=\"left\" Far-Infrared\n\nAstronomers typically divide the infrared spectrum as follows:\n\n! style=\"width:100pt; text-align:left;\" Designation\n! style=\"width:100pt; text-align:center;\" Abbreviation\n! style=\"width:150pt; text-align:center;\" Wavelength\nalign=\"left\" Near-Infrared\nalign=\"left\" Mid-Infrared\nalign=\"left\" Far-Infrared\n\nThese divisions are not precise and can vary depending on the publication. The three regions are used for observation of different temperature ranges, and hence different environments in space.\n\nThe most common photometric system used in astronomy allocates capital letters to different spectral regions according to filters used; I, J, H, and K cover the near-infrared wavelengths; L, M, N, and Q refer to the mid-infrared region. These letters are commonly understood in reference to atmospheric windows and appear, for instance, in the titles of many papers.\n\nA third scheme divides up the band based on the response of various detectors:\n\nBULLET::::- Near-infrared: from 0.7 to 1.0 μm (from the approximate end of the response of the human eye to that of silicon).\nBULLET::::- Short-wave infrared: 1.0 to 3 μm (from the cut-off of silicon to that of the MWIR atmospheric window). InGaAs covers to about 1.8 μm; the less sensitive lead salts cover this region.\nBULLET::::- Mid-wave infrared: 3 to 5 μm (defined by the atmospheric window and covered by indium antimonide [InSb] and mercury cadmium telluride [HgCdTe] and partially by lead selenide [PbSe]).\nBULLET::::- Long-wave infrared: 8 to 12, or 7 to 14 μm (this is the atmospheric window covered by HgCdTe and microbolometers).\nBULLET::::- Very-long wave infrared (VLWIR) (12 to about 30 μm, covered by doped silicon).\n\nNear-infrared is the region closest in wavelength to the radiation detectable by the human eye. mid- and far-infrared are progressively further from the visible spectrum. Other definitions follow different physical mechanisms (emission peaks, vs. bands, water absorption) and the newest follow technical reasons (the common silicon detectors are sensitive to about 1,050 nm, while InGaAs's sensitivity starts around 950 nm and ends between 1,700 and 2,600 nm, depending on the specific configuration). No international standards for these specifications are currently available.\n\nThe onset of infrared is defined (according to different standards) at various values typically between 700 nm and 800 nm, but the boundary between visible and infrared light is not precisely defined. The human eye is markedly less sensitive to light above 700 nm wavelength, so longer wavelengths make insignificant contributions to scenes illuminated by common light sources. However, particularly intense near-IR light (e.g., from IR lasers, IR LED sources, or from bright daylight with the visible light removed by colored gels) can be detected up to approximately 780 nm, and will be perceived as red light. Intense light sources providing wavelengths as long as 1050 nm can be seen as a dull red glow, causing some difficulty in near-IR illumination of scenes in the dark (usually this practical problem is solved by indirect illumination). Leaves are particularly bright in the near IR, and if all visible light leaks from around an IR-filter are blocked, and the eye is given a moment to adjust to the extremely dim image coming through a visually opaque IR-passing photographic filter, it is possible to see the Wood effect that consists of IR-glowing foliage.\n\nIn optical communications, the part of the infrared spectrum that is used is divided into seven bands based on availability of light sources transmitting/absorbing materials (fibers) and detectors:\n\n!Band\n!Descriptor\n!Wavelength range\nO band\nOriginal\n1260–1360 nm\nE band\nExtended\n1360–1460 nm\nS band\nShort wavelength\n1460–1530 nm\nC band\nConventional\n1530–1565 nm\nL band\nLong wavelength\n1565–1625 nm\nU band\nUltralong wavelength\n1625–1675 nm\n\nThe C-band is the dominant band for long-distance telecommunication networks. The S and L bands are based on less well established technology, and are not as widely deployed.\n\nInfrared radiation is popularly known as \"heat radiation\", but light and electromagnetic waves of any frequency will heat surfaces that absorb them. Infrared light from the Sun accounts for 49% of the heating of Earth, with the rest being caused by visible light that is absorbed then re-radiated at longer wavelengths. Visible light or ultraviolet-emitting lasers can char paper and incandescently hot objects emit visible radiation. Objects at room temperature will emit radiation concentrated mostly in the 8 to 25 μm band, but this is not distinct from the emission of visible light by incandescent objects and ultraviolet by even hotter objects (see black body and Wien's displacement law).\n\nHeat is energy in transit that flows due to a temperature difference. Unlike heat transmitted by thermal conduction or thermal convection, thermal radiation can propagate through a vacuum. Thermal radiation is characterized by a particular spectrum of many wavelengths that are associated with emission from an object, due to the vibration of its molecules at a given temperature. Thermal radiation can be emitted from objects at any wavelength, and at very high temperatures such radiation is associated with spectra far above the infrared, extending into visible, ultraviolet, and even X-ray regions (e.g. the solar corona). Thus, the popular association of infrared radiation with thermal radiation is only a coincidence based on typical (comparatively low) temperatures often found near the surface of planet Earth.\n\nThe concept of emissivity is important in understanding the infrared emissions of objects. This is a property of a surface that describes how its thermal emissions deviate from the idea of a black body. To further explain, two objects at the same physical temperature will not show the same infrared image if they have differing emissivity. For example, for any pre-set emissivity value, objects with higher emissivity will appear hotter, and those with a lower emissivity will appear cooler. For that reason, incorrect selection of emissivity will give inaccurate results when using infrared cameras and pyrometers.\n\n Infrared is used in night vision equipment when there is insufficient visible light to see. Night vision devices operate through a process involving the conversion of ambient light photons into electrons that are then amplified by a chemical and electrical process and then converted back into visible light. Infrared light sources can be used to augment the available ambient light for conversion by night vision devices, increasing in-the-dark visibility without actually using a visible light source.\n\nThe use of infrared light and night vision devices should not be confused with thermal imaging, which creates images based on differences in surface temperature by detecting infrared radiation (heat) that emanates from objects and their surrounding environment.\n\nInfrared radiation can be used to remotely determine the temperature of objects (if the emissivity is known). This is termed thermography, or in the case of very hot objects in the NIR or visible it is termed pyrometry. Thermography (thermal imaging) is mainly used in military and industrial applications but the technology is reaching the public market in the form of infrared cameras on cars due to greatly reduced production costs.\n\nThermographic cameras detect radiation in the infrared range of the electromagnetic spectrum (roughly 900–14,000 nanometers or 0.9–14 μm) and produce images of that radiation. Since infrared radiation is emitted by all objects based on their temperatures, according to the black-body radiation law, thermography makes it possible to \"see\" one's environment with or without visible illumination. The amount of radiation emitted by an object increases with temperature, therefore thermography allows one to see variations in temperature (hence the name).\n\nA hyperspectral image is a \"picture\" containing continuous spectrum through a wide spectral range at each pixel. Hyperspectral imaging is gaining importance in the field of applied spectroscopy particularly with NIR, SWIR, MWIR, and LWIR spectral regions. Typical applications include biological, mineralogical, defence, and industrial measurements.\n\nThermal infrared hyperspectral imaging can be similarly performed using a thermographic camera, with the fundamental difference that each pixel contains a full LWIR spectrum. Consequently, chemical identification of the object can be performed without a need for an external light source such as the Sun or the Moon. Such cameras are typically applied for geological measurements, outdoor surveillance and UAV applications.\n\nIn infrared photography, infrared filters are used to capture the near-infrared spectrum. Digital cameras often use infrared blockers. Cheaper digital cameras and camera phones have less effective filters and can \"see\" intense near-infrared, appearing as a bright purple-white color. This is especially pronounced when taking pictures of subjects near IR-bright areas (such as near a lamp), where the resulting infrared interference can wash out the image. There is also a technique called 'T-ray' imaging, which is imaging using far-infrared or terahertz radiation. Lack of bright sources can make terahertz photography more challenging than most other infrared imaging techniques. Recently T-ray imaging has been of considerable interest due to a number of new developments such as terahertz time-domain spectroscopy.\n\nInfrared tracking, also known as infrared homing, refers to a passive missile guidance system, which uses the emission from a target of electromagnetic radiation in the infrared part of the spectrum to track it. Missiles that use infrared seeking are often referred to as \"heat-seekers\" since infrared (IR) is just below the visible spectrum of light in frequency and is radiated strongly by hot bodies. Many objects such as people, vehicle engines, and aircraft generate and retain heat, and as such, are especially visible in the infrared wavelengths of light compared to objects in the background.\n\nInfrared radiation can be used as a deliberate heating source. For example, it is used in infrared saunas to heat the occupants. It may also be used in other heating applications, such as to remove ice from the wings of aircraft (de-icing). Infrared can be used in cooking and heating food as it predominantly heats the opaque, absorbent objects, rather than the air around them.\n\nInfrared heating is also becoming more popular in industrial manufacturing processes, e.g. curing of coatings, forming of plastics, annealing, plastic welding, and print drying. In these applications, infrared heaters replace convection ovens and contact heating.\n\nEfficiency is achieved by matching the wavelength of the infrared heater to the absorption characteristics of the material.\n\nA variety of technologies or proposed technologies take advantage of infrared emissions to cool buildings or other systems. The LWIR (8–15 μm) region is especially useful since some radiation at these wavelengths can escape into space through the atmosphere.\n\nIR data transmission is also employed in short-range communication among computer peripherals and personal digital assistants. These devices usually conform to standards published by IrDA, the Infrared Data Association. Remote controls and IrDA devices use infrared light-emitting diodes (LEDs) to emit infrared radiation that is focused by a plastic lens into a narrow beam. The beam is modulated, i.e. switched on and off, to prevent interference from other sources of infrared (like sunlight or artificial lighting). The receiver uses a silicon photodiode to convert the infrared radiation to an electric current. It responds only to the rapidly pulsing signal created by the transmitter, and filters out slowly changing infrared radiation from ambient light. Infrared communications are useful for indoor use in areas of high population density. IR does not penetrate walls and so does not interfere with other devices in adjoining rooms. Infrared is the most common way for remote controls to command appliances.\nInfrared remote control protocols like RC-5, SIRC, are used to communicate with infrared.\n\nFree space optical communication using infrared lasers can be a relatively inexpensive way to install a communications link in an urban area operating at up to 4 gigabit/s, compared to the cost of burying fiber optic cable, except for the radiation damage. \"Since the eye cannot detect IR, blinking or closing the eyes to help prevent or reduce damage may not happen.\"\n\nInfrared lasers are used to provide the light for optical fiber communications systems. Infrared light with a wavelength around 1,330 nm (least dispersion) or 1,550 nm (best transmission) are the best choices for standard silica fibers.\n\nIR data transmission of encoded audio versions of printed signs is being researched as an aid for visually impaired people through the RIAS (Remote Infrared Audible Signage) project.\nTransmitting IR data from one device to another is sometimes referred to as beaming.\n\nInfrared vibrational spectroscopy (see also near-infrared spectroscopy) is a technique that can be used to identify molecules by analysis of their constituent bonds. Each chemical bond in a molecule vibrates at a frequency characteristic of that bond. A group of atoms in a molecule (e.g., CH) may have multiple modes of oscillation caused by the stretching and bending motions of the group as a whole. If an oscillation leads to a change in dipole in the molecule then it will absorb a photon that has the same frequency. The vibrational frequencies of most molecules correspond to the frequencies of infrared light. Typically, the technique is used to study organic compounds using light radiation from 4000–400 cm, the mid-infrared. A spectrum of all the frequencies of absorption in a sample is recorded. This can be used to gain information about the sample composition in terms of chemical groups present and also its purity (for example, a wet sample will show a broad O-H absorption around 3200 cm). The unit for expressing radiation in this application, cm, is the spectroscopic wavenumber. It is the frequency divided by the speed of light in vacuum.\n\nIn the semiconductor industry, infrared light can be used to characterize materials such as thin films and periodic trench structures. By measuring the reflectance of light from the surface of a semiconductor wafer, the index of refraction (n) and the extinction Coefficient (k) can be determined via the Forouhi-Bloomer dispersion equations. The reflectance from the infrared light can also be used to determine the critical dimension, depth, and sidewall angle of high aspect ratio trench structures.\n\nWeather satellites equipped with scanning radiometers produce thermal or infrared images, which can then enable a trained analyst to determine cloud heights and types, to calculate land and surface water temperatures, and to locate ocean surface features. The scanning is typically in the range 10.3–12.5 μm (IR4 and IR5 channels).\n\nHigh, cold ice clouds such as cirrus or cumulonimbus show up bright white, lower warmer clouds such as stratus or stratocumulus show up as grey, with intermediate clouds shaded accordingly. Hot land surfaces will show up as dark-grey or black. One disadvantage of infrared imagery is that low cloud such as stratus or fog can have a temperature similar to the surrounding land or sea surface and does not show up. However, using the difference in brightness of the IR4 channel (10.3–11.5 μm) and the near-infrared channel (1.58–1.64 μm), low cloud can be distinguished, producing a \"fog\" satellite picture. The main advantage of infrared is that images can be produced at night, allowing a continuous sequence of weather to be studied.\n\nThese infrared pictures can depict ocean eddies or vortices and map currents such as the Gulf Stream, which are valuable to the shipping industry. Fishermen and farmers are interested in knowing land and water temperatures to protect their crops against frost or increase their catch from the sea. Even El Niño phenomena can be spotted. Using color-digitized techniques, the gray-shaded thermal images can be converted to color for easier identification of desired information.\n\nThe main water vapour channel at 6.40 to 7.08 μm can be imaged by some weather satellites and shows the amount of moisture in the atmosphere.\nIn the field of climatology, atmospheric infrared radiation is monitored to detect trends in the energy exchange between the earth and the atmosphere. These trends provide information on long-term changes in Earth's climate. It is one of the primary parameters studied in research into global warming, together with solar radiation.\nA pyrgeometer is utilized in this field of research to perform continuous outdoor measurements. This is a broadband infrared radiometer with sensitivity for infrared radiation between approximately 4.5 μm and 50 μm.\n\nAstronomers observe objects in the infrared portion of the electromagnetic spectrum using optical components, including mirrors, lenses and solid state digital detectors. For this reason it is classified as part of optical astronomy. To form an image, the components of an infrared telescope need to be carefully shielded from heat sources, and the detectors are chilled using liquid helium.\n\nThe sensitivity of Earth-based infrared telescopes is significantly limited by water vapor in the atmosphere, which absorbs a portion of the infrared radiation arriving from space outside of selected atmospheric windows. This limitation can be partially alleviated by placing the telescope observatory at a high altitude, or by carrying the telescope aloft with a balloon or an aircraft. Space telescopes do not suffer from this handicap, and so outer space is considered the ideal location for infrared astronomy.\n\nThe infrared portion of the spectrum has several useful benefits for astronomers. Cold, dark molecular clouds of gas and dust in our galaxy will glow with radiated heat as they are irradiated by imbedded stars. Infrared can also be used to detect protostars before they begin to emit visible light. Stars emit a smaller portion of their energy in the infrared spectrum, so nearby cool objects such as planets can be more readily detected. (In the visible light spectrum, the glare from the star will drown out the reflected light from a planet.)\n\nInfrared light is also useful for observing the cores of active galaxies, which are often cloaked in gas and dust. Distant galaxies with a high redshift will have the peak portion of their spectrum shifted toward longer wavelengths, so they are more readily observed in the infrared.\n\nInfrared cleaning is a technique used by some motion picture film scanners, film scanners and flatbed scanners to reduce or remove the effect of dust and scratches upon the finished scan. It works by collecting an additional infrared channel from the scan at the same position and resolution as the three visible color channels (red, green, and blue). The infrared channel, in combination with the other channels, is used to detect the location of scratches and dust. Once located, those defects can be corrected by scaling or replaced by inpainting.\n\nInfrared reflectography, as called by art conservators, can be applied to paintings to reveal underlying layers in a completely non-destructive manner, in particular the underdrawing or outline drawn by the artist as a guide. This often reveals the artist's use of carbon black, which shows up well in reflectograms, as long as it has not also been used in the ground underlying the whole painting. Art conservators are looking to see whether the visible layers of paint differ from the underdrawing or layers in between – such alterations are called pentimenti when made by the original artist. This is very useful information in deciding whether a painting is the prime version by the original artist or a copy, and whether it has been altered by over-enthusiastic restoration work. In general, the more pentimenti, the more likely a painting is to be the prime version. It also gives useful insights into working practices.\n\nAmong many other changes in the Arnolfini Portrait of 1434 (left), the man's face was originally higher by about the height of his eye; the woman's was higher, and her eyes looked more to the front. Each of his feet was underdrawn in one position, painted in another, and then overpainted in a third. These alterations are seen in infrared reflectograms.\n\nRecent progress in the design of infrared sensitive cameras made it possible to discover and depict not only underpaintings and pentimenti, but entire paintings that were later overpainted by the artist. Notable examples are Picasso's \"Woman Ironing\" and \"Blue Room\", where in both cases a portrait of a man has been made visible under the painting as it is known today.\n\nSimilar uses of infrared are made by conservators and scientists on various types of objects, especially very old written documents such as the Dead Sea Scrolls, the Roman works in the Villa of the Papyri, and the Silk Road texts found in the Dunhuang Caves. Carbon black used in ink can show up extremely well.\n\nThe pit viper has a pair of infrared sensory pits on its head. There is uncertainty regarding the exact thermal sensitivity of this biological infrared detection system.\n\nOther organisms that have thermoreceptive organs are pythons (family Pythonidae), some boas (family Boidae), the Common Vampire Bat (\"Desmodus rotundus\"), a variety of jewel beetles (\"Melanophila acuminata\"), darkly pigmented butterflies (\"Pachliopta aristolochiae\" and \"Troides rhadamantus plateni\"), and possibly blood-sucking bugs (\"Triatoma infestans\").\n\nSome fungi like \"Venturia inaequalis\" require near-infrared light for ejection\n\nAlthough near-infrared vision (780–1000 nm) has long been deemed impossible due to noise in visual pigments, sensation of near-infrared light was reported in the common carp and in three cichlid species. Fish use NIR to capture prey and for phototactic swimming orientation. NIR sensation in fish may be relevant under poor lighting conditions during twilight and in turbid surface waters.\n\nNear-infrared light, or photobiomodulation, is used for treatment of chemotherapy-induced oral ulceration as well as wound healing. There is some work relating to anti-herpes virus treatment. Research projects include work on central nervous system healing effects via cytochrome c oxidase upregulation and other possible mechanisms.\n\nStrong infrared radiation in certain industry high-heat settings may be hazardous to the eyes, resulting in damage or blindness to the user. Since the radiation is invisible, special IR-proof goggles must be worn in such places.\n\nThe discovery of infrared radiation is ascribed to William Herschel, the astronomer, in the early 19th century. Herschel published his results in 1800 before the Royal Society of London. Herschel used a prism to refract light from the sun and detected the infrared, beyond the red part of the spectrum, through an increase in the temperature recorded on a thermometer. He was surprised at the result and called them \"Calorific Rays\". The term \"infrared\" did not appear until late 19th century.\n\nOther important dates include:\n\nBULLET::::- 1737: Émilie du Châtelet predicted what is today known as infrared radiation in .\nBULLET::::- 1830: Leopoldo Nobili made the first thermopile IR detector.\nBULLET::::- 1840: John Herschel produces the first thermal image, called a thermogram.\nBULLET::::- 1860: Gustav Kirchhoff formulated the blackbody theorem formula_1.\nBULLET::::- 1873: Willoughby Smith discovered the photoconductivity of selenium.\nBULLET::::- 1878: Samuel Pierpont Langley invents the first bolometer, a device which is able to measure small temperature fluctuations, and thus the power of far infrared sources.\nBULLET::::- 1879: Stefan–Boltzmann law formulated empirically that the power radiated by a blackbody is proportional to \"T\".\nBULLET::::- 1880s and 1890s: Lord Rayleigh and Wilhelm Wien solved part of the blackbody equation, but both solutions diverged in parts of the electromagnetic spectrum. This problem was called the \"ultraviolet catastrophe and infrared catastrophe\".\nBULLET::::- 1892: Willem Henri Julius published infrared spectra of 20 organic compounds measured with a bolometer in units of angular displacement.\nBULLET::::- 1901: Max Planck published the blackbody equation and theorem. He solved the problem by quantizing the allowable energy transitions.\nBULLET::::- 1905: Albert Einstein developed the theory of the photoelectric effect.\nBULLET::::- 1905–1908: William Coblentz published infrared spectra in units of wavelength (micrometers) for several chemical compounds in \"Investigations of Infra-Red Spectra\".\nBULLET::::- 1917: Theodore Case developed the thallous sulfide detector; British scientist built the first infra-red search and track (IRST) device able to detect aircraft at a range of one mile (1.6 km).\nBULLET::::- 1935: Lead salts – early missile guidance in World War II.\nBULLET::::- 1938: Teau Ta predicted that the pyroelectric effect could be used to detect infrared radiation.\nBULLET::::- 1945: The Zielgerät 1229 \"Vampir\" infrared weapon system was introduced as the first portable infrared device for military applications.\nBULLET::::- 1952: H. Welker grew synthetic InSb crystals.\nBULLET::::- 1950s: Paul Kruse (at Honeywell) and Texas Instruments recorded infrared images.\nBULLET::::- 1950s and 1960s: Nomenclature and radiometric units defined by Fred Nicodemenus, G. J. Zissis and R. Clark; Robert Clark Jones defined \"D\"*.\nBULLET::::- 1958: W. D. Lawson (Royal Radar Establishment in Malvern) discovered IR detection properties of HgCdTe.\nBULLET::::- 1958: Falcon and Sidewinder missiles were developed using infrared technology.\nBULLET::::- 1961: J. Cooper demonstrated pyroelectric detection.\nBULLET::::- 1964: W. G. Evans discovered infrared thermoreceptors in a pyrophile beetle.\nBULLET::::- 1965: First IR handbook; first commercial imagers (Barnes, Agema (now part of FLIR Systems Inc.)); Richard Hudson's landmark text; F4 TRAM FLIR by Hughes; phenomenology pioneered by Fred Simmons and A. T. Stair; U.S. Army's night vision lab formed (now Night Vision and Electronic Sensors Directorate (NVESD)), and Rachets develops detection, recognition and identification modeling there.\nBULLET::::- 1970: Willard Boyle and George E. Smith proposed CCD at Bell Labs for picture phone.\nBULLET::::- 1972: Common module program started by NVESD.\nBULLET::::- 1978: Infrared imaging astronomy came of age, observatories planned, IRTF on Mauna Kea opened; 32 × 32 and 64 × 64 arrays produced using InSb, HgCdTe and other materials.\nBULLET::::- 2013: On 14 February, researchers developed a neural implant that gives rats the ability to sense infrared light, which for the first time provides living creatures with new abilities, instead of simply replacing or augmenting existing abilities.\n\nBULLET::::- Infrared: A Historical Perspective (Omega Engineering)\nBULLET::::- Infrared Data Association, a standards organization for infrared data interconnection\nBULLET::::- SIRC Protocol\nBULLET::::- How to build an USB infrared receiver to control PC's remotely\nBULLET::::- Infrared Waves: detailed explanation of infrared light. (NASA)\nBULLET::::- Herschel's original paper from 1800 announcing the discovery of infrared light\nBULLET::::- The thermographic's library, collection of thermogram\nBULLET::::- Infrared reflectography in analysis of paintings at ColourLex\nBULLET::::- Molly Faries, Techniques and Applications – Analytical Capabilities of Infrared Reflectography: An Art Historian s Perspective, in Scientific Examination of Art: Modern Techniques in Conservation and Analysis, Sackler NAS Colloquium, 2005\n"}
{"id": "15023", "url": "https://en.wikipedia.org/wiki?curid=15023", "title": "Icosidodecahedron", "text": "Icosidodecahedron\n\nIn geometry, an icosidodecahedron is a polyhedron with twenty (icosi) triangular faces and twelve (dodeca) pentagonal faces. An icosidodecahedron has 30 identical vertices, with two triangles and two pentagons meeting at each, and 60 identical edges, each separating a triangle from a pentagon. As such it is one of the Archimedean solids and more particularly, a quasiregular polyhedron.\n\nAn icosidodecahedron has icosahedral symmetry, and its first stellation is the compound of a dodecahedron and its dual icosahedron, with the vertices of the icosidodecahedron located at the midpoints of the edges of either.\n\nIts dual polyhedron is the rhombic triacontahedron. An icosidodecahedron can be split along any of six planes to form a pair of pentagonal rotundae, which belong among the Johnson solids.\n\nThe icosidodecahedron can be considered a \"pentagonal gyrobirotunda\", as a combination of two rotundae (compare pentagonal orthobirotunda, one of the Johnson solids). In this form its symmetry is D, [10,2], (2*5), order 20.\n\nThe wire-frame figure of the icosidodecahedron consists of six flat regular decagons, meeting in pairs at each of the 30 vertices.\n\nThe icosidodecahedron has 6 central decagons. Projected into a sphere, they define 6 great circles. Buckminster Fuller used these 6 great circles, along with 15 and 10 others in two other polyhedra to define his 31 great circles of the spherical icosahedron.\n\nConvenient Cartesian coordinates for the vertices of an icosidodecahedron with unit edges are given by the even permutations of:\nBULLET::::- (0, 0, ±\"φ\")\nwhere \"φ\" is the golden ratio, .\n\nThe icosidodecahedron has four special orthogonal projections, centered on a vertex, an edge, a triangular face, and a pentagonal face. The last two correspond to the A and H Coxeter planes.\n+ Orthogonal projections\n!Centered by\n!Vertex\n!Edge\n!Face<br>Triangle\n!Face<br>Pentagon\n!Solid\n\n!Wireframe\n\n!Projective<br>symmetry\n[2]\n[2]\n[6]\n[10]\n!Dual\n\nThe surface area \"A\" and the volume \"V\" of the icosidodecahedron of edge length \"a\" are:\n\nThe icosidodecahedron can also be represented as a spherical tiling, and projected onto the plane via a stereographic projection. This projection is conformal, preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.\n<br>Pentagon-centered\n<br>Triangle-centered\n!Orthographic projection\n!colspan=2Stereographic projections\n\n!colspan=\"5\" Orthographic projections\ncolspan=\"3\" 2-fold, 3-fold and 5-fold symmetry axes\n\nThe icosidodecahedron is a rectified dodecahedron and also a rectified icosahedron, existing as the full-edge truncation between these regular solids.\n\nThe icosidodecahedron contains 12 pentagons of the dodecahedron and 20 triangles of the icosahedron:\nThe icosidodecahedron exists in a sequence of symmetries of quasiregular polyhedra and tilings with vertex configurations (3.\"n\"), progressing from tilings of the sphere to the Euclidean plane and into the hyperbolic plane. With orbifold notation symmetry of *\"n\"32 all of these tilings are wythoff construction within a fundamental domain of symmetry, with generator points at the right angle corner of the domain.\n\nThe icosidodecahedron is related to the Johnson solid called a pentagonal orthobirotunda created by two pentagonal rotunda connected as mirror images. The \"icosidodecahedron\" can therefore be called a \"pentagonal gyrobirotunda\" with the gyration between top and bottom halves.\n\nalign=center<br>(Dissection)\nalign=center valign=bottom<br>Icosidodecahedron<br>(\"pentagonal gyrobirotunda\")\nalign=center valign=bottom<br>Pentagonal orthobirotunda\nalign=center valign=bottom<br>Pentagonal rotunda\n\nThe truncated cube can be turned into an icosidodecahedron by dividing the octagons into two pentagons and two triangles. It has pyritohedral symmetry.\n\nEight uniform star polyhedra share the same vertex arrangement. Of these, two also share the same edge arrangement: the small icosihemidodecahedron (having the triangular faces in common), and the small dodecahemidodecahedron (having the pentagonal faces in common). The vertex arrangement is also shared with the compounds of five octahedra and of five tetrahemihexahedra.\nalign=center<br>Icosidodecahedron\nalign=center<br>Small icosihemidodecahedron\nalign=center<br>Small dodecahemidodecahedron\nalign=center<br>Great icosidodecahedron\nalign=center<br>Great dodecahemidodecahedron\nalign=center<br>Great icosihemidodecahedron\nalign=center<br>Dodecadodecahedron\nalign=center<br>Small dodecahemicosahedron\nalign=center<br>Great dodecahemicosahedron\nalign=center<br>Compound of five octahedra\nalign=center<br>Compound of five tetrahemihexahedra\n\nIn four-dimensional geometry the icosidodecahedron appears in the regular 600-cell as the equatorial slice that belongs to the vertex-first passage of the 600-cell through 3D space. In other words: the 30 vertices of the 600-cell which lie at arc distances of 90 degrees on its circumscribed hypersphere from a pair of opposite vertices, are the vertices of an icosidodecahedron. The wire frame figure of the 600-cell consists of 72 flat regular decagons. Six of these are the equatorial decagons to a pair of opposite vertices. They are precisely the six decagons which form the wire frame figure of the icosidodecahedron.\n\nIn the mathematical field of graph theory, a icosidodecahedral graph is the graph of vertices and edges of the icosidodecahedron, one of the Archimedean solids. It has 30 vertices and 60 edges, and is a quartic graph Archimedean graph.\n\nIn Star Trek Universe, the Vulcan game of logic Kal-Toh has the goal to create a holographic icosidodecahedron.\n\nIn \"The Wrong Stars\", book one of the Axiom series, by Tim Pratt, Elena has a icosidodecahedron machine on either side of her. [Paperback p 336]\n\nBULLET::::- Cuboctahedron\nBULLET::::- Great truncated icosidodecahedron\nBULLET::::- Icosahedron\nBULLET::::- Rhombicosidodecahedron\nBULLET::::- Truncated icosidodecahedron\n\nBULLET::::- (Section 3-9)\n\nBULLET::::- Editable printable net of an icosidodecahedron with interactive 3D view\nBULLET::::- The Uniform Polyhedra\nBULLET::::- Virtual Reality Polyhedra The Encyclopedia of Polyhedra\n"}
{"id": "15024", "url": "https://en.wikipedia.org/wiki?curid=15024", "title": "ISO 8601", "text": "ISO 8601\n\nISO 8601 \"Data elements and interchange formats – Information interchange – Representation of dates and times\" is an international standard covering the exchange of date- and time-related data. It was issued by the International Organization for Standardization (ISO) and was first published in 1988. The purpose of this standard is to provide an unambiguous and well-defined method of representing dates and times, so as to avoid misinterpretation of numeric representations of dates and times, particularly when data is transferred between countries with different conventions for writing numeric dates and times.\n\nIn general, ISO 8601 applies to representations and formats of dates in the Gregorian (and potentially proleptic Gregorian) calendar, of times based on the 24-hour timekeeping system (with optional UTC offset), of , and combinations thereof. The standard does not assign any specific meaning to elements of the date/time to be represented; the meaning will depend on the context of its use. In addition, dates and times to be represented cannot include words with no specified numerical meaning in the standard (e.g., names of years in the Chinese calendar) or that do not use characters (e.g., images, sounds).\n\nIn representations for interchange, dates and times are arranged so the largest temporal term (the year) is placed to the left and each successively smaller term is placed to the right of the previous term. Representations must be written in a combination of Arabic numerals and certain characters (such as \"-\", \":\", \"T\", \"W\", and \"Z\") that are given specific meanings within the standard; the implication is that some commonplace ways of writing parts of dates, such as \"January\" or \"Thursday\", are not allowed in interchange representations.\n\nThe first edition of the ISO 8601 standard was published as \"ISO 8601:1988\" in 1988. It unified and replaced a number of older ISO standards on various aspects of date and time notation: ISO 2014, ISO 2015, ISO 2711, ISO 3307, and ISO 4031. It has been superseded by a second edition \"ISO 8601:2000\" in 2000, by a third edition \"ISO 8601:2004\" published on 1 December 2004, and withdrawn and revised by \"ISO 8601-1:2019\" and \"ISO 8601-2:2019\" on 25 February 25 2019. ISO 8601 was prepared by, and is under the direct responsibility of, ISO Technical Committee TC 154.\n\nISO 2014, though superseded, is the standard that originally introduced the all-numeric date notation in most-to-least-significant order . The ISO week numbering system was introduced in ISO 2015, and the identification of days by ordinal dates was originally defined in ISO 2711.\n\nThe draft ISO/DIS 8601-1:2016 represents the slightly updated contents of the current ISO 8601 standard, whereas the draft ISO/DIS 8601-2:2016 defines various extensions such as uncertainties or parts of the Extended Date/Time Format (EDTF).\n\n+ List\n! Name  Description\n\nBULLET::::- Date and time values are ordered from the largest to smallest unit of time: year, month (or week), day, hour, minute, second, and fraction of second. The lexicographical order of the representation thus corresponds to chronological order, except for date representations involving negative years or time offset. This allows dates to be naturally sorted by, for example, file systems.\nBULLET::::- Each date and time value has a fixed number of digits that must be padded with leading zeros.\nBULLET::::- Representations can be done in one of two formatsa basic format with a minimal number of separators or an extended format with separators added to enhance human readability. The standard notes that \"The basic format should be avoided in plain text.\" The separator used between date values (year, month, week, and day) is the hyphen, while the colon is used as the separator between time values (hours, minutes, and seconds). For example, the 6th day of the 1st month of the year 2009 may be written as in the extended format or simply as \"20090106\" in the basic format without ambiguity.\nBULLET::::- For reduced accuracy, any number of values may be dropped from any of the date and time representations, but in the order from the least to the most significant. For example, \"2004-05\" is a valid ISO 8601 date, which indicates May (the fifth month) 2004. This format will never represent the 5th day of an unspecified month in 2004, nor will it represent a time-span extending from 2004 into 2005.\nBULLET::::- If necessary for a particular application, the standard supports the addition of a decimal fraction to the smallest time value in the representation.\n\nThe standard uses the Gregorian calendar, which serves as an international standard for civil use.\n\nISO 8601 fixes a reference calendar date to the Gregorian calendar of 20 May 1875 as the date the (Metre Convention) was signed in Paris. However, ISO calendar dates before the Convention are still compatible with the Gregorian calendar all the way back to the official introduction of the Gregorian calendar on . Earlier dates, in the proleptic Gregorian calendar, may be used by mutual agreement of the partners exchanging information. The standard states that every date must be consecutive, so usage of the Julian calendar would be contrary to the standard (because at the switchover date, the dates would not be consecutive).\n\n YYYY\nISO 8601 prescribes, as a minimum, a four-digit year [YYYY] to avoid the year 2000 problem. It therefore represents years from 0000 to 9999, year 0000 being equal to 1 BC and all others AD. However, years prior to 1583 are not automatically allowed by the standard. Instead \"values in the range [0000] through [1582] shall only be used by mutual agreement of the partners in information interchange.\"\n\nTo represent years before 0000 or after 9999, the standard also permits the expansion of the year representation but only by prior agreement between the sender and the receiver. An expanded year representation [±YYYYY] must have an agreed-upon number of extra year digits beyond the four-digit minimum, and it must be prefixed with a + or − sign instead of the more common AD/BC (or CE/BCE) notation; by convention 1 BC is labelled +0000, 2 BC is labeled −0001, and so on.\n\n \"or\" YYYYMMDD\n\nCalendar date representations are in the form shown in the adjacent box. [YYYY] indicates a four-digit year, 0000 through 9999. [MM] indicates a two-digit month of the year, 01 through 12. [DD] indicates a two-digit day of that month, 01 through 31. For example, \"5 April 1981\" may be represented as either in the \"extended format\" or \"19810405\" in the \"basic format\".\n\nThe standard also allows for calendar dates to be written with reduced accuracy. For example, one may write to mean \"1981 April\". The 2000 version allowed writing to mean \"April 5\" but the 2004 version does not allow omitting the year when a month is present. One may simply write \"1981\" to refer to that year or \"19\" to refer to the century from 1900 to 1999 inclusive. Although the standard allows both the and YYYYMMDD formats for complete calendar date representations, if the day [DD] is omitted then only the format is allowed. By disallowing dates of the form YYYYMM, the standard avoids confusion with the truncated representation YYMMDD (still often used).\n\n YYYY-Www \"or\" YYYYWww\nWeek date representations are in the formats as shown in the adjacent box. [YYYY] indicates the \"ISO week-numbering year\" which is slightly different from the traditional Gregorian calendar year (see below). [Www] is the \"week number\" prefixed by the letter \"W\", from W01 through W53. [D] is the \"weekday number\", from 1 through 7, beginning with Monday and ending with Sunday.\n\nThere are several mutually equivalent and compatible descriptions of week 01:\nBULLET::::- the week with the year's first Thursday in it (the formal ISO definition),\nBULLET::::- the week with 4 January in it,\nBULLET::::- the first week with the majority (four or more) of its days in the starting year, and\nBULLET::::- the week starting with the Monday in the period 29 December – 4 January.\n\nAs a consequence, if 1 January is on a Monday, Tuesday, Wednesday or Thursday, it is in week 01. If 1 January is on a Friday, Saturday or Sunday, it is in week 52 or 53 of the previous year (there is no week 00). 28 December is always in the last week of its year.\n\nThe week number can be described by counting the Thursdays: week 12 contains the 12th Thursday of the year.\n\nThe \"ISO week-numbering year\" starts at the first day (Monday) of week 01 and ends at the Sunday before the new ISO year (hence without overlap or gap). It consists of 52 or 53 full weeks. The first ISO week of a year may have up to three days that are actually in the Gregorian calendar year that is ending; if three, they are Monday, Tuesday and Wednesday. Similarly, the last ISO week of a year may have up to three days that are actually in the Gregorian calendar year that is starting; if three, they are Friday, Saturday, and Sunday. The Thursday of each ISO week is always in the Gregorian calendar year denoted by the ISO week-numbering year.\n\nExamples:\nBULLET::::- is written \"\"\nBULLET::::- is written \"\"\n\n YYYY-DDD \"or\" YYYYDDD\n\nAn ordinal date is a simple form for occasions when the arbitrary nature of week and month definitions are more of an impediment than an aid, for instance, when comparing dates from different calendars. As represented above, [YYYY] indicates a year. [DDD] is the day of that year, from 001 through 365 (366 in leap years). For example, is also .\n\nThis format is used with simple hardware systems that have a need for a date system, but where including full calendar calculation software may be a significant nuisance. This system is sometimes referred to as \"Julian Date\", but this can cause confusion with the astronomical Julian day, a sequential count of the number of days since day 0 beginning Greenwich noon, Julian proleptic calendar (or noon on ISO date which uses the Gregorian proleptic calendar with a year 0000).\n\nhh:mm:ss.sss\n\"or\"\nhhmmss.sss\n\nISO 8601 uses the 24-hour clock system. The \"basic format\" is [hh][mm][ss] and the \"extended format\" is [hh]:[mm]:[ss].\nBULLET::::- [hh] refers to a zero-padded hour between 00 and 24 (where 24 is only used to denote midnight at the end of a calendar day).\nBULLET::::- [mm] refers to a zero-padded minute between 00 and 59.\nBULLET::::- [ss] refers to a zero-padded second between 00 and 60 (where 60 is only used to denote an added leap second).\nSo a time might appear as either \"134730\" in the \"basic format\" or \"13:47:30\" in the \"extended format\".\n\nEither the seconds, or the minutes and seconds, may be omitted from the basic or extended time formats for greater brevity but decreased accuracy; the resulting reduced accuracy time formats are:\nBULLET::::- [hh][mm] in \"basic format\" or [hh]:[mm] in \"extended format\", when seconds are omitted.\nBULLET::::- [hh], when both seconds and minutes are omitted.\n\n\"Midnight\" is a special case and may be referred to as either \"00:00\" or \"24:00\". The notation \"00:00\" is used at the beginning of a calendar day and is the more frequently used. At the end of a day use \"24:00\". \"2007-04-05T24:00\" is the same instant as \"2007-04-06T00:00\" (see \"Combined date and time representations\" below).\n\nDecimal fractions may be added to any of the three time elements. However, a fraction may only be added to the lowest order time element in the representation. A decimal mark, either a comma or a dot (without any preference as stated in resolution 10 of the 22nd General Conference CGPM in 2003, but with a preference for a comma according to ISO 8601:2004) is used as a separator between the time element and its fraction. To denote \"14 hours, 30 and one half minutes\", do not include a seconds figure. Represent it as \"14:30,5\", \"1430,5\", \"14:30.5\", or \"1430.5\". There is no limit on the number of decimal places for the decimal fraction. However, the number of decimal places needs to be agreed to by the communicating parties. For example, in Microsoft SQL Server, the precision of a decimal fraction is 3, i.e., \"yyyy-mm-ddThh:mm:ss[.mmm]\".\n\n <time>Z\n\nTime zones in ISO 8601 are represented as local time (with the location unspecified), as UTC, or as an offset from UTC.\n\nIf no UTC relation information is given with a time representation, the time is assumed to be in local time. While it \"may\" be safe to assume local time when communicating in the same time zone, it is ambiguous when used in communicating across different time zones. Even within a single geographic time zone, some local times will be ambiguous if the region observes daylight saving time. It is usually preferable to indicate a time zone (zone designator) using the standard's notation.\n\nIf the time is in UTC, add a \"Z\" directly after the time without a space. \"Z\" is the zone designator for the zero UTC offset. \"09:30 UTC\" is therefore represented as \"09:30Z\" or \"0930Z\". \"14:45:15 UTC\" would be \"14:45:15Z\" or \"144515Z\".\n\nThe \"Z\" suffix in the ISO 8601 time representation is sometimes referred to as \"Zulu time\" because the same letter is used to designate the Zulu time zone. However the ACP 121 standard that defines the list of military time zones makes no mention of UTC and derives the \"Zulu time\" from the Greenwich Mean Time which was formerly used as the international civil time standard. GMT is no longer precisely defined by the scientific community and can refer to either UTC or UT1 depending on context.\n\nThe UTC offset is appended to the time in the same way that 'Z' was above, in the form ±[hh]:[mm], ±[hh][mm], or ±[hh]. \n\nNegative UTC offsets describe a time zone west of , where the civil time is behind (or earlier) than UTC so the zone designator will look like \"−03:00\",\"−0300\", or \"−03\".\n\nPositive UTC offsets describe a time zone east of , where the civil time is ahead (or later) than UTC so the zone designator will look like \"+02:00\",\"+0200\", or \"+02\".\n\nExamples\nBULLET::::- \"−05:00\" for New York on standard time ()\nBULLET::::- \"−04:00\" for New York on daylight savings time ()\nBULLET::::- \"+00:00\" (but not \"-00:00) for London ()\nBULLET::::- \"+02:00\" for Cairo ()\nBULLET::::- \"+05:30\" for Mumbai ()\nBULLET::::- \"+14:00\" for Kiribati ()\n\nSee List of UTC time offsets for other UTC offsets.\n\nTo represent a negative offset, ISO 8601 specifies using either a hyphen–minus or a minus sign character. If the interchange character set is limited and does not have a minus sign character, then the hyphen–minus should be used. ASCII does not have a minus sign, so its hyphen–minus character (code is 45 decimal or 2D hexadecimal) would be used. If the character set has a minus sign, then that character should be used. Unicode has a minus sign, and its character code is U+2212 (2212 hexadecimal); the HTML character entity invocation is codice_1.\n\nThe following times all refer to the same moment: \"18:30Z\", \"22:30+04\", \"1130−0700\", and \"15:00−03:30\". Nautical time zone letters are not used with the exception of Z. To calculate UTC time one has to subtract the offset from the local time, e.g. for \"15:00−03:30\" do 15:00 − (−03:30) to get 18:30 UTC.\n\nAn offset of zero, in addition to having the special representation \"Z\", can also be stated numerically as \"+00:00\", \"+0000\", or \"+00\". However, it is not permitted to state it numerically with a negative sign, as \"−00:00\", \"−0000\", or \"−00\". The section dictating sign usage (section 3.4.2 in the 2004 edition of the standard) states that a plus sign must be used for a positive or zero value, and a minus sign for a negative value. Contrary to this rule, RFC 3339, which is otherwise a profile of ISO 8601, permits the use of \"-00\", with the same denotation as \"+00\" but a differing connotation.\n\n <date>T<time>\n\nA single point in time can be represented by concatenating a complete date expression, the letter \"\"T\"\" as a delimiter, and a valid time expression. For example, . It is permitted to omit the \"\"T\"\" character by mutual agreement as in .\nSeparating date and time parts with other characters such as space is not allowed in ISO 8601, but allowed in its profile RFC 3339.\n\nIf a time zone designator is required, it follows the combined date and time. For example, or .\n\nEither basic or extended formats may be used, but both date and time must use the same format. The date expression may be calendar, week, or ordinal, and must use a complete representation. The time may be represented using a specified reduced accuracy format.\n\n PnYnMnDTnHnMnS\nDurations define the amount of intervening time in a time interval and are represented by the format P[n]Y[n]M[n]DT[n]H[n]M[n]S or P[n]W as shown to the right. In these representations, the [n] is replaced by the value for each of the date and time elements that follow the [n]. Leading zeros are not required, but the maximum number of digits for each element should be agreed to by the communicating parties. The capital letters \"P\", \"Y\", \"M\", \"W\", \"D\", \"T\", \"H\", \"M\", and \"S\" are designators for each of the date and time elements and are not replaced.\n\nBULLET::::- \"P\" is the duration designator (for \"period\") placed at the start of the duration representation.\nBULLET::::- \"Y\" is the year designator that follows the value for the number of years.\nBULLET::::- \"M\" is the month designator that follows the value for the number of months.\nBULLET::::- \"W\" is the week designator that follows the value for the number of weeks.\nBULLET::::- \"D\" is the day designator that follows the value for the number of days.\nBULLET::::- \"T\" is the time designator that precedes the time components of the representation.\nBULLET::::- \"H\" is the hour designator that follows the value for the number of hours.\nBULLET::::- \"M\" is the minute designator that follows the value for the number of minutes.\nBULLET::::- \"S\" is the second designator that follows the value for the number of seconds.\n\nFor example, \"P3Y6M4DT12H30M5S\" represents a duration of \"three years, six months, four days, twelve hours, thirty minutes, and five seconds\".\n\nDate and time elements including their designator may be omitted if their value is zero, and lower order elements may also be omitted for reduced precision. For example, \"P23DT23H\" and \"P4Y\" are both acceptable duration representations. However, at least one element must be present, thus \"P\" is not a valid representation for a duration of 0 seconds. \"PT0S\" or \"P0D\", however, are both valid and represent the same duration.\n\nTo resolve ambiguity, \"P1M\" is a one-month duration and \"PT1M\" is a one-minute duration (note the time designator, T, that precedes the time value). The smallest value used may also have a decimal fraction, as in \"P0.5Y\" to indicate half a year. This decimal fraction may be specified with either a comma or a full stop, as in \"P0,5Y\" or \"P0.5Y\". The standard does not prohibit date and time values in a duration representation from exceeding their \"carry over points\" except as noted below. Thus, \"PT36H\" could be used as well as \"P1DT12H\" for representing the same duration. But keep in mind that \"PT36H\" is not the same as \"P1DT12H\" when switching from or to Daylight saving time.\n\nAlternatively, a format for duration based on combined date and time representations may be used by agreement between the communicating parties either in the basic format PYYYYMMDDThhmmss or in the extended format . For example, the first duration shown above would be . However, individual date and time values cannot exceed their moduli (e.g. a value of 13 for the month or 25 for the hour would not be permissible).\n\nAlthough the standard describes a duration as part of time intervals, which are discussed in the next section, the duration format (or a subset thereof) is widely used independent of time intervals, as with the Java 8 Duration class.\n\n <start>/<end>\nA time interval is the intervening time between two time points. The amount of intervening time is expressed by a duration (as described in the previous section). The two time points (start and end) are expressed by either a combined date and time representation or just a date representation.\n\nThere are four ways to express a time interval:\n\nBULLET::::1. Start and end, such as \"2007-03-01T13:00:00Z/2008-05-11T15:30:00Z\"\nBULLET::::2. Start and duration, such as \"2007-03-01T13:00:00Z/P1Y2M10DT2H30M\"\nBULLET::::3. Duration and end, such as \"P1Y2M10DT2H30M/2008-05-11T15:30:00Z\"\nBULLET::::4. Duration only, such as \"P1Y2M10DT2H30M\", with additional context information\n\nOf these, the first three require two values separated by an \"interval designator\" which is usually a solidus (more commonly referred to as a forward slash \"/\"). Section 4.4.2 of the standard notes that: \"In certain application areas a double hyphen is used as a separator instead of a solidus.\" The standard does not define the term \"double hyphen\", but previous versions used notations like \"2000--2002\". Use of a double hyphen instead of a solidus allows inclusion in computer filenames. A solidus is a reserved character and not allowed in a filename in common operating systems.\n\nFor <start>/<end> expressions, if any elements are missing from the end value, they are assumed to be the same as for the start value including the time zone. This feature of the standard allows for concise representations of time intervals. For example, the date of a two-hour meeting including the start and finish times could be simply shown as \"2007-12-14T13:30/15:30\", where \"/15:30\" implies \"/2007-12-14T15:30\" (the same date as the start), or the beginning and end dates of a monthly billing period as \"2008-02-15/03-14\", where \"/03-14\" implies \"/2008-03-14\" (the same year as the start).\n\nIf greater precision is desirable to represent the time interval, then more time elements can be added to the representation. An interval denoted can start at any time on and end at any time on , whereas includes the start and end times.\nTo explicitly include all of the start and end dates, the interval would be represented as .\n\n Rn/<interval>\nRepeating intervals are specified in clause \"4.5 Recurring time interval\". They are formed by adding \"R[n]/\" to the beginning of an interval expression, where \"R\" is used as the letter itself and [n] is replaced by the number of repetitions. Leaving out the value for [n] means an unbounded number of repetitions. If the interval specifies the start (forms 1 and 2 above), then this is the start of the repeating interval. If the interval specifies the end but not the start (form 3 above), then this is the end of the repeating interval. For example, to repeat the interval of \"P1Y2M10DT2H30M\" five times starting at , use .\n\nISO 8601:2000 allowed truncation (by agreement), where leading components of a date or time are omitted. Notably, this allowed two-digit years to be used and the ambiguous formats YY-MM-DD and YYMMDD. This provision was removed in ISO 8601:2004.\n\nOn the Internet, the World Wide Web Consortium (W3C) uses ISO 8601 in defining a profile of the standard that restricts the supported date and time formats to reduce the chance of error and the complexity of software.\n\nISO 8601 is referenced by several specifications, but the full range of options of ISO 8601 is not always used. For example, the various electronic program guide standards for TV, digital radio, etc. use several forms to describe points in time and durations. The ID3 audio meta-data specification also makes use of a subset of ISO 8601.\nThe X.690 encoding standard's GeneralizedTime makes use of another subset of ISO 8601.\n\nThe ISO 8601 week date, as of 2006, appeared in its basic form on major brand commercial packaging in the United States. Its appearance depended on the particular packaging, canning, or bottling plant more than any particular brand. The format is particularly useful for quality assurance, so that production errors can be readily traced to work weeks, and products can be correctly targeted for recall.\n\nRFC 3339 defines a profile of ISO 8601 for use in Internet protocols and standards. It explicitly excludes durations and dates before the common era. The more complex formats such as week numbers and ordinal days are not permitted.\n\nRFC 3339 deviates from ISO 8601 in allowing a zero time zone offset to be specified as \"-00:00\", which ISO 8601 forbids. RFC 3339 intends \"-00:00\" to carry the connotation that it is not stating a preferred time zone, whereas the conforming \"+00:00\" or any non-zero offset connotes that the offset being used is preferred. This convention regarding \"-00:00\" is derived from earlier RFCs, such as RFC 2822 which uses it for timestamps in email headers. RFC 2822 made no claim that any part of its timestamp format conforms to ISO 8601, and so was free to use this convention without conflict.\n\n! style=\"text-align:left\"  Australia \n! style=\"text-align:left\"  Austria \n! style=\"text-align:left\"  Belgium \n! style=\"text-align:left\"  Brazil \n! style=\"text-align:left\"  Canada \n! style=\"text-align:left\"  Colombia \n! style=\"text-align:left\"  China \n! style=\"text-align:left\"  Czech Republic \n! style=\"text-align:left\"  Denmark \n! style=\"text-align:left\"  Estonia \n! style=\"text-align:left\"  European Norm \n! style=\"text-align:left\"  Finland \n! style=\"text-align:left\"  France \n! style=\"text-align:left\"  Germany \n! style=\"text-align:left\"  Greece \n! style=\"text-align:left\"  Hungary \n! style=\"text-align:left\"  Iceland \n! style=\"text-align:left\"  India \n! style=\"text-align:left\"  Ireland \n! style=\"text-align:left\"  Italy \n! style=\"text-align:left\"  Japan \n! style=\"text-align:left\"  Korea, Republic of \n! style=\"text-align:left\"  Latvia \n! style=\"text-align:left\"  Lithuania \n! style=\"text-align:left\"  Luxembourg \n! style=\"text-align:left\"  Netherlands \n! style=\"text-align:left\"  Norway \n! style=\"text-align:left\"  Poland \n! style=\"text-align:left\"  Portugal \n! style=\"text-align:left\"  Russia \n! style=\"text-align:left\"  South Africa \n! style=\"text-align:left\"  Spain \n! style=\"text-align:left\"  Sweden \n! style=\"text-align:left\"  Switzerland \n! style=\"text-align:left\"  Taiwan \n! style=\"text-align:left\"  Thailand \n! style=\"text-align:left\"  Turkey \n! style=\"text-align:left\"  Ukraine \n! style=\"text-align:left\"  United Kingdom \n! style=\"text-align:left\"  United States \n! style=\"text-align:left\"  Vietnam \n\nBULLET::::- Astronomical year numbering\nBULLET::::- Date and time representation by country\nBULLET::::- Date format by country\nBULLET::::- Horology\n\nBULLET::::- ISO's catalog entry for ISO 8601:2004\nBULLET::::- The latest prototype of ISO 8601-1 (ISO/TC 154 N)\nBULLET::::- The latest prototype of ISO 8601-2 (ISO/TC 154 N)\nBULLET::::- Use international date format (ISO) – Quality Web Tips The World Wide Web Consortium (W3C)\nBULLET::::- ISO 8601 summary by Markus Kuhn\nBULLET::::- The Mathematics of the ISO 8601 Calendar\nBULLET::::- W3C Specification about UTC Date and Time, based on ISO 8601:1988\nBULLET::::- IETF RFC 3339, based on ISO 8601:2000\n\nImplementation overview\nBULLET::::- ISO 8601 Implementation Around The World\n"}
{"id": "15027", "url": "https://en.wikipedia.org/wiki?curid=15027", "title": "Isa", "text": "Isa\n\nIsa or ISA may refer to:\n\nBULLET::::- Isa, Kagoshima, Japan\nBULLET::::- Isa, Nigeria\nBULLET::::- Isa District, Kagoshima, former district in Japan\nBULLET::::- Isa Town, middle class town located in Bahrain\nBULLET::::- Mount Isa, Queensland, Australia\n\nBULLET::::- Isa (name), an Arabic name corresponding to Jesus in English\nBULLET::::- Isa Tengblad (born 1998), Swedish singer using the mononym Isa\n\nBULLET::::- Information Systems Associates FZE, an aviation software house\n\nBULLET::::- ISA (\"Days of Our Lives\"), spy agency in TV series\nBULLET::::- Isa the iguana, in TV series \"Dora the Explorer\"\nBULLET::::- Interplanetary Strategic Alliance (ISA), military alliance in videogame saga \"Killzone\"\n\nBULLET::::- \"Isa\" (album), a 2004 album by Enslaved\nBULLET::::- Isa (comics), of graphic novel series \"Les Passagers du vent\"\nBULLET::::- \"Isa\" (film), a 2014 television film\nBULLET::::- Isa, a dance in music of the Canary Islands\n\nBULLET::::- Industry Standard Architecture, a PC computer bus standard\nBULLET::::- Instruction set architecture of a computer architecture\nBULLET::::- Is-a, a relationship between abstractions in programming\nBULLET::::- Internet Security and Acceleration, a network router, firewall, antivirus program, VPN server and web cache from Microsoft Corporation\n\nBULLET::::- Independent Schools Association (Australia), mainly for sports\nBULLET::::- Independent Schools Association (UK)\nBULLET::::- Iniciativa de Salud de las Americas, Spanish name for Health Initiative of the Americas\nBULLET::::- Institute for the Study of the Americas, University of London, England\nBULLET::::- Instituto Superior de Agronomia, an agronomy faculty in Lisbon, Portugal\nBULLET::::- Instituto Superior de Arte, an art school in Havana, Cuba\nBULLET::::- International School Amsterdam, the Netherlands\nBULLET::::- International School Augsburg\nBULLET::::- International School of Aleppo, Syria\nBULLET::::- International School of Athens, Greece\nBULLET::::- International School of the Americas, San Antonio, Texas\nBULLET::::- International Studies Association\nBULLET::::- Islamic Saudi Academy\n\nBULLET::::- Income share agreement, US, a borrowing agreement sometimes used for tuition loans\nBULLET::::- Individual savings account, UK\nBULLET::::- International Standards on Auditing\nBULLET::::- Israel Securities Authority\n\nBULLET::::- Independent Safeguarding Authority, former UK child protection agency\nBULLET::::- Intelligence Services Act 1994, UK\nBULLET::::- Intelligence Support Activity, of the US Army\nBULLET::::- International Seabed Authority, for mineral-related activities\nBULLET::::- International Searching Authority, for patents\nBULLET::::- International Solar Alliance\nBULLET::::- Interoperability Solutions for European Public Administrations, EU\nBULLET::::- Iranian Space Agency\nBULLET::::- Israel Security Agency or Shin Bet, Israel\nBULLET::::- Israel Space Agency\nBULLET::::- Israel State Archives, the national archive of Israel\nBULLET::::- Italian Space Agency\n\nIn alphabetical order:\nBULLET::::- International Society of Arboriculture, a non-profit botanical organization\nBULLET::::- International Society of Automation, a non-profit professional organization for engineers, technicians, and students\nBULLET::::- International Sociological Association\nBULLET::::- International Soling Association\nBULLET::::- International Surfing Association, the world governing authority for the sport of surfing\nBULLET::::- Irish Sailing Association, the governing body for sailing in Ireland\n\nBULLET::::- Isosaccharinic acid, a six-carbon sugar acid\nBULLET::::- Intrinsic sympatheticomimetic activity, a term used with beta blockers that are partial agonists\nBULLET::::- Infectious salmon anemia, a viral disease of salmon\nBULLET::::- International Standard Atmosphere, an atmospheric model of the Earth\n\nBULLET::::- Intelligent speed adaptation, systems to automatically enforce vehicle speed limits\nBULLET::::- International Somali Awards, an award ceremony\nBULLET::::- International Symbol of Access, blue and white wheelchair symbol\nBULLET::::- ISA Brown, type of chicken\nBULLET::::- Ideological state apparatus, a theory by Louis Althusser\n\nBULLET::::- Internal Security Act (disambiguation)\nBULLET::::- Isaz, the \"I\" rune in the Scandinavian runic alphabet\n"}
{"id": "15028", "url": "https://en.wikipedia.org/wiki?curid=15028", "title": "International Seabed Authority", "text": "International Seabed Authority\n\nThe International Seabed Authority (ISA) () is an intergovernmental body based in Kingston, Jamaica, that was established to organize, regulate and control all mineral-related activities in the international seabed area beyond the limits of national jurisdiction, an area underlying most of the world's oceans. It is an organization established by the United Nations Convention on the Law of the Sea.\n\nFollowing at least ten preparatory meetings over the years, the Authority held its first inaugural meeting in its host country, Jamaica, on 16 November 1994, the day the Convention came into force. The articles governing the Authority have been made \"noting the political and economic changes, including market-oriented approaches, affecting the implementation\" of the Convention. The Authority obtained its observer status to the United Nations in October 1996.\n\nCurrently, the Authority has 167 members and the European Union, composed of all parties to the United Nations Convention on the Law of the Sea.\n\nTwo principal organs establish the policies and govern the work of the Authority: the Assembly, in which all members are represented, and a 36-member Council elected by the Assembly. Council members are chosen according to a formula designed to ensure equitable representation of countries from various groups, including those engaged in seabed mineral exploration and the land-based producers of minerals found on the seabed. The Authority holds one annual session, usually of two weeks' duration.\n\nAlso established is a 30-member Legal and Technical Commission which advises the Council and a 15-member Finance Committee that deals with budgetary and related matters. All members are experts nominated by governments and elected to serve in their individual capacity.\n\nThe Authority operates by contracting with private and public corporations and other entities authorizing them to explore, and eventually exploit, specified areas on the deep seabed for mineral resources essential for building most technological products. The Convention also established a body called the Enterprise which is to serve as the Authority's own mining operator, but no concrete steps have been taken to bring this into being.\n\nThe Authority currently has a Secretariat of 37 authorized posts and a biennial budget of $9.1 million for 2017 and $8.9 million for 2018. In July 2016, the Assembly of the Authority elected Michael Lodge of the United Kingdom, for a four-year term as Secretary-General beginning 1 January 2017. He succeeds Nii Allotey Odunton of Ghana, who had served two consecutive four-year terms since 2008.\n\nThe exploitation system envisaged in the UN Convention on the Law of the Sea, overseen by the Authority, came to life with the signature in 2001/02 of 15-year contracts with seven organizations that had applied for specific seabed areas in which they were authorized to explore for polymetallic nodules. In 2006, a German entity was added to the list.\n\nThese contractors are: Yuzhmorgeologya (Russian Federation); Interoceanmetal Joint Organization (IOM) (Bulgaria, Cuba, Slovakia, Czech Republic, Poland and Russian Federation); the Government of the Republic of Korea; China Ocean Minerals Research and Development Association (COMRA) (China); Deep Ocean Resources Development Company (DORD) (Japan); Institut français de recherche pour l’exploitation de la mer (IFREMER) (France); the Government of India, the Federal Institute for Geosciences and Natural Resources of Germany.\n\nAll but one of the current areas of exploration are in the Clarion-Clipperton Zone, in the Equatorial North Pacific Ocean south and southeast of Hawaii. The remaining area, being explored by India, is in the Central Indian Basin of the Indian Ocean.\n\nEach area is limited to , of which half is to be relinquished to the Authority after eight years. Each contractor is required to report once a year on its activities in its assigned area. So far, none of them has indicated any serious move to begin commercial exploitation.\n\nIn 2008, the Authority received two new applications for authorization to explore for polymetallic nodules, coming for the first time from private firms in developing island nations of the Pacific. Sponsored by their respective governments, they were submitted by Nauru Ocean Resources Inc. and Tonga Offshore Mining Limited. A 15-year exploration contract was granted by the Authority to Nauru Ocean Resources Inc. on 22 July 2011 and to Tonga Offshore Mining Limited on 12 January 2012.\n\nFifteen-year exploration contracts for polymetallic nodules were also granted to G-TECH Sea Mineral Resources NV (Belgium) on 14 January 2013; Marawa Research and Exploration Ltd (Kiribati) on 19 January 2015; Ocean Mineral Singapore Pte Ltd on 22 January 2015; UK Seabed Resources Ltd (two contracts on 8 February 2013 and 29 March 2016 respectively); Cook Islands Investment Corporation on 15 July 2016 and more recently China Minmetals Corporation on 12 May 2017.\n\nThe Authority has signed seven contracts for the exploration for polymetallic sulphides in the South West Indian Ridge, Central Indian Ridge and Mid-Atlantic Ridge with China Ocean Mineral Resources Research and Development Association (18 November 2011); the Government of Russia (29 October 2012); Government of the Republic of Korea (24 June 2014); Institut français de recherche pour l’exploitation de la mer (Ifremer,France, 18 November 2014); the Federal Institute for Geosciences and Natural Resources of Germany (6 May 2015); and the Government of India (26 September 2016) and the Government of the Republic of Poland (12 February 2018).\n\nThe Authority also holds five contracts for the exploration of cobalt-rich ferromanganese crusts in the Western Pacific Ocean with China Ocean Mineral Resources Research and Development Association (29 April 2014); Japan Oil Gas and Metals National Corporation (JOGMEC, 27 January 2014); Ministry of Natural Resources and Environment of the Russian Federation (10 March 2015), Companhia De Pesquisa de Recursos Minerais (9 November 2015) and the Government of the Republic of Korea (27 March 2018).\n\nThe Authority's main legislative accomplishment to date has been the adoption, in the year 2000, of regulations governing exploration for polymetallic nodules. These resources, also called manganese nodules, contain varying amounts of manganese, cobalt, copper and nickel. They occur as potato-sized lumps scattered about on the surface of the ocean floor, mainly in the central Pacific Ocean but with some deposits in the Indian Ocean.\n\nThe Council of the Authority began work, in August 2002, on another set of regulations, covering polymetallic sulfides and cobalt-rich ferromanganese crusts, which are rich sources of such minerals as copper, iron, zinc, silver and gold, as well as cobalt. The sulphides are found around volcanic hot springs, especially in the western Pacific Ocean, while the crusts occur on oceanic ridges and elsewhere at several locations around the world. The Council decided in 2006 to prepare separate sets of regulations for sulphides and for crusts, with priority given to sulphides. It devoted most of its sessions in 2007 and 2008 to this task, but several issues remained unresolved. Chief among these were the definition and configuration of the area to be allocated to contractors for exploration, the fees to be paid to the Authority and the question of how to deal with any overlapping claims that might arise. Meanwhile, the Legal and Technical Commission reported progress on ferromanganese crusts.\n\nIn addition to its legislative work, the Authority organizes annual workshops on various aspects of seabed exploration, with emphasis on measures to protect the marine environment from any harmful consequences. It disseminates the results of these meetings through publications. Studies over several years covering the key mineral area of the Central Pacific resulted in a technical study on biodiversity, species ranges and gene flow in the abyssal Pacific nodule province, with emphasis on predicting and managing the impacts of deep seabed mining A workshop at Manoa, Hawaii, in October 2007 produced a rationale and recommendations for the establishment of \"preservation reference areas\" in the Clarion-Clipperton Zone, where nodule mining would be prohibited in order to leave the natural environment intact. The most recent workshop, held at Chennai, India, in February 2008, concerned polymetallic nodule mining technology, with special reference to its current status and challenges ahead\n\nContrary to early hopes that seabed mining would generate extensive revenues for both the exploiting countries and the Authority, no technology has yet been developed for gathering deep-sea minerals at costs that can compete with land-based mines. Until recently, the consensus has been that economic mining of the ocean depths might be decades away. Moreover, the United States, with some of the most advanced ocean technology in the world, has not yet ratified the Law of the Sea Convention and is thus not a member of the Authority.\n\nIn recent years, however, interest in deep-sea mining, especially with regard to ferromanganese crusts and polymetallic sulphides, has picked up among several firms now operating in waters within the national zones of Papua New Guinea, Fiji and Tonga. Papua New Guinea was the first country in the world to grant commercial exploration licenses for seafloor massive sulphide deposits when it granted the initial license to Nautilus Minerals in 1997. Japan's new ocean policy emphasizes the need to develop methane hydrate and hydrothermal deposits within Japan's exclusive economic zone and calls for the commercialization of these resources within the next 10 years. Reporting on these developments in his annual report to the Authority in April 2008, Secretary-General Nandan referred also to the upward trend in demand and prices for cobalt, copper, nickel and manganese, the main metals that would be derived from seabed mining, and he noted that technologies being developed for offshore extraction could be adapted for deep sea mining.\n\nIn its preamble, UNCLOS defines the international seabed area—the part under ISA jurisdiction—as \"the seabed and ocean floor and the subsoil thereof, beyond the limits of national jurisdiction\". There are no maps annexed to the Convention to delineate this area. Rather, UNCLOS outlines the areas of national jurisdiction, leaving the rest for the international portion. National jurisdiction over the seabed normally leaves off at seaward from baselines running along the shore, unless a nation can demonstrate that its continental shelf is naturally prolonged beyond that limit, in which case it may claim up to . ISA has no role in determining this boundary. Rather, this task is left to another body established by UNCLOS, the Commission on the Limits of the Continental Shelf, which examines scientific data submitted by coastal states that claim a broader reach. Maritime boundaries between states are generally decided by bilateral negotiation (sometimes with the aid of judicial bodies), not by ISA.\n\nRecently, there has been much interest in the possibility of exploiting seabed resources in the Arctic Ocean, bordered by Canada, Denmark, Iceland, Norway, Russia and the United States (see Territorial claims in the Arctic). Mineral exploration and exploitation activities in any seabed area not belonging to these states would fall under ISA jurisdiction.\n\nIn 2006 the Authority established an Endowment Fund to Support Collaborative Marine Scientific Research on the International Seabed Area. The Fund will aid experienced scientists and technicians from developing countries to participate in deep-sea research organized by international and national institutions. A campaign was launched in February 2008 to identify participants, establish a network of cooperating bodies and seek outside funds to augment the initial $3 million endowment from the Authority.\n\nThe International Seabed Authority Endowment Fund promotes and encourages the conduct of collaborative marine scientific research in the international seabed area through two main activities:\nBULLET::::- \"By supporting the participation of qualified scientists and technical personnel from developing countries in marine scientific research programmes and activities.\"\nBULLET::::- \"By providing opportunities to these scientists to participate in relevant initiatives.\"\n\nThe Secretariat of the International Seabed Authority is facilitating these activities by creating and maintaining an ongoing list of opportunities for scientific collaboration, including research cruises, deep-sea sample analysis, and training and internship programmes. This entails building a network of co-operating groups interested in (or presently undertaking) these types of activities and programmes, such as universities, institutions, contractors with the Authority and other entities.\n\nThe Secretariat is also actively seeking applications from scientists and other technical personnel from developing nations to be considered for assistance under the Fund. Application guidelines have been prepared for potential recipients to participate in marine scientific research programmes or other scientific co-operation activity, to enroll in training programmes, and to qualify for technical assistance. An advisory panel will evaluate all incoming applications and make recommendations to the Secretary-General of the International Seabed Authority so successful applicants may be awarded with Fund assistance.\n\nTo maximize opportunities for and participation in the Fund, the Secretariat is also seeking donations and in-kind contributions to build on the initial investment of US$3 million. This entails raising awareness of the Fund, reporting on its successes and encouraging new activities and participants.\n\nIn 2017, the Authority registered seven voluntary commitments with the UN Oceans Conference for Sustainable Development Goal 14. These were:\nBULLET::::1. OceanAction15467 – Enhancing the role of women in marine scientific research through capacity building\nBULLET::::2. OceanAction15796 – Encouraging dissemination of research results through the ISA Secretary-General Award for Excellence in Deep-Sea Research\nBULLET::::3. OceanAction16538 – Abyssal Initiative for Blue Growth (with UN-DESA)\nBULLET::::4. OceanAction16494 – Fostering cooperation to promote the sustainable development of Africa's deep seabed resources in support of Africas Blue Economy\nBULLET::::5. OceanAction17746 – Enhancing the assessment of essential ecological functions of the deep sea oceans through long-term underwater oceanographic observatories in the Area;\nBULLET::::6. OceanAction17776 – Enhancing deep sea marine biodiversity assessment through the creation of online taxonomic atlases linked to deep sea mining activities in the Area\n\nThe exact nature of the ISA's mission and authority has been questioned by opponents of the Law of the Sea Treaty who are generally skeptical of multilateral engagement by the United States. The United States is the only major maritime power that has not ratified the Convention (see United States non-ratification of the UNCLOS), with one of the main anti-ratification arguments being a charge that the ISA is flawed or unnecessary. In its original form, the Convention included certain provisions that some found objectionable, such as:\nBULLET::::- Imposition of permit requirements, fees and taxation on seabed mining; ban on mining absent ISA permission\nBULLET::::- Use of collected money for wealth redistribution in addition to ISA administration\nBULLET::::- Mandatory technology transfer\n\nBecause of these concerns, the United States pushed for modification of the Convention, obtaining a 1994 Agreement on Implementation that somewhat mitigates them and thus modifies the ISA's authority. Despite this change the United States has not ratified the Convention and so is not a member of ISA, although it sends sizable delegations to participate in meetings as an observer.\n\nBULLET::::- International waters\nBULLET::::- Seabed Arms Control Treaty\nBULLET::::- United Nations Trusteeship Council\n\nBULLET::::- International Seabed Authority\nBULLET::::- Overview – Convention & Related Agreements. UN: United Nations Convention on the Law of the Sea (1982).\n"}
{"id": "15029", "url": "https://en.wikipedia.org/wiki?curid=15029", "title": "Industry Standard Architecture", "text": "Industry Standard Architecture\n\nIndustry Standard Architecture (ISA) is the 16-bit internal bus of IBM PC/AT and similar computers based on the Intel 80286 and its immediate successors during the 1980s. The bus was (largely) backward compatible with the 8-bit bus of the 8088-based IBM PC, including the IBM PC/XT as well as IBM PC compatibles.\n\nOriginally referred to as the PC/AT-bus, it was also termed \"I/O Channel\" by IBM. The ISA term was coined as a retronym by competing PC-clone manufacturers in the late 1980s or early 1990s as a reaction to IBM attempts to replace the AT-bus with its new and incompatible Micro Channel architecture.\n\nThe 16-bit ISA bus was also used with 32-bit processors for several years. An attempt to extend it to 32 bits, called Extended Industry Standard Architecture (EISA), was not very successful, however. Later buses such as VESA Local Bus and PCI were used instead, often along with ISA slots on the same mainboard. Derivatives of the AT bus structure were and still are used in ATA/IDE, the PCMCIA standard, Compact Flash, the PC/104 bus, and internally within Super I/O chips.\n\nThe ISA bus was developed by a team led by Mark Dean at IBM as part of the IBM PC project in 1981 Compaq created the term \"Industry Standard Architecture\" (ISA) to replace \"PC compatible\". ISA originated as an 8-bit system. A 16-bit version, the IBM AT bus, was introduced with the release of the IBM PC/AT in 1984. In 1988, the 32-bit Extended Industry Standard Architecture (EISA) standard was proposed by the \"Gang of Nine\" group of PC-compatible manufacturers that included Compaq. In the process, they retroactively renamed the AT bus to \"ISA\" to avoid infringing IBM's trademark on its PC/AT computer.\n\nIBM designed the 8-bit version as a buffered interface to the motherboard buses of the Intel 8088 (16/8 bit) CPU in the IBM PC and PC/XT. The 16-bit version was an upgrade for the motherboard buses of the Intel 80286 CPU used in the IBM AT. The ISA bus was therefore synchronous with the CPU clock, until sophisticated buffering methods were implemented by chipsets to interface ISA to much faster CPUs.\n\nISA was designed to connect peripheral cards to the motherboard and allows for bus mastering. Only the first 16 MB of main memory is addressable. The original 8-bit bus ran from the 4.77 MHz clock of the 8088 CPU in the IBM PC and PC/XT. The original 16-bit bus ran from the CPU clock of the 80286 in IBM PC/AT computers, which was 6 MHz in the first models and 8 MHz in later models. The IBM RT PC also used the 16-bit bus. ISA was also used in some non-IBM compatible machines such as Motorola 68k-based Apollo (68020) and Amiga 3000 (68030) workstations, the short-lived AT&T Hobbit and the later PowerPC-based BeBox.\n\nCompanies like Dell improved the AT bus's performance but in 1987, IBM replaced the AT bus with its proprietary Micro Channel Architecture (MCA). MCA overcame many of the limitations then apparent in ISA but was also an effort by IBM to regain control of the PC architecture and the PC market. MCA was far more advanced than ISA and had many features that would later appear in PCI. However, MCA was also a closed standard whereas IBM had released full specifications and circuit schematics for ISA. Computer manufacturers responded to MCA by developing the Extended Industry Standard Architecture (EISA) and the later VESA Local Bus (VLB). VLB used some electronic parts originally intended for MCA because component manufacturers already were equipped to manufacture them. Both EISA and VLB were backwards-compatible expansions of the AT (ISA) bus.\n\nUsers of ISA-based machines had to know special information about the hardware they were adding to the system. While a handful of devices were essentially \"plug-n-play\", this was rare. Users frequently had to configure parameters when adding a new device, such as the IRQ line, I/O address, or DMA channel. MCA had done away with this complication and PCI actually incorporated many of the ideas first explored with MCA, though it was more directly descended from EISA.\n\nThis trouble with configuration eventually led to the creation of ISA PnP, a plug-n-play system that used a combination of modifications to hardware, the system BIOS, and operating system software to automatically manage resource allocations. In reality, ISA PnP could be troublesome and did not become well-supported until the architecture was in its final days.\n\nPCI slots were the first physically-incompatible expansion ports to directly squeeze ISA off the motherboard. At first, motherboards were largely ISA, including a few PCI slots. By the mid-1990s, the two slot types were roughly balanced, and ISA slots soon were in the minority of consumer systems. Microsoft's PC 99 specification recommended that ISA slots be removed entirely, though the system architecture still required ISA to be present in some vestigial way internally to handle the floppy drive, serial ports, etc., which was why the software compatible LPC bus was created. ISA slots remained for a few more years, and towards the turn of the century it was common to see systems with an Accelerated Graphics Port (AGP) sitting near the central processing unit, an array of PCI slots, and one or two ISA slots near the end. In late 2008, even floppy disk drives and serial ports were disappearing, and the extinction of vestigial ISA (by then the LPC bus) from chipsets was on the horizon.\n\nPCI slots are \"rotated\" compared to their ISA counterparts—PCI cards were essentially inserted \"upside-down,\" allowing ISA and PCI connectors to squeeze together on the motherboard. Only one of the two connectors can be used in each slot at a time, but this allowed for greater flexibility.\n\nThe AT Attachment (ATA) hard disk interface is directly descended from the 16-bit ISA of the PC/AT. ATA has its origins in hardcards that integrated a hard disk drive (HDD) and a hard disk controller (HDC) onto one card. This was at best awkward and at worst damaging to the motherboard, as ISA slots were not designed to support such heavy devices as HDDs. The next generation of Integrated Drive Electronics drives moved both the drive and controller to a drive bay and used a ribbon cable and a very simple interface board to connect it to an ISA slot. ATA is basically a standardization of this arrangement plus a uniform command structure for software to interface with the HDC within the drive. ATA has since been separated from the ISA bus and connected directly to the local bus, usually by integration into the chipset, for much higher clock rates and data throughput than ISA could support. ATA has clear characteristics of 16-bit ISA, such as a 16-bit transfer size, signal timing in the PIO modes and the interrupt and DMA mechanisms.\n\n style=\"vertical-align:top; padding:0; border-spacing:0; border-collapse: collapse;\"  <br>\n\nThe PC/XT-bus is an eight-bit ISA bus used by Intel 8086 and Intel 8088 systems in the IBM PC and IBM PC XT in the 1980s. Among its 62 pins were demultiplexed and electrically buffered versions of the 8 data and 20 address lines of the 8088 processor, along with power lines, clocks, read/write strobes, interrupt lines, etc. Power lines included −5 V and ±12 V in order to directly support pMOS and enhancement mode nMOS circuits such as dynamic RAMs among other things. The XT bus architecture uses a single Intel 8259 PIC, giving eight vectorized and prioritized interrupt lines. It has four DMA channels originally provided by the Intel 8237, 3 of the DMA channels are brought out to the XT bus expansion slots; of these, 2 are normally already allocated to machine functions (diskette drive and hard disk controller):\n\n! DMA channel !! Expansion !! Standard function\nThe PC/AT-bus, a 16-bit (or 80286-) version of the PC/XT bus, was introduced with the IBM PC/AT. This bus was officially termed \"I/O Channel\" by IBM. It extends the XT-bus by adding a second shorter edge connector in-line with the eight-bit XT-bus connector, which is unchanged, retaining compatibility with most 8-bit cards. The second connector adds four additional address lines for a total of 24, and 8 additional data lines for a total of 16. It also adds new interrupt lines connected to a second 8259 PIC (connected to one of the lines of the first) and 4 × 16-bit DMA channels, as well as control lines to select 8- or 16-bit transfers.\n\nThe 16-bit AT bus slot originally used two standard edge connector sockets in early IBM PC/AT machines. However, with the popularity of the AT-architecture and the 16-bit ISA bus, manufacturers introduced specialized 98-pin connectors that integrated the two sockets into one unit. These can be found in almost every AT-class PC manufactured after the mid-1980s. The ISA slot connector is typically black (distinguishing it from the brown EISA connectors and white PCI connectors).\n\nMotherboard devices have dedicated IRQs (not present in the slots). 16-bit devices can use either PC-bus or PC/AT-bus IRQs. It is therefore possible to connect up to 6 devices that use one 8-bit IRQ each, or up to 5 devices that use one 16-bit IRQ each. At the same time, up to 4 devices may use one 8-bit DMA channel each, while up to 3 devices can use one 16-bit DMA channel each.\n\nOriginally, the bus clock was synchronous with the CPU clock, resulting in varying bus clock frequencies among the many different IBM \"clones\" on the market (sometimes as high as 16 or 20 MHz), leading to software or electrical timing problems for certain ISA cards at bus speeds they were not designed for. Later motherboards or integrated chipsets used a separate clock generator, or a clock divider which either fixed the ISA bus frequency at 4, 6, or 8 MHz or allowed the user to adjust the frequency via the BIOS setup. When used at a higher bus frequency, some ISA cards (certain Hercules-compatible video cards, for instance), could show significant performance improvements.\n\nMemory address decoding for the selection of 8 or 16-bit transfer mode was limited to 128 KiB sections, leading to problems when mixing 8- and 16-bit cards as they could not co-exist in the same 128 KiB area. This is because the MEMCS16 line is required to be set based on the value of LA17-23 only.\n\nISA is still used today for specialized industrial purposes. In 2008 IEI Technologies released a modern motherboard for Intel Core 2 Duo processors which, in addition to other special I/O features, is equipped with two ISA slots. It is marketed to industrial and military users who have invested in expensive specialized ISA bus adaptors, which are not available in PCI bus versions.\n\nSimilarly, ADEK Industrial Computers is releasing a motherboard in early 2013 for Intel Core i3/i5/i7 processors, which contains one (non-DMA) ISA slot.\n\nThe PC/104 bus, used in industrial and embedded applications, is a derivative of the ISA bus, utilizing the same signal lines with different connectors. The LPC bus has replaced the ISA bus as the connection to the legacy I/O devices on recent motherboards; while physically quite different, LPC looks just like ISA to software, so that the peculiarities of ISA such as the 16 MiB DMA limit (which corresponds to the full address space of the Intel 80286 CPU used in the original IBM AT) are likely to stick around for a while.\n\nAs explained in the \"History\" section, ISA was the basis for development of the ATA interface, used for ATA (a.k.a. IDE) and more recently Serial ATA (SATA) hard disks. Physically, ATA is essentially a simple subset of ISA, with 16 data bits, support for exactly one IRQ and one DMA channel, and 3 address bits. To this ISA subset, ATA adds two IDE address select (\"chip select\") lines and a few unique signal lines specific to ATA/IDE hard disks (such as the Cable Select/Spindle Sync. line.) In addition to the physical interface channel, ATA goes beyond and far outside the scope of ISA by also specifying a set of physical device registers to be implemented on every ATA (IDE) drive and a full set of protocols and device commands for controlling fixed disk drives using these registers. The ATA device registers are accessed using the address bits and address select signals in the ATA physical interface channel, and all operations of ATA hard disks are performed using the ATA-specified protocols through the ATA command set. The earliest versions of the ATA standard featured a few simple protocols and a basic command set comparable to the command sets of MFM and RLL controllers (which preceded ATA controllers), but the latest ATA standards have much more complex protocols and instruction sets that include optional commands and protocols providing such advanced optional-use features as sizable hidden system storage areas, password security locking, and programmable geometry translation.\n\nA further deviation between ISA and ATA is that while the ISA bus remained locked into a single standard clock rate (for backward hardware compatibility), the ATA interface offered many different speed modes, could select among them to match the maximum speed supported by the attached drives, and kept adding faster speeds with later versions of the ATA standard (up to 133 MB/s for ATA-6, the latest.) In most forms, ATA ran much faster than ISA, provided it was connected directly to a local bus faster than the ISA bus.\n\nBefore the 16-bit ATA/IDE interface, there was an 8-bit XT-IDE (also known as XTA) interface for hard disks. It was not nearly as popular as ATA has become, and XT-IDE hardware is now fairly hard to find. Some XT-IDE adapters were available as 8-bit ISA cards, and XTA sockets were also present on the motherboards of Amstrad's later XT clones as well as a short-lived line of Philips units. The XTA pinout was very similar to ATA, but only eight data lines and two address lines were used, and the physical device registers had completely different meanings. A few hard drives (such as the Seagate ST351A/X) could support either type of interface, selected with a jumper.\n\nMany later AT (and AT successor) motherboards had no integrated hard drive interface but relied on a separate hard drive interface plugged into an ISA/EISA/VLB slot. There were even a few 80486 based units shipped with MFM/RLL interfaces and drives instead of the increasingly common AT-IDE.\n\nCommodore built the XT-IDE based peripheral hard drive / memory expansion unit A590 for their Amiga 500 and 500+ computers that also supported a SCSI drive. AT-IDE type interfaces only entered the keyboard-cased Amiga line upon introduction of the A600 and A1200 which have an integrated interface and 44 pin connector. Many owners removed the 2,5 inch bracket and installed a 3,5 inch drive with an adapter cable.\n\nThe PCMCIA specification can be seen as a superset of ATA. The standard for PCMCIA hard disk interfaces, which included PCMCIA flash drives, allows for the mutual configuration of the port and the drive in an ATA mode. As a de facto extension, most PCMCIA flash drives additionally allow for a simple ATA mode that is enabled by pulling a single pin low, so that PCMCIA hardware and firmware are unnecessary to use them as an ATA drive connected to an ATA port. PCMCIA flash drive to ATA adapters are thus simple and inexpensive, but are not guaranteed to work with any and every standard PCMCIA flash drive. Further, such adapters cannot be used as generic PCMCIA ports, as the PCMCIA interface is much more complex than ATA.\n\nAlthough most modern computers do not have physical ISA buses, all IBM compatible computers — x86, and x86-64 (most non-mainframe, non-embedded) — have ISA buses allocated in physical address space. Embedded controller chips (southbridge) and CPUs themselves provide services such as temperature monitoring and voltage readings through these buses as ISA devices.\n\nIEEE started a standardization of the ISA bus in 1985, called the P996 specification. However, despite there even having been books published on the P996 specification, it never officially progressed past draft status.\nThere still is an existing user base with old computers, so some ISA cards are still manufactured, e.g. with USB ports or complete single board computers based on modern processors, USB 3.0, and SATA.\n\nBULLET::::- PC/104 - Embedded variant of ISA\nBULLET::::- Low Pin Count (LPC) - Low pin count version of ISA\nBULLET::::- Extended Industry Standard Architecture (EISA)\nBULLET::::- Micro Channel architecture (MCA)\nBULLET::::- VESA Local Bus (VLB)\nBULLET::::- Accelerated Graphics Port (AGP)\nBULLET::::- PCI-X\nBULLET::::- Peripheral Component Interconnect (PCI)\nBULLET::::- PCI Express (PCI-E or PCIe)\nBULLET::::- List of computer bus interfaces\nBULLET::::- Amiga Zorro II\nBULLET::::- NuBus\nBULLET::::- Switched fabric\nBULLET::::- List of device bandwidths\nBULLET::::- CompactPCI\nBULLET::::- PC card\nBULLET::::- Universal Serial Bus\nBULLET::::- Legacy port\nBULLET::::- Backplane\n\nBULLET::::- \"Intel ISA Bus Specification and Application Notes - Rev 2.01\"; Intel; 73 pages; 1989.\n\n"}
{"id": "15030", "url": "https://en.wikipedia.org/wiki?curid=15030", "title": "Intergovernmental Panel on Climate Change", "text": "Intergovernmental Panel on Climate Change\n\nThe Intergovernmental Panel on Climate Change (IPCC) is an intergovernmental body of the United Nations that is dedicated to providing the world with objective, scientific information relevant to understanding the scientific basis of the\nrisk of human-inducedclimate change, its natural, political, and economic impacts and risks, and possible response options.\n\nThe IPCC was established in 1988 by the World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP) and was later endorsed by the United Nations General Assembly. Membership is open to all members of the WMO and UN.\nThe IPCC produces reports that contribute to the work of the United Nations Framework Convention on Climate Change (UNFCCC), the main international treaty on climate change. The objective of the UNFCCC is to \"stabilize greenhouse gas concentrations in the atmosphere at a level that would prevent dangerous anthropogenic (human-induced) interference with the climate system\". The IPCC's Fifth Assessment Report was a critical scientific input into the UNFCCC's Paris Agreement in 2015.\n\nIPCC reports cover the \"scientific, technical and socio-economic information relevant to understanding the scientific basis of risk of human-induced climate change, its potential impacts and options for adaptation and mitigation.\" The IPCC does not carry out original research, nor does it monitor climate or related phenomena itself. Rather, it assesses published literature, including peer-reviewed and non-peer-reviewed sources. However, the IPCC can be said to stimulate research in climate science. Chapters of IPCC reports often close with sections on limitations and knowledge or research gaps, and the announcement of an IPCC special report can catalyse research activity in that area.\n\nThousands of scientists and other experts contribute on a voluntary basis to writing and reviewing reports, which are then reviewed by governments. IPCC reports contain a \"Summary for Policymakers\", which is subject to line-by-line approval by delegates from all participating governments. Typically, this involves the governments of more than 120 countries.\n\nThe IPCC provides an internationally accepted authority on climate change, producing reports that have the agreement of leading climate scientists and consensus from participating governments. The 2007 Nobel Peace Prize was shared between the IPCC and Al Gore.\n\nFollowing the election of a new Bureau in 2015, the IPCC embarked on its sixth assessment cycle. Besides the Sixth Assessment Report, to be completed in 2022, the IPCC released the Special Report on Global Warming of 1.5 °C in October 2018, released an update to its 2006 Guidelines for National Greenhouse Gas Inventories—the 2019 Refinement—in May 2019, and delivered two further special reports in 2019: the Special Report on Climate Change and Land (SRCCL), published online on 7 August, and the Special Report on the Ocean and Cryosphere in a Changing Climate (SROCC), released on 25 September 2019. This makes the sixth assessment cycle the most ambitious in the IPCC's 30-year history. The IPCC also decided to prepare a special report on cities and climate change in the seventh assessment cycle and held a conference in March 2018 to stimulate research in this area.\n\nThe IPCC developed from an international scientific body, the Advisory Group on Greenhouse Gases set up in 1985 by the International Council of Scientific Unions, the United Nations Environment Programme (UNEP), and the World Meteorological Organization (WMO) to provide recommendations based on current research. This small group of scientists lacked the resources to cover the increasingly complex interdisciplinary nature of climate science. The United States Environmental Protection Agency and State Department wanted an international convention to agree restrictions on greenhouse gases, and the conservative Reagan Administration was concerned about unrestrained influence from independent scientists or from United Nations bodies including UNEP and the WMO. The U.S. government was the main force in forming the IPCC as an autonomous intergovernmental body in which scientists took part both as experts on the science and as official representatives of their governments, to produce reports which had the firm backing of all the leading scientists worldwide researching the topic, and which then had to gain consensus agreement from every one of the participating governments. In this way, it was formed as a hybrid between a scientific body and an intergovernmental political organisation.\n\nThe United Nations formally edorsed the creation of the IPCC in 1988. Some of the reasons the UN stated in its resolution include\n\nThe IPCC was tasked with reviewing peer-reviewed scientific literature and other relevant publications to provide information on the state of knowledge about climate change.\n\nThe IPCC does not conduct its own original research. It produces comprehensive assessments, reports on special topics, and methodologies. The assessments build on previous reports, highlighting the latest knowledge. For example, the wording of the reports from the first to the fifth assessment reflects the growing evidence for a changing climate caused by human activity.\n\nThe IPCC has adopted and published \"Principles Governing IPCC Work\", which states that the IPCC will assess:\nBULLET::::- the risk of human-induced climate change,\nBULLET::::- its potential impacts, and\nBULLET::::- possible options for prevention.\nThis document also states that IPCC will do this work by assessing \"on a comprehensive, objective, open and transparent basis the scientific, technical and socio-economic information relevant to understanding the scientific basis\" of these topics. The Principles also state that \"IPCC reports should be neutral with respect to policy, although they may need to deal objectively with scientific, technical and socio-economic factors relevant to the application of particular policies.\"\n\nKorean economist Hoesung Lee has been the chair of the IPCC since 8 October 2015, with the election of the new IPCC Bureau. \nBefore this election, the IPCC was led by Vice-Chair Ismail El Gizouli, who was designated acting Chair after the resignation of Rajendra K. Pachauri in February 2015. The previous chairs were Rajendra K. Pachauri, elected in May 2002; Robert Watson in 1997; and Bert Bolin in 1988. The chair is assisted by an elected bureau including vice-chairs and working group co-chairs, and by a secretariat.\n\nThe Panel itself is composed of representatives appointed by governments. Participation of delegates with appropriate expertise is encouraged. Plenary sessions of the IPCC and IPCC Working Groups are held at the level of government representatives. Non-Governmental and Intergovernmental Organizations admitted as observer organizations may also attend. Sessions of the Panel, IPCC Bureau, workshops, expert and lead authors meetings are by invitation only. About 500 people from 130 countries attended the 48th Session of the Panel in Incheon, Republic of Korea, in October 2018, including 290 government officials and 60 representatives of observer organizations. The opening ceremonies of sessions of the Panel and of Lead Author Meetings are open to media, but otherwise IPCC meetings are closed.\n\nThere are several major groups:\nBULLET::::- IPCC Panel: Meets in plenary session about once a year. It controls the organization's structure, procedures, and work programme, and accepts and approves IPCC reports. The Panel is the IPCC corporate entity.\nBULLET::::- Chair: Elected by the Panel.\nBULLET::::- Secretariat: Oversees and manages all activities. Supported by UNEP and WMO.\nBULLET::::- Bureau: Elected by the Panel. Chaired by the Chair. 34 members include IPCC Vice-Chairs, Co-Chairs of Working Groups and the Task Force, and Vice-Chairs of the Working Groups. It provides guidance to the Panel on the scientific and technical aspects of its work.\nBULLET::::- Working Groups: Each has two Co-Chairs, one from the developed and one from developing world, and a technical support unit. Sessions of the Working Group approve the Summary for Policymakers of special reports and working group contributions to an assessment report. Each Working Group has a Bureau comprising its Co-Chairs and Vice-Chairs, who are also members of the IPCC Bureau.\nBULLET::::- Working Group I: Assesses scientific aspects of the climate system and climate change. Co-Chairs: Valérie Masson-Delmotte and Panmao Zhai\nBULLET::::- Working Group II: Assesses vulnerability of socio-economic and natural systems to climate change, consequences, and adaptation options. Co-Chairs: Hans-Otto Pörtner and Debra Roberts\nBULLET::::- Working Group III: Assesses options for limiting greenhouse gas emissions and otherwise mitigating climate change. Co-Chairs: Priyadarshi R. Shukla and Jim Skea\nBULLET::::- Task Force on National Greenhouse Gas Inventories. Co-Chairs: Kiyoto Tanabe and Eduardo Calvo Buendía\nBULLET::::- Task Force Bureau: Comprises the two Co-Chairs, who are also members of the IPCC Bureau, and 12 members.\nBULLET::::- Executive Committee: Comprises the Chair, IPCC Vice-Chairs and the Co-Chairs of the Working Groups and Task Force. Its role includes addressing urgent issues that arise between sessions of the Panel.\n\nThe IPCC receives funding through the IPCC Trust Fund, established in 1989 by the United Nations Environment Programme (UNEP) and the World Meteorological Organization (WMO), Costs of the Secretary and of housing the secretariat are provided by the WMO, while UNEP meets the cost of the Depute Secretary. Annual cash contributions to the Trust Fund are made by the WMO, by UNEP, and by IPCC Members. Payments and their size are voluntary. The Panel is responsible for considering and adopting by consensus the annual budget. The organization is required to comply with the Financial Regulations and Rules of the WMO.\n\nThe IPCC has published five comprehensive assessment reports reviewing the latest climate science, as well as a number of special reports on particular topics. These reports are prepared by teams of relevant researchers selected by the Bureau from government nominations. Expert reviewers from a wide range of governments, IPCC observer organizations and other organizations are invited at different\nstages to comment on various aspects of the drafts.\n\nThe IPCC published its First Assessment Report (FAR) in 1990, a supplementary report in 1992, a Second Assessment Report (SAR) in 1995, a Third Assessment Report (TAR) in 2001, a Fourth Assessment Report (AR4) in 2007 and a Fifth Assessment Report (AR5) in 2014. The IPCC is currently preparing the Sixth Assessment Report (AR6), which will be completed in 2022.\n\nEach assessment report is in three volumes, corresponding to Working Groups I, II, and III. It is completed by a synthesis report that integrates the working group contributions and any special reports produced in that assessment cycle.\n\nThe IPCC does not carry out research nor does it monitor climate related data. Lead authors of IPCC reports assess the available information about climate change based on published sources. According to IPCC guidelines, authors should give priority to peer-reviewed sources. Authors may refer to non-peer-reviewed sources (the \"grey literature\"), provided that they are of sufficient quality. Examples of non-peer-reviewed sources include model results, reports from government agencies and non-governmental organizations, and industry journals. Each subsequent IPCC report notes areas where the science has improved since the previous report and also notes areas where further research is required.\n\nThere are generally three stages in the review process:\nBULLET::::- Expert review (6–8 weeks)\nBULLET::::- Government/expert review\nBULLET::::- Government review of:\nBULLET::::- Summaries for Policymakers\nBULLET::::- Overview Chapters\nBULLET::::- Synthesis Report\nReview comments are in an open archive for at least five years.\n\nThere are several types of endorsement which documents receive:\nBULLET::::- Approval. Material has been subjected to detailed, line by line discussion and agreement.\nBULLET::::- Working Group Summaries for Policymakers are \"approved\" by their Working Groups.\nBULLET::::- Synthesis Report Summary for Policymakers is \"approved\" by Panel.\nBULLET::::- Adoption. Endorsed section by section (and not line by line).\nBULLET::::- Panel \"adopts\" Overview Chapters of Methodology Reports.\nBULLET::::- Panel \"adopts\" IPCC Synthesis Report.\nBULLET::::- Acceptance. Not been subject to line by line discussion and agreement, but presents a comprehensive, objective, and balanced view of the subject matter.\nBULLET::::- Working Groups \"accept\" their reports.\nBULLET::::- Task Force Reports are \"accepted\" by the Panel.\nBULLET::::- Working Group Summaries for Policymakers are \"accepted\" by the Panel after group \"approval\".\n\nThe Panel is responsible for the IPCC and its endorsement of Reports allows it to ensure they meet IPCC standards.\n\nThere have been a range of commentaries on the IPCC's procedures, examples of which are discussed later in the article (see also IPCC Summary for Policymakers). Some of these comments have been supportive, while others have been critical. Some commentators have suggested changes to the IPCC's procedures.\n\nEach chapter has a number of authors who are responsible for writing and editing the material. A chapter typically has two \"coordinating lead authors\", ten to fifteen \"lead authors\", and a somewhat larger number of \"contributing authors\". The coordinating lead authors are responsible for assembling the contributions of the other authors, ensuring that they meet stylistic and formatting requirements, and reporting to the Working Group chairs. Lead authors are responsible for writing sections of chapters. Contributing authors prepare text, graphs or data for inclusion by the lead authors.\n\nAuthors for the IPCC reports are chosen from a list of researchers prepared by governments and participating organisations, and by the Working Group/Task Force Bureaux, as well as other experts known through their published work. The choice of authors aims for a range of views, expertise and geographical representation, ensuring representation of experts from developing and developed countries and countries with economies in transition.\n\nThe IPCC First Assessment Report (FAR) was completed in 1990, and served as the basis of the UNFCCC.\n\nThe executive summary of the WG I Summary for Policymakers report says they are certain that emissions resulting from human activities are substantially increasing the atmospheric concentrations of the greenhouse gases, resulting on average in an additional warming of the Earth's surface. They calculate with confidence that CO has been responsible for over half the enhanced greenhouse effect. They predict that under a \"business as usual\" (BAU) scenario, global mean temperature will increase by about 0.3 °C per decade during the [21st] century. They judge that global mean surface air temperature has increased by 0.3 to 0.6 °C over the last 100 years, broadly consistent with prediction of climate models, but also of the same magnitude as natural climate variability. The unequivocal detection of the enhanced greenhouse effect is not likely for a decade or more.\n\nThe 1992 supplementary report was an update, requested in the context of the negotiations on the UNFCCC at the Earth Summit (United Nations Conference on Environment and Development) in Rio de Janeiro in 1992.\n\nThe major conclusion was that research since 1990 did \"not affect our fundamental understanding of the science of the greenhouse effect and either confirm or do not justify alteration of the major conclusions of the first IPCC scientific assessment\". It noted that transient (time-dependent) simulations, which had been very preliminary in the FAR, were now improved, but did not include aerosol or ozone changes.\n\n\"Climate Change 1995\", the IPCC Second Assessment Report (SAR), was finished in 1996. It is split into four parts:\nBULLET::::- A synthesis to help interpret UNFCCC article 2.\nBULLET::::- \"The Science of Climate Change\" (WG I)\nBULLET::::- \"Impacts, Adaptations and Mitigation of Climate Change\" (WG II)\nBULLET::::- \"Economic and Social Dimensions of Climate Change\" (WG III)\n\nEach of the last three parts was completed by a separate Working Group (WG), and each has a Summary for Policymakers (SPM) that represents a consensus of national representatives. The SPM of the WG I report contains headings:\n\nBULLET::::1. Greenhouse gas concentrations have continued to increase\nBULLET::::2. Anthropogenic aerosols tend to produce negative radiative forcings\nBULLET::::3. Climate has changed over the past century (air temperature has increased by between 0.3 and 0.6 °C since the late 19th century; this estimate has not significantly changed since the 1990 report).\nBULLET::::4. The balance of evidence suggests a discernible human influence on global climate (considerable progress since the 1990 report in distinguishing between natural and anthropogenic influences on climate, because of: including aerosols; coupled models; pattern-based studies)\nBULLET::::5. Climate is expected to continue to change in the future (increasing realism of simulations increases confidence; important uncertainties remain but are taken into account in the range of model projections)\nBULLET::::6. There are still many uncertainties (estimates of future emissions and biogeochemical cycling; models; instrument data for model testing, assessment of variability, and detection studies)\n\nThe Third Assessment Report (TAR) was completed in 2001 and consists of four reports, three of them from its Working Groups:\nBULLET::::- Working Group I: The Scientific Basis\nBULLET::::- Working Group II: Impacts, Adaptation and Vulnerability\nBULLET::::- Working Group III: Mitigation\nBULLET::::- Synthesis Report\n\nA number of the TAR's conclusions are given quantitative estimates of how probable it is that they are correct, e.g., greater than 66% probability of being correct. These are \"Bayesian\" probabilities, which are based on an expert assessment of all the available evidence.\n\n\"Robust findings\" of the TAR Synthesis Report include:\nBULLET::::- \"Observations show Earth's surface is warming. Globally, 1990s very likely warmest decade in instrumental record\". Atmospheric concentrations of anthropogenic (i.e., human-emitted) greenhouse gases have increased substantially.\nBULLET::::- Since the mid-20th century, most of the observed warming is \"likely\" (greater than 66% probability, based on expert judgement) due to human activities.\nBULLET::::- Projections based on the \"Special Report on Emissions Scenarios\" suggest warming over the 21st century at a more rapid rate than that experienced for at least the last 10,000 years.\nBULLET::::- \"Projected climate change will have beneficial and adverse effects on both environmental and socio-economic systems, but the larger the changes and the rate of change in climate, the more the adverse effects predominate.\"\nBULLET::::- \"Ecosystems and species are vulnerable to climate change and other stresses (as illustrated by observed impacts of recent regional temperature changes) and some will be irreversibly damaged or lost.\"\nBULLET::::- \"Greenhouse gas emission reduction (mitigation) actions would lessen the pressures on natural and human systems from climate change.\"\nBULLET::::- \"Adaptation [to the effects of climate change] has the potential to reduce adverse effects of climate change and can often produce immediate ancillary benefits, but will not prevent all damages.\" An example of adaptation to climate change is building levees in response to sea level rise.\n\nIn 2001, 16 national science academies issued a joint statement on climate change.\nThe joint statement was made by the Australian Academy of Science, the Royal Flemish Academy of Belgium for Science and the Arts, the Brazilian Academy of Sciences, the Royal Society of Canada, the Caribbean Academy of Sciences, the Chinese Academy of Sciences, the French Academy of Sciences, the German Academy of Natural Scientists Leopoldina, the Indian National Science Academy, the Indonesian Academy of Sciences, the Royal Irish Academy, Accademia Nazionale dei Lincei (Italy), the Academy of Sciences Malaysia, the Academy Council of the Royal Society of New Zealand, the Royal Swedish Academy of Sciences, and the Royal Society (UK).\nThe statement, also published as an editorial in the journal Science, stated \"we support the [TAR's] conclusion that it is at least 90% certain that temperatures will continue to rise, with average global surface temperature projected to increase by between 1.4 and 5.8 °C above 1990 levels by 2100\".\nThe TAR has also been endorsed by the Canadian Foundation for Climate and Atmospheric Sciences, Canadian Meteorological and Oceanographic Society, and European Geosciences Union (refer to \"Endorsements of the IPCC\").\n\nIn 2001, the US National Research Council (US NRC) produced a report that assessed Working Group I's (WGI) contribution to the TAR. US NRC (2001) \"generally agrees\" with the WGI assessment, and describes the full WGI report as an \"admirable summary of research activities in climate science\".\n\nIPCC author Richard Lindzen has made a number of criticisms of the TAR. Among his criticisms, Lindzen has stated that the WGI Summary for Policymakers (SPM) does not faithfully summarize the full WGI report. For example, Lindzen states that the SPM understates the uncertainty associated with climate models. John Houghton, who was a co-chair of TAR WGI, has responded to Lindzen's criticisms of the SPM. Houghton has stressed that the SPM is agreed upon by delegates from many of the world's governments, and that any changes to the SPM must be supported by scientific evidence.\n\nIPCC author Kevin Trenberth has also commented on the WGI SPM. Trenberth has stated that during the drafting of the WGI SPM, some government delegations attempted to \"blunt, and perhaps obfuscate, the messages in the report\". However, Trenberth concludes that the SPM is a \"reasonably balanced summary\".\n\nUS NRC (2001) concluded that the WGI SPM and Technical Summary are \"consistent\" with the full WGI report. US NRC (2001) stated:\n[...] the full [WGI] report is adequately summarized in the Technical Summary. The full WGI report and its Technical Summary are not specifically directed at policy. The Summary for Policymakers reflects less emphasis on communicating the basis for uncertainty and a stronger emphasis on areas of major concern associated with human-induced climate change. This change in emphasis appears to be the result of a summary process in which scientists work with policy makers on the document. Written responses from U.S. coordinating and lead scientific authors to the committee indicate, however, that (a) no changes were made without the consent of the convening lead authors (this group represents a fraction of the lead and contributing authors) and (b) most changes that did occur lacked significant impact.\n\nThe Fourth Assessment Report (AR4) was published in 2007. Like previous assessment reports, it consists of four reports:\nBULLET::::- Working Group I: The Physical Science Basis\nBULLET::::- Working Group II: Impacts, Adaptation and Vulnerability\nBULLET::::- Working Group III: Mitigation\nBULLET::::- Synthesis Report\nPeople from over 130 countries contributed to the IPCC Fourth Assessment Report, which took 6 years to produce. Contributors to AR4 included more than 2500 scientific expert reviewers, more than 800 contributing authors, and more than 450 lead authors.\n\n\"Robust findings\" of the Synthesis report include:\nBULLET::::- \"Warming of the climate system is unequivocal, as is now evident from observations of increases in global average air and ocean temperatures, widespread melting of snow and ice and rising global average sea level\".\nBULLET::::- Most of the global average warming over the past 50 years is \"very likely\" (greater than 90% probability, based on expert judgement) due to human activities.\n\nBULLET::::- \"Impacts [of climate change] will very likely increase due to increased frequencies and intensities of some extreme weather events\".\n\nBULLET::::- \"Anthropogenic warming and sea level rise would continue for centuries even if GHG emissions were to be reduced sufficiently for GHG concentrations to stabilise, due to the time scales associated with climate processes and feedbacks\". Stabilization of atmospheric greenhouse gas concentrations is discussed in climate change mitigation.\nBULLET::::- \"Some planned adaptation (of human activities) is occurring now; more extensive adaptation is required to reduce vulnerability to climate change\".\nBULLET::::- \"Unmitigated climate change would, in the long term, be likely to exceed the capacity of natural, managed and human systems to adapt\".\nBULLET::::- \"Many impacts [of climate change] can be reduced, delayed or avoided by mitigation\".\n\nGlobal warming projections from AR4 are shown below. The projections apply to the end of the 21st century (2090–99), relative to temperatures at the end of the 20th century (1980–99). Add 0.7 °C to projections to make them relative to pre-industrial levels instead of 1980–99. (UK Royal Society, 2010, p=10. Descriptions of the greenhouse gas emissions scenarios can be found in Special Report on Emissions Scenarios.\n\n+ AR4 global warming projections\n\n! Emissionsscenario!! Best estimate(°C) !! \"Likely\" range(°C)\n\n\"Likely\" means greater than 66% probability of being correct, based on expert judgement.\n\nSeveral science academies have referred to and/or reiterated some of the conclusions of AR4. These include:\nBULLET::::- Joint-statements made in 2007, 2008 and 2009 by the science academies of Brazil, China, India, Mexico, South Africa and the G8 nations (the \"G8+5\").\nBULLET::::- Publications by the Australian Academy of Science.\nBULLET::::- A joint-statement made in 2007 by the Network of African Science Academies.\nBULLET::::- A statement made in 2010 by the Inter Academy Medical Panel This statement has been signed by 43 scientific academies.\n\nThe Netherlands Environmental Assessment Agency (PBL, \"et al.\", 2009; 2010) has carried out two reviews of AR4. These reviews are generally supportive of AR4's conclusions. PBL (2010) make some recommendations to improve the IPCC process. A literature assessment by the US National Research Council (US NRC, 2010) concludes:Climate change is occurring, is caused largely by human activities, and poses significant risks for—and in many cases is already affecting—a broad range of human and natural systems [\"emphasis in original text\"]. [...] This conclusion is based on a substantial array of scientific evidence, including recent work, and is consistent with the conclusions of recent assessments by the U.S. Global Change Research Program [...], the Intergovernmental Panel on Climate Change’s Fourth Assessment Report [...], and other assessments of the state of scientific knowledge on climate change.\n\nSome errors have been found in the IPCC AR4 Working Group II report. Two errors include the melting of Himalayan glaciers (see later section), and Dutch land area that is below sea level.\n\nThe IPCC's Fifth Assessment Report (AR5) was completed in 2014. AR5 followed the same general format as of AR4, with three Working Group reports and a Synthesis report. The Working Group I report (WG1) was published in September 2013.\n\nConclusions of AR5 are summarized below:\nBULLET::::- Working Group I:\nBULLET::::- \"Warming of the climate system is unequivocal, and since the 1950s, many of the observed changes are unprecedented over decades to millennia\".\nBULLET::::- \"Atmospheric concentrations of carbon dioxide, methane, and nitrous oxide have increased to levels unprecedented in at least the last 800,000 years\".\nBULLET::::- Human influence on the climate system is clear. It is extremely likely (95-100% probability) that human influence was the dominant cause of global warming between 1951-2010.\nBULLET::::- Working Group II:\nBULLET::::- \"Increasing magnitudes of [global] warming increase the likelihood of severe, pervasive, and irreversible impacts\"\nBULLET::::- \"A first step towards adaptation to future climate change is reducing vulnerability and exposure to present climate variability\"\nBULLET::::- \"The overall risks of climate change impacts can be reduced by limiting the rate and magnitude of climate change\"\nBULLET::::- Working Group III:\nBULLET::::- Without new policies to mitigate climate change, projections suggest an increase in global mean temperature in 2100 of 3.7 to 4.8 °C, relative to pre-industrial levels (median values; the range is 2.5 to 7.8 °C including climate uncertainty).\nBULLET::::- The current trajectory of global greenhouse gas emissions is not consistent with limiting global warming to below 1.5 or 2 °C, relative to pre-industrial levels. Pledges made as part of the Cancún Agreements are broadly consistent with cost-effective scenarios that give a \"likely\" chance (66-100% probability) of limiting global warming (in 2100) to below 3 °C, relative to pre-industrial levels.\n\nProjections in AR5 are based on \"Representative Concentration Pathways\" (RCPs). The RCPs are consistent with a wide range of possible changes in future anthropogenic greenhouse gas emissions. Projected changes in global mean surface temperature and sea level are given in the main RCP article.\n\nIn addition to climate assessment reports, the IPCC publishes Special Reports on specific topics. The preparation and approval process for all IPCC Special Reports follows the same procedures as for IPCC Assessment Reports. In the year 2011 two IPCC Special Report were finalized, the Special Report on Renewable Energy Sources and Climate Change Mitigation (SRREN) and the Special Report on Managing Risks of Extreme Events and Disasters to Advance Climate Change Adaptation (SREX). Both Special Reports were requested by governments.\n\nThe Special Report on Emissions Scenarios (SRES) is a report by the IPCC which was published in 2000. The SRES contains \"scenarios\" of future changes in emissions of greenhouse gases and sulfur dioxide. One of the uses of the SRES scenarios is to project future changes in climate, e.g., changes in global mean temperature. The SRES scenarios were used in the IPCC's Third and Fourth Assessment Reports.\n\nThe SRES scenarios are \"baseline\" (or \"reference\") scenarios, which means that they do not take into account any current or future measures to limit greenhouse gas (GHG) emissions (e.g., the Kyoto Protocol to the United Nations Framework Convention on Climate Change). SRES emissions projections are broadly comparable in range to the baseline projections that have been developed by the scientific community.\n\nThere have been a number of comments on the SRES. Parson \"et al.\" (2007) stated that the SRES represented \"a substantial advance from prior scenarios\". At the same time, there have been criticisms of the SRES.\n\nThe most prominently publicized criticism of SRES focused on the fact that all but one of the participating models compared gross domestic product (GDP) across regions using market exchange rates (MER), instead of the more correct purchasing-power parity (PPP) approach. This criticism is discussed in the main SRES article.\n\nThis report assesses existing literature on renewable energy commercialisation for the mitigation of climate change. It was published in 2012 and covers the six most important renewable energy technologies, as well as their integration into present and future energy systems. It also takes into consideration the environmental and social consequences associated with these technologies, the cost and strategies to overcome technical as well as non-technical obstacles to their application and diffusion. The full report in PDF form is found here\n\nMore than 130 authors from all over the world contributed to the preparation of IPCC Special Report on Renewable Energy Sources and Climate Change Mitigation (SRREN) on a voluntary basis – not to mention more than 100 scientists, who served as contributing authors.\n\nThe report was published in 2012. It assesses the effect that climate change has on the threat of natural disasters and how nations can better manage an expected change in the frequency of occurrence and intensity of severe weather patterns. It aims to become a resource for decision-makers to prepare more effectively for managing the risks of these events. A potentially important area for consideration is also the detection of trends in extreme events and the attribution of these trends to human influence. The full report, 594 pages in length, may be found here in PDF form.\n\nMore than 80 authors, 19 review editors, and more than 100 contributing authors from all over the world contributed to the preparation of SREX.\n\nWhen the Paris Agreement was adopted, the UNFCCC invited the Intergovernmental Panel on Climate Change to write a special report on \"How can humanity prevent the global temperature rise more than 1.5 degrees above pre-industrial level\". The completed report, Special Report on Global Warming of 1.5 °C (SR15), was released on 8 October 2018. Its full title is \"Global Warming of 1.5 °C, an IPCC special report on the impacts of global warming of 1.5 °C above pre-industrial levels and related global greenhouse gas emission pathways, in the context of strengthening the global response to the threat of climate change, sustainable development, and efforts to eradicate poverty\".\n\nThe finished report summarizes the findings of scientists, showing that maintaining a temperature rise to below 1.5 °C remains possible, but only through \"rapid and far-reaching transitions in energy, land, urban and infrastructure..., and industrial systems\". Meeting the Paris target of is possible but would require \"deep emissions reductions\", \"rapid\", \"far-reaching and unprecedented changes in all aspects of society\". In order to achieve the 1.5 °C target, CO2 emissions must decline by 45% (relative to 2010 levels) by 2030, reaching net zero by around 2050. Deep reductions in non-CO2 emissions (such as nitrous oxide and methane) will also be required to limit warming to 1.5 °C. Under the pledges of the countries entering the Paris Accord, a sharp rise of 3.1 to 3.7 °C is still expected to occur by 2100. Holding this rise to 1.5 °C avoids the worst effects of a rise by even 2 °C. However, a warming of even 1.5 degrees will still result in large-scale drought, famine, heat stress, species die-off, loss of entire ecosystems, and loss of habitable land, throwing more than 100 Million into poverty. Effects will be most drastic in arid regions including the Middle East and the Sahel in Africa, where fresh water will remain in some areas following a 1.5 °C rise in temperatures but are expected to dry up completely if the rise reaches 2 °C.\n\nThe final draft of the \"Special Report on climate change and land\" (SRCCL)—with the full title, \"Special Report on climate change, desertification, land degradation, sustainable land management, food security, and greenhouse gas fluxes in terrestrial ecosystems\" was published online on 7 August 2019. The SRCCL consists of seven chapters, Chapter 1: Framing and Context, Chapter 2: Land-Climate Interactions, Chapter 3: Desertification, Chapter 4: Land Degradation, Chapter 5: Food Security, Chapter 5 Supplementary Material, Chapter 6: Interlinkages between desertification, land degradation, food security and GHG fluxes: Synergies, trade-offs and Integrated Response Options, and Chapter 7: Risk management and decision making in relation to sustainable development.\n\nThe \"Special Report on the Ocean and Cryosphere in a Changing Climate\" (SROCC) was approved on 25 September 2019 in Monaco. Among other findings, the report concluded that sea level rises could be up to two feet higher by the year 2100, even if efforts to reduce greenhouse gas emissions and to limit global warming are successful; coastal cities across the world could see so-called \"storm[s] of the century\" at least once a year.\n\nWithin IPCC the National Greenhouse Gas Inventory Program develops methodologies to estimate emissions of greenhouse gases. This has been undertaken since 1991 by the IPCC WGI in close collaboration with the Organisation for Economic Co-operation and Development and the International Energy Agency.\nThe objectives of the National Greenhouse Gas Inventory Program are:\nBULLET::::- to develop and refine an internationally agreed methodology and software for the calculation and reporting of national greenhouse gas emissions and removals; and\nBULLET::::- to encourage the widespread use of this methodology by countries participating in the IPCC and by signatories of the UNFCCC.\n\nThe 1996 Guidelines for National Greenhouse Gas Investories provide the methodological basis for the estimation of national greenhouse gas emissions inventories. Over time these guidelines have been completed with good practice reports: \"Good Practice Guidance and Uncertainty Management in National Greenhouse Gas Inventories\" and \"Good Practice Guidance for Land Use, Land-Use Change and Forestry\".\n\nThe 1996 guidelines and the two good practice reports are to be used by parties to the UNFCCC and to the Kyoto Protocol in their annual submissions of national greenhouse gas inventories.\n\nThe 2006 \"IPCC Guidelines for National Greenhouse Gas Inventories\" is the latest version of these emission estimation methodologies, including a large number of default emission factors. Although the IPCC prepared this new version of the guidelines on request of the parties to the UNFCCC, the methods have not yet been officially accepted for use in national greenhouse gas emissions reporting under the UNFCCC and the Kyoto Protocol.\n\nThe IPCC concentrates its activities on the tasks allotted to it by the relevant WMO Executive Council and UNEP Governing Council resolutions and decisions as well as on actions in support of the UNFCCC process. While the preparation of the assessment reports is a major IPCC function, it also supports other activities, such as the Data Distribution Centre and the National Greenhouse Gas Inventories Programme, required under the UNFCCC. This involves publishing default emission factors, which are factors used to derive emissions estimates based on the levels of fuel consumption, industrial production and so on.\n\nThe IPCC also often answers inquiries from the UNFCCC Subsidiary Body for Scientific and Technological Advice (SBSTA).\n\nIn December 2007, the IPCC was awarded the Nobel Peace Prize \"for their efforts to build up and disseminate greater knowledge about man-made climate change, and to lay the foundations for the measures that are needed to counteract such change\". The award is shared with Former U.S. Vice-President Al Gore for his work on climate change and the documentary \"An Inconvenient Truth\".\n\nThere is widespread support for the IPCC in the scientific community, which is reflected in publications by other scientific bodies and experts. However, criticisms of the IPCC have been made.\n\nSince 2010 the IPCC has come under yet unparalleled public and political scrutiny. The global IPCC consensus approach has been challenged internally and externally, for example, during the 2009 Climatic Research Unit email controversy (\"Climategate\"). It is contested by some as an information monopoly with results for both the quality and the impact of the IPCC work as such.\n\nA paragraph in the 2007 Working Group II report (\"Impacts, Adaptation and Vulnerability\"), chapter 10 included a projection that Himalayan glaciers could disappear by 2035\n\nThis projection was not included in the final summary for policymakers. The IPCC has since acknowledged that the date is incorrect, while reaffirming that the conclusion in the final summary was robust. They expressed regret for \"the poor application of well-established IPCC procedures in this instance\". The date of 2035 has been correctly quoted by the IPCC from the WWF report, which has misquoted its own source, an ICSI report \"Variations of Snow and Ice in the past and at present on a Global and Regional Scale\".\n\nRajendra K. Pachauri responded in an interview with \"Science\".\n\nFormer IPCC chairman Robert Watson said, regarding the Himalayan glaciers estimation, \"The mistakes all appear to have gone in the direction of making it seem like climate change is more serious by overstating the impact. That is worrying. The IPCC needs to look at this trend in the errors and ask why it happened\". Martin Parry, a climate expert who had been co-chair of the IPCC working group II, said that \"What began with a single unfortunate error over Himalayan glaciers has become a clamour without substance\" and the IPCC had investigated the other alleged mistakes, which were \"generally unfounded and also marginal to the assessment\".\n\nThe third assessment report (TAR) prominently featured a graph labeled \"Millennial Northern Hemisphere temperature reconstruction\" based on a 1999 paper by Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes (MBH99), which has been referred to as the \"hockey stick graph\". This graph extended the similar graph in from the IPCC Second Assessment Report of 1995, and differed from a schematic in the first assessment report that lacked temperature units, but appeared to depict larger global temperature variations over the past 1000 years, and higher temperatures during the Medieval Warm Period than the mid 20th century. The schematic was not an actual plot of data, and was based on a diagram of temperatures in central England, with temperatures increased on the basis of documentary evidence of Medieval vineyards in England. Even with this increase, the maximum it showed for the Medieval Warm Period did not reach temperatures recorded in central England in 2007. The MBH99 finding was supported by cited reconstructions by , , and , using differing data and methods. The Jones et al. and Briffa reconstructions were overlaid with the MBH99 reconstruction in Figure 2.21 of the IPCC report.\n\nThese studies were widely presented as demonstrating that the current warming period is exceptional in comparison to temperatures between 1000 and 1900, and the MBH99 based graph featured in publicity. Even at the draft stage, this finding was disputed by contrarians: in May 2000 Fred Singer's Science and Environmental Policy Project held a press event on Capitol Hill, Washington, D.C., featuring comments on the graph Wibjörn Karlén and Singer argued against the graph at a United States Senate Committee on Commerce, Science and Transportation hearing on 18 July 2000. Contrarian John Lawrence Daly featured a modified version of the IPCC 1990 schematic, which he mis-identified as appearing in the IPCC 1995 report, and argued that \"Overturning its own previous view in the 1995 report, the IPCC presented the 'Hockey Stick' as the new orthodoxy with hardly an apology or explanation for the abrupt U-turn since its 1995 report\". Criticism of the MBH99 reconstruction in a review paper, which was quickly discredited in the Soon and Baliunas controversy, was picked up by the Bush administration, and a Senate speech by US Republican senator James Inhofe alleged that \"manmade global warming is the greatest hoax ever perpetrated on the American people\". The data and methodology used to produce the \"hockey stick graph\" was criticized in papers by Stephen McIntyre and Ross McKitrick, and in turn the criticisms in these papers were examined by other studies and comprehensively refuted by , which showed errors in the methods used by McIntyre and McKitrick.\n\nOn 23 June 2005, Rep. Joe Barton, chairman of the House Committee on Energy and Commerce wrote joint letters with Ed Whitfield, chairman of the Subcommittee on Oversight and Investigations demanding full records on climate research, as well as personal information about their finances and careers, from Mann, Bradley and Hughes. Sherwood Boehlert, chairman of the House Science Committee, said this was a \"misguided and illegitimate investigation\" apparently aimed at intimidating scientists, and at his request the U.S. National Academy of Sciences arranged for its National Research Council to set up a special investigation. The National Research Council's report agreed that there were some statistical failings, but these had little effect on the graph, which was generally correct. In a 2006 letter to \"Nature\", Mann, Bradley, and Hughes pointed out that their original article had said that \"more widespread high-resolution data are needed before more confident conclusions can be reached\" and that the uncertainties were \"the point of the article\".\n\nThe IPCC Fourth Assessment Report (AR4) published in 2007 featured a graph showing 12 proxy based temperature reconstructions, including the three highlighted in the 2001 Third Assessment Report (TAR); as before, and had both been calibrated by newer studies. In addition, analysis of the Medieval Warm Period cited reconstructions by (as cited in the TAR) and . Ten of these 14 reconstructions covered 1,000 years or longer. Most reconstructions shared some data series, particularly tree ring data, but newer reconstructions used additional data and covered a wider area, using a variety of statistical methods. The section discussed the divergence problem affecting certain tree ring data.\n\nSome critics have contended that the IPCC reports tend to be conservative by consistently underestimating the pace and impacts of global warming, and report only the \"lowest common denominator\" findings.\n\nOn the eve of the publication of IPCC's Fourth Assessment Report in 2007 another study was published suggesting that temperatures and sea levels have been rising at or above the maximum rates proposed during IPCC's 2001 Third Assessment Report. The study compared IPCC 2001 projections on temperature and sea level change with observations. Over the six years studied, the actual temperature rise was near the top end of the range given by IPCC's 2001 projection, and the actual sea level rise was above the top of the range of the IPCC projection.\n\nAnother example of scientific research which suggests that previous estimates by the IPCC, far from overstating dangers and risks, have actually understated them is a study on projected rises in sea levels. When the researchers' analysis was \"applied to the possible scenarios outlined by the Intergovernmental Panel on Climate Change (IPCC), the researchers found that in 2100 sea levels would be 0.5–1.4 m [50–140 cm] above 1990 levels. These values are much greater than the 9–88 cm as projected by the IPCC itself in its Third Assessment Report, published in 2001\". This may have been due, in part, to the expanding human understanding of climate.\n\nGreg Holland from the National Center for Atmospheric Research, who reviewed a multi-meter sea level rise study by Jim Hansen, noted “\"There is no doubt that the sea level rise, within the IPCC, is a very conservative number, so the truth lies somewhere between IPCC and Jim.\"”\n\nIn reporting criticism by some scientists that IPCC's then-impending January 2007 report understates certain risks, particularly sea level rises, an AP story quoted Stefan Rahmstorf, professor of physics and oceanography at Potsdam University as saying \"In a way, it is one of the strengths of the IPCC to be very conservative and cautious and not overstate any climate change risk\".\n\nIn his December 2006 book, \"\", and in an interview on Fox News on 31 January 2007, energy expert Joseph Romm noted that the IPCC Fourth Assessment Report is already out of date and omits recent observations and factors contributing to global warming, such as the release of greenhouse gases from thawing tundra.\n\nPolitical influence on the IPCC has been documented by the release of a memo by ExxonMobil to the Bush administration, and its effects on the IPCC's leadership. The memo led to strong Bush administration lobbying, evidently at the behest of ExxonMobil, to oust Robert Watson, a climate scientist, from the IPCC chairmanship, and to have him replaced by Pachauri, who was seen at the time as more mild-mannered and industry-friendly.\n\nMichael Oppenheimer, a long-time participant in the IPCC and coordinating lead author of the Fifth Assessment Report conceded in \"Science Magazine's State of the Planet 2008–2009\" some limitations of the IPCC consensus approach and asks for concurring, smaller assessments of special problems instead of the large scale approach as in the previous IPCC assessment reports. It has become more important to provide a broader exploration of uncertainties. Others see as well mixed blessings of the drive for consensus within the IPCC process and ask to include dissenting or minority positions or to improve statements about uncertainties.\n\nThe IPCC process on climate change and its efficiency and success has been compared with dealings with other environmental challenges (compare Ozone depletion and global warming). In case of the Ozone depletion, global regulation based on the Montreal Protocol has been successful. In case of Climate Change, the Kyoto Protocol failed. The Ozone case was used to assess the efficiency of the IPCC process.\nThe lockstep situation of the IPCC is having built a broad science consensus while states and governments still follow different, if not opposing goals. The underlying linear model of policy-making of \"the more knowledge we have, the better the political response will be\" is being doubted.\n\nAccording to Sheldon Ungar's comparison with global warming, the actors in the ozone depletion case had a better understanding of scientific ignorance and uncertainties. The ozone case communicated to lay persons \"with easy-to-understand bridging metaphors derived from the popular culture\" and related to \"immediate risks with everyday relevance\", while the public opinion on climate change sees no imminent danger. The stepwise mitigation of the ozone layer challenge was based as well on successfully reducing regional burden sharing conflicts. In case of the IPCC conclusions and the failure of the Kyoto Protocol, varying regional cost-benefit analysis and burden-sharing conflicts with regard to the distribution of emission reductions remain an unsolved problem. In the UK, a report for a House of Lords committee asked to urge the IPCC to involve better assessments of costs and benefits of climate change, but the Stern Review, ordered by the UK government, made a stronger argument in favor to combat human-made climate change.\n\nSince the IPCC does not carry out its own research, it operates on the basis of scientific papers and independently documented results from other scientific bodies, and its schedule for producing reports requires a deadline for submissions prior to the report's final release. In principle, this means that any significant new evidence or events that change our understanding of climate science between this deadline and publication of an IPCC report cannot be included. In an area of science where our scientific understanding is rapidly changing, this has been raised as a serious shortcoming in a body which is widely regarded as the ultimate authority on the science. However, there has generally been a steady evolution of key findings and levels of scientific confidence from one assessment report to the next.\n\nThe submission deadlines for the Fourth Assessment Report (AR4) differed for the reports of each Working Group. Deadlines for the Working Group I report were adjusted during the drafting and review process in order to ensure that reviewers had access to unpublished material being cited by the authors. The final deadline for cited publications was 24 July 2006. The final WG I report was released on 30 April 2007 and the final AR4 Synthesis Report was released on 17 November 2007.Rajendra Pachauri, the IPCC chair, admitted at the launch of this report that since the IPCC began work on it, scientists have recorded \"much stronger trends in climate change\", like the unforeseen dramatic melting of polar ice in the summer of 2007, and added, \"that means you better start with intervention much earlier\".\n\nScientists who participate in the IPCC assessment process do so without any compensation other than the normal salaries they receive from their home institutions. The process is labor-intensive, diverting time and resources from participating scientists' research programs. Concerns have been raised that the large uncompensated time commitment and disruption to their own research may discourage qualified scientists from participating.\n\nIn May 2010, Pachauri noted that the IPCC currently had no process for responding to errors or flaws once it issued a report. The problem, according to Pachauri, was that once a report was issued the panels of scientists producing the reports were disbanded.\n\nIn February 2010, in response to controversies regarding claims in the Fourth Assessment Report, five climate scientists – all contributing or lead IPCC report authors – wrote in the journal \"Nature\" calling for changes to the IPCC. They suggested a range of new organizational options, from tightening the selection of lead authors and contributors, to dumping it in favor of a small permanent body, or even turning the whole climate science assessment process into a moderated \"living\" Wikipedia-IPCC. Other recommendations included that the panel employ a full-time staff and remove government oversight from its processes to avoid political interference.\n\nThe 2018 report \"What Lies Beneath\" by the Breakthrough - National Centre for Climate Restoration, with contributions from Kevin Anderson, James Hansen, Michael E. Mann, Michael Oppenheimer, Naomi Oreskes, Stefan Rahmstorf, Eric Rignot, Hans Joachim Schellnhuber, Kevin Trenberth, and others, urges the IPCC, the wider UNFCCC negotiations, and national policy makers to change their approach. The authors note, \"We urgently require a reframing of scientific research within an existential risk-management framework.\"\n\nIn March 2010, at the invitation of the United Nations secretary-general and the chair of the IPCC, the InterAcademy Council (IAC) was asked to review the IPCC's processes for developing its reports. The IAC panel, chaired by Harold Tafler Shapiro, convened on 14 May 2010 and released its report on 1 September 2010.\n\nThe IAC found that, \"The IPCC assessment process has been successful overall\". The panel, however, made seven formal recommendations for improving the IPCC's assessment process, including:\nBULLET::::1. establish an executive committee;\nBULLET::::2. elect an executive director whose term would only last for one assessment;\nBULLET::::3. encourage review editors to ensure that all reviewer comments are adequately considered and genuine controversies are adequately reflected in the assessment reports;\nBULLET::::4. adopt a better process for responding to reviewer comments;\nBULLET::::5. working groups should use a qualitative level-of-understanding scale in the Summary for Policy Makers and Technical Summary;\nBULLET::::6. \"Quantitative probabilities (as in the likelihood scale) should be used to describe the probability of well-defined outcomes only when there is sufficient evidence\"; and\nBULLET::::7. implement a communications plan that emphasizes transparency and establish guidelines for who can speak on behalf of the organization.\nThe panel also advised that the IPCC avoid appearing to advocate specific policies in response to its scientific conclusions. Commenting on the IAC report, \"Nature News\" noted that \"The proposals were met with a largely favourable response from climate researchers who are eager to move on after the media scandals and credibility challenges that have rocked the United Nations body during the past nine months\".\n\nPapers and electronic files of certain working groups of the IPCC, including reviews and comments on drafts of their Assessment Reports, are archived at the Environmental Science and Public Policy Archives in the Harvard Library.\n\nVarious scientific bodies have issued official statements endorsing and concurring with the findings of the IPCC.\n\nBULLET::::- Joint science academies' statement of 2001. \"The work of the Intergovernmental Panel on Climate Change (IPCC) represents the consensus of the international scientific community on climate change science. We recognise IPCC as the world's most reliable source of information on climate change and its causes, and we endorse its method of achieving this consensus\".\nBULLET::::- Canadian Foundation for Climate and Atmospheric Sciences. \"We concur with the climate science assessment of the Intergovernmental Panel on Climate Change (IPCC) in 2001 ... We endorse the conclusions of the IPCC assessment...\"\nBULLET::::- Canadian Meteorological and Oceanographic Society. \"CMOS endorses the process of periodic climate science assessment carried out by the Intergovernmental Panel on Climate Change and supports the conclusion, in its Third Assessment Report, which states that the balance of evidence suggests a discernible human influence on global climate.\"\nBULLET::::- European Geosciences Union. \"The Intergovernmental Panel on Climate Change [...] is the main representative of the global scientific community [...][The] IPCC third assessment report [...] represents the state-of-the-art of climate science supported by the major science academies around the world and by the vast majority of scientific researchers and investigations as documented by the peer-reviewed scientific literature\".\nBULLET::::- International Council for Science (ICSU). \"...the IPCC 4th Assessment Report represents the most comprehensive international scientific assessment ever conducted. This assessment reflects the current collective knowledge on the climate system, its evolution to date, and its anticipated future development\".\nBULLET::::- National Oceanic and Atmospheric Administration (USA). \"Internationally, the Intergovernmental Panel on Climate Change (IPCC)... is the most senior and authoritative body providing scientific advice to global policy makers\".\nBULLET::::- United States National Research Council. \"The IPCC Third Assessment Report'] conclusion that most of the observed warming of the last 50 years is likely to have been due to the increase in greenhouse gas concentrations accurately reflects the current thinking of the scientific community on this issue\".\nBULLET::::- Network of African Science Academies. \"The IPCC should be congratulated for the contribution it has made to public understanding of the nexus that exists between energy, climate and sustainability\".\nBULLET::::- Royal Meteorological Society, in response to the release of the Fourth Assessment Report, referred to the IPCC as \"The world's best climate scientists\".\nBULLET::::- Stratigraphy Commission of the Geological Society of London. \"The most authoritative assessment of climate change in the near future is provided by the Inter-Governmental Panel for Climate Change\".\n\nBULLET::::- 4 Degrees and Beyond International Climate Conference\nBULLET::::- Avoiding dangerous climate change\nBULLET::::- Indian Network on Climate Change Assessment\nBULLET::::- Intergovernmental Science-Policy Platform on Biodiversity and Ecosystem Services\nBULLET::::- Post–Kyoto Protocol negotiations on greenhouse gas emissions\nBULLET::::- Robust decision making\n\nBULLET::::- . Statement website.\n\nBULLET::::- . Statement website.\n\nBULLET::::- . Statement website.\n\nBULLET::::- . Low-resolution (2 Mb) or high-resolution PDF (25 Mb).\nBULLET::::- . High-resolution PDF versions: HL 12-I (report), HL 12-II (evidence).\n\nBULLET::::- . Archived\n\nBULLET::::- . Archived\n\nBULLET::::- . Climate Change 2013 Working Group 1 website.}}\n\nBULLET::::- (pb: )\n\nBULLET::::- (pb: )\n\nBULLET::::- (pb: ).\n\nBULLET::::- (pb: )\n\nBULLET::::- (pb: )\n\nBULLET::::- (pb: )\n\nBULLET::::- (pb: )\n\nBULLET::::- , (pb: , ). Also published in English (html) (PDF) on the IPCC website. Summary for policymakers available in French, Russian, and Spanish.\n\nBULLET::::- (pb: ) pdf.\n\nBULLET::::- (pb: )\n\nBULLET::::- .\nBULLET::::- . Statement website.\nBULLET::::- (78 researchers, corresponding author Darrell S. Kaufman).\n\nBULLET::::- . Report website.\n\nBULLET::::- . Report website.\n\nBULLET::::- . Also available as a PDF\n\nBULLET::::- . Report website.\n\nBULLET::::- The World Bank climate change and water sector, 2009 Water and Climate Change: Understanding the Risks and Making Climate-Smart Investment Decisions. Retrieved 31 October 2013.\n\nBULLET::::- Intergovernmental Panel on Climate Change\nBULLET::::- IPCC Organisation\nBULLET::::- IPCC publications\nBULLET::::- IPCC AR4 WG1 Report Available for Purchase\nBULLET::::- Summaries for Policymakers (SPMs) of the IPCC Fourth Assessment Report:\nBULLET::::- — The collection of drafts, review-comments and other documents relating to the work of the Working Group I of the Assessment Report 4.\nBULLET::::- A summary of the Fifth Assessment Report WG1 TS and of the Fourth Assessment Report SPMs by GreenFacts.org\nBULLET::::- The World Bank – Climate Change and concerns over water resources The World Bank's portal to climate change and water publications.\nBULLET::::- IPCC article at the Encyclopedia of Earth – General overview of the IPCC\nBULLET::::- Climate Change – What Is the IPCC by\nBULLET::::- Climate Change Freeview Video Interview 2006 – Sherwood Rowland, Nobel Laureate (1995) for work on ozone depletion discusses climate change. Provided by the Vega Science Trust.\nBULLET::::- Evolution of Climate Science in the IPCC Assessments: Understanding the 20th Century Climate Change. A video of a lecture given at Princeton University by Venkatachalam Ramaswamy, Acting Director and Senior Scientist, Geophysical Fluid Dynamics Laboratory (GFDL), Professor in Geosciences and Atmospheric and Oceanic Sciences, Princeton University.\nBULLET::::- IPCC Data Distribution Centre Climate data and guidance on its use.\n"}
{"id": "15031", "url": "https://en.wikipedia.org/wiki?curid=15031", "title": "IPCC (disambiguation)", "text": "IPCC (disambiguation)\n\nIPCC, or Intergovernmental Panel on Climate Change, is a scientific body under the auspices of the United Nations.\n\nIPCC may also refer to:\nBULLET::::- Independent Police Complaints Commission, of England and Wales\nBULLET::::- Independent Police Complaints Council, of Hong Kong\nBULLET::::- Integrated Professional Competency Course, a course of the Institute of Chartered Accountants of India\nBULLET::::- Interworld Police Coordinating Company, a fictional organization in Jack Vance's novels\nBULLET::::- Irish Peatland Conservation Council\n"}
{"id": "15032", "url": "https://en.wikipedia.org/wiki?curid=15032", "title": "IBM Personal Computer", "text": "IBM Personal Computer\n\nThe IBM Personal Computer, commonly known as the IBM PC, is the original version of the IBM PC compatible hardware platform. It is IBM model number 5150 and was introduced on August 12, 1981. It was created by a team of engineers and designers under the direction of Philip Don Estridge of the IBM Entry Systems Division in Boca Raton, Florida.\n\nThe generic term \"personal computer\" (\"PC\") was in use years before 1981, applied as early as 1972 to the Xerox PARC's Alto, but the term \"PC\" came to mean more specifically a desktop microcomputer compatible with IBM's \"Personal Computer\" branded products. The machine was based on open architecture, and third-party suppliers sprang up to provide peripheral devices, expansion cards, and software. IBM had a substantial influence on the personal computer market in standardizing a platform for personal computers, and \"IBM compatible\" became an important criterion for sales growth. Only the Apple Macintosh family kept a significant share of the microcomputer market after the 1980s without compatibility to the IBM personal computer.\n\nInternational Business Machines (IBM) was one of the world's largest companies and had a 62-percent share of the mainframe computer market in 1982. In the late 1970s, the new personal computer industry was dominated by the Commodore PET, Atari 8-bit family, Apple II series, Tandy Corporation's TRS-80, and various CP/M machines. The microcomputer market was large enough for IBM's attention, with $150 million in sales by 1979 and projected annual growth of more than 40-percent in the early 1980s. Other large technology companies had entered it, such as Hewlett-Packard (HP), Texas Instruments (TI), and Data General, and some large IBM customers were buying Apples, so IBM saw introducing its own personal computer as both an experiment in a new market and a defense against rivals.\n\nIn 1980 and 1981, rumors spread of an IBM personal computer, perhaps a miniaturized version of the IBM System/370, while Matsushita acknowledged that it had discussed with IBM the possibility of manufacturing a personal computer for the American company. The Japanese project was codenamed \"Go\", but it ended before the 1981 release of the American-designed IBM PC codenamed \"Chess\", and two simultaneous projects further confused rumors about the forthcoming product.\n\nWhether IBM had waited too long to enter an industry in which Tandy, Atari and others were already successful was unclear. Data General and TI's small computers were not very successful, but observers expected AT&T to soon enter the computer industry, and other large companies such as Exxon, Montgomery Ward, Pentel, and Sony were designing their own microcomputers. Xerox quickly produced the 820 to introduce a personal computer before IBM, becoming the second Fortune 500 company after Tandy to do so, and had its Xerox PARC laboratory's sophisticated technology.\n\nAn observer stated that \"IBM bringing out a personal computer would be like teaching an elephant to tap dance\". Successful microcomputer company Vector Graphic's fiscal 1980 revenue was $12 million. A single IBM computer in the early 1960s cost as much as $9 million, occupied of air-conditioned space, and had a staff of 60 people; in 1980 its least-expensive computer, the 5120, still cost about $13,500. The \"Colossus of Armonk\" only sold through its own sales force, had no experience with resellers or retail stores, and did not introduce the first product designed to work with non-IBM equipment until 1980.\n\nAnother observer claimed that IBM made decisions so slowly that, when tested, \"what they found is that it would take at least nine months to ship an empty box\", and an employee complained that \"IBM has more committees than the U.S. Government\". As with other large computer companies, its new products typically required about four to five years for development. IBM had to learn how to quickly develop, mass-produce, and market new computers. While the company traditionally let others pioneer a new market—IBM released its first commercial computer a year after Remington Rand's UNIVAC in 1951, but within five years had 85% of the market—the personal-computer development and pricing cycles were much faster than for mainframes, with products designed in a few months and obsolete quickly.\n\nMany in the microcomputer industry resented IBM's power and wealth, and disliked the perception that an industry founded by startups needed a latecomer so staid that it had a strict dress code and employee songbook, and prohibited salesmen with client visits in the afternoon from drinking alcohol at lunch. The potential importance to microcomputers of a company so prestigious, that a popular saying in American companies stated \"No one ever got fired for buying IBM\", was nonetheless clear. \"InfoWorld\", which described itself as \"The Newsweekly for Microcomputer Users\", stated that \"for my grandmother, and for millions of people like her, \"IBM \" and \"computer\" are synonymous\". \"Byte\" (\"The Small Systems Journal\") stated in an editorial just before the announcement of the IBM PC:\n\nThe editorial acknowledged that \"some factions in our industry have looked upon IBM as the 'enemy, but concluded with optimism: \"I want to see personal computing take a giant step.\"\n\nDesktop sized programmable calculators by HP had evolved into the HP 9830 BASIC language computer by 1972. In 1972–1973 a team led by Dr. Paul Friedl at the IBM Los Gatos Scientific Center developed a portable computer prototype called SCAMP (Special Computer APL Machine Portable) based on the IBM PALM processor with a Philips compact cassette drive, small CRT, and full-function keyboard. SCAMP emulated an IBM 1130 minicomputer to run APL\\1130. In 1973 APL was generally available only on mainframe computers, and most desktop sized microcomputers such as the Wang 2200 or HP 9800 offered only BASIC. Because it was the first to emulate APL\\1130 performance on a portable, single-user computer, \"PC Magazine\" in 1983 designated SCAMP a \"revolutionary concept\" and \"the world's first personal computer\". The prototype is in the Smithsonian Institution. A non-working industrial design model was also created in 1973 by industrial designer Tom Hardy illustrating how the SCAMP engineering prototype could be transformed into a usable product design for the marketplace. This design model was requested by IBM executive Bill Lowe to complement the engineering prototype in his early efforts to demonstrate the viability of creating a single-user computer.\n\nSuccessful demonstrations of the 1973 SCAMP prototype led to the IBM 5100 portable microcomputer in 1975. In the late 1960s such a machine would have been nearly as large as two desks and would have weighed about half a ton. The 5100 was a complete computer system programmable in BASIC or APL, with a small built-in CRT monitor, keyboard, and tape drive for data storage. It was also very expensive, up to US$20,000; the computer was designed for professional and scientific customers, not business users or hobbyists. \"BYTE\" in 1975 announced the 5100 with the headline \"Welcome, IBM, to personal computing\", but \"PC Magazine\" in 1984 described 5100s as \"little mainframes\" and stated that \"as personal computers, these machines were dismal failures ... the antithesis of user-friendly\", with no IBM support for third-party software. Despite news reports that it was the first IBM product without a model number, when the PC was introduced in 1981 it was designated as the IBM 5150, putting it in the \"5100\" series though its architecture was not directly descended from the IBM 5100. Later models followed in the trend: For example, the IBM Portable Personal Computer, PC/XT, and PC AT are IBM machine types 5155, 5160, and 5170, respectively.\n\nFollowing SCAMP, the IBM Boca Raton, Florida Laboratory created several single-user computer design concepts to support Lowe's ongoing effort to convince IBM there was a strategic opportunity in the personal computer business. A selection of these early IBM design concepts created by Hardy is highlighted in the book \"DELETE: A Design History of Computer Vapourware\". One such concept in 1977, code-named Aquarius, was a working prototype utilizing advanced bubble memory cartridges. While this design was more powerful and smaller than Apple II launched the same year, the advanced bubble technology was deemed unstable and not ready for mass production.\n\nSome employees opposed IBM entering the market. One said, \"Why on earth would you care about the personal computer? It has nothing at all to do with office automation\". The company considered personal computer designs but had determined that IBM was unable to build a personal computer profitably. For example, Walden C. Rhines of TI met with a Boca Raton group considering the TMS9900 microprocessor for a secret 16-bit microprocessor-based project in 1978.\n\nIBM President John Opel was not among those skeptical of personal computers. He and CEO Frank Cary had created more than a dozen semi-autonomous \"Independent Business Units\" (IBU) to encourage innovation; \"Fortune\" called them \"how to start your own company without leaving IBM\". Lowe became the first head of the Entry Level Systems IBU in Boca Raton, and his team researched the market. Computer dealers were very interested in selling an IBM product, but they told Lowe that the company could not design, sell, or service it as IBM had previously done. An IBM microcomputer, they said, must be composed of standard parts that store employees could repair. Dealers disliked Apple's business practices, including a shortage of the Apple II while the company focused on the more sophisticated Apple III, but they saw no alternative because they doubted that IBM's traditional sales methods and bureaucracy would change.\n\nSchools in Broward County—near Boca Raton—purchased Apples, a consequence of IBM lacking a personal computer. Atari proposed in 1980 that it act as original equipment manufacturer for an IBM microcomputer. Lowe was aware that the company needed to enter the market quickly, so he met with Opel, Cary, and others on the Corporate Management Committee in 1980. He demonstrated the proposal with an industrial design model by Hardy based on the Atari 800 platform, and suggested acquiring Atari \"because we can't do this within the culture of IBM\".\n\nCary agreed about the culture, observing that IBM would need \"four years and three hundred people\" to develop its own personal computer; Lowe promised one in a year if done without traditional IBM methods. Instead of acquiring Atari, the committee allowed him to form an independent group of employees called \"the Dirty Dozen\", led by engineer Bill Sydnes, and Lowe promised that they could design a prototype in 30 days. The crude prototype barely worked when Lowe demonstrated it in August, but he presented a detailed business plan which proposed that the new computer have an open architecture, use non-proprietary components and software, and be sold through retail stores, all contrary to IBM practice.\n\nThe committee agreed that Lowe's approach was the most likely to succeed, and it approved turning the group into another IBU code named \"Project Chess\" to develop \"Acorn\", with unusually large funding to help achieve the goal of introducing the product within one year of the August demonstration. Don Estridge became the head of Chess. Cary told the team to do whatever was necessary to develop an IBM personal computer quickly, and they made their first internal demonstration of the computer by January 1981. Other key members included Sydnes, Lewis Eggebrecht, David Bradley, Mark Dean, and David O'Connor. Many were already hobbyists who owned their own computers, including Estridge, who had an Apple II. Industrial designer Hardy was also assigned to the project. The team received permission to expand to 150 people by the end of 1980, and more than 500 IBM employees called in one day asking to join.\n\nIBM normally was vertically integrated, only purchasing components like transformers and semiconductors. It internally developed all important hardware and software and discouraged customers from purchasing third-party products compatible with IBM products. For the PC the company avoided vertical integration as much as possible; choosing, for example, to license Microsoft BASIC despite having a BASIC of its own for mainframes. (Estridge said that unlike IBM's own version \"Microsoft BASIC had hundreds of thousands of users around the world. How are you going to argue with that?\") Although the company denied doing so, many observers concluded that IBM intentionally emulated Apple when designing the PC. The many Apple II owners on the team influenced its decision to design the computer with an open architecture and publish technical information so others could create software and expansion slot peripherals.\n\nAlthough the company knew that it could not avoid competition from third-party software on proprietary hardware—Digital Research released CP/M-86 for the IBM Displaywriter, for example—it considered using the IBM 801 RISC processor and its operating system, developed at the Thomas J. Watson Research Center in Yorktown Heights, New York. The 801 processor was more than an order of magnitude more powerful than the Intel 8088, and the operating system more advanced than the PC DOS 1.0 operating system from Microsoft. Ruling out an in-house solution made the team's job much easier and may have avoided a delay in the schedule, but the ultimate consequences of this decision for IBM were far-reaching.\n\nIBM had recently developed the Datamaster business microcomputer, which used a processor and other chips from Intel; familiarity with them and the immediate availability of the 8088 was a reason for choosing it for the PC. The 62-pin expansion bus slots were designed to be similar to the Datamaster slots. Differences from the Datamaster included avoiding an all-in-one design while limiting the computer's size so that it would still fit on a standard desktop with the keyboard (also similar to the Datamaster's), and 5.25\" disk drives instead of 8\". Delays due to in-house development of the Datamaster software was a reason why IBM chose Microsoft BASIC—already available for the 8088—and published available technical information to encourage third-party developers. IBM chose the 8088 over the similar but superior 8086 because Intel offered a better price on the former and could provide more units, and the 8088's 8-bit bus reduced the cost of the rest of the computer.\n\nThe design for the computer was essentially complete by April 1981, when the manufacturing team took over the project. IBM could not only use its own hardware and make a profit with \"Acorn\". To save time and money, the IBU built the machine with commercial off-the-shelf parts from original equipment manufacturers whenever possible, with assembly occurring in Boca Raton at a plant Estridge designed. The IBU would decide whether it would be more economical to \"Make or Buy\" each manufacturing step. Various IBM divisions for the first time competed with outsiders to build parts of the new computer; a North Carolina IBM factory built the keyboard, the Endicott, New York factory had to lower its bid for printed circuit boards, and a Taiwanese company built the monitor. The IBU chose an existing monitor from IBM Japan and an Epson printer. Because of the off-the-shelf parts only the system unit and keyboard has unique IBM industrial design elements. The IBM copyright appears in only the ROM BIOS and on the company logo, and the company reportedly received no patents on the PC, with outsiders manufacturing 90% of it. Because the product would carry the IBM logo, the only corporate division the IBU could not bypass was the Quality Assurance Unit. A component manufacturer described the process of being selected as a supplier as rigorous and \"absolutely amazing\", with IBM inspectors even testing solder flux. They stayed after selection, monitoring and helping to improve the manufacturing process. IBM's size overwhelmed other companies; \"a hundred IBM engineers\" reportedly visited Mitel to meet with two of the latter's employees about a problem, according to \"The New York Times\".\n\nAnother aspect of IBM that did not change was its emphasis on secrecy; employees at Yorktown knew nothing of Boca Raton's activities. Those working on the project, within and outside of IBM, were under strict confidentiality agreements. When an individual mentioned in public on a Saturday that his company was working on software for a new IBM computer, IBM security appeared at the company on Monday to investigate the leak. After an IBM official discovered printouts in a supplier's garbage, the former company persuaded the latter to purchase a paper shredder. Management Science America did not know until after agreeing to buy Peachtree Software in 1981 that the latter was working on software for the PC. Developers such as Software Arts received breadboard prototype computers in boxes lined with lead to block X-rays and sealed with solder, and had to keep them in locked, windowless rooms; to develop software, Microsoft emulated the PC on a DEC minicomputer and used the prototype for debugging. After the PC's debut, IBM Boca Raton employees continued to decline to discuss their jobs in public. One writer compared the \"silence\" after asking one about his role at the company to \"hit[ting] the wall at the Boston Marathon: the conversation is over\".\n\nAfter developing it in 12 months—faster than any other hardware product in company history—IBM announced the Personal Computer on August 12, 1981. Pricing started at for a configuration with 16K RAM, Color Graphics Adapter, and no disk drives. The company intentionally set prices for it and other configurations that were comparable to those of Apple and other rivals; what Dan Bricklin described as \"pretty competitive\" pricing surprised him and other Software Arts employees. One analyst stated that IBM \"has taken the gloves off\", while the company said \"we suggest [the PC's price] invites comparison\". Microsoft, Personal Software, and Peachtree Software were among the developers of nine launch titles, including EasyWriter and VisiCalc. In addition to the existing corporate sales force IBM opened its own Product Center retail stores. After studying Apple's successful distribution network, the company for the first time sold through others, ComputerLand and Sears Roebuck. Because retail stores receive revenue from repairing computers and providing warranty service, IBM broke a 70-year tradition by permitting and training non-IBM service personnel to fix the PC.\n\n\"BYTE\" described IBM as having \"the strongest marketing organization in the world\", but the PC's marketing also differed from that of previous products. The company was aware of its strong corporate reputation among potential customers; an early advertisement began \"Presenting the IBM of Personal Computers\". The advertisements emphasized the novelty of an individual owning an IBM computer, describing \"a product \"you\" may have a personal interest in\" and asking readers to think of My own IBM computer. Imagine that' ... it's yours. For your business, your project, your department, your class, your family and, indeed, for yourself.\"\n\nIBM considered Alan Alda, Beverly Sills, Kermit the Frog, and Billy Martin to be celebrity endorsers of the PC, but chose Charlie Chaplin's The Little Tramp character for a series of advertisements based on Chaplin's films, played by Billy Scudder. Chaplin's film \"Modern Times\" expressed his opposition to big business, mechanization, and technological efficiency, but the $36-million marketing campaign made Chaplin the (according to \"Creative Computing\") \"warm cuddly\" mascot of one of the world's largest companies.\n\nChaplin and his character became so widely associated with IBM that others used his bowler hat and cane to represent or satirize the company. Chaplin's estate sued those who used the trademark without permission, yet \"PC Magazine\"s April 1983 issue had 12 advertisements which referred to the Little Tramp.\n\nPerhaps Chess's most unusual decision for IBM was to publish the PC's technical specifications, allowing outsiders to create products for it. \"We encourage third-party suppliers ... we are delighted to have them\", the company stated. Although the team began managing its own business operations on prototypes before the PC's debut, and despite IBM's $5.3 billion R&D budget in 1982—larger than the total revenue of many competitors—the company did not sell internally developed PC software until April 1984, instead relying on already established software companies. The company contacted Microsoft even before the official approval of Chess, and it and others received cooperation that was, one writer said, \"unheard of\" for IBM. Such openness surprised observers; \"BYTE\" called it \"striking\" and \"startling\", and one developer reported that \"it's a very different IBM\". Another said \"They were very open and helpful about giving us all the technical information we needed. The feeling was so radically different—it's like stepping out into a warm breeze.\" He concluded, \"After years of hassling—fighting the Not-Invented-Here attitude—we're the gods.\"\n\nMost other personal-computer companies did not disclose technical details. Tandy hoped to monopolize sales of TRS-80 software and peripherals. Its RadioShack stores only sold Tandy products; third-party developers found selling their offerings difficult. TI intentionally made developing third-party TI-99/4A software difficult, even requiring a lockout chip in cartridges. IBM itself kept its mainframe technology so secret that rivals were indicted for industrial espionage. For the PC, however, IBM immediately released detailed information. The US$36 \"IBM PC Technical Reference Manual\" included complete circuit schematics, commented ROM BIOS source code, and other engineering and programming information for all of IBM's PC-related hardware, plus instructions on designing third-party peripherals. It was so comprehensive that one reviewer suggested that the manual could serve as a university textbook, and so clear that a developer claimed that he could design an expansion card without seeing the physical computer.\n\nIBM marketed the technical manual in full-page color print advertisements, stating that \"our software story is still being written. Maybe by you\". Sydnes stated that \"The definition of a personal computer \"is\" third-party hardware and software.\" Estridge said that IBM did not keep software development proprietary because it would have to \"out-VisiCalc VisiCorp and out-Peachtree Peachtree—and you just can't do that\".\n\nAnother advertisement told developers that the company would consider publishing software for \"Education. Entertainment. Personal finance. Data management. Self-improvement. Games. Communications. And yes, business.\" Estridge explicitly invited small, \"cottage\" amateur and professional developers to create products \"with\", he said, \"our logo and our support\". IBM sold the PC at a large discount to employees, encouraged them to write software, and distributed a catalog of inexpensive software written by individuals that might not otherwise appear in public.\n\nThe press reported on most details of the PC before the official announcement; only IBM not providing internally developed software, including DOS (86-DOS) as the operating system, surprised observers. \"BYTE\" was correct in predicting that an IBM personal computer would nonetheless receive much public attention. Its rapid development amazed observers, as did the willingness of the Colossus of Armonk to sell as a launch title Microsoft \"Adventure\" (a video game that, its press release stated, brought \"players into a fantasy world of caves and treasures\"); the company even offered an optional joystick port. Future Computing estimated that \"IBM's Billion Dollar Baby\" would have $2.3 billion in hardware sales by 1986. David Bunnell, an editor at Osborne/McGraw-Hill, recalled that\n\nWithin seven weeks Bunnell helped found \"PC Magazine\", the first periodical for the new computer.\n\n\"InfoWorld\" reported that \"On the morning of the announcement, phone calls to IBM's competitors revealed that almost everyone was having an 'executive meeting' involving the high-level officials who might be in a position to publicly react to the IBM announcement\". Claiming that the new IBM computer competed against rivals' products, they were publicly skeptical about the PC. Adam Osborne said that unlike his Osborne I, \"when you buy a computer from IBM, you buy a la carte. By the time you have a computer that does anything, it will cost more than an Apple. I don't think Apple has anything to worry about\". Apple's Mike Markkula agreed that IBM's product was more expensive than the Apple II, and claimed that the Apple III \"offers better performance\". He denied that the IBM PC offered more memory, stating that his company could offer more than 128K \"but frankly we don't know what anyone would do with that memory\". At Tandy, John Roach said \"I don't think it's that significant\"; Jon Shirley admitted that IBM had a \"legendary service reputation\" but claimed that the thousands of Radio Shack stores \"can provide better service\", while predicting that the IBM PC's \"major market will be IBM addicts\"; another executive claimed that Tandy could undersell a $3,000 IBM computer by $1,000.\n\nMany criticized the PC's design as outdated and not innovative, and believed that its alleged weaknesses, such as the use of single-sided, single-density disks with less storage than the computer's RAM, and limited graphics capability (customers who wanted both color and high-quality text had to purchase two graphics cards and two monitors), existed because the company was uncertain about the market and was experimenting before releasing a better computer. (Estridge later boasted, \"Many ... said that there was nothing technologically new in this machine. That was the best news we could have had; we actually had done what we had set out to do.\")\n\nRivals such as Apple, Tandy, and Commodore—together with more than 50% of the personal-computer market—had many advantages. While IBM began with one microcomputer, little available hardware or software, and a couple of hundred dealers, Radio Shack had already sold more than 350,000 computers. It had 14 million customers and 8,000 stores—more than McDonald's—that only sold its broad range of computers and accessories. Apple had sold more than 250,000 computers and had five times as many dealers in the US as IBM and an established international distribution network. Hundreds of independent developers produced software and peripherals for both companies' computers; at least ten Apple databases and ten word processors were available, while the PC had no databases and one word processor. Altos, Vector Graphic, Cromemco, and Zenith were among the companies that were making CP/M, \"InfoWorld\" said, \"\"the\" small-computer operating system\" in many different markets.\n\nRadio Shack and Apple stated their hope that an IBM personal computer would help grow the market. Steve Jobs at Apple ordered a team to examine an IBM PC. After finding it unimpressive—Chris Espinosa called the computer \"a half-assed, hackneyed attempt\"—the company confidently purchased a full-page advertisement in \"The Wall Street Journal\" with the headline \"Welcome, IBM. Seriously\". Microsoft head Bill Gates was at Apple headquarters the day of IBM's announcement and later said \"They didn't seem to care. It took them a full year to realize what had happened\".\n\nThe IBM PC was immediately successful. The PC was small, light weight, and easy to use. Because it was advertised as a personal computer for anyone and not just large corporations, and because it was small and could fit easily into people's homes, it became a device of popular choice for many people. It couldn't have hurt that IBM also advertised it with the lovable Charlie Chaplin's tramp character, who after seeing the computer, falls in love with it and purchases the PC. Chaplin's character became the face of the company's PC.\n\"BYTE\" reported a rumor that more than 40,000 were ordered on the day of the announcement; John Dvorak recalled that one dealer that day praised the computer as an \"incredible winner, and IBM knows how to treat us — none of the Apple arrogance\". One dealer received 22 $1,000 deposits from customers although he could not promise a delivery date. The company could have sold its entire projected first-year production to employees, and IBM customers that were reluctant to purchase Apples were glad to buy microcomputers from their traditional supplier. The computer began shipping in October, ahead of schedule; by then some referred to it simply as \"the PC\".\n\n\"BYTE\" estimated that 90% of the 40,000 first-day orders were from software developers. By COMDEX in November Tecmar developed 20 products including memory expansion and expansion chassis, surprising even IBM. Jerry Pournelle reported after attending the West Coast Computer Faire in early 1982 that because IBM \"encourages amateurs\" with \"documents that tell all\", \"an explosion of [third-party] hardware and software\" was visible at the convention. Many manufacturers of professional business application software, who had been planning/developing versions for the Apple II, promptly switched their efforts over to the IBM PC when it was announced. Often, these products needed the capacity and speed of a hard-disk. Although IBM did not offer a hard-disk option for almost two years following introduction of its PC, business sales were nonetheless catalyzed by the simultaneous availability of hard-disk subsystems, like those of Tallgrass Technologies which sold in ComputerLand stores alongside the IBM 5150 at the introduction in 1981.\n\nOne year after the PC's release, although IBM had sold fewer than 100,000 computers, \"PC World\" counted 753 software packages for the PC—more than four times the number available for the Apple Macintosh one year after its 1984 release—including 422 applications and almost 200 utilities and languages. \"InfoWorld\" reported that \"most of the major software houses have been frantically adapting their programs to run on the PC\", with new PC-specific developers composing \"an entire subindustry that has formed around the PC's open system\", which Dvorak described as a \"de facto standard microcomputer\". The magazine estimated that \"hundreds of tiny garage-shop operations\" were in \"bloodthirsty\" competition to sell peripherals, with 30 to 40 companies in a price war for memory-expansion cards, for example. \"PC Magazine\" renamed its planned \"1001 Products to Use with Your IBM PC\" special issue after the number of product listings it received exceeded the figure. Tecmar and other companies that benefited from IBM's openness rapidly grew in size and importance, as did \"PC Magazine\"; within two years it expanded from 96 bimonthly to 800 monthly pages, including almost 500 pages of advertisements.\n\nGates estimated that IBM would sell \"not far from 200,000\" PCs in 1982. The company did so; by the end of that year it was selling one every minute of the business day. The company estimated that 50 to 70% of PCs sold in retail stores went to the home, and the publicity from selling a popular product to consumers caused IBM to, a spokesman said, \"enter the world\" by familiarizing them with the Colossus of Armonk. Although the PC only provided two to three percent of sales the company found that demand had exceeded its estimate by as much as 800%. Because its prices were based on forecasts of much lower volume—250,000 over five years, which would have made the PC a very successful IBM product—the PC became very profitable; at times the company sold almost that many computers per month. Estridge claimed in 1983 that from October 1982 to March 1983 customer demand quadrupled. He stated that the company had increased production three times in one year, and warned of a component shortage if demand continued to increase. Many small suppliers' sales to IBM grew rapidly, both pleasing their executives and causing them to worry about being overdependent on it. Miniscribe, for example, in 1983 received 61% of its hard drive orders from IBM; the company's stock price fell by more than one third in one day after IBM reduced orders in January 1984. Suppliers often found, however, that the prestige of having IBM as a customer led to additional sales elsewhere.\n\nBy mid-1983, Yankee Group estimated that ten new IBM PC-related products appeared every day. In August the Chess IBU, with 4,000 employees, became the Entry Systems Division, which observers believed indicated that the PC was significantly important to IBM overall, and no longer an experiment. In November the Associated Press stated that the PC \"in two years [had] effectively set a new standard in desktop computers\". It surpassed the Apple II as the best-selling personal computer with more than 750,000 sold by the end of the year, while DEC only sold 69,000 microcomputers in the first nine months of the year despite offering three models for different markets. Retailers also benefited, with 65% of BusinessLand's revenue coming from the PC. Demand still so exceeded supply two years after its debut that, despite IBM shipping 40,000 PCs a month, dealers reportedly received 60% or less of their desired quantity. Pournelle received the PC he paid for in early July 1983 on 1 November, and IBM Boca Raton employees and neighbors had to wait five weeks to buy the computers assembled there.\n\nYankee Group also stated that the PC had by 1983 \"destroyed the market for some older machines\" from companies like Vector Graphic, North Star, and Cromemco. \"inCider\" wrote \"This may be an Apple magazine, but let's not kid ourselves, IBM has devoured competitors like a cloud of locusts\". By February 1984 \"BYTE\" reported on \"the phenomenal market acceptance of the IBM PC\", and by fall concluded that the company \"has given the field its third major standard, after the Apple II and CP/M\". Some rivals speculated that the government might again prosecute IBM for antitrust, and Ben Rosen claimed that the company's dominance \"is having a chilling effect on new ventures, a fear factor\".\n\nBy that time, Apple was less welcoming of the rival that \"inCider\" stated had a \"godlike\" reputation. Its focus on the III had delayed improvements to the II, and the sophisticated Lisa was unsuccessful in part because, unlike the II and the PC, Apple discouraged third-party developers. The head of a retail chain said \"It appears that IBM had a better understanding of why the Apple II was successful than had Apple.\" Jobs, after trying to recruit Estridge to become Apple's president, admitted that in two years IBM had joined Apple as \"the industry's two strongest competitors\". He warned in a speech before previewing the forthcoming \"1984\" Super Bowl commercial: \"It appears IBM wants it \"all\" ... Will Big Blue dominate the entire computer industry? The entire information age? Was George Orwell right about 1984?\"\n\nIBM had $4 billion in annual PC revenue by 1984, more than twice that of Apple and as much as the sales of Apple, Commodore, HP, and Sperry combined, and 6% of total revenue. A \"Fortune\" survey found that 56% of American companies with personal computers used IBM PCs, compared to Apple's 16%. A 1983 study of corporate customers similarly found that two thirds of large customers standardizing on one computer chose the PC, compared to 9% for Apple. IBM had defeated UNIVAC with an inferior computer; IBM's own documentation similarly described the PC as inferior to competitors' less-expensive products. The company generally did not compete on price; rather, the study found that customers preferred \"IBM's hegemony\" because of its support. Most companies with mainframes used their PCs with the larger computers, which likely benefited IBM's mainframe sales and discouraged their purchasing non-IBM hardware.\n\nIn 1984, IBM introduced the PC/AT, unlike its predecessor the most sophisticated personal computer from any major company. By 1985, the PC family had more than doubled Future Computing's 1986 revenue estimate, with more than 12,000 applications and 4,500 dealers and distributors worldwide. The PC was similarly dominant in Europe, two years after release there. In his 1985 obituary, \"The New York Times\" wrote that Estridge had led the \"extraordinarily successful entry of the International Business Machines Corporation into the personal computer field\". The Entry Systems Division had 10,000 employees and by itself would have been the world's third-largest computer company behind IBM and DEC, with more revenue than IBM's minicomputer business despite its much later start. IBM was the only major company with significant minicomputer and microcomputer businesses, in part because rivals like DEC and Wang did not adjust to the retail market.\n\nRumors of \"lookalike\", compatible computers, created without IBM's approval, began almost immediately after the IBM PC's release. Other manufacturers soon reverse engineered the BIOS to produce their own non-infringing functional copies. Columbia Data Products introduced the first IBM-PC compatible computer in June 1982. In November 1982, Compaq Computer Corporation announced the \"Compaq Portable\", the first portable IBM PC compatible. The first models were shipped in January 1983.\n\nThe success of the IBM computer led other companies to develop \"IBM Compatibles\", which in turn led to branding like diskettes being advertised as \"IBM format\". An IBM PC clone could be built with off-the-shelf parts, but the BIOS required some reverse engineering. Companies like Compaq, Phoenix Software Associates, American Megatrends, Award, and others achieved fully functional versions of the BIOS, allowing companies like Dell, Gateway and HP to manufacture PCs that worked like IBM's product. The IBM PC became the industry standard.\n\nBecause IBM had no retail experience, the retail chains ComputerLand and Sears Roebuck provided important knowledge of the marketplace. They became the main outlets for the new product. More than 190 ComputerLand stores already existed, while Sears was in the process of creating a handful of in-store computer centers for sale of the new product. This guaranteed IBM widespread distribution across the U.S.\n\nTargeting the new PC at the home market, Sears Roebuck sales failed to live up to expectations. This unfavorable outcome revealed that the strategy of targeting the office market was the key to higher sales.\n\n+ The IBM PC line\n! Model name!!Model #!!Introduced!!CPU!! style=\"text-align:left;\"Features\n\nAll IBM personal computers are software backwards-compatible with each other in general, but not every program will work in every machine. Some programs are time sensitive to a particular speed class. Older programs will not take advantage of newer higher-resolution and higher-color display standards, while some newer programs require newer display adapters. (Note that as the display adapter was an adapter card in all of these IBM models, newer display hardware could easily be, and often was, retrofitted to older models.) A few programs, typically very early ones, are written for and require a specific version of the IBM PC BIOS ROM. Most notably, BASICA which was dependent on the BIOS ROM had a sister program called GW-BASIC which supported more functions, was 100% backwards compatible and could run independently from the BIOS ROM.\n\nThe CGA video card, with a suitable modulator, could use an NTSC television set or an RGBi monitor for display; IBM's RGBi monitor was their display model 5153. The other option that was offered by IBM was an MDA and their monochrome display model 5151. It was possible to install both an MDA and a CGA card and use both monitors concurrently if supported by the application program. For example, AutoCAD, Lotus 1-2-3 and others allowed use of a CGA Monitor for graphics and a separate monochrome monitor for text menus. Some model 5150 PCs with CGA monitors and a printer port also included the MDA adapter by default, because IBM provided the MDA port and printer port on the same adapter card; it was in fact an MDA/printer port combo card.\n\nAlthough cassette tape was originally envisioned by IBM as a low-budget storage alternative, the most commonly used medium was the floppy disk. The 5150 was available with one or two \" floppy drives – with two drives the program disc(s) would be in drive A, while drive B would hold the disc(s) for working files; with one drive the user had to swap program and file discs into the single drive. For models without any drives or storage medium, IBM intended users to connect their own cassette recorder via the 5150's cassette socket. The cassette tape socket was physically the same DIN plug as the keyboard socket and next to it, but electrically completely different.\n\nA hard disk could not be installed into the 5150's system unit without changing to a higher-rated power supply (although later drives with lower power consumption have been known to work with the standard 63.5 Watt unit). The \"IBM 5161 Expansion Chassis\" came with its own power supply and one 10 MB hard disk and allowed the installation of a second hard disk. The system unit had five expansion slots, and the expansion unit had eight; however, one of the system unit's slots and one of the expansion unit's slots had to be occupied by the Extender Card and Receiver Card, respectively, which were needed to connect the expansion unit to the system unit and make the expansion unit's other slots available, for a total of 11 slots. A working configuration required that some of the slots be occupied by display, disk, and I/O adapters, as none of these were built into the 5150's motherboard; the only motherboard external connectors were the keyboard and cassette ports.\n\nThe simple PC speaker sound hardware was also on board.\n\nThe original PC's maximum memory using IBM parts was 256 kB, achievable through the installation of 64 kB on the motherboard and three 64 kB expansion cards. The processor was an Intel 8088 running at 4.77 MHz, 4/3 the standard NTSC color burst frequency of 315/88 = 3.579 MHz. (In early units, the Intel 8088 used was a 1978 version, later were 1978/81/2 versions of the Intel chip; second-sourced AMDs were used after 1983). Some owners replaced the 8088 with an NEC V20 for a slight increase in processing speed and support for real mode 80186 instructions. The V20 gained its speed increase through the use of a hardware multiplier which the 8088 lacked. An Intel 8087 coprocessor could also be added for hardware floating-point arithmetic.\n\nIBM sold the first IBM PCs in configurations with 16 or 64 kB of RAM preinstalled using either nine or thirty-six 16-kilobit DRAM chips. (The ninth bit was used for parity checking of memory.) In November 1982, the hardware was changed to allow the use of 64-Kbit chips (as opposed to the original 16-Kbit chips) - the same RAM configuration as the soon-to-be-released IBM XT. (64 kB in one bank, expandable to 256kB by populating the other three banks.)\n\nAlthough the TV-compatible video board, cassette port and Federal Communications Commission Class B certification were all aimed at making it a home computer, the original PC proved too expensive for the home market. At introduction, a PC with 64 kB of RAM and a single 5.25-inch floppy drive and monitor sold for (), while the cheapest configuration () that had no floppy drives, only 16 kB RAM, and no monitor (again, under the expectation that users would connect their existing TV sets and cassette recorders) proved too unattractive and low-spec, even for its time (cf. footnotes to the above IBM PC range table). While the 5150 did not become a top selling home computer, its floppy-based configuration became an unexpectedly large success with businesses.\n\nThe \"IBM Personal Computer XT\", IBM model 5160, was introduced two years after the PC and featured a 10 megabyte hard drive. It had eight expansion slots but the same processor and clock speed as the PC. The XT had no cassette jack, but still had the Cassette Basic interpreter in ROMs.\n\nThe XT could take 256 kB of memory on the main board (using 64 kbit DRAM); later models were expandable to 640 kB. The remaining 384 kilobytes of the 8088 address space (between 640 KB and 1 MB) were used for the BIOS ROM, adapter ROM and RAM space, including video RAM space. It was usually sold with a Monochrome Display Adapter (MDA) video card or a CGA video card.\n\nThe eight expansion slots were the same as the model 5150 but were spaced closer together. Although rare, a card designed for the 5150 could be wide enough to obstruct the adjacent slot in an XT. Because of the spacing, an XT motherboard would not fit into a case designed for the PC motherboard, but the slots and peripheral cards were compatible. The XT expansion bus (later called \"8-bit Industry Standard Architecture\" (ISA) by competitors) was retained in the IBM AT, which added connectors for some slots to allow 16-bit transfers; 8-bit cards could be used in an AT.\n\nThe \"IBM Personal Computer XT/370\" was an XT with three custom 8-bit cards: the processor card (370PC-P) contained a modified Motorola 68000 chip, microcoded to execute System/370 instructions, a second 68000 to handle bus arbitration and memory transfers, and a modified 8087 to emulate the S/370 floating point instructions. The second card (370PC-M) connected to the first and contained 512 kB of memory. The third card (PC3277-EM), was a 3270 terminal emulator necessary to install the system software for the VM/PC software to run the processors.\n\nThe computer booted into DOS, then ran the VM/PC Control Program.\n\nThe \"IBM PCjr\" was IBM's first attempt to enter the market for relatively inexpensive educational and home-use personal computers. The PCjr, IBM model number 4860, retained the IBM PC's 8088 CPU and BIOS interface for compatibility, but its cost and differences in the PCjr's architecture, as well as other design and implementation decisions (chief among these was the use of a \"chiclet\" keyboard, which was difficult to type with), eventually led to the PCjr, and the related IBM JX, being commercial failures.\n\nThe \"IBM Portable Personal Computer\" 5155 model 68 was an early portable computer developed by IBM after the success of Compaq's suitcase-size portable machine (the Compaq Portable). It was released in February 1984, and was eventually replaced by the IBM Convertible.\n\nThe Portable was an XT motherboard, transplanted into a Compaq-style luggable case. The system featured 256 kilobytes of memory (expandable to 512 KB), an added CGA card connected to an internal monochrome (amber) composite monitor, and one or two half-height 5.25\" 360 KB floppy disk drives. Unlike the Compaq Portable, which used a dual-mode monitor and special display card, IBM used a stock CGA board and a composite monitor, which had lower resolution. It could however, display color if connected to an external monitor or television.\n\nThe \"IBM Personal Computer/AT\" (model 5170), announced August 15, 1984, used an Intel 80286 processor, originally running at 6 MHz. It had a 16-bit ISA bus and 20 MB hard drive. A faster model, running at 8 MHz and sporting a 30-megabyte hard disk was introduced in 1986.\n\nThe AT was designed to support multitasking; the new SysRq (system request) key, little noted and often overlooked, is part of this design, as is the 80286 itself, the first Intel 16-bit processor with multitasking features (i.e. the 80286 protected mode). IBM made some attempt at marketing the AT as a multi-user machine, but it sold mainly as a faster PC for power users. For the most part, IBM PC/ATs were used as more powerful DOS (single-tasking) personal computers, in the literal sense of the PC name.\n\nEarly PC/ATs were plagued with reliability problems, in part because of some software and hardware incompatibilities, but mostly related to the internal 20 MB hard disk, and High Density Floppy Disk Drive.\n\nWhile some people blamed IBM's hard disk controller card and others blamed the hard disk manufacturer Computer Memories Inc. (CMI), the IBM controller card worked fine with other drives, including CMI's 33-MB model. The problems introduced doubt about the computer and, for a while, even about the 286 architecture in general, but after IBM replaced the 20 MB CMI drives, the PC/AT proved reliable and became a lasting industry standard.\n\nIBM AT's Drive parameter table listed the CMI-33 as having 615 cylinders instead of the 640 the drive was designed with, as to make the size an even 30 MB. Those who re-used the drives mostly found that the 616th cylinder was bad due to it being used as a landing area.\n\nThe \"IBM Personal Computer AT/370\" was an AT with two custom 16-bit cards, running almost exactly the same setup as the XT/370.\n\nThe IBM PC Convertible, released April 3, 1986, was IBM's first laptop computer and was also the first IBM computer to utilize the 3.5\" floppy disk which went on to become the standard. Like modern laptops, it featured power management and the ability to run from batteries. It was the follow-up to the IBM Portable and was model number 5140. The concept and the design of the body was made by the German industrial designer Richard Sapper.\n\nIt utilized an Intel 80c88 CPU (a CMOS version of the Intel 8088) running at 4.77 MHz, 256 kB of RAM (expandable to 640 kB), dual 720 kB 3.5\" floppy drives, and a monochrome CGA-compatible LCD screen at a price of $2,000. It weighed and featured a built-in carrying handle.\n\nThe PC Convertible had expansion capabilities through a proprietary ISA bus-based port on the rear of the machine. Extension modules, including a small printer and a video output module, could be snapped into place. The machine could also take an internal modem, but there was no room for an internal hard disk.\n\nThe IBM PS/2 line was introduced in 1987. The Model 30 at the bottom end of the lineup was very similar to earlier models; it used an 8086 processor and an ISA bus. The Model 30 was not \"IBM compatible\" in that it did not have standard 5.25-inch drive bays; it came with a 3.5-inch floppy drive and optionally a 3.5-inch-sized hard disk. Most models in the PS/2 line further departed from \"IBM compatible\" by replacing the ISA bus completely with Micro Channel Architecture. The MCA bus was not received well by the customer base for PC's, since it was proprietary to IBM. It was rarely implemented by any of the other PC-compatible makers. Eventually IBM would abandon this architecture entirely and return to the standard ISA bus.\n\nThe main circuit board in a PC is called the motherboard (IBM terminology calls it a \"planar\"). This mainly carries the CPU and RAM, and has a bus with slots for expansion cards. Also on the motherboard are the ROM subsystem, DMA and IRQ controllers, coprocessor socket, sound (PC speaker, tone generation) circuitry, and keyboard interface. The original PC also has a cassette interface.\n\nThe bus used in the original PC became very popular, and it was later named ISA. It was originally known as the PC-bus or XT-bus; the term \"ISA\" arose later when industry leaders chose to continue manufacturing machines based on the IBM PC AT architecture rather than license the PS/2 architecture and its Micro Channel bus from IBM. The XT-bus was then retroactively named \"8-bit ISA\" or \"XT ISA\", while the unqualified term \"ISA\" usually refers to the 16-bit AT-bus (as better defined in the ISA specifications). The AT-bus is an extension of the PC-/XT-bus and is in use to this day in computers for industrial use, where its relatively low speed, 5-volt signals, and relatively simple, straightforward design (all by year 2011 standards) give it technical advantages (e.g. noise immunity for reliability).\nA monitor and any floppy or hard disk drives are connected to the motherboard through cables connected to graphics adapter and disk controller cards, respectively, installed in expansion slots. Each expansion slot on the motherboard has a corresponding opening in the back of the computer case through which the card can expose connectors; a blank metal cover plate covers this case opening (to prevent dust and debris intrusion and control airflow) when no expansion card is installed. Memory expansion beyond the amount installable on the motherboard was also done with boards installed in expansion slots, and I/O devices such as parallel, serial, or network ports were likewise installed as individual expansion boards. For this reason, it was easy to fill the five expansion slots of the PC, or even the eight slots of the XT, even without installing any special hardware. Companies like Quadram and AST addressed this with their popular multi-I/O cards which combine several peripherals on one adapter card that uses only one slot; Quadram offered the QuadBoard and AST the SixPak.\n\nIntel 8086 and 8088-based PCs require expanded memory (EMS) boards to work with more than 640 kB of memory. (Though the 8088 can address one megabyte of memory, the last 384 kB of that is used or reserved for the BIOS ROM, BASIC ROM, extension ROMs installed on adapter cards, and memory address space used by devices including display adapter RAM and even the 64 kB EMS page frame itself.) The original IBM PC AT used an Intel 80286 processor which can access up to 16 MB of memory (though standard DOS applications cannot use more than one megabyte without using additional APIs). Intel 80286-based computers running under OS/2 can work with the maximum memory.\n\nThe set of peripheral chips selected for the original IBM PC defined the functionality of an IBM compatible. These became the de facto base for later application-specific integrated circuits (ASICs) used in compatible products.\n\nThe original system chips were one Intel 8259 programmable interrupt controller (PIC) (at I/O address ), one Intel 8237 direct memory access (DMA) controller (at I/O address ), and an Intel 8253 programmable interval timer (PIT) (at I/O address ). The PIT provides the clock ticks, dynamic memory refresh timing, and can be used for speaker output; one DMA channel is used to perform the memory refresh.\n\nThe mathematics coprocessor was the Intel 8087 using I/O address 0xF0. This was an option for users who needed extensive floating-point arithmetic, such as users of computer-aided drafting.\n\nThe IBM PC AT added a second, slave 8259 PIC (at I/O address ), a second 8237 DMA controller for 16-bit DMA (at I/O address ), a DMA address register (implemented with a 74LS612 IC) (at I/O address ), and a Motorola MC146818 real-time clock (RTC) with nonvolatile memory (NVRAM) used for system configuration (replacing the DIP switches and jumpers used for this purpose in PC and PC/XT models (at I/O address ). On expansion cards, the Intel 8255 programmable peripheral interface (PPI) (at I/O addresses is used for parallel I/O controls the printer, and the 8250 universal asynchronous receiver/transmitter (UART) (at I/O address or ) controls the serial communication at the (pseudo-) RS-232 port.\n\nIBM offered a Game Control Adapter for the PC, which supported analog joysticks similar to those on the Apple II. Although analog controls proved inferior for arcade-style games, they were an asset in certain other genres such as flight simulators. The joystick port on the IBM PC supported two controllers, but required a Y-splitter cable to connect both at once. It remained the standard joystick interface on IBM compatibles until being replaced by USB during the 2000s.\n\nThe keyboard that came with the IBM 5150 was an extremely reliable and high-quality electronic keyboard originally developed in North Carolina for the Datamaster. Each key was rated to be reliable to over 100 million keystrokes. For the IBM PC, a separate keyboard housing was designed with a novel usability feature that allowed users to adjust the keyboard angle for personal comfort. Compared with the keyboards of other small computers at the time, the IBM PC keyboard was far superior and played a significant role in establishing a high-quality impression. For example, the industrial design of the adjustable keyboard, together with the system unit, was recognized with a major design award. \"Byte\" magazine in the fall of 1981 went so far as to state that the keyboard was 50% of the reason to buy an IBM PC. The importance of the keyboard was definitely established when the 1983 IBM PCjr flopped, in very large part for having a much different and mediocre Chiclet keyboard that made a poor impression on customers. Oddly enough, the same thing almost happened to the original IBM PC when in early 1981 management seriously considered substituting a cheaper and lower quality keyboard. This mistake was narrowly avoided on the advice of one of the original development engineers.\n\nHowever, the original 1981 IBM PC 83-key keyboard was criticized by typists for its non-standard placement of the and left keys, and because it did not have separate cursor and numeric pads that were popular on the pre-PC DEC VT100 series video terminals. In 1982, Key Tronic introduced a 101-key PC keyboard, albeit not with the now-familiar layout. In 1984, IBM corrected the and left keys on its AT keyboard, but shortened the key, making it harder to reach. In 1986, IBM introduced the 101 key Enhanced Keyboard, which added the separate cursor and numeric key pads, relocated all the function keys and the keys, and the key was also relocated to the opposite side of the keyboard. The Enhanced Keyboard was an option for the PC XT/AT in 1986, both of which were also available with their original keyboards, and introduced the key layout that's still the industry standard.\n\nAnother feature of the original keyboard is the relatively loud \"click\" sound each key made when pressed. Since typewriter users were accustomed to keeping their eyes on the hardcopy they were typing from and had come to rely on the mechanical sound that was made as each character was typed onto the paper to ensure that they had pressed the key hard enough (and only once), the PC keyboard used a keyswitch that produced a click and tactile bump intended to provide that same reassurance.\n\nThe IBM PC keyboard is very robust and flexible. The low-level interface for each key is the same: each key sends a signal when it is pressed and another signal when it is released. An integrated microcontroller in the keyboard scans the keyboard and encodes a \"scan code\" and \"release code\" for each key as it is pressed and released separately. Any key can be used as a shift key, and a large number of keys can be held down simultaneously and separately sensed. The controller in the keyboard handles typematic operation, issuing periodic repeat scan codes for a depressed key and then a single release code when the key is finally released.\n\nAn \"IBM PC compatible\" may have a keyboard that does not recognize every key combination a true IBM PC does, such as shifted cursor keys. In addition, the \"compatible\" vendors sometimes used proprietary keyboard interfaces, preventing the keyboard from being replaced.\n\nAlthough the PC/XT and AT used the same style of keyboard connector, the low-level protocol for reading the keyboard was different between these two series. The AT keyboard uses a bidirectional interface which allows the computer to send commands to the keyboard. An AT keyboard could not be used in an XT, nor the reverse. Third-party keyboard manufacturers provided a switch on some of their keyboards to select either the AT-style or XT-style protocol for the keyboard.\n\nThe original IBM PC used the 7-bit ASCII alphabet as its basis, but extended it to 8 bits with nonstandard character codes. This character set was not suitable for some international applications, and soon a veritable cottage industry emerged providing variants of the original character set in various national variants. In IBM tradition, these variants were called code pages. These codings are now obsolete, having been replaced by more systematic and standardized forms of character coding, such as ISO 8859-1, Windows-1251 and Unicode. The original character set is known as code page 437.\n\nIBM equipped the model 5150 with a cassette port for connecting a cassette drive and assumed that home users would purchase the low-end model and save files to cassette tapes as was typical of home computers of the time. However, adoption of the floppy- and monitor-less configuration was low; few (if any) IBM PCs left the factory without a floppy disk drive installed. Also, DOS was not available on cassette tape, only on floppy disks (hence \"Disk Operating System\"). 5150s with just external cassette recorders for storage could only use the built-in ROM BASIC as their operating system. As DOS saw increasing adoption, the incompatibility of DOS programs with PCs that used only cassettes for storage made this configuration even less attractive. The ROM BIOS supported cassette operations.\n\nThe IBM PC cassette interface encodes data using frequency modulation with a variable data rate. Either a one or a zero is represented by a single cycle of a square wave, but the square wave frequencies differ by a factor of two, with ones having the lower frequency. Therefore, the bit periods for zeros and ones also differ by a factor of two, with the unusual effect that a data stream with more zeros than ones will use less tape (and time) than an equal-length (in bits) data stream containing more ones than zeros, or equal numbers of each.\n\nIBM also had an exclusive license agreement with Microsoft to include BASIC in the ROM of the PC; clone manufacturers could not have ROM BASIC on their machines, but it also became a problem as the XT, AT, and PS/2 eliminated the cassette port and IBM was still required to install the (now useless) BASIC with them. The agreement finally expired in 1991 when Microsoft replaced BASICA/GW-BASIC with QBASIC. The main core BASIC resided in ROM and \"linked\" up with the RAM-resident BASIC.COM/BASICA.COM included with PC DOS (they provided disk support and other extended features not present in ROM BASIC). Because BASIC was over 50 kB in size, this served a useful function during the first three years of the PC when machines only had 64–128 kB of memory, but became less important by 1985. For comparison, clone makers such as Compaq were forced to include a version of BASIC that resided entirely in RAM.\n\nThe first IBM 5150 PCs had two 5.25-inch 160 KiB single sided double density (SSDD) floppy disk drives. As two heads drives became available in the spring of 1982, later IBM PC and compatible computers could read 320 KiB double sided double density (DSDD) disks with software support of MS-DOS 1.25 and higher. The same type of physical diskette media could be used for both drives but a disk formatted for double-sided use could not be read on a single-sided drive. PC DOS 2.0 added support for 180 KiB and 360 KiB SSDD and DSDD floppy disks, using the same physical media again.\n\nThe disks were Modified Frequency Modulation (MFM) coded in 512-byte sectors, and were soft-sectored. They contained 40 tracks per side at the 48 track per inch (TPI) density, and initially were formatted to contain eight sectors per track. This meant that SSDD disks initially had a formatted capacity of 160 kB, while DSDD disks had a capacity of 320 kB. However, the PC DOS 2.0 and later operating systems allowed formatting the disks with nine sectors per track. This yielded a formatted capacity of 180 kB with SSDD disks/drives, and 360 kB with DSDD disks/drives. The \"unformatted\" capacity of the floppy disks was advertised as \"250KB\" for SSDD and \"500KB\" for DSDD (\"KB\" ambiguously referring to either 1000 or 1024 bytes; essentially the same for rounded-off values), however these \"raw\" 250/500 kB were not the same thing as the usable formatted capacity; under DOS, the maximum capacity for SSDD and DSDD disks was 180 kB and 360 kB, respectively. Regardless of type, the file system of all floppy disks (under DOS) was FAT12.\n\nAfter the upgraded 64k-256k motherboard PCs arrived in early 1983, single-sided drives and the cassette model were discontinued.\n\nIBM's original floppy disk controller card also included an external 37-pin D-shell connector. This allowed users to connect additional external floppy drives by third party vendors, but IBM did not offer their own external floppies until 1986.\n\nThe industry-standard way of setting floppy drive numbers was via setting jumper switches on the drive unit, however IBM chose to instead use a method known as the \"cable twist\" which had a floppy data cable with a bend in the middle of it that served as a switch for the drive motor control. This eliminated the need for users to adjust jumpers while installing a floppy drive.\n\nThe 5150 could not itself power hard drives without retrofitting a stronger power supply, but IBM later offered the 5161 Expansion Unit, which not only provided more expansion slots, but also included a 10 MB (later 20 MB) hard drive powered by the 5161's own separate 130-watt power supply. The IBM 5161 Expansion Unit was released in early 1983.\n\nDuring the first year of the IBM PC, it was commonplace for users to install third-party Winchester hard disks which generally connected to the floppy controller and required a patched version of PC DOS which treated them as a giant floppy disk (there was no subdirectory support).\n\nIBM began offering hard disks with the XT, however the original PC was never sold with them. Nonetheless, many users installed hard disks and upgraded power supplies in them.\n\nAfter floppy disks became obsolete in the early 2000s, the letters A and B became unused. But for 25 years, virtually all DOS-based PC software assumed the program installation drive was C, so the primary HDD continues to be \"the C drive\" even today.\nOther operating system families (e.g. Unix) are not bound to these designations.\n\nWhich operating system IBM customers would choose was at first unclear. Although the company expected that most would use PC DOS IBM supported using CP/M-86—which became available six months after DOS—or UCSD p-System as operating systems. IBM promised that it would not favor one operating system over the others; the CP/M-86 support surprised Gates, who claimed that IBM was \"blackmailed into it\". IBM was correct, nonetheless, in its expectation; one survey found that 96.3% of PCs were ordered with the $40 DOS compared to 3.4% for the $240 CP/M-86.\n\nThe IBM PC's ROM BASIC and BIOS supported cassette tape storage. PC DOS itself did not support cassette tape storage. PC DOS version 1.00 supported only 160 kB SSDD floppies, but version 1.1, which was released nine months after the PC's introduction, supported 160 kB SSDD and 320 kB DSDD floppies. Support for the slightly larger nine sector per track 180 kB and 360 kB formats arrived 10 months later in March 1983.\n\nThe BIOS (Basic Input/Output System) provided the core ROM code for the PC. It contained a library of functions that software could call for basic tasks such as video output, keyboard input, and disk access in addition to interrupt handling, loading the operating system on boot-up, and testing memory and other system components.\n\nThe original IBM PC BIOS was 8k in size and occupied four 2k ROM chips on the motherboard, with a fifth and sixth empty slot left for any extra ROMs the user wished to install. IBM offered three different BIOS revisions during the PC's lifespan. The initial BIOS was dated April 1981 and came on the earliest models with single-sided floppy drives and PC DOS 1.00. The second version was dated October 1981 and arrived on the \"Revision B\" models sold with double-sided drives and PC DOS 1.10. It corrected some bugs, but was otherwise unchanged. Finally, the third BIOS version was dated October 1982 and found on all IBM PCs with the newer 64k-256k motherboard. This revision was more-or-less identical to the XT's BIOS. It added support for detecting ROMs on expansion cards as well as the ability to use 640k of memory (the earlier BIOS revisions had a limit of 544k). Unlike the XT, the original PC remained functionally unchanged from 1983 until its discontinuation in early 1987 and did not get support for 101-key keyboards or 3.5\" floppy drives, nor was it ever offered with half-height floppies.\n\nIBM initially offered two video adapters for the PC, the Color/Graphics Adapter and the Monochrome Display and Printer Adapter. CGA was intended to be a typical home computer display; it had NTSC output and could be connected to a composite monitor or a TV set with an RF modulator in addition to RGB for digital RGBI-type monitors, although IBM did not offer their own RGB monitor until 1983. Supported graphics modes were 40 or 80×25 color text with 8×8 character resolution, 320×200 bitmap graphics with two fixed 4-color palettes, or 640×200 monochrome graphics.\n\nThe MDA card and its companion 5151 monitor supported only 80×25 text with a 9×14 character resolution (total pixel resolution was 720×350). It was mainly intended for the business market and so also included a printer port.\n\nDuring 1982, the first third-party video card for the PC appeared when Hercules Computer Technologies released a clone of the MDA that could use bitmap graphics. Although not supported by the BIOS, the Hercules Graphics Adapter became extremely popular for business use due to allowing sharp, high resolution graphics plus text and itself was widely cloned by other manufacturers.\n\nIn 1985, after the launch of the IBM AT, the new Enhanced Graphics Adapter became available which could support 320×200 or 640×200 in 16 colors in addition to high-resolution 640×350 16 color graphics.\n\nIBM also offered a video board for the PC, XT, and AT known as the Professional Graphics Adapter during 1984–86, mainly intended for CAD design. It was extremely expensive, required a special monitor, and was rarely ordered by customers.\n\nVGA graphics cards could also be installed in IBM PCs and XTs, although they were introduced after the computer's discontinuation.\n\nThe serial port is an 8250 or a derivative (such as the 16450 or 16550), mapped to eight consecutive IO addresses and one interrupt request line.\n\n! COM port\n! IRQ\n! Base port address [Hex]\n\nOnly COM1: and COM2: addresses were defined by the original PC. Attempts to share IRQ3 and IRQ4 to use additional ports require special measures in hardware and software, since shared IRQs were not defined in the original PC design. The most typical devices plugged into the serial port were modems and mice. Plotters and serial printers were also among the more commonly used serial peripherals, and there were numerous other more unusual uses such as operating cash registers, factory equipment, and connecting terminals.\n\nIBM made a deal with Japan-based Epson to produce printers for the PC and all IBM-branded printers were manufactured by that company (Epson of course also sold printers with their own name). There was a considerable amount of controversy when IBM included a printer port on the PC that did not follow the industry-standard Centronics design, and it was rumored that this had been done to prevent customers from using non-Epson/IBM printers with their machines (plugging a Centronics printer into an IBM PC could damage the printer, the parallel port, or both). Although third-party cards were available with Centronics ports on them, PC clones quickly copied the IBM printer port and by the late 80s, it had largely displaced the Centronics standard.\n\n\"BYTE\" wrote in October 1981 that the IBM PC's \"hardware is impressive, but even more striking are two decisions made by IBM: to use outside suppliers already established in the microcomputer industry, and to provide information and assistance to independent, small-scale software writers and manufacturers of peripheral devices\". It praised the \"smart\" hardware design and stated that its price was not much higher than the 8-bit machines from Apple and others. The reviewer admitted that the computer \"came as a shock. I expected that the giant would stumble by overestimating or underestimating the capabilities the public wants and stubbornly insisting on incompatibility with the rest of the microcomputer world. But IBM didn't stumble at all; instead, the giant jumped leagues in front of the competition ... the only disappointment about the IBM Personal Computer is its dull name\".\n\nIn a more detailed review in January 1982, \"BYTE\" called the IBM PC \"a synthesis of the best the microcomputer industry has offered to date ... as well designed on the inside as it is on the outside\". The magazine praised the keyboard as \"bar none, the best ... on any microcomputer\", describing the unusual Shift key locations as \"minor [problems] compared to some of the gigantic mistakes made on almost every other microcomputer keyboard\". The review also complimented IBM's manuals, which it predicted \"will set the standard for all microcomputer documentation in the future. Not only are they well packaged, well organized, and easy to understand, but they are also \"complete\"\". Observing that detailed technical information was available \"much earlier ... than it has been for other machines\", the magazine predicted that \"given a reasonable period of time, plenty of hardware and software will probably be developed for\" the computer. The review stated that although the IBM PC cost more than comparably configured Apple II and TRS-80 computers, and the insufficient number of slots for all desirable expansion cards was its most serious weakness, \"you get a \"lot\" more for your money\" and concluded, \"In two years or so, I think [it] will be one of the most popular and best-supported ... IBM should be proud of the people who designed it\".\n\nIn a special 1984 issue dedicated to the IBM PC, \"BYTE\" concluded that the PC had succeeded both because of its features like an 80-column screen, open architecture, and high-quality keyboard, and \"the failure of other major companies to provide these same fundamental features earlier. In retrospect, it seems IBM stepped into a void that remained, paradoxically, at the center of a crowded market\". \"Creative Computing\" that year named the PC the best desktop computer between $2000 and $4000, praising its vast hardware and software selection, manufacturer support, and resale value.\n\nMany IBM PCs have remained in service long after their technology became largely obsolete. In June 2006, IBM PC and XT models were still in use at the majority of U.S. National Weather Service upper-air observing sites, used to process data as it is returned from the ascending radiosonde, attached to a weather balloon, although they have been slowly phased out. Factors that have contributed to the 5150 PC's longevity are its flexible modular design, its open technical standard (making information needed to adapt, modify, and repair it readily available), use of few special nonstandard parts, and rugged high-standard IBM manufacturing, which provided for exceptional long-term reliability and durability.\n\nSome of the mechanical aspects of the slot specifications are still used in current PCs. A few systems still come with PS/2 style keyboard and mouse connectors.\n\nThe IBM model 5150 Personal Computer has become a collectable among vintage computer collectors, due to the system being the first true “PC” as we know them today. , the system had a market value of $50–$500. The IBM model 5150 has proven to be reliable; despite their age of 30 years or more, some still function as they did when new.\n\nBULLET::::- 386SLC\nBULLET::::- Apple Macintosh\nBULLET::::- Commodore Amiga\nBULLET::::- Atari ST\nBULLET::::- Conventional memory\nBULLET::::- IBM 5120\nBULLET::::- IBM Aptiva\nBULLET::::- IBM PCjr\nBULLET::::- IBM Portable Personal Computer\nBULLET::::- IBM token ring networks\nBULLET::::- Input/Output Base Address\nBULLET::::- List of IBM products\nBULLET::::- ThinkCentre\nBULLET::::- ThinkPad\nBULLET::::- Cited references\n\nBULLET::::- General references\n\nBULLET::::- Norton, Peter (1986). \"Inside the IBM PC. Revised and enlarged\". New York. Brady. .\nBULLET::::- August 12, 1981 press release announcing the IBM PC (PDF format).\nBULLET::::- Mueller, Scott (1992). \"Upgrading and Repairing PCs, Second Edition\", Que Books,\nBULLET::::- IBM (1983). \"Personal Computer Hardware Reference Library: Guide to Operations, Personal Computer XT\". IBM Part Number 6936831.\nBULLET::::- IBM (1984). \"Personal Computer Hardware Reference Library: Guide to Operations, Portable Personal Computer\". IBM Part Numbers 6936571 and 1502332.\nBULLET::::- IBM (1986). \"Personal Computer Hardware Reference Library: Guide to Operations, Personal Computer XT Model 286\". IBM Part Number 68X2523.\n\nBULLET::::- \"Birth of the IBM PC\", IBM Corporation History Archives website\n\nBULLET::::- IBM SCAMP\nBULLET::::- IBM 5150 information at www.minuszerodegrees.net\nBULLET::::- IBM PC 5150 System Disks and ROMs\nBULLET::::- IBM PC from IT Dictionary\nBULLET::::- IBM PC history and technical information\nBULLET::::- What a legacy! The IBM PC's 25 year legacy\nBULLET::::- CNN.com - IBM PC turns 25\nBULLET::::- IBM-5150 and collection of old digital and analog computers at oldcomputermuseum.com\nBULLET::::- IBM PC images and information\nBULLET::::- A brochure from November, 1982 advertising the IBM PC\nBULLET::::- A Picture of the XT/370 cards, showing the dual 68000 processors\n"}
{"id": "15033", "url": "https://en.wikipedia.org/wiki?curid=15033", "title": "Counties of Ireland", "text": "Counties of Ireland\n\nThe counties of Ireland (; Ulster-Scots: \"coonties o Airlann\") are sub-national divisions that have been, and in some cases continue to be, used to geographically demarcate areas of local government. These land divisions were formed following the Norman invasion of Ireland in imitation of the counties then in use as units of local government in the Kingdom of England. The older term ‘shire’ was historically equivalent to ‘county’. The principal function of the county was to impose royal control in the areas of taxation, security and the administration of justice at the local level. Cambro-Norman control was initially limited to the southeastern parts of Ireland; a further four centuries elapsed before the entire island was shired. At the same time, the now obsolete concept of county corporate elevated a small number of towns and cities to a status which was deemed to be no less important than the existing counties in which they lay. This double control mechanism of 32 counties plus 10 counties corporate remained unchanged for a little over two centuries until the early 19th century. Since then, counties have been adapted and in some cases divided by legislation to meet new administrative and political requirements.\n\nThe powers exercised by the Cambro-Norman barons and the Old English nobility waned over time. New offices of political control came to be established at a county level. In the Republic of Ireland, some counties have been split resulting in the creation of new counties. Along with certain defined cities, counties still form the basis for the demarcation of areas of local government in the Republic of Ireland. Currently, there are 26 county level, 3 city level and 2 city and county entities – the modern equivalent of counties corporate – that are used to demarcate areas of local government in the Republic.\n\nIn Northern Ireland, counties are no longer used for local government; districts are instead used. Upon the partition of Ireland in 1921, the county became one of the basic land divisions employed, along with county boroughs.\n\nThe word \"county\" has come to be used in different senses for different purposes. In common usage, many people have in mind the 32 counties that existed prior to 1838 – the so-called traditional counties. However, in official usage in the Republic of Ireland, the term often refers to the 28 modern counties. The term is also conflated with the 31 areas currently used to demarcate areas of local government in the Republic of Ireland at the level of LAU 1.\n\nIn Ireland, usage of the word \"county\" nearly always comes before rather than after the county name; thus \"\"County\" Roscommon\" in Ireland as opposed to \"Roscommon \"County\"\" in Michigan, United States. The former \"King's County\" and \"Queen's County\" were exceptions; these are now County Offaly and County Laois, respectively. The abbreviation Co. is used, as in \"Co. Roscommon\". A further exception occurs in the case of those counties created after 1994 which often drop the word \"county\" entirely, or use it after the name; thus for example internet search engines show many more uses (on Irish sites) of \"Fingal\" than of either \"County Fingal\" or \"Fingal County\". There appears to be no official guidance in the matter, as even the local council uses all three forms. In informal use, the word \"county\" is often dropped except where necessary to distinguish between county and town or city; thus \"Offaly\" rather than \"County Offaly\", but \"County Antrim\" to distinguish it from Antrim town. The synonym \"shire\" is not used for Irish counties, although the Marquessate of Downshire was named in 1789 after County Down.\n\nParts of some towns and cities were exempt from the jurisdiction of the counties that surrounded them. These towns and cities had the status of a County corporate, many granted by Royal Charter, which had all the judicial, administrative and revenue raising powers of the regular counties.\n\nThe political geography of Ireland can be traced with some accuracy from the 6th century. At that time Ireland was divided into a patchwork of petty kingdoms with a fluid political hierarchy which, in general, had three traditional grades of king. The lowest level of political control existed at the level of the \"túath\" (pl. \"túatha\"). A \"túath\" was an autonomous group of people of independent political jurisdiction under a rí túaithe, that is, a local petty king. About 150 such units of government existed. Each \"rí túaithe\" was in turn subject to a regional or \"over-king\" (). There may have been as many as 20 genuine ruiri in Ireland at any time.\n\nA \"king of over-kings\" () was often a provincial () or semi-provincial king to whom several ruiri were subordinate. No more than six genuine rí ruirech were ever contemporary. Usually, only five such \"king of over-kings\" existed contemporaneously and so are described in the Irish annals as \"fifths\" (). The areas under the control of these kings were: Ulster (), Leinster (), Connacht (), Munster () and Mide (). Later record-makers dubbed them \"provinces\", in imitation of Roman provinces. In the Norman period, the historic fifths of Leinster and Meath gradually merged, mainly due to the impact of the Pale, which straddled both, thereby forming the present-day province of Leinster.\n\nThe use of provinces as divisions of political power was supplanted by the system of counties after the Norman invasion. In modern times clusters of counties have been attributed to certain provinces but these clusters have no legal status. They are today seen mainly in a sporting context, as Ireland's four professional rugby teams play under the names of the provinces, and the Gaelic Athletic Association has separate Provincial councils and Provincial championships.\n\nWith the arrival of Cambro-Norman knights in 1169, the Norman invasion of Ireland commenced. This was followed in 1172 by the invasion of King Henry II of England, commencing English royal involvement.\n\nAfter his intervention in Ireland, Henry II effectively divided the English colony into liberties also known as lordships. These were effectively palatine counties and differed from ordinary counties in that they were disjoined from the crown and that whoever they were granted to essentially had the same authority as the king and that the king's writ had no effect except a writ of error. This covered all land within the county that was not church land. The reasons for the creating of such powerful entities in Ireland was due to the lack of authority the English crown had there. The same process occurred after the Norman conquest of England where despite there being a strong central government, county palatines were needed in border areas with Wales and Scotland. In Ireland this meant that the land was divided and granted to Richard de Clare and his followers who became lords (and sometimes called earls), with the only land which the English crown had any direct control over being the sea-coast towns and territories immediately adjacent.\n\nOf Henry II's grants, at least three of them—Leinster to Richard de Clare; Meath to Walter de Lacy; Ulster to John de Courcy—were equivalent to palatine counties in their bestowing of royal jurisdiction to the grantees. Other grants include the liberties of Connaught and Tipperary.\n\nThese initial lordships were later subdivided into smaller \"liberties\", which appear to have enjoyed the same privileges as their predecessors. The division of Leinster and Munster into smaller counties is commonly attributed to King John, mostly due to lack of prior documentary evidence, which has been destroyed. However, they may have had an earlier origin. These counties were: in Leinster: Carlow (also known as Catherlogh), Dublin, Kildare, Kilkenny, Louth (also known as Uriel), Meath, Wexford, Waterford; in Munster: Cork, Limerick, Kerry and Tipperary. It is thought that these counties did not have the administrative purpose later attached to them until late in the reign of King John, and that no new counties were created until the Tudor dynasty.\n\nThe most important office in those that were palatine was that of seneschal. In those liberties that came under Crown control this office was held by a sheriff. The sovereign could and did appoint sheriffs in palatines; however, their power was confined to the church lands, and they became known as sheriffs of a County of the Cross, of which there seem to have been as many in Ireland as there were counties palatine.\n\nThe exact boundaries of the liberties and shrievalties appears to have been in constant flux throughout the Plantagenet period, seemingly in line with the extent of English control. For example, in 1297 it is recorded that Kildare had extended to include the lands that now comprise the modern-day counties of Offaly, Laois (Leix) and Wicklow (Arklow). Some attempts had also been made to extend the county system to Ulster.\n\nHowever the Bruce Invasion of Ireland in 1315 resulted in the collapse of effective English rule in Ireland, with the land controlled by the crown continually shrinking to encompass Dublin, and parts of Meath, Louth and Kildare. Throughout the rest of Ireland, English rule was upheld by the earls of Desmond, Ormond, and Kildare (all created in the 14th-century), with the extension of the county system all but impossible. During the reign of Edward III (1327–77) all franchises, grants and liberties had been temporarily revoked with power passed to the king's sheriffs over the seneschals. This may have been due to the disorganisation caused by the Bruce invasion as well as the renouncing of the Connaught Burkes of their alligence to the crown.\n\nThe Earls of Ulster divided their territory up into counties; however, these are not considered part of the Crown's shiring of Ireland. In 1333, the Earldom of Ulster is recorded as consisting of seven counties: Antrim, Blathewyc, Cragferus, Coulrath, del Art, Dun (also known as Ladcathel), and Twescard.\n\nOf the original lordships or palatine counties:\nBULLET::::- Leinster had passed from Richard de Clare to his daughter, Isabel de Clare, who had married William Marshal, 1st Earl of Pembroke (second creation of title). This marriage was confirmed by King John, with Isabel's lands given to William as consort. The liberty was afterwards divided into five—Carlow, Kildare, Kilkenny, Leix and Wexford—one for each of Marshal's co-heiresses.\nBULLET::::- Meath was divided between the granddaughters of Walter de Lacy: Maud and Margery. Maud's half became the liberty of Trim, and she married Geoffrey de Geneville. Margery's half retained the name Meath, and she married John de Verdon. After the marriage of Maud's daughter Joan to Roger Mortimer, 1st Earl of March, Trim later passed via their descendants to the English Crown. Meath, which had passed to the Talbots, was resumed by Henry VIII under the Statute of Absentees.\nBULLET::::- Ulster was regranted to the de Lacys from John de Courcy, whilst Connaught, which had been granted to William de Burgh, was at some point divided into the liberties of Connaught and Roscommon. William's grandson Walter de Burgh was in 1264 also made lord of Ulster, bringing both Connaught and Ulster under the same lord. In 1352 Elizabeth de Burgh, 4th Countess of Ulster married Lionel of Antwerp, a son of king Edward III. Their daughter Philippa married Edmund Mortimer, 3rd Earl of March. Upon the death of Edmund Mortimer, 5th Earl of March in 1425, both lordships were inherited by Richard of York, 3rd Duke of York and thus passed to the Crown.\nBULLET::::- Tipperary was resumed by King James I, however under Charles II in 1662 was reconstituted for James Butler, 1st Duke of Ormonde.\n\nWith the passing of liberties to the Crown, the number of Counties of the Cross declined, and only one, Tipperary, survived into the Stuart era; the others had ceased to exist by the reign of Henry VIII.\n\nIt was not until the Tudors, specifically the reign of Henry VIII (1509–47), that crown control started to once again extend throughout Ireland. Having declared himself King of Ireland in 1541, Henry VIII went about converting Irish chiefs into feudal subjects of the crown with land divided into districts, which were eventually amalgamated into the modern counties. County boundaries were still ill-defined; however, in 1543 Meath was split into Meath and Westmeath. Around 1545, the Byrnes and O'Tooles, both native septs who had constantly been a pain for the English administration of the Pale, petitioned the Lord Deputy of Ireland to turn their district into its own county, Wicklow, however this was ignored.\n\nDuring the reigns of the last two Tudor monarchs, Mary I (1553–58) and Elizabeth I (1568–1603), the majority of the work for the foundation of the modern counties was carried out under the auspices of three Lord Deputies: Thomas Radclyffe, 3rd Earl of Sussex, Sir Henry Sydney, and Sir John Perrot.\n\nMary's reign saw the first addition of actual new counties since the reign of King John. Radclyffe had conquered the districts of Glenmaliry, Irry, Leix, Offaly, and Slewmargy from the O'Moores and O'Connors, and in 1556 a statute decreed that Offaly and part of Glenmaliry would be made into the county of King's County, whilst the rest of Glenmarliry along with Irry, Leix and Slewmargy was formed into Queen's County. Radclyffe brought forth legislation to shire all land as yet unshired throughout Ireland and sought to divide the island into six parts—Connaught, Leinster, Meath, Nether Munster, Ulster, and Upper Munster. However, his administrative reign in Ireland was cut short, and it was not until the reign of Mary's successor, Elizabeth, that this legislation was re-adopted. Under Elizabeth, Radclyffe was brought back to implement it.\n\nSydney during his three tenures as Lord Deputy created two presidencies to administer Connaught and Munster. He shired Connaught into the counties of Galway, Mayo, Roscommon, and Sligo. In 1565 the territory of the O'Rourkes within Roscommon was made into the county of Leitrim. In an attempt to reduce the importance of the province of Munster, Sydney, using the River Shannon as a natural boundary took the former kingdom of Thomond (North Munster) and made it into the county of Clare as part of the presidency of Connaught in 1569. A commission headed by Perrot and others in 1571 declared that the territory of Desmond in Munster was to be made a county of itself, and it had its own sheriff appointed, however in 1606 it was merged with the county of Kerry. In 1575 Sydney made an expedition to Ulster to plan its shiring. However, nothing came to bear.\n\nIn 1578 the go-ahead was given for turning the districts of the Byrnes and O'Tooles into the county of Wicklow. However, with the outbreak of war in Munster and then Ulster, they resumed their independence. Sydney also sought to split Wexford into two smaller counties, the northern half of which was to be called Ferns, but the matter was dropped as it was considered impossible to properly administer. The territory of the O'Farrells of Annaly, however, which was in Westmeath, in 1583 was formed into the county of Longford and transferred to Connaught. The Desmond rebellion (1579–83) that was taking place in Munster stopped Sydney's work and by the time it had been defeated Sir John Perrot was now Lord Deputy, being appointed in 1584.\n\nPerrot would be most remembered for shiring the only province of Ireland that remained effectively outside of English control, that of Ulster. Prior to his tenancy the only proper county in Ulster was Louth, which had been part of the Pale. There were two other long recognised entities north of Louth—Antrim and Down—that had at one time been \"counties\" of the Earldom of Ulster and were regarded as apart from the unreformed parts of the province. The date Antrim and Down became constituted is unknown. Perrot was recalled in 1588 and the shiring of Ulster would for two decades basically exist on paper as the territory affected remained firmly outside of English control until the defeat of Hugh O'Neill, Earl of Tyrone in the Nine Years' War. These counties were: Armagh, Cavan, Coleraine, Donegal, Fermanagh, Monaghan, and Tyrone. Cavan was formed from the territory of the O'Reilly's of East Breifne in 1584 and had been transferred from Connaught to Ulster. After O'Neill and his allies fled Ireland in 1607 in the Flight of the Earls, their lands became escheated to the Crown and the county divisions designed by Perrot were used as the basis for the grants of the subsequent Plantation of Ulster effected by King James I, which officially started in 1609.\n\nAround 1600 near the end of Elizabeth's reign, Clare was made an entirely distinct presidency of its own under the Earls of Thomond and would not return to being part of Munster until after the Restoration in 1660.\n\nIt was not until the subjugation of the Byrnes and O'Tooles by Lord Deputy Sir Arthur Chichester that in 1606 Wicklow was finally shired. This county was one of the last to be created, yet was the closest to the center of English power in Ireland.\n\nCounty Londonderry was incorporated in 1613 by the merger of County Coleraine with the barony of Loughinsholin (in County Tyrone), the North West Liberties of Londonderry (in County Donegal), and the North East Liberties of Coleraine (in County Antrim).\n\nThroughout the Elizabethan era and the reign of her successor James I, the exact boundaries of the provinces and the counties they consisted of remained uncertain. In 1598 Meath is considered a province in Hayne's \"Description of Ireland\", and included the counties of Cavan, East Meath, Longford, and Westmeath. This contrasts to George Carew's 1602 survey where there were only four provinces with Longford part of Connaught and Cavan not mentioned at all with only three counties mentioned for Ulster. During Perrot's tenure as Lord President of Munster before he became Lord Deputy, Munster contained as many as eight counties rather than the six it later consisted of. These eight counties were: the five English counties of Cork, Limerick, Kerry, Tipperary, and Waterford; and the three Irish counties of Desmond, Ormond, and Thomond.\n\nPerrot's divisions in Ulster were for the main confirmed by a series of inquisitions between 1606 and 1610 that settled the demarcation of the counties of Connaught and Ulster. John Speed's \"Description of the Kingdom of Ireland\" in 1610 showed that there was still a vagueness over what counties constituted the provinces, however Meath was no longer reckoned a province. By 1616 when the Attorney General for Ireland Sir John Davies departed Ireland, almost all counties had been delimited. The only exception was the county of Tipperary, which still belonged to the palatinate of Ormond.\n\nTipperary would remain an anomaly being in effect two counties, one palatine, the other of the Cross until 1715 during the reign of King George I where an act abolished the \"royalties and liberties of the County of Tipperary\" and \"that whatsoever hath been denominated or called Tipperary or Cross Tipperary, shall henceforth be and remain one county for ever, under the name of the County of Tipperary.\"\n\nTo correspond with the subdivisions of the English shires into honours or baronies, Irish counties were granted out to the Anglo-Norman noblemen in cantreds, later known as baronies, which in turn were subdivided, as in England, into parishes. Parishes were composed of townlands. However, in many cases, these divisions correspond to earlier, pre-Norman, divisions. While there are 331 baronies in Ireland, and more than a thousand civil parishes, there are around sixty thousand townlands that range in size from one to several thousand hectares. Townlands were often traditionally divided into smaller units called \"quarters\", but these subdivisions are not legally defined.\n\nThe following towns/cities had charters specifically granting them the status of a county corporate:\nBULLET::::- County of the Town of Carrickfergus (by 1325)\nBULLET::::- County of the City of Cork (1608)\nBULLET::::- County of the Town of Drogheda (1412)\nBULLET::::- County of the City of Dublin (1548)\nBULLET::::- County of the Town of Galway (1610)\nBULLET::::- County of the City of Kilkenny (1610)\nBULLET::::- County of the City of Limerick (1609)\nBULLET::::- County of the City of Waterford (1574)\nThe only entirely new counties created in 1898 were the county boroughs of Londonderry and Belfast. Carrickfergus, Drogheda and Kilkenny were abolished; Galway was also abolished, but recreated in 1986.\n\nRegional presidencies of Connacht and Munster remained in existence until 1672, with special powers over their subsidiary counties. Tipperary remained a county palatine until the passing of the County Palatine of Tipperary Act 1715, with different officials and procedures from other counties. At the same time, Dublin, until the 19th century, had ecclesiastical liberties with rules outside those applying to the rest of Dublin city and county. Exclaves of the county of Dublin existed in counties Kildare and Wicklow. At least eight other enclaves of one county inside another, or between two others, existed. The various enclaves and exclaves were merged into neighbouring and surrounding counties, primarily in the mid-19th century under a series of Orders in Council.\n\nThe Church of Ireland exercised functions at the level of civil parish that would later be exercised by county authorities. Vestigial feudal power structures of major old estates remained well into the 18th century. Urban corporations operated individual royal charters. Management of counties came to be exercised by grand juries. Members of grand juries were the local payers of rates who historically held judicial functions, taking maintenance roles in regard to roads and bridges, and the collection of \"county cess\" taxes. They were usually composed of wealthy \"country gentlemen\" (i.e. landowners, farmers and merchants):A country gentleman as a member of a Grand Jury...levied the local taxes, appointed the nephews of his old friends to collect them, and spent them when they were gathered in. He controlled the boards of guardians and appointed the dispensary doctors, regulated the diet of paupers, inflicted fines and administered the law at petty sessions. The counties were initially used for judicial purposes, but began to take on some governmental functions in the 17th century, notably with grand juries.\n\nIn 1836, the use of counties as local government units was further developed, with grand-jury powers extended under the Grand Jury (Ireland) Act 1836. The traditional county of Tipperary was split into two judicial counties (or ridings) following the establishment of assize courts in 1838. Also in that year, local poor law boards, with a mix of magistrates and elected \"guardians\" took over the health and social welfare functions of the grand juries.\n\nSixty years later, a more radical reorganisation of local government took place with the passage of the Local Government (Ireland) Act 1898. This Act established a county council for each of the thirty-three Irish administrative counties. Elected county councils took over the powers of the grand juries. The boundaries of the traditional counties changed on a number of occasions. The 1898 Act changed the boundaries of Counties Galway, Clare, Mayo, Roscommon, Sligo, Waterford, Kilkenny, Meath and Louth, and others. County Tipperary was divided into two regions: North Riding and South Riding. Areas of the cities of Belfast, Cork, Dublin, Limerick, Derry and Waterford were carved from their surrounding counties to become county boroughs in their own right and given powers equivalent to those of administrative counties.\n\nUnder the Government of Ireland Act 1920, the island was partitioned between Southern Ireland and Northern Ireland. For the purposes of the Act, ... Northern Ireland shall consist of the parliamentary counties of Antrim, Armagh, Down, Fermanagh, Londonderry and Tyrone, and the parliamentary boroughs of Belfast and Londonderry, and Southern Ireland shall consist of so much of Ireland as is not comprised within the said parliamentary counties and boroughs.\n\nThe county and county borough borders were thus used to determine the line of partition. Southern Ireland shortly afterwards became the Irish Free State. This partition was entrenched in the Anglo-Irish Treaty, which was ratified in 1922, by which the Irish Free State left the United Kingdom with Northern Ireland making the decision to not separate two days later.\n\nUnder the Local Government Provisional Order Confirmation Act 1976, part of the urban area of Drogheda, which lay in County Meath, was transferred to County Louth on 1 January 1977. This resulted in the land area of County Louth increasing slightly at the expense of County Meath. The possibility of a similar action with regard to Waterford City has been raised in recent years, though opposition from Kilkenny has been strong.\n\nAreas that were shired by 1607 and continued as counties until the local government reforms of 1836, 1898 and 2001 are sometimes referred to as \"traditional\" or \"historic\" counties. These were distinct from the counties corporate that existed in some of the larger towns and cities, although linked to the county at large for other purposes. From 1898 to 2001, areas with county councils were known as administrative counties, while the counties corporate were designated as county boroughs. In other cases, the \"traditional\" county was divided to form two administrative counties. From 2001, certain administrative counties, which were originally \"traditional\" counties, underwent further splitting.\n\nIn the Republic of Ireland the traditional counties are, in general, the basis for local government, planning and community development purposes, are governed by county councils and are still generally respected for other purposes. Administrative borders have been altered to allocate various towns (e.g. Bray) exclusively into one county having been originally split between two counties.\n\nThere are now 26 county councils, three city councils, and two city and county councils – a total of 31 local government entities.\n\nCounty Tipperary was split into North and South Ridings in 1838. These Ridings were established as separate administrative counties under the Local Government (Ireland) Act, 1898.\n\nCounty Dublin was abolished as an administrative county in 1994, while also remaining a point of reference for purposes other than local government. Its territory was divided into three administrative counties: Dún Laoghaire–Rathdown, Fingal, and South Dublin. The county borough of Dublin, together with the county boroughs of Cork, Galway, Limerick and Waterford, were re-styled as city councils under the Local Government Act 2001, with the same status in law as county councils.\n\nThe city councils of Limerick and Waterford were merged with their respective county councils by the Local Government Reform Act 2014, to form new city and county councils. Anomalously, the city of Kilkenny does not have a \"city council\" as it was a borough but not a county borough. It is now administered by its eponymous county council but is, exceptionally, permitted to retain the style of \"city\" for ornament only. Also, the 2014 Act abolished North Tipperary and South Tipperary, and re-established County Tipperary.\n\nThese 31 \"county-level\" entities correspond to the first level of local administrative unit for EU and Eurostat purposes. The second level of local administrative unit (LAU) is the District electoral division. Of the administrative structures established under the 1898 Local Government Act, the only type to have been completely abolished was the Rural District, which was rendered void in the early years of the Irish Free State amidst widespread allegations of corruption. At a level above that of LAU is the Region which clusters counties together for NUTS purposes. The Regions are administered by Regional Authorities which were established by the Local Government Act 1991 and came into existence in 1994.\n\nIn 2013 Education and Training Boards (ETBs) were formed throughout the Republic of Ireland, replacing the system of Vocational Education Committees (VECs) created in 1930. Originally, VECs were formed for each administrative county and county borough, and also in a number of larger towns. In 1997 the majority of town VECs were absorbed by the surrounding county. The 33 VEC areas were reduced to 16 ETB areas, with each consisting of one or more local government county or city.\n\nThe Institute of technology system was organised on the committee areas or \"functional areas\", these still remain legal but are not as important as originally envisioned as the institutes are now more national in character and are only really applied today when selecting governing councils, similarly Dublin Institute of Technology was originally a group of several colleges of the City of Dublin committee.\n\nWhere possible, parliamentary constituencies in the Republic of Ireland follow county boundaries. Under the Electoral Act 1997 a Constituency Commission is established following the publication of census figures every five years. The Commission is charged with defining constituency boundaries, and the 1997 Act provides that \"the breaching of county boundaries shall be avoided as far as practicable\". This provision does not apply to the boundaries between cities and counties, or between the three counties in the Dublin area.\n\nThis system usually results in more populated counties having several constituencies: Dublin, including Dublin city, is subdivided into twelve constituencies, Cork into five. On the other hand, smaller counties such as Carlow and Kilkenny or Laois and Offaly may be paired to form constituencies. An extreme case is the splitting of Ireland's least populated county of Leitrim between the constituencies of Sligo–North Leitrim and Roscommon–South Leitrim.\n\nEach county or city is divided into Local electoral areas for the election of councillors. The boundaries of the areas and the number of councillors assigned are fixed from time to time by order of the Minister for the Environment, Community and Local Government, following a report by the Local Government Commission, and based on population changes recorded in the census.\n\nIn Northern Ireland, a major reorganisation of local government in 1973 replaced the six traditional counties and two county boroughs (Belfast and Derry) with 26 single-tier districts for local government purposes. In 2015, as a result of a reform process that started in 2005, these districts were merged to form 11 new single-tier \"super districts\".\n\nThe six traditional counties remain in use for some purposes, including the three-letter coding of vehicle number plates, the Royal Mail Postcode Address File (which records counties in all addresses although they are no longer required for postcoded mail) and Lord Lieutenancies (for which the former county boroughs are also used). There are no longer official 'county towns'. However the counties are still very widely acknowledged, for example as administrative divisions for sporting and cultural organisations.\n\nThe administrative division of the island along the lines of the traditional 32 counties was also adopted by non-governmental and cultural organisations. In particular the Gaelic Athletic Association continues to organise its activities on the basis of GAA counties that, throughout the island, correspond almost exactly to the 32 traditional counties in use at the time of the foundation of that organisation in 1884. The GAA also uses the term \"county\" for some of its organisational units in Britain and further afield.\n\nThe 35 divisions listed below include the ‘traditional’ counties of Ireland as well as those created or re-created after the 19th century. Twenty four counties still delimit the remit of local government divisions in the Republic of Ireland (in some cases with slightly redrawn boundaries). In Northern Ireland, the counties listed no longer serve this purpose. The Irish-language names of counties in the Republic of Ireland are prescribed by ministerial order, which in the case of three newer counties, omits the word \"contae\" (county). Irish names form the basis for all English-language county names except Waterford, Wexford, and Wicklow, which are of Norse origin.\n\nIn the ’Region’ column of the table below, except for the six Northern Ireland counties the reference is to NUTS 3 statistical regions of the Republic of Ireland. ’County town’ is the current or former administrative capital of the county.\n\nCities which, in the Republic, are currently administered outside the county system, but with the same legal status as administrative counties, are not shown separately: these are Cork, Dublin and, Galway. Also omitted are the former county boroughs of Londonderry and Belfast which in Northern Ireland had the same legal status as the six counties until the reorganisation of local government in 1973. County Dublin, which was officially abolished in 1994, is included, as are the three new administrative counties which took over the functions of the defunct Dublin County Council.\n\n!colspan=2County\n!Irish name\n!Ulster-Scots<br>name(s)<br>\n!County town\n!Mostpopulouscity/town\n!Province\n!Region\n\ncolspan=2Antrim\nAnthrimAntrìmEntrim\nBallymena \nBelfast (part)\nUlster\nNorthern Ireland – UKN0\n\ncolspan=2Armagh\nAirmagh\nArmagh\nCraigavon\nUlster\nNorthern Ireland – UKN0  \n\ncolspan=2Carlow\n\nCarlow\nCarlow\nLeinster\nSouth-East – IE024\n\ncolspan=2Cavan\n\nCavan\nCavan\nUlster\nBorder – IE011\n\ncolspan=2Clare\n\nEnnis\nEnnis\nMunster\nMid-West – IE023\n\ncolspan=2Cork\nCoark\nCork\nCork\nMunster\nSouth-West – IE025\n\ncolspan=2Donegal\nDinnygalDunnygal\nLifford\nLetterkenny\nUlster\nBorder – IE011\n\ncolspan=2Down\nDoonDoun\nDownpatrick\nBelfast (part)\nUlster\nNorthern Ireland – UKN0\n\ncolspan=2Dublin\n\nDublin\nDublin\nLeinster\nDublin – IE021\n\n–Rathdown\n\nLeinster\nDublin – IE021\n\nFingal\n\nSwords\nSwords\nLeinster\nDublin – IE021\n\nSouth Dublin\n\nTallaght\nTallaght\nLeinster\nDublin – IE021\n\ncolspan=2Fermanagh\nFermanay\nEnniskillen\nEnniskillen\nUlster\nNorthern Ireland – UKN0\n\ncolspan=2Galway\n\nGalway\nGalway\nWest – IE013\n\ncolspan=2Kerry\n\nTralee\nTralee\nMunster\nSouth-West – IE025\n\ncolspan=2Kildare\n\nNaas\nNewbridge\nLeinster\nMid-East – IE022\n\ncolspan=2Kilkenny\n\nKilkenny\nKilkenny\nLeinster\nSouth-East – IE024\n\ncolspan=2Laois\n\nPortlaoise\nPortlaoise\nLeinster\nMidlands – IE012\n\ncolspan=2Leitrim\n\nCarrick-on-Shannon\nCarrick-on-Shannon\nBorder – IE011\n\ncolspan=2Limerick\nLïmerick\nLimerick\nLimerick\nMunster\nMid-West – IE023\n\ncolspan=2Londonderry\nLunnonderrie\nColeraine\nDerry\nUlster\nNorthern Ireland – UKN0\n\ncolspan=2Longford\nLangfurd\nLongford\nLongford\nLeinster\nMidlands – IE012\n\ncolspan=2Louth\n\nDundalk\nDrogheda\nLeinster\nMid-East – IE022\n\ncolspan=2Mayo\n\nCastlebar\nCastlebar\nWest – IE013\n\ncolspan=2Meath\n\nNavan\nNavan\nLeinster\nMid-East – IE022\n\ncolspan=2Monaghan\nRonelann\nMonaghan\nMonaghan\nUlster\nBorder – IE011\n\ncolspan=2Offaly\n\nTullamore \nTullamore\nLeinster\nMidlands – IE012\n\ncolspan=2Roscommon\n\nRoscommon\nRoscommon\nWest – IE013\n\ncolspan=2Sligo\n\nSligo\nSligo\nBorder – IE011\n\ncolspan=2Tipperary\n\nNenagh  \nClonmel\nMunster\n‡\ncolspan=2Tyrone\nOwenslann\nOmagh\nOmagh\nUlster\nNorthern Ireland – UKN0\n\ncolspan=2Waterford\nWattèrford\nWaterford\nWaterford\nMunster\nSouth-East – IE024\n\ncolspan=2Westmeath\n\nMullingar\nAthlone\nLeinster\nSouth-East – IE024\n\ncolspan=2Wexford\n\nWexford\nWexford\nLeinster\nSouth-East – IE024\n\ncolspan=2Wicklow\n\nWicklow\nBray\nLeinster\nMid-East – IE022\n‡ Also administrative\n\nBULLET::::- Gaelic Athletic Association county\nBULLET::::- History of Ireland\nBULLET::::- History of the United Kingdom\nBULLET::::- Vehicle registration plates of the Republic of Ireland\nBULLET::::- Vehicle registration plates of Northern Ireland\nBULLET::::- List of Irish counties by area\nBULLET::::- List of Irish counties by population\nBULLET::::- List of Irish counties' coats of arms\nBULLET::::- List of Irish county towns\n\nBULLET::::- Common Licensed Photos from all the Counties\nBULLET::::- The Baronies of Ireland -Clans and Baronies\n"}
{"id": "15034", "url": "https://en.wikipedia.org/wiki?curid=15034", "title": "Information Sciences Institute", "text": "Information Sciences Institute\n\nThe USC Information Sciences Institute (ISI) is a component of the University of Southern California (USC) Viterbi School of Engineering, and specializes in research and development in information processing, computing, and communications technologies. It is located in Marina del Rey, California.\n\nISI actively participated in the information revolution, and it played a leading role in developing and managing the early Internet and its predecessor ARPAnet. The Institute conducts basic and applied research supported by more than 20 U.S. government agencies involved in defense, science, health, homeland security, energy and other areas. Annual funding is about $100 million.\n\nISI employs about 350 research scientists, research programmers, graduate students and administrative staff at its Marina del Rey, California headquarters and in Arlington, Virginia. About half of the research staff hold PhD degrees, and about 40 are research faculty who teach at USC and advise graduate students. Several senior researchers are tenured USC faculty in the Viterbi School.\n\nISI research spans artificial intelligence (AI), cybersecurity, grid computing, quantum computing, microelectronics, supercomputing, nano-satellites and many other areas. AI expertise includes natural language processing, in which ISI has an international reputation, reconfigurable robotics, information integration, motion analysis and social media analysis. Hardware/software expertise includes cyber-physical system security, data mining, reconfigurable computing and cloud computing. In networking, ISI explores Internet resilience, Internet traffic analysis and photonics, among other areas. Researchers also work in scientific data management, wireless technologies, biomimetics and electrical smart grid, in which ISI is advising the Los Angeles Department of Water and Power on a major demonstration project. Another current initiative involves big data brain imaging jointly with the Keck School of Medicine of USC.\n\nFederal agency sponsors include the Air Force Office of Scientific Research, Department of Defense Advanced Research Projects Agency, Department of Education, Department of Energy, Department of Homeland Security, National Institutes of Health, National Science Foundation, and other scientific, technical and defense-related agencies.\n\nCorporate partners include Chevron Corp. in the Center for Interactive Smart Oilfield Technologies (CiSoft), Lockheed Martin Company in the USC-Lockheed Martin Quantum Computation Center, and Parsons Corp. subsidiary Sparta Inc. in the DETER Project, a cybersecurity research initiative and international testbed. ISI also has partnered with businesses including IBM Corporation, Samsung Electronics Company, the Raytheon Company, GlobalFoundries Inc., Northrop Grumman Corporation and Carl Zeiss AG, and currently is working with Micron Technology, Inc., Altera Corporation and Fujitsu Ltd.\n\nISI also operates the Metal Oxide Semiconductor Implementation Service (MOSIS), a multi-project electronic circuit wafer service that has prototyped more than 60,000 chips since 1981. MOSIS provides design tools and pools circuit designs to produce specialty and low-volume chips for corporations, universities and other research entities worldwide. The Institute also has given rise to several startup and spinoff companies in grid software, geospatial information fusion, machine translation, data integration and other technologies.\n\nISI was founded by Keith Uncapher, who headed the computer research group at RAND Corporation in the 1960s and early 1970s. Uncapher decided to leave RAND after his group's funding was cut in 1971. He approached the University of California at Los Angeles about creating an off-campus technology institute, but was told that a decision would take 15 months. He then presented the concept to USC, which approved the proposal in five days. ISI was launched with three employees in 1972. Its first proposal was funded by the Defense Advanced Research Projects Agency (DARPA) in 30 days for $6 million.\n\nISI became one of the earliest nodes on ARPANET, the predecessor to the Internet, and in 1977 figured prominently in a demonstration of its international viability. ISI also helped refine the TCP/IP communications protocols fundamental to Net operations, and researcher Paul Mockapetris developed the now-familiar Domain Name System characterized by .com, .org, .net, .gov, and .edu on which the Net still operates. (The names .com, .org et al. were invented at SRI International, an ongoing collaborator.) Steve Crocker originated the Request for Comments (RFC) series, the written record of the network's technical structure and operation that both documented and shaped the emerging Internet. Another ISI researcher, Danny Cohen, became first to implement packet voice and packet video over ARPANET, demonstrating the viability of packet switching for real-time applications.\n\nJonathan Postel collaborated in development of TCP/IP, DNS and the SMTP protocol that supports email. He also edited the RFC for nearly three decades until his sudden death in 1998, when ISI colleagues assumed responsibility. The Institute retained that role until 2009. Postel simultaneously directed the Internet Assigned Number Authority (IANA) and its predecessor, which assign Internet addresses. IANA was administered from ISI until a nonprofit organization, ICANN, was created for that purpose in 1998.\n\nSome of the first Net security applications, and one of the world's first portable computers, also originated at ISI.\n\nISI researchers also created or co-created the:\nBULLET::::- GLOBUS grid computing standard\nBULLET::::- LOOM knowledge representation language and environment, or LOOM (ontology)\nBULLET::::- MONARCH supercomputer-on-a-chip\nBULLET::::- Soar (cognitive architecture) for developing intelligent behavioral systems\n\nIn 2011, several ISI natural language experts advised the IBM team that created Watson, the computer that became the first machine to win against human competitors on the \"Jeopardy!\" TV show. In 2012, ISI's Kevin Knight spearheaded a successful drive to crack the Copiale cipher, a lengthy encrypted manuscript that had remained unreadable for 250 years. Also in 2012, the USC-Lockheed Martin Quantum Computation Center (QCC) became the first organization to operate a quantum annealing system outside of its manufacturer, D-Wave Systems, Inc. USC, ISI and Lockheed Martin now are performing basic and applied research into quantum computing. A second quantum annealing system is located at NASA Ames Research Center, and is operated jointly by NASA and Google.\n\nThe USC Andrew and Erna Viterbi School of Engineering was ranked among the nation's top 10 engineering graduate schools by \"US News & World Report\" in 2015. Including ISI, USC is ranked first nationally in federal computer science research and development expenditures.\n\nISI is organized into six divisions focused on differing areas of research expertise:\nBULLET::::- Internet and Networked Systems: networking software and hardware, Internet security, decision systems and related areas\nBULLET::::- Computational Systems and Technology: quantum computing; supercomputing; cloud, wireless, reconfigurable and multicore computing; microarchitecture and electronics; science automation technologies; social networks and space systems\nBULLET::::- Intelligent Systems: artificial intelligence, including natural language, machine translation, information integration, social networks, knowledge engineering, multi-agent systems and robotics\nBULLET::::- Informatics: application of grid computing and other methods to healthcare discovery processes, practice and delivery\nBULLET::::- Advanced Electronics: MOSIS shared-services integrated circuit research and fabrication, CMOS and post-CMOS concepts, and biomimetics\nBULLET::::- Space Technology and Systems: space research and hands-on involvement for students through the Space Engineering Research Center, operated jointly by ISI and USC.\n\nSmaller, specialized research groups operate within almost all divisions.\n\nISI is led by Executive Director Prem Natarajan, previously an executive vice president and principal scientist at Raytheon BBN Technologies. He is a natural language specialist with research interests that focus on optical character recognition, speech processing, and multimedia analysis. Natarajan joined ISI in 2013, succeeding USC Viterbi School vice dean John O'Brien, who served as interim executive director in 2012 and 2013. From 1988 to 2012, ISI was led by former IBM executive Herbert Schorr.\n\nBULLET::::- Official USC Information Sciences Institute (ISI) website\nBULLET::::- Official USC Viterbi School of Engineering website\nBULLET::::- Youtube.com\nBULLET::::- Dwavesys.com\n"}
{"id": "15036", "url": "https://en.wikipedia.org/wiki?curid=15036", "title": "Information security", "text": "Information security\n\nInformation security, sometimes shortened to infosec, is the practice of protecting information by mitigating information risks. It is part of information risk management. It typically involves preventing or at least reducing the probability of unauthorized/inappropriate access, use, disclosure, disruption, deletion/destruction, corruption, modification, inspection, recording or devaluation, although it may also involve reducing the adverse impacts of incidents. Information may take any form, e.g. electronic or physical., tangible (e.g. paperwork) or intangible (e.g. knowledge). Information security's primary focus is the balanced protection of the confidentiality, integrity and availability of data (also known as the CIA triad) while maintaining a focus on efficient policy implementation, all without hampering organization productivity. This is largely achieved through a structured risk management process that involves: \nBULLET::::- Identifying information and related assets, plus potential threats, vulnerabilities and impacts;\nBULLET::::- Evaluating the risks;\nBULLET::::- Deciding how to address or treat the risks i.e. to avoid, mitigate, share or accept them;\nBULLET::::- Where risk mitigation is required, selecting or designing appropriate security controls and implementing them;\nBULLET::::- Monitoring the activities, making adjustments as necessary to address any issues, changes and improvement opportunities.\n\nTo standardize this discipline, academics and professionals collaborate to offer guidance, policies, and industry standards on password, antivirus software, firewall, encryption software, legal liability, security awareness and training, and so forth. This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, transferred and destroyed. However, the implementation of any standards and guidance within an entity may have limited effect if a culture of continual improvement isn't adopted.\n\nAt the core of information security is information assurance, the act of maintaining the confidentiality, integrity and availability (CIA) of information, ensuring that information is not compromised in any way when critical issues arise. These issues include but are not limited to natural disasters, computer/server malfunction, and physical theft. While paper-based business operations are still prevalent, requiring their own set of information security practices, enterprise digital initiatives are increasingly being emphasized, with information assurance now typically being dealt with by information technology (IT) security specialists. These specialists apply information security to technology (most often some form of computer system). It is worthwhile to note that a computer does not necessarily mean a home desktop. A computer is any device with a processor and some memory. Such devices can range from non-networked standalone devices as simple as calculators, to networked mobile computing devices such as smartphones and tablet computers. IT security specialists are almost always found in any major enterprise/establishment due to the nature and value of the data within larger businesses. They are responsible for keeping all of the technology within the company secure from malicious cyber attacks that often attempt to acquire critical private information or gain control of the internal systems.\n\nThe field of information security has grown and evolved significantly in recent years. It offers many areas for specialization, including securing networks and allied infrastructure, securing applications and databases, security testing, information systems auditing, business continuity planning, electronic record discovery, and digital forensics. Information security professionals are very stable in their employment. more than 80 percent of professionals had no change in employer or employment over a period of a year, and the number of professionals is projected to continuously grow more than 11 percent annually from 2014 to 2019.\n\nInformation security threats come in many different forms. Some of the most common threats today are software attacks, theft of intellectual property, identity theft, theft of equipment or information, sabotage, and information extortion. Most people have experienced software attacks of some sort. Viruses, worms, phishing attacks and Trojan horses are a few common examples of software attacks. The theft of intellectual property has also been an extensive issue for many businesses in the information technology (IT) field. Identity theft is the attempt to act as someone else usually to obtain that person's personal information or to take advantage of their access to vital information through social engineering. Theft of equipment or information is becoming more prevalent today due to the fact that most devices today are mobile, are prone to theft and have also become far more desirable as the amount of data capacity increases. Sabotage usually consists of the destruction of an organization's website in an attempt to cause loss of confidence on the part of its customers. Information extortion consists of theft of a company's property or information as an attempt to receive a payment in exchange for returning the information or property back to its owner, as with ransomware. There are many ways to help protect yourself from some of these attacks but one of the most functional precautions is conduct periodical user awareness. The number one threat to any organisation are users or internal employees, they are also called insider threats.\n\nGovernments, military, corporations, financial institutions, hospitals, non-profit organisations and private businesses amass a great deal of confidential information about their employees, customers, products, research and financial status. Should confidential information about a business' customers or finances or new product line fall into the hands of a competitor or a black hat hacker, a business and its customers could suffer widespread, irreparable financial loss, as well as damage to the company's reputation. From a business perspective, information security must be balanced against cost; the Gordon-Loeb Model provides a mathematical economic approach for addressing this concern.\n\nFor the individual, information security has a significant effect on privacy, which is viewed very differently in various cultures.\n\nPossible responses to a security threat or risk are:\nBULLET::::- reduce/mitigate – implement safeguards and countermeasures to eliminate vulnerabilities or block threats\nBULLET::::- assign/transfer – place the cost of the threat onto another entity or organization such as purchasing insurance or outsourcing\nBULLET::::- accept – evaluate if the cost of the countermeasure outweighs the possible cost of loss due to the threat\n\nSince the early days of communication, diplomats and military commanders understood that it was necessary to provide some mechanism to protect the confidentiality of correspondence and to have some means of detecting tampering. Julius Caesar is credited with the invention of the Caesar cipher c. 50 B.C., which was created in order to prevent his secret messages from being read should a message fall into the wrong hands; however, for the most part protection was achieved through the application of procedural handling controls. Sensitive information was marked up to indicate that it should be protected and transported by trusted persons, guarded and stored in a secure environment or strong box. As postal services expanded, governments created official organizations to intercept, decipher, read and reseal letters (e.g., the U.K.'s Secret Office, founded in 1653).\n\nIn the mid-nineteenth century more complex classification systems were developed to allow governments to manage their information according to the degree of sensitivity. For example, the British Government codified this, to some extent, with the publication of the Official Secrets Act in 1889. By the time of the First World War, multi-tier classification systems were used to communicate information to and from various fronts, which encouraged greater use of code making and breaking sections in diplomatic and military headquarters. Encoding became more sophisticated between the wars as machines were employed to scramble and unscramble information. The volume of information shared by the Allied countries during the Second World War necessitated formal alignment of classification systems and procedural controls. An arcane range of markings evolved to indicate who could handle documents (usually officers rather than enlisted troops) and where they should be stored as increasingly complex safes and storage facilities were developed. The Enigma Machine, which was employed by the Germans to encrypt the data of warfare and was successfully decrypted by Alan Turing, can be regarded as a striking example of creating and using secured information. Procedures evolved to ensure documents were destroyed properly, and it was the failure to follow these procedures which led to some of the greatest intelligence coups of the war (e.g., the capture of U-570).\n\nThe end of the twentieth century and the early years of the twenty-first century saw rapid advancements in telecommunications, computing hardware and software, and data encryption. The availability of smaller, more powerful and less expensive computing equipment made electronic data processing within the reach of small business and the home user. These computers quickly became interconnected through the internet.\n\nThe rapid growth and widespread use of electronic data processing and electronic business conducted through the internet, along with numerous occurrences of international terrorism, fueled the need for better methods of protecting the computers and the information they store, process and transmit. The academic disciplines of computer security and information assurance emerged along with numerous professional organizations, all sharing the common goals of ensuring the security and reliability of information systems.\n\nVarious definitions of information security are suggested below, summarized from different sources:\n\nBULLET::::1. \"Preservation of confidentiality, integrity and availability of information. Note: In addition, other properties, such as authenticity, accountability, non-repudiation and reliability can also be involved.\" (ISO/IEC 27000:2009)\nBULLET::::2. \"The protection of information and information systems from unauthorized access, use, disclosure, disruption, modification, or destruction in order to provide confidentiality, integrity, and availability.\" (CNSS, 2010)\nBULLET::::3. \"Ensures that only authorized users (confidentiality) have access to accurate and complete information (integrity) when required (availability).\" (ISACA, 2008)\nBULLET::::4. \"Information Security is the process of protecting the intellectual property of an organisation.\" (Pipkin, 2000)\nBULLET::::5. \"...information security is a risk management discipline, whose job is to manage the cost of information risk to the business.\" (McDermott and Geer, 2001)\nBULLET::::6. \"A well-informed sense of assurance that information risks and controls are in balance.\" (Anderson, J., 2003)\nBULLET::::7. \"Information security is the protection of information and minimizes the risk of exposing information to unauthorized parties.\" (Venter and Eloff, 2003)\nBULLET::::8. \"Information Security is a multidisciplinary area of study and professional activity which is concerned with the development and implementation of security mechanisms of all available types (technical, organizational, human-oriented and legal) in order to keep information in all its locations (within and outside the organization's perimeter) and, consequently, information systems, where information is created, processed, stored, transmitted and destroyed, free from threats.Threats to information and information systems may be categorized and a corresponding security goal may be defined for each category of threats. A set of security goals, identified as a result of a threat analysis, should be revised periodically to ensure its adequacy and conformance with the evolving environment. The currently relevant set of security goals may include: \"confidentiality, integrity, availability, privacy, authenticity & trustworthiness, non-repudiation, accountability and auditability.\"\" (Cherdantseva and Hilton, 2013)\n\nThe CIA triad of confidentiality, integrity, and availability is at the heart of information security. (The members of the classic InfoSec triad—confidentiality, integrity and availability—are interchangeably referred to in the literature as security attributes, properties, security goals, fundamental aspects, information criteria, critical information characteristics and basic building blocks.) However, debate continues about whether or not this CIA triad is sufficient to address rapidly changing technology and business requirements, with recommendations to consider expanding on the intersections between availability and confidentiality, as well as the relationship between security and privacy. Other principles such as \"accountability\" have sometimes been proposed; it has been pointed out that issues such as non-repudiation do not fit well within the three core concepts.\n\nIn 1992 and revised in 2002, the OECD's \"Guidelines for the Security of Information Systems and Networks\" proposed the nine generally accepted principles: awareness, responsibility, response, ethics, democracy, risk assessment, security design and implementation, security management, and reassessment. Building upon those, in 2004 the NIST's \"Engineering Principles for Information Technology Security\" proposed 33 principles. From each of these derived guidelines and practices.\n\nIn 1998, Donn Parker proposed an alternative model for the classic CIA triad that he called the six atomic elements of information. The elements are confidentiality, possession, integrity, authenticity, availability, and utility. The merits of the Parkerian Hexad are a subject of debate amongst security professionals.\n\nIn 2011, The Open Group published the information security management standard O-ISM3. This standard proposed an operational definition of the key concepts of security, with elements called \"security objectives\", related to access control (9), availability (3), data quality (1), compliance and technical (4). In 2009, DoD Software Protection Initiative released the Three Tenets of Cybersecurity which are System Susceptibility, Access to the Flaw, and Capability to Exploit the Flaw. Neither of these models are widely adopted.\n\nIn information security, confidentiality \"is the property, that information is not made available or disclosed to unauthorized individuals, entities, or processes.\" While similar to \"privacy,\" the two words aren't interchangeable. Rather, confidentiality is a component of privacy that implements to protect our data from unauthorized viewers. Examples of confidentiality of electronic data being compromised include laptop theft, password theft, or sensitive emails being sent to the incorrect individuals.\n\nIn information security, data integrity means maintaining and assuring the accuracy and completeness of data over its entire lifecycle. This means that data cannot be modified in an unauthorized or undetected manner. This is not the same thing as referential integrity in databases, although it can be viewed as a special case of consistency as understood in the classic ACID model of transaction processing. Information security systems typically provide message integrity alongside confidentiality.\n\nFor any information system to serve its purpose, the information must be available when it is needed. This means the computing systems used to store and process the information, the security controls used to protect it, and the communication channels used to access it must be functioning correctly. High availability systems aim to remain available at all times, preventing service disruptions due to power outages, hardware failures, and system upgrades. Ensuring availability also involves preventing denial-of-service attacks, such as a flood of incoming messages to the target system, essentially forcing it to shut down.\n\nIn the realm of information security, availability can often be viewed as one of the most important parts of a successful information security program. Ultimately end-users need to be able to perform job functions; by ensuring availability an organization is able to perform to the standards that an organization's stakeholders expect. This can involve topics such as proxy configurations, outside web access, the ability to access shared drives and the ability to send emails. Executives oftentimes do not understand the technical side of information security and look at availability as an easy fix, but this often requires collaboration from many different organizational teams, such as network operations, development operations, incident response and policy/change management. A successful information security team involves many different key roles to mesh and align for the CIA triad to be provided effectively.\n\nIn law, non-repudiation implies one's intention to fulfill their obligations to a contract. It also implies that one party of a transaction cannot deny having received a transaction, nor can the other party deny having sent a transaction.\n\nIt is important to note that while technology such as cryptographic systems can assist in non-repudiation efforts, the concept is at its core a legal concept transcending the realm of technology. It is not, for instance, sufficient to show that the message matches a digital signature signed with the sender's private key, and thus only the sender could have sent the message, and nobody else could have altered it in transit (data integrity). The alleged sender could in return demonstrate that the digital signature algorithm is vulnerable or flawed, or allege or prove that his signing key has been compromised. The fault for these violations may or may not lie with the sender, and such assertions may or may not relieve the sender of liability, but the assertion would invalidate the claim that the signature necessarily proves authenticity and integrity. As such, the sender may repudiate the message (because authenticity and integrity are pre-requisites for non-repudiation).\n\nThe \"Certified Information Systems Auditor (CISA) Review Manual 2006\" provides the following definition of risk management: \"Risk management is the process of identifying vulnerabilities and threats to the information resources used by an organization in achieving business objectives, and deciding what countermeasures, if any, to take in reducing risk to an acceptable level, based on the value of the information resource to the organization.\"\n\nThere are two things in this definition that may need some clarification. First, the \"process\" of risk management is an ongoing, iterative process. It must be repeated indefinitely. The business environment is constantly changing and new threats and vulnerabilities emerge every day. Second, the choice of countermeasures (controls) used to manage risks must strike a balance between productivity, cost, effectiveness of the countermeasure, and the value of the informational asset being protected.\n\nRisk analysis and risk evaluation processes have their limitations since, when security incidents occur, they emerge in a context, and their rarity and uniqueness give rise to unpredictable threats. The analysis of these phenomena, which are characterized by breakdowns, surprises and side-effects, requires a theoretical approach that is able to examine and interpret subjectively the detail of each incident.\n\nRisk is the likelihood that something bad will happen that causes harm to an informational asset (or the loss of the asset). A vulnerability is a weakness that could be used to endanger or cause harm to an informational asset. A threat is anything (man-made or act of nature) that has the potential to cause harm.\n\nThe likelihood that a threat will use a vulnerability to cause harm creates a risk. When a threat does use a vulnerability to inflict harm, it has an impact. In the context of information security, the impact is a loss of availability, integrity, and confidentiality, and possibly other losses (lost income, loss of life, loss of real property). It should be pointed out that it is not possible to identify all risks, nor is it possible to eliminate all risk. The remaining risk is called \"residual risk.\"\n\nA risk assessment is carried out by a team of people who have knowledge of specific areas of the business. Membership of the team may vary over time as different parts of the business are assessed. The assessment may use a subjective qualitative analysis based on informed opinion, or where reliable dollar figures and historical information is available, the analysis may use quantitative analysis.\n\nResearch has shown that the most vulnerable point in most information systems is the human user, operator, designer, or other human. The ISO/IEC 27002:2005 Code of practice for information security management recommends the following be examined during a risk assessment:\nBULLET::::- security policy,\nBULLET::::- organization of information security,\nBULLET::::- asset management,\nBULLET::::- human resources security,\nBULLET::::- physical and environmental security,\nBULLET::::- communications and operations management,\nBULLET::::- access control,\nBULLET::::- information systems acquisition, development and maintenance,\nBULLET::::- information security incident management,\nBULLET::::- business continuity management, and\nBULLET::::- regulatory compliance.\n\nIn broad terms, the risk management process consists of:\nBULLET::::1. Identification of assets and estimating their value. Include: people, buildings, hardware, software, data (electronic, print, other), supplies.\nBULLET::::2. Conduct a threat assessment. Include: Acts of nature, acts of war, accidents, malicious acts originating from inside or outside the organization.\nBULLET::::3. Conduct a vulnerability assessment, and for each vulnerability, calculate the probability that it will be exploited. Evaluate policies, procedures, standards, training, physical security, quality control, technical security.\nBULLET::::4. Calculate the impact that each threat would have on each asset. Use qualitative analysis or quantitative analysis.\nBULLET::::5. Identify, select and implement appropriate controls. Provide a proportional response. Consider productivity, cost effectiveness, and value of the asset.\nBULLET::::6. Evaluate the effectiveness of the control measures. Ensure the controls provide the required cost effective protection without discernible loss of productivity.\n\nFor any given risk, management can choose to accept the risk based upon the relative low value of the asset, the relative low frequency of occurrence, and the relative low impact on the business. Or, leadership may choose to mitigate the risk by selecting and implementing appropriate control measures to reduce the risk. In some cases, the risk can be transferred to another business by buying insurance or outsourcing to another business. The reality of some risks may be disputed. In such cases leadership may choose to deny the risk.\n\nSelecting and implementing proper security controls will initially help an organization bring down risk to acceptable levels. Control selection should follow and should be based on the risk assessment. Controls can vary in nature, but fundamentally they are ways of protecting the confidentiality, integrity or availability of information. ISO/IEC 27001 has defined controls in different areas. Organizations can implement additional controls according to requirement of the organization. ISO/IEC 27002 offers a guideline for organizational information security standards.\n\nAdministrative controls consist of approved written policies, procedures, standards and guidelines. Administrative controls form the framework for running the business and managing people. They inform people on how the business is to be run and how day-to-day operations are to be conducted. Laws and regulations created by government bodies are also a type of administrative control because they inform the business. Some industry sectors have policies, procedures, standards and guidelines that must be followed – the Payment Card Industry Data Security Standard (PCI DSS) required by Visa and MasterCard is such an example. Other examples of administrative controls include the corporate security policy, password policy, hiring policies, and disciplinary policies.\n\nAdministrative controls form the basis for the selection and implementation of logical and physical controls. Logical and physical controls are manifestations of administrative controls, which are of paramount importance.\n\nLogical controls (also called technical controls) use software and data to monitor and control access to information and computing systems. Passwords, network and host-based firewalls, network intrusion detection systems, access control lists, and data encryption are examples of logical controls.\n\nAn important logical control that is frequently overlooked is the principle of least privilege, which requires that an individual, program or system process not be granted any more access privileges than are necessary to perform the task. A blatant example of the failure to adhere to the principle of least privilege is logging into Windows as user Administrator to read email and surf the web. Violations of this principle can also occur when an individual collects additional access privileges over time. This happens when employees' job duties change, employees are promoted to a new position, or employees are transferred to another department. The access privileges required by their new duties are frequently added onto their already existing access privileges, which may no longer be necessary or appropriate.\n\nPhysical controls monitor and control the environment of the work place and computing facilities. They also monitor and control access to and from such facilities and include doors, locks, heating and air conditioning, smoke and fire alarms, fire suppression systems, cameras, barricades, fencing, security guards, cable locks, etc. Separating the network and workplace into functional areas are also physical controls.\n\nAn important physical control that is frequently overlooked is separation of duties, which ensures that an individual can not complete a critical task by himself. For example, an employee who submits a request for reimbursement should not also be able to authorize payment or print the check. An applications programmer should not also be the server administrator or the database administrator; these roles and responsibilities must be separated from one another.\n\nInformation security must protect information throughout its lifespan, from the initial creation of the information on through to the final disposal of the information. The information must be protected while in motion and while at rest. During its lifetime, information may pass through many different information processing systems and through many different parts of information processing systems. There are many different ways the information and information systems can be threatened. To fully protect the information during its lifetime, each component of the information processing system must have its own protection mechanisms. The building up, layering on and overlapping of security measures is called \"defense in depth.\" In contrast to a metal chain, which is famously only as strong as its weakest link, the defense in depth strategy aims at a structure where, should one defensive measure fail, other measures will continue to provide protection.\n\nRecall the earlier discussion about administrative controls, logical controls, and physical controls. The three types of controls can be used to form the basis upon which to build a defense in depth strategy. With this approach, defense in depth can be conceptualized as three distinct layers or planes laid one on top of the other. Additional insight into defense in depth can be gained by thinking of it as forming the layers of an onion, with data at the core of the onion, people the next outer layer of the onion, and network security, host-based security and application security forming the outermost layers of the onion. Both perspectives are equally valid, and each provides valuable insight into the implementation of a good defense in depth strategy.\n\nAn important aspect of information security and risk management is recognizing the value of information and defining appropriate procedures and protection requirements for the information. Not all information is equal and so not all information requires the same degree of protection. This requires information to be assigned a security classification. The first step in information classification is to identify a member of senior management as the owner of the particular information to be classified. Next, develop a classification policy. The policy should describe the different classification labels, define the criteria for information to be assigned a particular label, and list the required security controls for each classification.\n\nSome factors that influence which classification information should be assigned include how much value that information has to the organization, how old the information is and whether or not the information has become obsolete. Laws and other regulatory requirements are also important considerations when classifying information. The Information Systems Audit and Control Association (ISACA) and its \"Business Model for Information Security\" also serves as a tool for security professionals to examine security from a systems perspective, creating an environment where security can be managed holistically, allowing actual risks to be addressed.\n\nThe type of information security classification labels selected and used will depend on the nature of the organization, with examples being:\nBULLET::::- In the business sector, labels such as: Public, Sensitive, Private, Confidential.\nBULLET::::- In the government sector, labels such as: Unclassified, Unofficial, Protected, Confidential, Secret, Top Secret and their non-English equivalents.\nBULLET::::- In cross-sectoral formations, the Traffic Light Protocol, which consists of: White, Green, Amber, and Red.\n\nAll employees in the organization, as well as business partners, must be trained on the classification schema and understand the required security controls and handling procedures for each classification. The classification of a particular information asset that has been assigned should be reviewed periodically to ensure the classification is still appropriate for the information and to ensure the security controls required by the classification are in place and are followed in their right procedures.\n\nAccess to protected information must be restricted to people who are authorized to access the information. The computer programs, and in many cases the computers that process the information, must also be authorized. This requires that mechanisms be in place to control the access to protected information. The sophistication of the access control mechanisms should be in parity with the value of the information being protected; the more sensitive or valuable the information the stronger the control mechanisms need to be. The foundation on which access control mechanisms are built start with identification and authentication.\n\nAccess control is generally considered in three steps: identification, authentication, and authorization.\n\nIdentification is an assertion of who someone is or what something is. If a person makes the statement \"Hello, my name is John Doe\" they are making a claim of who they are. However, their claim may or may not be true. Before John Doe can be granted access to protected information it will be necessary to verify that the person claiming to be John Doe really is John Doe. Typically the claim is in the form of a username. By entering that username you are claiming \"I am the person the username belongs to\".\n\nAuthentication is the act of verifying a claim of identity. When John Doe goes into a bank to make a withdrawal, he tells the bank teller he is John Doe, a claim of identity. The bank teller asks to see a photo ID, so he hands the teller his driver's license. The bank teller checks the license to make sure it has John Doe printed on it and compares the photograph on the license against the person claiming to be John Doe. If the photo and name match the person, then the teller has authenticated that John Doe is who he claimed to be. Similarly, by entering the correct password, the user is providing evidence that he/she is the person the username belongs to.\n\nThere are three different types of information that can be used for authentication:\nBULLET::::- Something you know: things such as a PIN, a password, or your mother's maiden name\nBULLET::::- Something you have: a driver's license or a magnetic swipe card\nBULLET::::- Something you are: biometrics, including palm prints, fingerprints, voice prints and retina (eye) scans\n\nStrong authentication requires providing more than one type of authentication information (two-factor authentication). The username is the most common form of identification on computer systems today and the password is the most common form of authentication. Usernames and passwords have served their purpose, but they are increasingly inadequate. Usernames and passwords are slowly being replaced or supplemented with more sophisticated authentication mechanisms such as Time-based One-time Password algorithms.\n\nAfter a person, program or computer has successfully been identified and authenticated then it must be determined what informational resources they are permitted to access and what actions they will be allowed to perform (run, view, create, delete, or change). This is called authorization. Authorization to access information and other computing services begins with administrative policies and procedures. The policies prescribe what information and computing services can be accessed, by whom, and under what conditions. The access control mechanisms are then configured to enforce these policies. Different computing systems are equipped with different kinds of access control mechanisms. Some may even offer a choice of different access control mechanisms. The access control mechanism a system offers will be based upon one of three approaches to access control, or it may be derived from a combination of the three approaches.\n\nThe non-discretionary approach consolidates all access control under a centralized administration. The access to information and other resources is usually based on the individuals function (role) in the organization or the tasks the individual must perform. The discretionary approach gives the creator or owner of the information resource the ability to control access to those resources. In the mandatory access control approach, access is granted or denied basing upon the security classification assigned to the information resource.\n\nExamples of common access control mechanisms in use today include role-based access control, available in many advanced database management systems; simple file permissions provided in the UNIX and Windows operating systems; Group Policy Objects provided in Windows network systems; and Kerberos, RADIUS, TACACS, and the simple access lists used in many firewalls and routers.\n\nTo be effective, policies and other security controls must be enforceable and upheld. Effective policies ensure that people are held accountable for their actions. The U.S. Treasury's guidelines for systems processing sensitive or proprietary information, for example, states that all failed and successful authentication and access attempts must be logged, and all access to information must leave some type of audit trail.\n\nAlso, the need-to-know principle needs to be in effect when talking about access control. This principle gives access rights to a person to perform their job functions. This principle is used in the government when dealing with difference clearances. Even though two employees in different departments have a top-secret clearance, they must have a need-to-know in order for information to be exchanged. Within the need-to-know principle, network administrators grant the employee the least amount of privileges to prevent employees from accessing more than what they are supposed to. Need-to-know helps to enforce the confidentiality-integrity-availability triad. Need-to-know directly impacts the confidential area of the triad.\n\nInformation security uses cryptography to transform usable information into a form that renders it unusable by anyone other than an authorized user; this process is called encryption. Information that has been encrypted (rendered unusable) can be transformed back into its original usable form by an authorized user who possesses the cryptographic key, through the process of decryption. Cryptography is used in information security to protect information from unauthorized or accidental disclosure while the information is in transit (either electronically or physically) and while information is in storage.\n\nCryptography provides information security with other useful applications as well, including improved authentication methods, message digests, digital signatures, non-repudiation, and encrypted network communications. Older, less secure applications such as Telnet and File Transfer Protocol (FTP) are slowly being replaced with more secure applications such as Secure Shell (SSH) that use encrypted network communications. Wireless communications can be encrypted using protocols such as WPA/WPA2 or the older (and less secure) WEP. Wired communications (such as ITU‑T G.hn) are secured using AES for encryption and X.1035 for authentication and key exchange. Software applications such as GnuPG or PGP can be used to encrypt data files and email.\n\nCryptography can introduce security problems when it is not implemented correctly. Cryptographic solutions need to be implemented using industry-accepted solutions that have undergone rigorous peer review by independent experts in cryptography. The length and strength of the encryption key is also an important consideration. A key that is weak or too short will produce weak encryption. The keys used for encryption and decryption must be protected with the same degree of rigor as any other confidential information. They must be protected from unauthorized disclosure and destruction and they must be available when needed. Public key infrastructure (PKI) solutions address many of the problems that surround key management.\n\nThe terms \"reasonable and prudent person,\" \"due care\" and \"due diligence\" have been used in the fields of finance, securities, and law for many years. In recent years these terms have found their way into the fields of computing and information security. U.S. Federal Sentencing Guidelines now make it possible to hold corporate officers liable for failing to exercise due care and due diligence in the management of their information systems.\n\nIn the business world, stockholders, customers, business partners and governments have the expectation that corporate officers will run the business in accordance with accepted business practices and in compliance with laws and other regulatory requirements. This is often described as the \"reasonable and prudent person\" rule. A prudent person takes due care to ensure that everything necessary is done to operate the business by sound business principles and in a legal, ethical manner. A prudent person is also diligent (mindful, attentive, ongoing) in their due care of the business.\n\nIn the field of information security, Harris\noffers the following definitions of due care and due diligence:\n\n\"\"Due care are steps that are taken to show that a company has taken responsibility for the activities that take place within the corporation and has taken the necessary steps to help protect the company, its resources, and employees.\"\" And, <nowiki>[Due diligence are the]</nowiki> \"\"continual activities that make sure the protection mechanisms are continually maintained and operational.\"\"\nAttention should be made to two important points in these definitions. First, in due care, steps are taken to show; this means that the steps can be verified, measured, or even produce tangible artifacts. Second, in due diligence, there are continual activities; this means that people are actually doing things to monitor and maintain the protection mechanisms, and these activities are ongoing.\n\nOrganizations have a responsibility with practicing duty of care when applying information security. The Duty of Care Risk Analysis Standard (DoCRA) provides principles and practices for evaluating risk. It considers all parties that could be affected by those risks. DoCRA helps evaluate safeguards if they are appropriate in protecting others from harm while presenting a reasonable burden. With increased data breach litigation, companies must balance security controls, compliance, and its mission.\n\nThe Software Engineering Institute at Carnegie Mellon University, in a publication titled \"Governing for Enterprise Security (GES) Implementation Guide\", defines characteristics of effective security governance. These include:\nBULLET::::- An enterprise-wide issue\nBULLET::::- Leaders are accountable\nBULLET::::- Viewed as a business requirement\nBULLET::::- Risk-based\nBULLET::::- Roles, responsibilities, and segregation of duties defined\nBULLET::::- Addressed and enforced in policy\nBULLET::::- Adequate resources committed\nBULLET::::- Staff aware and trained\nBULLET::::- A development life cycle requirement\nBULLET::::- Planned, managed, measurable, and measured\nBULLET::::- Reviewed and audited\n\nAn incident response plan is a group of policies that dictate an organizations reaction to a cyber attack. Once an security breach has been identified the plan is initiated. It is important to note that there can be legal implications to a data breach. Knowing local and federal laws is critical. Every plan is unique to the needs of the organization, and it can involve skill set that are not part of an IT team. For example, a lawyer may be included in the response plan to help navigate legal implications to a data breach.\n\nAs mentioned above every plan is unique but most plans will include the following:\n\nGood preparation includes the development of an Incident Response Team. Skills need to be used by this team would be, penetration testing, computer forensics, network security, etc. This team should also keep track of trends in cybersecurity and modern attack strategies. A training program for end users is important as well as most modern attack strategies target users on the network.\n\nThis part of the incident response plan identifies if there was a security event. When an end user reports information or an admin notices irregularities, an investigation is launched. An incident log is a crucial part of this step. All of the members of the team should be updating this log to ensure that information flows as fast as possible. If it has been identified that a security breach has occurred the next step should be activated.\n\nIn this phase, the IRT works to isolate the areas that the breach took place to limit the scope of the security event. During this phase it is important to preserve information forensically so it can be analyzed later in the process. Containment could be as simple as physically containing a server room or as complex as segmenting a network to not allow the spread of a virus.\n\nThis is where the threat that was identified is removed from the affected systems. This could include using deleting malicious files, terminating compromised accounts, or deleting other components. Some events do not require this step, however it is important to fully understand the event before moving to this step. This will help to ensure that the threat is completely removed.\n\nThis stage is where the systems are restored back to original operation. This stage could include the recovery of data, changing user access information, or updating firewall rules or policies to prevent a breach in the future. Without executing this step, the system could still be vulnerable to future security threats.\n\nIn this step information that has been gathered during this process is used to make future decisions on security. This step is crucial to the ensure that future events are prevented. Using this information to further train admins is critical to the process. This step can also be used to process information that is distributed from other entities who have experienced a security event.\n\nChange management is a formal process for directing and controlling alterations to the information processing environment. This includes alterations to desktop computers, the network, servers and software. The objectives of change management are to reduce the risks posed by changes to the information processing environment and improve the stability and reliability of the processing environment as changes are made. It is not the objective of change management to prevent or hinder necessary changes from being implemented.\n\nAny change to the information processing environment introduces an element of risk. Even apparently simple changes can have unexpected effects. One of management's many responsibilities is the management of risk. Change management is a tool for managing the risks introduced by changes to the information processing environment. Part of the change management process ensures that changes are not implemented at inopportune times when they may disrupt critical business processes or interfere with other changes being implemented.\n\nNot every change needs to be managed. Some kinds of changes are a part of the everyday routine of information processing and adhere to a predefined procedure, which reduces the overall level of risk to the processing environment. Creating a new user account or deploying a new desktop computer are examples of changes that do not generally require change management. However, relocating user file shares, or upgrading the Email server pose a much higher level of risk to the processing environment and are not a normal everyday activity. The critical first steps in change management are (a) defining change (and communicating that definition) and (b) defining the scope of the change system.\n\nChange management is usually overseen by a change review board composed of representatives from key business areas, security, networking, systems administrators, database administration, application developers, desktop support and the help desk. The tasks of the change review board can be facilitated with the use of automated work flow application. The responsibility of the change review board is to ensure the organization's documented change management procedures are followed. The change management process is as follows\n\nBULLET::::- Request: Anyone can request a change. The person making the change request may or may not be the same person that performs the analysis or implements the change. When a request for change is received, it may undergo a preliminary review to determine if the requested change is compatible with the organizations business model and practices, and to determine the amount of resources needed to implement the change.\nBULLET::::- Approve: Management runs the business and controls the allocation of resources therefore, management must approve requests for changes and assign a priority for every change. Management might choose to reject a change request if the change is not compatible with the business model, industry standards or best practices. Management might also choose to reject a change request if the change requires more resources than can be allocated for the change.\nBULLET::::- Plan: Planning a change involves discovering the scope and impact of the proposed change; analyzing the complexity of the change; allocation of resources and, developing, testing and documenting both implementation and back-out plans. Need to define the criteria on which a decision to back out will be made.\nBULLET::::- Test: Every change must be tested in a safe test environment, which closely reflects the actual production environment, before the change is applied to the production environment. The backout plan must also be tested.\nBULLET::::- Schedule: Part of the change review board's responsibility is to assist in the scheduling of changes by reviewing the proposed implementation date for potential conflicts with other scheduled changes or critical business activities.\nBULLET::::- Communicate: Once a change has been scheduled it must be communicated. The communication is to give others the opportunity to remind the change review board about other changes or critical business activities that might have been overlooked when scheduling the change. The communication also serves to make the help desk and users aware that a change is about to occur. Another responsibility of the change review board is to ensure that scheduled changes have been properly communicated to those who will be affected by the change or otherwise have an interest in the change.\nBULLET::::- Implement: At the appointed date and time, the changes must be implemented. Part of the planning process was to develop an implementation plan, testing plan and, a back out plan. If the implementation of the change should fail or, the post implementation testing fails or, other \"drop dead\" criteria have been met, the back out plan should be implemented.\nBULLET::::- Document: All changes must be documented. The documentation includes the initial request for change, its approval, the priority assigned to it, the implementation, testing and back out plans, the results of the change review board critique, the date/time the change was implemented, who implemented it, and whether the change was implemented successfully, failed or postponed.\nBULLET::::- Post-change review: The change review board should hold a post-implementation review of changes. It is particularly important to review failed and backed out changes. The review board should try to understand the problems that were encountered, and look for areas for improvement.\n\nChange management procedures that are simple to follow and easy to use can greatly reduce the overall risks created when changes are made to the information processing environment. Good change management procedures improve the overall quality and success of changes as they are implemented. This is accomplished through planning, peer review, documentation and communication.\n\nISO/IEC 20000, The Visible OPS Handbook: Implementing ITIL in 4 Practical and Auditable Steps (Full book summary), and Information Technology Infrastructure Library all provide valuable guidance on implementing an efficient and effective change management program information security.\n\nBusiness continuity management (BCM) concerns arrangements aiming to protect an organization's critical business functions from interruption due to incidents, or at least minimize the effects. BCM is essential to any organization to keep technology and business in line with current threats to the continuation of business as usual. The BCM should be included in an organizations risk analysis plan to ensure that all of the necessary business functions have what they need to keep going in the event of any type of threat to any business function.\n\nIt encompasses:\nBULLET::::- Analysis of requirements, e.g., identifying critical business functions, dependencies and potential failure points, potential threats and hence incidents or risks of concern to the organization;\nBULLET::::- Specification, e.g., maximum tolerable outage periods; recovery point objectives (maximum acceptable periods of data loss);\nBULLET::::- Architecture and design, e.g., an appropriate combination of approaches including resilience (e.g. engineering IT systems and processes for high availability, avoiding or preventing situations that might interrupt the business), incident and emergency management (e.g., evacuating premises, calling the emergency services, triage/situation assessment and invoking recovery plans), recovery (e.g., rebuilding) and contingency management (generic capabilities to deal positively with whatever occurs using whatever resources are available);\nBULLET::::- Implementation, e.g., configuring and scheduling backups, data transfers, etc., duplicating and strengthening critical elements; contracting with service and equipment suppliers;\nBULLET::::- Testing, e.g., business continuity exercises of various types, costs and assurance levels;\nBULLET::::- Management, e.g., defining strategies, setting objectives and goals; planning and directing the work; allocating funds, people and other resources; prioritization relative to other activities; team building, leadership, control, motivation and coordination with other business functions and activities (e.g., IT, facilities, human resources, risk management, information risk and security, operations); monitoring the situation, checking and updating the arrangements when things change; maturing the approach through continuous improvement, learning and appropriate investment;\nBULLET::::- Assurance, e.g., testing against specified requirements; measuring, analyzing and reporting key parameters; conducting additional tests, reviews and audits for greater confidence that the arrangements will go to plan if invoked.\n\nWhereas BCM takes a broad approach to minimizing disaster-related risks by reducing both the probability and the severity of incidents, a disaster recovery plan (DRP) focuses specifically on resuming business operations as quickly as possible after a disaster. A disaster recovery plan, invoked soon after a disaster occurs, lays out the steps necessary to recover critical information and communications technology (ICT) infrastructure. Disaster recovery planning includes establishing a planning group, performing risk assessment, establishing priorities, developing recovery strategies, preparing inventories and documentation of the plan, developing verification criteria and procedure, and lastly implementing the plan.\n\nBelow is a partial listing of governmental laws and regulations in various parts of the world that have, had, or will have, a significant effect on data processing and information security. Important industry sector regulations have also been included when they have a significant impact on information security.\n\nBULLET::::- The UK Data Protection Act 1998 makes new provisions for the regulation of the processing of information relating to individuals, including the obtaining, holding, use or disclosure of such information. The European Union Data Protection Directive (EUDPD) requires that all E.U. members adopt national regulations to standardize the protection of data privacy for citizens throughout the E.U.\nBULLET::::- The Computer Misuse Act 1990 is an Act of the U.K. Parliament making computer crime (e.g., hacking) a criminal offense. The act has become a model upon which several other countries, including Canada and the Republic of Ireland, have drawn inspiration from when subsequently drafting their own information security laws.\nBULLET::::- The E.U.'s Data Retention Directive (annulled) required internet service providers and phone companies to keep data on every electronic message sent and phone call made for between six months and two years.\nBULLET::::- The Family Educational Rights and Privacy Act (FERPA) ( g; 34 CFR Part 99) is a U.S. Federal law that protects the privacy of student education records. The law applies to all schools that receive funds under an applicable program of the U.S. Department of Education. Generally, schools must have written permission from the parent or eligible student in order to release any information from a student's education record.\nBULLET::::- The Federal Financial Institutions Examination Council's (FFIEC) security guidelines for auditors specifies requirements for online banking security.\nBULLET::::- The Health Insurance Portability and Accountability Act (HIPAA) of 1996 requires the adoption of national standards for electronic health care transactions and national identifiers for providers, health insurance plans, and employers. Additionally, it requires health care providers, insurance providers and employers to safeguard the security and privacy of health data.\nBULLET::::- The Gramm–Leach–Bliley Act of 1999 (GLBA), also known as the Financial Services Modernization Act of 1999, protects the privacy and security of private financial information that financial institutions collect, hold, and process.\nBULLET::::- Section 404 of the Sarbanes–Oxley Act of 2002 (SOX) requires publicly traded companies to assess the effectiveness of their internal controls for financial reporting in annual reports they submit at the end of each fiscal year. Chief information officers are responsible for the security, accuracy and the reliability of the systems that manage and report the financial data. The act also requires publicly traded companies to engage with independent auditors who must attest to, and report on, the validity of their assessments.\nBULLET::::- The Payment Card Industry Data Security Standard (PCI DSS) establishes comprehensive requirements for enhancing payment account data security. It was developed by the founding payment brands of the PCI Security Standards Council — including American Express, Discover Financial Services, JCB, MasterCard Worldwide and Visa International — to help facilitate the broad adoption of consistent data security measures on a global basis. The PCI DSS is a multifaceted security standard that includes requirements for security management, policies, procedures, network architecture, software design and other critical protective measures.\nBULLET::::- State security breach notification laws (California and many others) require businesses, nonprofits, and state institutions to notify consumers when unencrypted \"personal information\" may have been compromised, lost, or stolen.\nBULLET::::- The Personal Information Protection and Electronics Document Act (PIPEDA) of Canada supports and promotes electronic commerce by protecting personal information that is collected, used or disclosed in certain circumstances, by providing for the use of electronic means to communicate or record information or transactions and by amending the Canada Evidence Act, the Statutory Instruments Act and the Statute Revision Act.\nBULLET::::- Greece's Hellenic Authority for Communication Security and Privacy (ADAE) (Law 165/2011) establishes and describes the minimum information security controls that should be deployed by every company which provides electronic communication networks and/or services in Greece in order to protect customers' confidentiality. These include both managerial and technical controls (e.g., log records should be stored for two years).\nBULLET::::- Greece's Hellenic Authority for Communication Security and Privacy (ADAE) (Law 205/2013) concentrates around the protection of the integrity and availability of the services and data offered by Greek telecommunication companies. The law forces these and other related companies to build, deploy and test appropriate business continuity plans and redundant infrastructures.<ref name=\"205/2013\"></ref>\n\nDescribing more than simply how security aware employees are, information security culture is the ideas, customs, and social behaviors of an organization that impact information security in both positive and negative ways. Cultural concepts can help different segments of the organization work effectively or work against effectiveness towards information security within an organization. The way employees think and feel about security and the actions they take can have a big impact on information security in organizations. Roer & Petric (2017) identify seven core dimensions of information security culture in organizations:\n\nBULLET::::- Attitudes: Employees’ feelings and emotions about the various activities that pertain to the organizational security of information.\nBULLET::::- Behaviors: Actual or intended activities and risk-taking actions of employees that have direct or indirect impact on information security.\nBULLET::::- Cognition: Employees' awareness, verifiable knowledge, and beliefs regarding practices, activities, and self-efficacy relation that are related to information security.\nBULLET::::- Communication: Ways employees communicate with each other, sense of belonging, support for security issues, and incident reporting.\nBULLET::::- Compliance: Adherence to organizational security policies, awareness of the existence of such policies and the ability to recall the substance of such policies.\nBULLET::::- Norms: Perceptions of security-related organizational conduct and practices that are informally deemed either normal or deviant by employees and their peers, e.g. hidden expectations regarding security behaviors and unwritten rules regarding uses of information-communication technologies.\nBULLET::::- Responsibilities: Employees' understanding of the roles and responsibilities they have as a critical factor in sustaining or endangering the security of information, and thereby the organization.\n\nAndersson and Reimers (2014) found that employees often do not see themselves as part of the organization Information Security \"effort\" and often take actions that ignore organizational information security best interests. Research shows information security culture needs to be improved continuously. In \"Information Security Culture from Analysis to Change\", authors commented, \"It's a never ending process, a cycle of evaluation and change or maintenance.\" To manage the information security culture, five steps should be taken: pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.\n\nBULLET::::- Pre-Evaluation: to identify the awareness of information security within employees and to analyze current security policy\nBULLET::::- Strategic Planning: to come up a better awareness-program, we need to set clear targets. Clustering people is helpful to achieve it\nBULLET::::- Operative Planning: create a good security culture based on internal communication, management buy-in, security awareness and training programs\nBULLET::::- Implementation: should feature commitment of management, communication with organizational members, courses for all organizational members, and commitment of the employees\nBULLET::::- Post-evaluation: to better gauge the effectiveness of the prior steps and build on continuous improvement\n\nThe International Organization for Standardization (ISO) is a consortium of national standards institutes from 157 countries, coordinated through a secretariat in Geneva, Switzerland. ISO is the world's largest developer of standards. ISO 15443: \"Information technology – Security techniques – A framework for IT security assurance\", ISO/IEC 27002: \"Information technology – Security techniques – Code of practice for information security management\", ISO-20000: \"Information technology – Service management\", and ISO/IEC 27001: \"Information technology – Security techniques – Information security management systems – Requirements\" are of particular interest to information security professionals.\n\nThe US National Institute of Standards and Technology (NIST) is a non-regulatory federal agency within the U.S. Department of Commerce. The NIST Computer Security Division\ndevelops standards, metrics, tests and validation programs as well as publishes standards and guidelines to increase secure IT planning, implementation, management and operation. NIST is also the custodian of the U.S. Federal Information Processing Standard publications (FIPS).\n\nThe Internet Society is a professional membership society with more than 100 organizations and over 20,000 individual members in over 180 countries. It provides leadership in addressing issues that confront the future of the internet, and it is the organizational home for the groups responsible for internet infrastructure standards, including the Internet Engineering Task Force (IETF) and the Internet Architecture Board (IAB). The ISOC hosts the Requests for Comments (RFCs) which includes the Official Internet Protocol Standards and the RFC-2196 Site Security Handbook.\n\nThe Information Security Forum (ISF) is a global nonprofit organization of several hundred leading organizations in financial services, manufacturing, telecommunications, consumer goods, government, and other areas. It undertakes research into information security practices and offers advice in its biannual Standard of Good Practice and more detailed advisories for members.\n\nThe Institute of Information Security Professionals (IISP) is an independent, non-profit body governed by its members, with the principal objective of advancing the professionalism of information security practitioners and thereby the professionalism of the industry as a whole. The institute developed the IISP Skills Framework. This framework describes the range of competencies expected of information security and information assurance professionals in the effective performance of their roles. It was developed through collaboration between both private and public sector organizations and world-renowned academics and security leaders.\n\nThe German Federal Office for Information Security (in German \"Bundesamt für Sicherheit in der Informationstechnik (BSI)\") BSI-Standards 100-1 to 100-4 are a set of recommendations including \"methods, processes, procedures, approaches and measures relating to information security\". The BSI-Standard 100-2 \"IT-Grundschutz Methodology\" describes how information security management can be implemented and operated. The standard includes a very specific guide, the IT Baseline Protection Catalogs (also known as IT-Grundschutz Catalogs). Before 2005, the catalogs were formerly known as \"IT Baseline Protection Manual\". The Catalogs are a collection of documents useful for detecting and combating security-relevant weak points in the IT environment (IT cluster). The collection encompasses as of September 2013 over 4,400 pages with the introduction and catalogs. The IT-Grundschutz approach is aligned with to the ISO/IEC 2700x family.\n\nThe European Telecommunications Standards Institute standardized a catalog of information security indicators, headed by the Industrial Specification Group (ISG) ISI.\n\nBULLET::::- Backup\nBULLET::::- Data breach\nBULLET::::- Data-centric security\nBULLET::::- Enterprise information security architecture\nBULLET::::- Identity-based security\nBULLET::::- Information infrastructure\nBULLET::::- Information security audit\nBULLET::::- Information security indicators\nBULLET::::- Information security management\nBULLET::::- Information security standards\nBULLET::::- Information technology security audit\nBULLET::::- IT risk\nBULLET::::- ITIL security management\nBULLET::::- Kill chain\nBULLET::::- List of Computer Security Certifications\nBULLET::::- Mobile security\nBULLET::::- Network Security Services\nBULLET::::- Privacy engineering\nBULLET::::- Privacy software\nBULLET::::- Privacy-enhancing technologies\nBULLET::::- Security bug\nBULLET::::- Security convergence\nBULLET::::- Security information management\nBULLET::::- Security level management\nBULLET::::- Security of Information Act\nBULLET::::- Security service (telecommunication)\nBULLET::::- Single sign-on\nBULLET::::- Verification and validation\nBULLET::::- Anderson, K., \"IT Security Professionals Must Evolve for Changing Market\", \"SC Magazine\", October 12, 2006.\nBULLET::::- Aceituno, V., \"On Information Security Paradigms\", \"ISSA Journal\", September 2005.\nBULLET::::- Dhillon, G., \"Principles of Information Systems Security: text and cases\", John Wiley & Sons, 2007.\nBULLET::::- Easttom, C., \"Computer Security Fundamentals (2nd Edition)\" Pearson Education, 2011.\nBULLET::::- Lambo, T., \"ISO/IEC 27001: The future of infosec certification\", \"ISSA Journal\", November 2006.\nBULLET::::- Dustin, D., \" Awareness of How Your Data is Being Used and What to Do About It\", \"CDR Blog\", May 2017.\n\n\nBULLET::::- DoD IA Policy Chart on the DoD Information Assurance Technology Analysis Center web site.\nBULLET::::- patterns & practices Security Engineering Explained\nBULLET::::- Open Security Architecture- Controls and patterns to secure IT systems\nBULLET::::- IWS – Information Security Chapter\nBULLET::::- Ross Anderson's book \"Security Engineering\"\n"}
{"id": "15037", "url": "https://en.wikipedia.org/wiki?curid=15037", "title": "Income", "text": "Income\n\nIncome is the consumption and saving opportunity gained by an entity within a specified timeframe, which is generally expressed in monetary terms. \n\nFor households and individuals, \"income is the sum of all the wages, salaries, profits, interest payments, rents, and other forms of earnings received in a given period of time.\" (also known as gross income). Net income is defined as the gross income minus taxes and other deductions (e.g., mandatory pension contributions), and is usually the basis to calculate how much income tax is owed. \n\nIn the field of public economics, the concept may comprise the accumulation of both monetary and non-monetary consumption ability, with the former (monetary) being used as a proxy for total income.\n\nFor a firm, gross income can be defined as sum of all revenue minus the cost of goods sold. Net income nets out expenses: net income equals revenue minus cost of goods sold, expenses, depreciation, interest, and taxes.\n\nIn economics, \"factor income\" is the return accruing for a person, or a nation, derived from the \"factors of production\": rental income, wages generated by labor, the interest created by capital, and profits from entrepreneurial ventures.\n\nIn consumer theory 'income' is another name for the \"budget constraint,\" an amount formula_1 to be spent on different goods x and y in quantities formula_2 and formula_3 at prices formula_4 and formula_5. The basic equation for this is\nThis equation implies two things. First buying one more unit of good x implies buying formula_7 less units of good y. So, formula_7 is the \"relative\" price of a unit of x as to the number of units given up in y. Second, if the price of x falls for a fixed formula_1 and fixed formula_10 then its relative price falls. The usual hypothesis, the law of demand, is that the quantity demanded of x would increase at the lower price. The analysis can be generalized to more than two goods.\n\nThe theoretical generalization to more than one period is a multi-period wealth and income constraint. For example, the same person can gain more productive skills or acquire more productive income-earning assets to earn a higher income. In the multi-period case, something might also happen to the economy beyond the control of the individual to reduce (or increase) the flow of income. Changing measured income and its relation to consumption over time might be modeled accordingly, such as in the permanent income hypothesis.\n\n\"Full income\" refers to the accumulation of both the monetary and the non-monetary consumption-ability of any given entity, such as a person or a household. According to what the economist Nicholas Barr describes as the \"classical definition of income\" (the 1938 Haig-Simons definition): \"income may be defined as the... sum of (1) the market value of rights exercised in consumption and (2) the change in the value of the store of property rights...\" Since the consumption potential of non-monetary goods, such as leisure, cannot be measured, monetary income may be thought of as a proxy for full income. As such, however, it is criticized for being unreliable, \"i.e.\" failing to accurately reflect affluence (and thus the consumption opportunities) of any given agent. It omits the utility a person may derive from non-monetary income and, on a macroeconomic level, fails to accurately chart social welfare. According to Barr, \"in practice money income as a proportion of total income varies widely and unsystematically. Non-observability of full-income prevent a complete characterization of the individual opportunity set, forcing us to use the unreliable yardstick of money income.\n\nIncome per capita has been increasing steadily in most countries. Many factors contribute to people having a higher income, including education, globalisation and favorable political circumstances such as economic freedom and peace. Increases in income also tend to lead to people choosing to work fewer hours.\nDeveloped countries (defined as countries with a \"developed economy\") have higher incomes as opposed to developing countries tending to have lower incomes.\n\nIncome inequality is the extent to which income is distributed in an uneven manner. It can be measured by various methods, including the Lorenz curve and the Gini coefficient. Many economists argue that certain amounts of inequality are necessary and desirable but that excessive inequality leads to efficiency problems and social injustice.\n\nNational income, measured by statistics such as net national income (NNI), measures the total income of individuals, corporations, and government in the economy. For more information see Measures of national income and output.\n\nThroughout history, many have written about the impact of income on morality and society. Saint Paul wrote 'For the love of money is a root of all kinds of evil:' ( (ASV)).\n\nSome scholars have come to the conclusion that material progress and prosperity, as manifested in continuous income growth at both the individual and the national level, provide the indispensable foundation for sustaining any kind of morality. This argument was explicitly given by Adam Smith in his \"Theory of Moral Sentiments\", and has more recently been developed by Harvard economist Benjamin Friedman in his book \"The Moral Consequences of Economic Growth\".\n\nThe International Accounting Standards Board (IASB) uses the following definition: \"Income is increases in economic benefits during the accounting period in the form of inflows or enhancements of assets or decreases of liabilities that result in increases in equity, other than those relating to contributions from equity participants.\" [F.70] (IFRS Framework).\n\nAccording :) to John Hicks' definitions, income \"is the maximum amount which can be spent during a period if there is to be an expectation of maintaining intact, the capital value of prospective receipts (in money terms)”.\n\nJohn Hicks used \"I\" for income, but Keynes wrote to him in 1937, \"\"after trying both, I believe it is easier to use Y for income and I for investment.\"\" Some consider Y as an alternative letter for the phonem I in languages like Spanish, although Y as the \"Greek I\" was actually pronounced like the modern German ü or the phonetic /y/.\n\nBULLET::::- Basic income\nBULLET::::- Comprehensive income\nBULLET::::- Income tax\nBULLET::::- Unpaid work\n\nBULLET::::- D. Usher (1987). \"real income,\" \"The \", v. 4, pp. 104–5\n"}
{"id": "15039", "url": "https://en.wikipedia.org/wiki?curid=15039", "title": "Iona", "text": "Iona\n\nIona (, sometimes simply \"Ì\") is a small island in the Inner Hebrides off the Ross of Mull on the western coast of Scotland. It is mainly known for Iona Abbey, though there are other buildings on the island. Iona Abbey was a centre of Gaelic monasticism for three centuries and is today known for its relative tranquility and natural environment. It is a tourist destination and a place for spiritual retreats. Its modern Scottish Gaelic name means \"Iona of (Saint) Columba\" (formerly anglicised \"Icolmkill\").\n\nThe Hebrides have been occupied by the speakers of several languages since the Iron Age, and as a result many of the names of these islands have more than one possible meaning. Nonetheless few, if any, can have accumulated as many different names over the centuries as the island now known in English as \"Iona\".\n\nThe earliest forms of the name enabled place-name scholar William J. Watson to show that the name originally meant something like \"yew-place\". The element \"Ivo-\", denoting \"yew\", occurs in Ogham inscriptions (\"Iva-cattos\" [genitive], \"Iva-geni\" [genitive]) and in Gaulish names (\"Ivo-rix\", \"Ivo-magus\") and may form the basis of early Gaelic names like \"Eogan\" (ogham: \"Ivo-genos\"). It is possible that the name is related to the mythological figure, \"Fer hÍ mac Eogabail\", foster-son of Manannan, the forename meaning \"man of the yew\".\n\nMac an Tàilleir (2003) lists the more recent Gaelic names of \"Ì\", \"Ì Chaluim Chille\" and \"Eilean Idhe\" noting that the first named is \"generally lengthened to avoid confusion\" to the second, which means \"Calum's (i.e. in latinised form \"Columba's\") Iona\" or \"island of Calum's monastery\". The confusion results from \"ì\", despite its original etymology as the name of the island, being confused with the Gaelic noun \"ì\" \"island\" (now obsolete) of Old Norse origin (\"ey\" \"island\", \"Eilean Idhe\" means \"the isle of Iona\", also known as \"Ì nam ban bòidheach\" (\"the isle of beautiful women\"). The modern English name comes of yet another variant, \"Ioua\", which was either just Adomnán's attempt to make the Gaelic name fit Latin grammar or else a genuine derivative from \"Ivova\" (\"yew place\"). \"Ioua\"'s change to \"Iona\", attested from c.1274, results from a transcription mistake resulting from the similarity of \"n\" and \"u\" in Insular Minuscule.\n\nDespite the continuity of forms in Gaelic between the pre-Norse and post-Norse eras, Haswell-Smith (2004) speculates that the name may have a Norse connection, \"Hiōe\" meaning \"island of the den of the brown bear\", The medieval English language version was \"Icolmkill\" (and variants thereof).\n! style=\"background:#dddcec;\" colspan=\"5\"Table of earliest forms (incomplete)\n! style=\"background:#ddddec; width:15%;\"Form\n! style=\"background:#ddddec; width:25%;\"Source\n! style=\"background:#ddddec; width:10%;\"Language\n! style=\"background:#ddddec; width:50%;\"Notes\nIoua insula\nAdomnán's \"Vita Columbae\" (c. 700)\nLatin\nAdomnán calls Eigg \"Egea insula\" and Skye \"Scia insula\"\nHii, Hy\nBede's \"Historia ecclesiastica gentis Anglorum\"\nLatin\nEoa, Iae, Ie,<br>I Cholaim Chille\n\"Annals of Ulster\"\nIrish, Latin\nU563 \"Nauigatio Coluim Chille ad Insolam Iae\"<br>\"The journey of St Columba to Í\"<br>U716 \"Pascha comotatur in Eoa ciuitate\"<br>\"The date of Easter is changed in the monastery of Í\")<br>U717 \"Expulsio familie Ie\"<br>\"The expulsion of the community of Í\"<br>U778 \"Niall...a nn-I Cholaim Chille\"<br>\"Niall... in Í Cholaim Chille\"\nHi, Eu\n\"Lebor na hUidre\"\nIrish\n\"Hi con ilur a mmartra\"<br>\"Hi with the multitude of its relics\"<br> \"in tan conucaib a chill hi tosuċ .i. Eu\"<br>\"the time he raised his church first i.e. Eu\"\nEo\nWalafrid Strabo (c. 831)\nLatin\n\"Insula Pictorum quaedam monstratur in oris fluctivago suspensa salo, cognominis Eo\"<br>\"On the coasts of the Picts is pointed out an isle poised in the rolling sea, whose name is \"Eo\"\"\nEuea insula\n\"Life of St Cathróe of Metz\"\nLatin\n\nMurray (1966) claims that the \"ancient\" Gaelic name was \"Innis nan Druinich\" (\"the isle of Druidic hermits\") and repeats a Gaelic story (which he admits is apocryphal) that as Columba's coracle first drew close to the island one of his companions cried out \"\"Chì mi i\"\" meaning \"I see her\" and that Columba's response was \"Henceforth we shall call her Ì\".\n\nIona lies about from the coast of Mull. It is about wide and long with a resident population of 125. The geology of the island consists mainly of Precambrian Lewisian gneiss with Torridonian sedimentary rocks on the eastern side and small outcrops of pink granite on the eastern beaches. Like other places swept by ocean breezes, there are few trees; most of them are near the parish church.\n\nIona's highest point is Dùn Ì, , an Iron Age hill fort dating from 100 BC – AD 200. Iona's geographical features include the Bay at the Back of the Ocean and \"Càrn Cùl ri Éirinn\" (the Hill/Cairn of [turning the] Back to Ireland), said to be adjacent to the beach where St. Columba first landed.\n\nThe main settlement, located at St. Ronan's Bay on the eastern side of the island, is called \"Baile Mòr\" and is also known locally as \"The Village\". The primary school, post office, the island's two hotels, the Bishop's House and the ruins of the Nunnery are here. The Abbey and MacLeod Centre are a short walk to the north. Port Bàn (white port) beach on the west side of the island is home to the Iona Beach Party.\n\nThere are numerous offshore islets and skerries: Eilean Annraidh (island of storm) and Eilean Chalbha (calf island) to the north, Rèidh Eilean and Stac MhicMhurchaidh to the west and Eilean Mùsimul (mouse holm island) and Soa Island to the south are amongst the largest. The steamer \"Cathcart Park\" carrying a cargo of salt from Runcorn to Wick ran aground on Soa on 15 April 1912, the crew of 11 escaping in two boats.\n\nOn a map of 1874, the following territorial subdivision is indicated (from north to south):\nBULLET::::- Ceann Tsear\nBULLET::::- Sliabh Meanach\nBULLET::::- Machar\nBULLET::::- Sliginach\nBULLET::::- Sliabh Siar\nBULLET::::- Staonaig\n\nIn the early Historic Period Iona lay within the Gaelic kingdom of Dál Riata, in the region controlled by the Cenél Loairn (i.e. Lorn, as it was then). The island was the site of a highly important monastery (see Iona Abbey) during the Early Middle Ages. According to tradition the monastery was founded in 563 by the monk Columba, also known as Colm Cille, who had been exiled from his native Ireland as a result of his involvement in the Battle of Cul Dreimhne. Columba and twelve companions went into exile on Iona and founded a monastery there. The monastery was hugely successful, and played a crucial role in the conversion to Christianity of the Picts of present-day Scotland in the late 6th century and of the Anglo-Saxon kingdom of Northumbria in 635. Many satellite institutions were founded, and Iona became the centre of one of the most important monastic systems in Great Britain and Ireland.\n\nIona became a renowned centre of learning, and its scriptorium produced highly important documents, probably including the original texts of the Iona Chronicle, thought to be the source for the early Irish annals. The monastery is often associated with the distinctive practices and traditions known as Celtic Christianity. In particular, Iona was a major supporter of the \"Celtic\" system for calculating the date of Easter at the time of the Easter controversy, which pitted supporters of the Celtic system against those favoring the \"Roman\" system used elsewhere in Western Christianity. The controversy weakened Iona's ties to Northumbria, which adopted the Roman system at the Synod of Whitby in 664, and to Pictland, which followed suit in the early 8th century. Iona itself did not adopt the Roman system until 715, according to the Anglo-Saxon historian Bede. Iona's prominence was further diminished over the next centuries as a result of Viking raids and the rise of other powerful monasteries in the system, such as the Abbey of Kells.\n\nThe Book of Kells may have been produced or begun on Iona towards the end of the 8th century. Around this time the island's exemplary high crosses were sculpted; these may be the first such crosses to contain the ring around the intersection that became characteristic of the \"Celtic cross\". The series of Viking raids on Iona began in 794 and, after its treasures had been plundered many times, Columba's relics were removed and divided two ways between Scotland and Ireland in 849 as the monastery was abandoned.\n\nAs the Norse domination of the west coast of Scotland advanced, Iona became part of the Kingdom of the Isles. The Norse \"Rex plurimarum insularum\" Amlaíb Cuarán died in 980 or 981 whilst in \"religious retirement\" on Iona. Nonetheless the island was sacked twice by his successors, on Christmas night 986 and again in 987. Although Iona was never again important to Ireland, it rose to prominence once more in Scotland following the establishment of the Kingdom of Scotland in the later 9th century; the ruling dynasty of Scotland traced its origin to Iona, and the island thus became an important spiritual centre for the new kingdom, with many of its early kings buried there. However, a campaign by Magnus Barelegs led to the formal acknowledgement of Norwegian control of Argyll, in 1098.\n\nSomerled, the brother-in-law of Norway's governor of the region (the \"King of the Isles\"), launched a revolt, and made the kingdom independent. A convent for Benedictine nuns was established in about 1208, with Bethóc, Somerled's daughter, as first prioress. The present Benedictine abbey, Iona Abbey, was built in about 1203. \n\nOn Somerled's death, nominal Norwegian overlordship of the Kingdom was re-established, but de facto control was split between Somerled's sons, and his brother-in-law.\n\nFollowing the 1266 Treaty of Perth the Hebrides were transferred from Norwegian to Scottish overlordship. At the end of the century, king John Balliol was challenged for the throne by Robert de Bruys (Robert The Bruce). By this point, Somerled's descendants had split into three groups, the MacRory, MacDougalls, and MacDonalds. The MacDougalls backed Balliol, so when he was defeated by de Bruys, the latter exiled the MacDougalls and transferred their island territories to the MacDonalds; by marrying the heir of the MacRorys, the heir of the MacDonalds re-unified most of Somerled's realm, creating the Lordship of the Isles, under nominal Scottish authority. Iona, which had been a MacDougall territory (together with the rest of Lorn), was given to the Campbells, where it remained for half a century.\n\nIn 1354, though in exile and without control of his ancestral lands, John, the MacDougall heir, quitclaimed any rights he had over Mull and Iona to the Lord of the Isles (though this had no meaningful effect at the time). When Robert's son, David II, King of Scotland, became king, he spent some time in English captivity; following his release, in 1357, he restored MacDougall authority over Lorn. The 1354 quitclaim, which seems to have been an attempt to ensure peace in just such an eventuality, took automatic effect, splitting Mull and Iona from Lorn, and making it subject to the Lordship of the Isles. Iona remained part of the Lordship of the Isles for the next century and a half.\n\nFollowing the 1491 Raid on Ross, the Lordship of the Isles was dismantled, and Scotland gained full control of Iona for the second time. The monastery and nunnery continued to be active until the Reformation, when buildings were demolished and all but three of the 360 carved crosses destroyed. The Augustine nunnery now only survives as a number of 13th century ruins, including a church and cloister. By the 1760s little more of the nunnery remained standing than at present, though it is the most complete remnant of a medieval nunnery in Scotland.\n\nAfter a visit in 1773, the English writer Samuel Johnson remarked:\n\nHe estimated the population of the village at 70 families or perhaps 350 inhabitants.\n\nIn the 19th century green-streaked marble was commercially mined in the south-east of Iona; the quarry and machinery survive, see 'Marble Quarry remains' below.\n\nIona Abbey, now an ecumenical church, is of particular historical and religious interest to pilgrims and visitors alike. It is the most elaborate and best-preserved ecclesiastical building surviving from the Middle Ages in the Western Isles of Scotland. Though modest in scale in comparison to medieval abbeys elsewhere in Western Europe, it has a wealth of fine architectural detail, and monuments of many periods. The 8th Duke of Argyll presented the sacred buildings and sites of the island to the Iona Cathedral trust in 1899.\n\nIn front of the Abbey stands the 9th century St Martin's Cross, one of the best-preserved Celtic crosses in the British Isles, and a replica of the 8th century St John's Cross (original fragments in the Abbey museum).\n\nThe ancient burial ground, called the Rèilig Odhrain (Eng: Oran's \"burial place\" or \"cemetery\"), contains the 12th century chapel of St Odhrán (said to be Columba's uncle), restored at the same time as the Abbey itself. It contains a number of medieval grave monuments. The abbey graveyard is said to contain the graves of many early Scottish Kings, as well as Norse kings from Ireland and Norway. Iona became the burial site for the kings of Dál Riata and their successors. Notable burials there include:\nBULLET::::- Cináed mac Ailpín, king of the Picts (also known today as \"Kenneth I of Scotland\")\nBULLET::::- Domnall mac Causantín, alternatively \"king of the Picts\" or \"king of Scotland\"\nBULLET::::- Máel Coluim mac Domnaill, king of Scotland (\"Malcolm I\")\nBULLET::::- Donnchad mac Crínáin, king of Scotland (\"Duncan I\")\nBULLET::::- Mac Bethad mac Findlaích, king of Scotland (\"Macbeth\")\nBULLET::::- Domnall mac Donnchada, king of Scotland (\"Domnall Bán\" or \"Donald III\")\nBULLET::::- John Smith, Labour Party Leader\n\nIn 1549 an inventory of 48 Scottish, 8 Norwegian and 4 Irish kings was recorded. None of these graves are now identifiable (their inscriptions were reported to have worn away at the end of the 17th century). Saint Baithin and Saint Failbhe may also be buried on the island. The Abbey graveyard is also the final resting place of John Smith, the former Labour Party leader, who loved Iona. His grave is marked with an epitaph quoting Alexander Pope: \"An honest man's the noblest work of God\".\n\nLimited archaeological investigations commissioned by the National Trust for Scotland found some evidence for ancient burials in 2013. The excavations, conducted in the area of Martyrs Bay, revealed burials from the 6th-8th centuries, probably jumbled up and reburied in the 13-15th century. \n\nOther early Christian and medieval monuments have been removed for preservation to the cloister arcade of the Abbey, and the Abbey museum (in the medieval infirmary). The ancient buildings of Iona Abbey are now cared for by Historic Environment Scotland (entrance charge).\n\nThe remains of a marble quarrying enterprise can be seen in a small bay on the south-east shore of Iona. The quarry is the source of ‘Iona Marble’, a beautiful translucent green and white stone, much used in brooches and other jewellery. The stone has been known of for centuries and was credited with healing and other powers. While the quarry had been used in a small way, it was not until around the end of the 18th century when it was opened up on a more industrial scale by the Duke of Argyle. The then difficulties of extracting the hard stone and transporting it meant that the scheme was short lived. Another attempt was started in 1907, this time more successful with considerable quantities of stone extracted and indeed exported, but the First World War put paid to this as well, with little quarrying after 1914 and the operation finally closing in 1919. A painting showing the quarry in operation, \"The Marble Quarry, Iona\" (1909) by David Young Cameron, is in the collection of Cartwright Hall art gallery in Bradford.. Such is the site’s rarity that it has been designated as a Scheduled Ancient Monument.\n\nThe island, other than the land owned by the Iona Cathedral Trust, was purchased from the Duke of Argyll by Hugh Fraser in 1979 and donated to the National Trust for Scotland. In 2001 Iona's population was 125 and by the time of the 2011 census this had grown to 177 usual residents. During the same period Scottish island populations as a whole grew by 4% to 103,702.\n\nNot to be confused with the local island community, Iona (Abbey) Community are based within Iona Abbey. \n\nIn 1938 George MacLeod founded the Iona Community, an ecumenical Christian community of men and women from different walks of life and different traditions in the Christian church committed to seeking new ways of living the Gospel of Jesus in today's world. This community is a leading force in the present Celtic Christian revival.\n\nThe Iona Community runs 3 residential centres on the Isle of Iona and on Mull, where one can live together in community with people of every background from all over the world. Weeks at the centres often follow a programme related to the concerns of the Iona Community.\n\nThe 8 tonne \"Fallen Christ\" sculpture by Ronald Rae was permanently situated outside the MacLeod Centre in February 2008.\n\nVisitors can reach Iona by the 10-minute ferry trip across the Sound of Iona from Fionnphort on Mull. The most common route is via Oban in Argyll and Bute. Regular ferries connect to Craignure on Mull, from where the scenic road runs to Fionnphort. Tourist coaches and local bus services meet the ferries.\n\nCar ownership is lightly regulated, with no requirement for an MOT Certificate or payment of Road Tax for cars kept permanently on the island, but vehicular access is restricted to permanent residents and there are few cars. Visitors must leave their car in Fionnphort, but upon landing on Iona they will find the village, the shops, the post office, the cafe, the hotels and the abbey are all within walking distance. Bike hire is available at the pier, and on Mull.\n\nIn addition to the hotels, there are several bed and breakfasts on Iona and various self-catering properties. The Iona Hostel at Lagandorain and the Iona campsite at Cnoc Oran also offer accommodation.\n\nThe island of Iona has played an important role in Scottish landscape painting, especially during the Twentieth Century. As travel to north and west Scotland became easier from the mid C18 on, artists’ visits to the island steadily increased. The Abbey remains in particular became frequently recorded during this early period. Many of the artists are listed and illustrated in the valuable book, \"‘Iona Portrayed – The Island through Artists’ Eyes 1760-1960’\", which lists over 170 artists known to have painted on the island.\n\nThe C20 however saw the greatest period of influence on landscape painting, in particular through the many paintings of the island produced by F C B Cadell and S J Peploe, two of the ‘Scottish Colourists’. As with many artists, both professional and amateur, they were attracted by the unique quality of light, the white sandy beaches, the aquamarine colours of the sea and the landscape of rich greens and rocky outcrops. While Cadell and Peploe are perhaps best known, many major Scottish painters of the C20 worked on Iona and visited many times – for example George Houston, D Y Cameron, James Shearer, John Duncan and John Maclauchlan Milne, among many.\n\nSamuel Johnson wrote \"That man is little to be envied whose patriotism would not gain force upon the plains of Marathon, or whose piety would not grow warmer amid the ruins of Iona.\"\n\nIn Jules Verne's novel \"The Green Ray\", the heroes visit Iona in chapters 13 to 16. The inspiration is romantic, the ruins of the island are conducive to daydreaming. The young heroine, Helena Campbell, argues that Scotland in general and Iona in particular are the scene of the appearance of goblins and other familiar demons.\n\nIn Jean Raspail's novel \"The Fisherman's Ring\" (1995), his cardinal is one of the last to support the antipope Benedict XIII and his successors.\n\nIn the novel \"The Carved Stone\" (by Guillaume Prévost), the young Samuel Faulkner is projected in time as he searches for his father and lands on Iona in the year 800, then threatened by the Vikings.\n\n\"Peace of Iona\" is a song written by Mike Scott that appears on the studio album \"Universal Hall\" and on the live recording \"Karma to Burn\" by The Waterboys. Iona is the setting for the song \"Oran\" on the 1997 Steve McDonald album \"Stone of Destiny\".\n\nKenneth C. Steven published an anthology of poetry entitled \"Iona: Poems\" in 2000 inspired by his association with the island and the surrounding area.\n\nIona is featured prominently in the first episode (\"By the Skin of Our Teeth\") of the celebrated arts series \"\" (1969).\n\nIona is the setting of Jeanne M. Dams' Dorothy Martin mystery \"Holy Terror of the Hebrides\" (1998).\n\nThe Academy Award–nominated Irish animated film \"The Secret of Kells\" is about the creation of the Book of Kells. One of the characters, Brother Aiden, is a master illuminator from Iona Abbey who had helped to illustrate the Book, but had to escape the island with it during a Viking invasion.\n\nAfter his death in 2011, the cremated remains of songwriter/recording artist Gerry Rafferty were scattered on Iona.\n\nFrances Macdonald the contemporary Scottish artist based in Crinian, Argyll, regularly paints landscapes on Iona.\n\nIona Abbey is mentioned in Tori Amos's \"Twinkle\" from her 1996 album \"Boys for Pele\":\n\"And last time I knew, she worked at an abbey in Iona. She said 'I killed a man, T, I've gotta stay hidden in this abbey' \"\n\nIona is the name of a progressive Celtic rock band (first album released in 1990; not active at present), many of whose songs are inspired by the island of Iona and Columba's life.\n\nNeil Gaiman's poem \"In Relig Odhrain\", published in \"Trigger Warning: Short Fictions and Disturbances (2015)\", retells the story of Oran's death, and the creation of the chapel on Iona. This poem was made into a short stop-motion animated film, released in 2019. \n\nBULLET::::- List of islands of Scotland\nBULLET::::- Bishop's House Iona\nBULLET::::- Clann-an-oistir\nBULLET::::- Dál Riata\nBULLET::::- Statutes of Iona\n\nBULLET::::- Christian, J & Stiller, C (2000), \"Iona Portrayed – The Island through Artists’ Eyes 1760-1960\", The New Iona Press, Inverness, 96pp, numerous illustrations in B&W and colour, with list of artists.\nBULLET::::- Dwelly, Edward (1911). \"Faclair Gàidhlig gu Beurla le Dealbhan/The Illustrated [Scottish] Gaelic- English Dictionary\". Edinburgh. Birlinn. .\nBULLET::::- Gregory, Donald (1881) \"The History of the Western Highlands and Isles of Scotland 1493–1625\". Edinburgh. Birlinn. 2008 reprint – originally published by Thomas D. Morrison. .\nBULLET::::- Hunter, James (2000). \"Last of the Free: A History of the Highlands and Islands of Scotland\". Edinburgh. Mainstream.\nBULLET::::- Johnson, Samuel (1775). \"A Journey to the Western Islands of Scotland\". London: Chapman & Dodd. (1924 edition).\nBULLET::::- Marsden, John (1995). \"The Illustrated Life of Columba\". Edinburgh. Floris Books. .\nBULLET::::- Murray, W. H. (1966). \"The Hebrides\". London. Heinemann.\nBULLET::::- Ó Corráin, Donnchadh (1998) \"Vikings in Ireland and Scotland in the Ninth Century\" CELT.\nBULLET::::- Watson, W. J., \"The History of the Celtic Place-names of Scotland\". Reprinted with an introduction by Simon Taylor, Birlinn, Edinburgh, 2004. .\n\nBULLET::::- Campbell, George F. (2006). \"The First and Lost Iona\". Glasgow: Candlemas Hill Publishing. (and on Kindle).\nBULLET::::- MacArthur, E Mairi, \"Iona, Colin Baxter Island Guide\" (1997) Colin Baxter Photography, Grantown-on-Spey, 128pp.\n\nBULLET::::- Visit Mull & Iona (Official tourism website for the Isles of Mull and Iona)\nBULLET::::- Isle of Iona, Scotland (A visitors guide to the Isle)\nBULLET::::- The Iona Community\nBULLET::::- Computer-generated virtual panorama Summit of Iona Index\nBULLET::::- Photo Gallery of Iona by Enrico Martino\nBULLET::::- National Trust for Scotland property page\n"}
{"id": "15040", "url": "https://en.wikipedia.org/wiki?curid=15040", "title": "Ido language", "text": "Ido language\n\nIdo (, sometimes ) is a constructed language, derived from Reformed Esperanto, created to be a universal second language for speakers of diverse backgrounds. Ido was specifically designed to be grammatically, orthographically, and lexicographically regular, and above all easy to learn and use. In this sense, Ido is classified as a constructed international auxiliary language. It is the most successful of many Esperanto derivatives, called Esperantidos.\n\nIdo was created in 1907 out of a desire to reform perceived flaws in Esperanto, a language that had been created 20 years earlier to facilitate international communication. The name of the language traces its origin to the Esperanto word \"\", meaning \"offspring\", since the language is a \"descendant\" of Esperanto. After its inception, Ido gained support from some in the Esperanto community, but following the sudden death in 1914 of one of its most influential proponents, Louis Couturat, it declined in popularity. There were two reasons for this: first, the emergence of further schisms arising from competing reform projects; and second, a general lack of awareness of Ido as a candidate for an international language. These obstacles weakened the movement and it was not until the rise of the Internet that it began to regain momentum.\n\nIdo uses the same 26 letters as the English (Latin) alphabet, with no diacritics. It draws its vocabulary from English, French, German, Italian, Latin, Russian, Spanish and Portuguese, and is largely intelligible to those who have studied Esperanto.\n\nSeveral works of literature have been translated into Ido, including \"The Little Prince\", the Book of Psalms, and the Gospel of Luke. As of the year 2000, there were approximately 100–200 Ido speakers in the world.\n\nThe idea of a universal second language is not new, and constructed languages are not a recent phenomenon. The first known constructed language was Lingua Ignota, created in the 12th century. But the idea did not catch on in large numbers until the language Volapük was created in 1879. Volapük was popular for some time and apparently had a few thousand users, but was later eclipsed by the popularity of Esperanto, which arose in 1887. Several other languages such as Latino sine Flexione and Idiom Neutral had also been put forward. It was during this time that French mathematician Louis Couturat formed the \"Delegation for the Adoption of an International Auxiliary Language\".\n\nThis delegation made a formal request to the International Association of Academies in Vienna to select and endorse an international language; the request was rejected in May 1907. The Delegation then met as a Committee in Paris in October 1907 to discuss the adoption of a standard international language. Among the languages considered was a new language anonymously submitted at the last moment (and therefore against the Committee rules) under the pen name \"Ido\". In the end the Committee, always without plenary sessions and consisting of only 12 members, concluded the last day with 4 votes for and 1 abstention. They concluded that no language was completely acceptable, but that Esperanto could be accepted \"on condition of several modifications to be realized by the permanent Commission in the direction defined by the conclusions of the Report of the Secretaries [Louis Couturat and Léopold Leau] and by the Ido project\".\n\nEsperanto's inventor, L. L. Zamenhof, having heard a number of complaints, had suggested in 1894 a proposal for a Reformed Esperanto with several changes that Ido adopted and made it closer to French: eliminating the accented letters and the accusative case, changing the plural to an Italianesque \"-i\", and replacing the table of correlatives with more Latinate words. However, the Esperanto community voted and rejected Reformed Esperanto, and likewise most rejected the recommendations of the 1907 Committee composed by 12 members. Zamenhof deferred to their judgment, although doubtful. Furthermore, controversy ensued when the \"Ido project\" was found to have been primarily devised by Louis de Beaufront, whom Zamenhof had chosen to represent Esperanto before the Committee, as the Committee's rules dictated that the creator of a submitted language could not defend it. The Committee's language was French and not everyone could speak in French. When the president of the Committee asked who was the author of Ido's project, Couturat, Beaufront and Leau answered that they were not. Beaufront was the person who presented Ido's project and gave a description as a better, richer version of Esperanto. Couturat, Leau, Beaufront and Jespersen were finally the only members who voted, all of them for Ido's project. A month later, Couturat accidentally put Jespersen in a copy of a letter in which he acknowledged that Beaufront was the author of the Ido project. Jespersen was angered by this and asked for a public confession, which was never forthcoming.\n\nIt is estimated that some 20% of Esperanto leaders and 3–4% of ordinary Esperantists defected to Ido, which from then on suffered constant modifications seeking to perfect it, but which ultimately had the effect of causing many Ido speakers to give up on trying to learn it. Although it fractured the Esperanto movement, the schism gave the remaining Esperantists the freedom to concentrate on using and promoting their language as it stood. At the same time, it gave the Idists freedom to continue working on their own language for several more years before actively promoting it. The \"Uniono di la Amiki di la Linguo Internaciona\" (\"Union of Friends of the International Language\") was established along with an Ido Academy to work out the details of the new language.\n\nCouturat, who was the leading proponent of Ido, was killed in an automobile accident in 1914. This, along with World War I, practically suspended the activities of the Ido Academy from 1914 to 1920. In 1928 Ido's major intellectual supporter, the Danish linguist Otto Jespersen, published his own planned language, Novial. His defection from the Ido movement set it back even further.\n\nThe language still has active speakers today, and the Internet has sparked a renewal of interest in the language in recent years. A sample of 24 Idists on the Yahoo! group \"Idolisto\" during November 2005 showed that 57% had begun their studies of the language during the preceding three years, 32% from the mid-1990s to 2002, and 8% had known the language from before.\n\nFew changes have been made to Ido since 1922.\n\nCamiel de Cock was named secretary of linguistic issues in 1990, succeeding Roger Moureaux. He resigned after the creation of a linguistic committee in 1991. De Cock was succeeded by Robert C. Carnaghan, who held the position from 1992 to 2008. No new words were adopted between 2001 and 2006. Following the 2008–2011 elections of ULI's direction committee, Gonçalo Neves replaced Carnaghan as secretary of linguistic issues in February 2008. Neves resigned in August 2008. A new linguistic committee was formed in 2010. In April 2010, Tiberio Madonna was appointed as secretary of linguistic issues, succeeding Neves. \nIn January 2011, ULI approved eight new words. This was the first addition of words in many years. As of April 2012, the secretary of linguistic issues remains Tiberio Madonna.\n\nIdo has five vowel phonemes. The vowels and are interchangeable depending on speaker preference, as are and . The combinations /au/ and /eu/ become diphthongs in word roots but not when adding affixes.\n\n+ Ido vowels\n!  Front\n!  Back\n! Close\n! Mid\n! Open\n\n+Ido consonants\n! colspan=2 Width=13%  Labial\n! colspan=2 Width=13%  Alveolar\n! colspan=2 Width=13%  Post-alveolar\n! colspan=2 Width=13%  Palatal\n! colspan=2 Width=13%  Velar\n! colspan=2 Width=13%  Glottal\n! Nasal\nstyle=\"border-right: 0;\" style=\"border-left: 0;\"\nstyle=\"border-right: 0;\" style=\"border-left: 0;\"\n! Stop\nstyle=\"border-right: 0;\"style=\"border-left: 0;\"\nstyle=\"border-right: 0;\"style=\"border-left: 0;\"\nstyle=\"border-right: 0;\"style=\"border-left: 0;\"\n! Affricate\nstyle=\"border-right: 0;\"style=\"border-left: 0;\"\nstyle=\"border-right: 0;\"style=\"border-left: 0;\"\n! Fricative\nstyle=\"border-right: 0;\"style=\"border-left: 0;\"\nstyle=\"border-right: 0;\"style=\"border-left: 0;\"\nstyle=\"border-right: 0;\"style=\"border-left: 0;\"\nstyle=\"border-right: 0;\"style=\"border-left: 0;\"\n! Approximant\nstyle=\"border-right: 0;\" style=\"border-left: 0;\"\nstyle=\"border-right: 0;\" style=\"border-left: 0;\"\nstyle=\"border-right: 0;\" style=\"border-left: 0;\"\n! Flap\nstyle=\"border-right: 0;\" style=\"border-left: 0;\"\n\nAll polysyllabic words are stressed on the second-to-last syllable except for verb infinitives, which are stressed on the last syllableskolo, kafeo and lernas for \"school\", \"coffee\" and the present tense of \"to learn\", but irar, savar and drinkar for \"to go\", \"to know\" and \"to drink\". If an i or u precedes another vowel, the pair is considered part of the same syllable when applying the accent rulethus radio, familio and manuo for \"radio\", \"family\" and \"hand\", unless the two vowels are the only ones in the word, in which case the \"i\" or \"u\" is stressed: dio, frua for \"day\" and \"early\".\n\nIdo uses the same 26 letters as the English alphabet and ISO Basic Latin alphabet with three digraphs and no ligatures or diacritics. Where the table below lists two pronunciations, either is perfectly acceptable.\n\n! Letter\n! IPA\n! English\n\nThe digraphs are:\n\n! Digraph\n! IPA\n! English\n\nThe definite article is \"\"la\"\" and is invariable. The indefinite article (a/an) does not exist in Ido. Each word in the Ido vocabulary is built from a root word. A word consists of a root and a grammatical ending. Other words can be formed from that word by removing the grammatical ending and adding a new one, or by inserting certain affixes between the root and the grammatical ending.\n\nSome of the grammatical endings are defined as follows:\n\n! Grammatical form\n! Singular noun\n! Plural noun\n! Adjective\n! Adverb\n! Present tense infinitive\n! Past tense infinitive\n! Future tense infinitive\n! Present\n! Past\n! Future\n! Imperative\n! Conditional\n\nThese are the same as in Esperanto except for \"-i\", \"-ir\", \"-ar\", \"-or\" and \"-ez\". Esperanto marks noun plurals by an \"agglutinative\" ending \"-j\" (so plural nouns end in \"-oj\"), uses \"-i\" for verb infinitives (Esperanto infinitives are tenseless), and uses \"-u\" for the imperative. Verbs in Ido, as in Esperanto, do not conjugate depending on person, number or gender; the -as, -is, and -os endings suffice whether the subject is I, you, he, she, they, or anything else. For the word \"to be,\" Ido allows either \"\"esas\"\" or \"\"es\"\" in the present tense; however, the full forms must be used for the past tense \"\"esis\"\" and future tense \"\"esos\".\" Adjectives and adverbs are compared in Ido by means of the words \"plu\" = more, \"maxim\" = most, \"min\" = less, \"minim\" = least, \"kam\" = than/as. There exist in Ido three categories of adverbs: the simple, the derived, and the composed. The simple adverbs do not need special endings, for example: \"tre\" = very, \"tro\" = too, \"olim\" =formerly, \"nun\" = now, \"nur\" = only. The derived and composed adverbs, not being originally adverbs but derived from nouns, adjectives and verbs, have the ending -e.\n\nIdo word order is generally the same as English (subject–verb–object), so the sentence \"Me havas la blua libro\" is the same as the English \"I have the blue book\", both in meaning and word order. There are a few differences, however:\nBULLET::::- Adjectives can precede the noun as in English, or follow the noun as in Spanish. Thus, \"Me havas la libro blua\" means the same thing.\nBULLET::::- Ido has the accusative suffix \"-n\". Unlike Esperanto, this suffix is only required when the object of the sentence is not clear, for example, when the subject-verb-object word order is not followed. Thus, \"La blua libron me havas\" also means the same thing.\n\nIdo generally does not impose rules of grammatical agreement between grammatical categories within a sentence. For example, the verb in a sentence is invariable regardless of the number and person of the subject. Nor must the adjectives be pluralized as well the nounsin Ido \"the large books\" would be \"la granda libri\" as opposed to the French \"les grands livres\" or the Esperanto \"la grandaj libroj\".\n\nNegation occurs in Ido by simply adding ne before a verb: Me ne havas libro means \"I do not have a book\". This as well does not vary, and thus the \"I do not\", \"He does not\", \"They do not\" before a verb are simply Me ne, Il ne, and Li ne. In the same way, past tense and future tense negatives are formed by ne before the conjugated verb. \"I will not go\" and \"I did not go\" become Me ne iros and Me ne iris respectively.\n\nYes/no questions are formed by the particle ka in front of the question. \"I have a book\" (me havas libro) becomes Ka me havas libro? (do I have a book?). Ka can also be placed in front of a noun without a verb to make a simple question, corresponding to the English \"is it?\" Ka Mark? can mean, \"Are you Mark?\", \"Is it Mark?\", \"Do you mean Mark?\" depending on the context.\n\nThe pronouns of Ido were revised to make them more acoustically distinct than those of Esperanto, which all end in \"i\". Especially the singular and plural first-person pronouns \"mi\" and \"ni\" may be difficult to distinguish in a noisy environment, so Ido has \"me\" and \"ni\" instead. Ido also distinguishes between intimate (\"tu\") and formal (\"vu\") second-person singular pronouns as well as plural second-person pronouns (\"vi\") not marked for intimacy. Furthermore, Ido has a pan-gender third-person pronoun \"lu\" (it can mean \"he\", \"she\", or \"it\", depending on the context) in addition to its masculine (\"il\"), feminine (\"el\"), and neuter (\"ol\") third-person pronouns.\n\n+ Pronouns\n!rowspan=\"3\"\n!colspan=\"7\"singular\n!colspan=\"6\" plural\n!rowspan=\"3\"reflexive\n!rowspan=\"3\"indefinite\n!rowspan=\"2\"first\n!colspan=\"2\"second\n!colspan=\"4\"third\n!rowspan=\"2\"first\n!rowspan=\"2\"second\n!colspan=\"4\"third\n!\"familiar\"\n!\"formal\"\n!\"masc.\"\n!\"fem.\"\n!\"neuter\"\n!\"pan-gender\"\n!\"masc.\"\n!\"fem.\"\n!\"neuter\"\n!\"pan-gender\"\n!Ido\n!English\n\n!Esperanto\n\nBULLET::::1. \"ci\", although technically the familiar form of the word \"you\" in Esperanto, is seldom used. Esperanto's inventor himself did not include the pronoun in the first book on Esperanto and only later reluctantly; later he recommended against using \"ci\" because different cultures have conflicting traditions regarding the use of the familiar and formal forms of \"you\".\n\n\"ol\", like English \"it\" and Esperanto \"ĝi\", is not limited to inanimate objects, but can be used \"for entities whose sex is indeterminate: \"babies, children, humans, youths, elders, people, individuals, horses, [cattle], cats,\" etc.\"\n\n\"Lu\" is often mistakenly labeled an epicene pronoun, that is, one that refers to both masculine and feminine beings, but in fact, \"lu\" is more properly a \"pan-gender\" pronoun, as it is also used for referring to inanimate objects. From \"Kompleta Gramatiko Detaloza di la Linguo Internaciona Ido\" by Beaufront:\n\nIdo makes correlatives by combining entire words together and changing the word ending, with some irregularities to show distinction.\n! rowspan=\"2\" colspan=\"2\" \n!Relative and\ninterrogative\n!Demonstrative\n!Indeterminate\n!Most\nIndeterminate\n!Negative\n!Collective\n!qua, ∅\n!ita, ∅\n!ula, ∅\n!irga\n!nula\n!omna\n!Individual\n!-u\nulu\nirgu\nnulu\nomnu\n!Thing\n!-o\nquo\nito \nulo\nirgo\nnulo\nomno\n!Plural\n!-i\nqui\niti \nuli\nirgi\nnuli\nomni\n!Adjective\n!-a\nula\nirga\nnula\nomna\n!Motive\n!pro\npro quo\npro to\npro ulo\npro irgo\npro nulo\npro omno\n!Place\n!loke\nulaloke\nirgaloke\nnulaloke\nomnaloke\n!Time\n!tempe\nulatempe \nirgatempe\nnulatempe \n!Quality\n!-a, speca\nulaspeca \nirgaspeca\nnulaspeca \nomnaspeca\n!Manner\n!-e, maniere\nule, ulamaniere \nirge, irgamaniere\nnule, nulamaniere \nomne, omnamaniere\n!Quantity -\nadjective\n!quanta\nirgaquanta\nnulaquanta\nomnaquanta\n!Quantity -\nnoun\n!quanto\n\nBULLET::::1. The initial \"i\" can be omitted: \"ta\", \"to\", \"ti\", \"ta\".\nBULLET::::2. One can omit the initial \"a\": \"ultempe\", \"nultempe\", \"ulspeca\", \"nulspeca\", \"ulmaniere\", \"nulmaniere\".\nBULLET::::3. \"omnatempe\" is correct and usable, but \"sempre\" is the actual word.\nBULLET::::4. Instead of \"irga quanto\", \"nula quanto\" and \"la tota quanto\" one usually says \"irgo\", \"nulo\" and \"omno\".\n\nComposition in Ido obeys stricter rules than in Esperanto, especially formation of\nnouns, adjectives and verbs from a radical of a different\nclass. The reversibility principle assumes that for each composition rule (affix addition), the corresponding decomposition rule (affix removal) is valid.\n\nHence, while in Esperanto an adjective (for instance , formed on the noun radical , can mean an attribute ( “paper-made encyclopedia”) and a relation ( “paper-making factory”), Ido will distinguish the attribute (“paper” or “of paper” (not “paper-made” exactly)) from the relation (“paper-making”).\n\nSimilarly, means in both Esperanto and Ido the noun “crown”; where Esperanto allows formation of “to crown” by simply changing the ending from noun to verb (“crowning” is ), Ido requires an affix so the composition is reversible: (“the act of crowning” is ).\n\nAccording to Claude Piron, some modifications brought by Ido are in practice impossible to use and ruin spontaneous expression: Ido displays, on linguistic level, other drawbacks Esperanto succeeded to avoid, but I don’t have at hand documents which would allow me to go further in detail. For instance, if I remember correctly, where Esperanto only has the suffix *, Ido has several: **, **, **, which match subtleties which were meant to make language clearer, but that, in practice, inhibit natural expression.\n\nVocabulary in Ido is derived from French, Italian, Spanish, English, German, and Russian. Basing the vocabulary on various widespread languages was intended to make Ido as easy as possible for the greatest number of people possible. Early on, the first 5,371 Ido word roots were analyzed compared to the vocabulary of the six source languages, and the following result was found:\n\nBULLET::::- 2024 roots (38%) belong to 6 languages\nBULLET::::- 942 roots (17%) belong to 5 languages\nBULLET::::- 1111 roots (21%) belong to 4 languages\nBULLET::::- 585 roots (11%) belong to 3 languages\nBULLET::::- 454 roots (8%) belong to 2 languages\nBULLET::::- 255 roots (5%) belong to 1 language\n\nAnother analysis showed that:\n\nBULLET::::- 4880 roots (91%) are found in French\nBULLET::::- 4454 roots (83%) are found in Italian\nBULLET::::- 4237 roots (79%) are found in Spanish\nBULLET::::- 4219 roots (79%) are found in English\nBULLET::::- 3302 roots (61%) are found in German\nBULLET::::- 2821 roots (52%) are found in Russian\n\n+ Comparison of Ido vocabulary with its six source languages\n! Ido !! English !! Italian !! French !! German !! Russian !! Spanish\n! bona\n! donar\n! filtrar\n! gardeno\n! kavalo\n! maro\n! naciono\n! studiar\n! yuna\n\nVocabulary in Ido is often created through a number of official prefixes and suffixes that alter the meaning of the word. This allows a user to take existing words and modify them to create neologisms when necessary, and allows for a wide range of expression without the need to learn new vocabulary each time. Though their number is too large to be included in one article, some examples include:\nBULLET::::- The diminutive suffix -et-. Domo (house) becomes dometo (cottage), and libro (book) becomes libreto (novelette or short story).\nBULLET::::- The pejorative suffix -ach-. Domo becomes domacho (hovel), and libro becomes libracho (a shoddy piece of work, pulp fiction, etc.)\nBULLET::::- The prefix retro-, which implies a reversal. Irar (to go) becomes retroirar (to go back, backward) and venar (to come) becomes retrovenar (to return).\n\nNew vocabulary is generally created through an analysis of the word, its etymology, and reference to the six source languages. If a word can be created through vocabulary already existing in the language then it will usually be adopted without need for a new radical (such as wikipedio for \"Wikipedia\", which consists of wiki + enciklopedio for \"encyclopedia\"), and if not an entirely new word will be created. The word alternatoro for example was adopted in 1926, likely because five of the six source languages used largely the same orthography for the word, and because it was long enough to avoid being mistaken for other words in the existing vocabulary. Adoption of a word is done through consensus, after which the word will be made official by the union. Care must also be taken to avoid homonyms if possible, and usually a new word undergoes some discussion before being adopted. Foreign words that have a restricted sense and are not likely to be used in everyday life (such as the word \"intifada\" to refer to the conflict between Israel and Palestine) are left untouched, and often written in italics.\n\nIdo, unlike Esperanto, does not assume the male sex by default. For example, Ido does not derive the word for “waitress” by adding a feminine suffix to “waiter”, as Esperanto does. Instead, Ido words are defined as sex-neutral, and two different suffixes derive masculine and feminine words from the root: ' for a waiter of either sex, ' for a male waiter, and ' for a waitress. There are only two exceptions to this rule: First, ' for “father”, ' for “mother”, and ' for “parent”, and second, ' for “man”, ' for “woman”, and \"\" for “adult”.\n\nThe Lord's Prayer:\nPatro nia, qua esas en la cielo,\ntua nomo santigesez;\ntua regno advenez;\ntua volo facesez\nquale en la cielo, tale anke sur la tero.\nDonez a ni cadie l'omnadiala pano,\ne pardonez a ni nia ofensi,\nquale anke ni pardonas a nia ofensanti,\ne ne duktez ni aden la tento,\nma liberigez ni del malajo.\nOur Father, who art in heaven,\nhallowed be your name.\nThy kingdom come,\nThy will be done,\non earth as it is in heaven.\nGive us this day our daily bread,\nand forgive us our debts,\nas we also have forgiven our debtors.\nAnd lead us not into temptation,\nbut deliver us from evil.\n\nIdo has a number of publications that can be subscribed to or downloaded for free in most cases. \"Kuriero Internaciona\" is a magazine produced in France every few months with a range of topics. \"Adavane!\" is a magazine produced by the Spanish Ido Society every two months that has a range of topics, as well as a few dozen pages of work translated from other languages. \"Progreso\" is the official organ of the Ido movement and has been around since the inception of the movement in 1908. Other sites can be found with various stories, fables or proverbs along with a few books of the Bible translated into Ido on a smaller scale. The site \"publikaji\" has a few podcasts in Ido along with various songs and other recorded material.\n\nWikipedia includes an Ido-language edition (known in Ido as \"Wikipedio\"); in January 2012 it was the 81st most visited Wikipedia.\n\nULI organises Ido conventions yearly, and the conventions include a mix of tourism and work.\n\nBULLET::::- 2016: Valencia, Spain (Information)\nBULLET::::- 2015: Berlin, Germany (Information)\nBULLET::::- 2014: Paris, France (Information)\nBULLET::::- 2013: Ouroux-en-Morvan, France (Information)\nBULLET::::- 2012: Dessau, Germany (Information)\nBULLET::::- 2011: Echternach, Luxembourg (Information), 24 participants from 11 countries\nBULLET::::- 2010: Tübingen, Germany (Information)\nBULLET::::- 2009: Tallinn, Estonia (Information)\nBULLET::::- 2008: Wuppertal-Neviges, Germany, participants from 5 countries (Information)\nBULLET::::- 2007: Paris, France, 14 participants from 9 countries (Information, Photos)\nBULLET::::- 2006: Berlin, Germany, approx. 25 participants from 10 countries (Information)\nBULLET::::- 2005: Toulouse, France, 13 participants from 4 countries (Information)\nBULLET::::- 2004: Kiev, Ukraine, 17 participants from 9 countries (Information)\nBULLET::::- 2003: Großbothen, Germany, participants from 6 countries (Information)\nBULLET::::- 2002: Kraków, Poland, 14 participants from 6 countries (Information)\nBULLET::::- 2001: Nuremberg, Germany, 14 participants from 5 countries (Information)\nBULLET::::- 2000: Nürnberg, Germany\nBULLET::::- 1999: Waldkappel, Germany\nBULLET::::- 1998: Białobrzegi, Poland, 15 participants from 6 countries\nBULLET::::- 1997: Bakkum, Netherlands, 19 participants from 7 countries\nBULLET::::- 1995: Elsnigk, Germany\nBULLET::::- 1991: Ostend, Belgium, 21 participants\nBULLET::::- 1990: Waldkappel, Germany\nBULLET::::- 1989: Zürich-Thalwil, Switzerland\nBULLET::::- 1987: Eschwege, Germany\nBULLET::::- 1985: Antwerpen, Belgium\nBULLET::::- 1983: York, England\nBULLET::::- 1981: Jongny, Switzerland\nBULLET::::- 1980: Namur, Belgium, 35 participants\nBULLET::::- 1979: Uppsala, Sweden\nBULLET::::- 1978: Cambridge, England\nBULLET::::- 1977: Berlin-Tegel, Germany\nBULLET::::- 1976: Saint-Nazaire, France\nBULLET::::- 1975: Thun, Switzerland\nBULLET::::- 1974: Kiev, Ukraine\nBULLET::::- 1973: Cardiff, Wales\nBULLET::::- 1972: Chaux-de-Fonds, Switzerland\nBULLET::::- 1971: Trollhättan, Sweden\nBULLET::::- 1970: Luxembourg City, Luxembourg\nBULLET::::- 1969: Zürich, Switzerland\nBULLET::::- 1968: Berlin, Germany\nBULLET::::- 1967: Bourges, France\nBULLET::::- 1966: Biella, Italy\nBULLET::::- 1965: Lons-le-Saunier, France\nBULLET::::- 1964: Kiel, Germany\nBULLET::::- 1963: Barcelona, Spain\nBULLET::::- 1962: Thun, Switzerland\nBULLET::::- 1961: Zürich, Switzerland, participants\nBULLET::::- 1960: Colmar, France\nBULLET::::- 1959: Freiburg im Breisgau, Germany\nBULLET::::- 1957: Luxembourg City, Luxembourg\nBULLET::::- 1952: Berlin, Germany\nBULLET::::- 1951: Torino, Italy\nBULLET::::- 1950: Colmar, France\nBULLET::::- 1939: St. Gallen, Switzerland\nBULLET::::- 1937: Paris, France\nBULLET::::- 1936: Szombathely, Hungary\nBULLET::::- 1935: Fredericia, Denmark\nBULLET::::- 1934: Oostduinkerke, Belgium\nBULLET::::- 1933: Mondorf, Luxembourg\nBULLET::::- 1931: Lauenburg/Elbe, Germany\nBULLET::::- 1930: Sopron, Hungary\nBULLET::::- 1929: Freiburg im Breisgau, Germany\nBULLET::::- 1928: Zürich, Switzerland\nBULLET::::- 1927: Paris, France\nBULLET::::- 1926: Prague, Czechoslovakia\nBULLET::::- 1925: Torino, Italy\nBULLET::::- 1924: Luxembourg City, Luxembourg\nBULLET::::- 1923: Kassel, Germany\nBULLET::::- 1922: Dessau, Germany\nBULLET::::- 1921: Wien, Austria\n\nBULLET::::- Comparison between Esperanto and Ido\nBULLET::::- Comparison between Ido and Novial\nBULLET::::- Comparison between Ido and Interlingua\nBULLET::::- Interhelpo\nBULLET::::- English false friends in Ido\nBULLET::::- Engelbert Pigal\n\nAdditional notes\nBULLET::::1. L. Couturat, L. Leau. \"Delegation pour l'adoption d'une Langue auxiliare internationale\" (15–24 October 1907). Coulommiers: Imprimerie Paul Brodard, 1907\n\nBULLET::::- The international language Ido\nBULLET::::- Union for the International Language Ido (in Ido)\nBULLET::::- The IDO foundation for language research in memory of Hellmut Röhnish\nBULLET::::- Otto Jespersen's history of Ido\nBULLET::::- Henry Jacob's history of Ido\nBULLET::::- Online Library of Free Books in Ido\nBULLET::::- Ido for all – English course for learning Ido\nBULLET::::- Online Ido library (in Ido)\nBULLET::::- Examples of Ido Phrases\n"}
{"id": "15041", "url": "https://en.wikipedia.org/wiki?curid=15041", "title": "Improvisational theatre", "text": "Improvisational theatre\n\nImprovisational theatre, often called improvisation or improv, is the form of theatre, often comedy, in which most or all of what is performed is unplanned or unscripted: created spontaneously by the performers. In its purest form, the dialogue, action, story, and characters are created collaboratively by the players as the improvisation unfolds in present time, without use of an already prepared, written script.\n\nImprovisational theatre exists in performance as a range of styles of improvisational comedy as well as some non-comedic theatrical performances. It is sometimes used in film and television, both to develop characters and scripts and occasionally as part of the final product.\n\nImprovisational techniques are often used extensively in drama programs to train actors for stage, film, and television and can be an important part of the rehearsal process. However, the skills and processes of improvisation are also used outside the context of performing arts - Applied Improvisation. It is used in classrooms as an educational tool and in businesses as a way to develop communication skills, creative problem solving, and supportive team-work abilities that are used by improvisational, ensemble players. It is sometimes used in psychotherapy as a tool to gain insight into a person's thoughts, feelings, and relationships.\n\nThe earliest well-documented use of improvisational theatre in Western history is found in the Atellan Farce of 391 BC. From the 16th to the 18th centuries, \"commedia dell'arte\" performers improvised based on a broad outline in the streets of Italy. In the 1890s, theatrical theorists and directors such as the Russian Konstantin Stanislavski and the French Jacques Copeau, founders of two major streams of acting theory, both heavily utilized improvisation in acting training and rehearsal.\n\nModern theatrical improvisation games began as drama exercises for children, which were a staple of drama education in the early 20th century thanks in part to the progressive education movement initiated by John Dewey in 1916. Some people credit American Dudley Riggs as the first vaudevillian to use audience suggestions to create improvised sketches on stage. Improvisation exercises were developed further by Viola Spolin in the 1940s, 50s, and 60s, and codified in her book \"Improvisation For The Theater\", the first book that gave specific techniques for learning to do and teach improvisational theater. In the 1970s in Canada, British playwright and director Keith Johnstone wrote \"\", a book outlining his ideas on improvisation, and invented Theatresports, which has become a staple of modern improvisational comedy and is the inspiration for the popular television show \"Whose Line Is It Anyway?\"\n\nSpolin influenced the first generation of modern American improvisers at The Compass Players in Chicago, which led to The Second City. Her son, Paul Sills, along with David Shepherd, started The Compass Players. Following the demise of the Compass Players, Paul Sills began The Second City. They were the first organized troupes in Chicago, and the modern Chicago improvisational comedy movement grew from their success.\n\nMany of the current \"rules\" of comedic improv were first formalized in Chicago in the late 1950s and early 1960s, initially among The Compass Players troupe, which was directed by Paul Sills. From most accounts, David Shepherd provided the philosophical vision of the Compass Players, while Elaine May was central to the development of the premises for its improvisations. Mike Nichols, Ted Flicker, and Del Close were her most frequent collaborators in this regard. When The Second City opened its doors on December 16, 1959, directed by Paul Sills, his mother Viola Spolin began training new improvisers through a series of classes and exercises which became the cornerstone of modern improv training. By the mid-1960s, Viola Spolin's classes were handed over to her protégé, Jo Forsberg, who further developed Spolin's methods into a one-year course, which eventually became The Players Workshop, the first official school of improvisation in the USA. During this time, Forsberg trained many of the performers who went on to star on The Second City stage.\n\nMany of the original cast of \"Saturday Night Live\" came from The Second City, and the franchise has produced such comedy stars as Mike Myers, Tina Fey, Bob Odenkirk, Amy Sedaris, Stephen Colbert, Eugene Levy, Jack McBrayer, Steve Carell, Chris Farley, Dan Aykroyd, and John Belushi.\n\nSimultaneously, Keith Johnstone's group The Theatre Machine, which originated in London, was touring Europe. This work gave birth to Theatresports, at first secretly in Johnstone's workshops, and eventually in public when he moved to Canada. Toronto has been home to a rich improv tradition.\n\nIn 1984, Dick Chudnow (Kentucky Fried Theater) founded ComedySportz in Milwaukee, WI. Expansion began with the addition of ComedySportz-Madison (WI), in 1985. The first Comedy League of America National Tournament was held in 1988, with 10 teams participating. The league is now known as CSz Worldwide and boasts a roster of 29 international cities.\n\nIn San Francisco, The Committee theater was active in North Beach during the 1960s. It was founded by alumni of Chicago's Second City, Alan Myerson and his wife Jessica. When The Committee disbanded in 1972, three major companies were formed: The Pitchell Players, The Wing, and Improvisation Inc. The only company that continued to perform Close's Harold was the latter one. Its two former members, Michael Bossier and John Elk, formed Spaghetti Jam in San Francisco's Old Spaghetti Factory in 1976, where shortform improv and Harolds were performed through 1983. Stand-up comedians performing down the street at the Intersection for the Arts would drop by and sit in. In 1979, Elk brought shortform to England, teaching workshops at Jacksons Lane Theatre, and he was the first American to perform at The Comedy Store, London, above a Soho strip club.\n\nModern political improvisation's roots include Jerzy Grotowski's work in Poland during the late 1950s and early 1960s, Peter Brook's \"happenings\" in England during the late 1960s, Augusto Boal's \"Forum Theatre\" in South America in the early 1970s, and San Francisco's The Diggers' work in the 1960s. Some of this work led to pure improvisational performance styles, while others simply added to the theatrical vocabulary and were, on the whole, avant-garde experiments.\n\nJoan Littlewood, an English actress and director who was active from the 1950s to 1960s, made extensive use of improv in developing plays for performance. However, she was successfully prosecuted twice for allowing her actors to improvise in performance. Until 1968, British law required scripts to be approved by the Lord Chamberlain's Office. The department also sent inspectors to some performances to check that the approved script was performed exactly as approved.\n\nIn 1987, Annoyance Theatre began as a club in Chicago that emphasizes longform improvisation. The Annoyance Theatre has grown into multiple locations in Chicago and New York City. It is the home of the longest running musical improv show in history at 11 years.\n\nIn 2012, Lebanese writer and director Lucien Bourjeily used improvisational theater techniques to create a multi-sensory play entitled \"66 Minutes in Damascus\". This play premiered at the London International Festival of Theater, and is considered one of the most extreme kinds of interactive improvised theater put on stage. The audience play the part of kidnapped tourists in today's Syria in a hyperreal sensory environment.\n\nRob Wittig and Mark C. Marino have developed a form of improv for online theatrical improvisation called netprov. The form relies on social media to engage audiences in the creation of dynamic fictional scenarios that evolve in real-time.\n\nModern improvisational comedy, as it is practiced in the West, falls generally into two categories: shortform and longform.\n\nShortform improv consists of short scenes usually constructed from a predetermined game, structure, or idea and driven by an audience suggestion. Many short form exercises were first created by Viola Spolin, who called them theatre games, influenced by her training from recreational games expert Neva Boyd. The short-form improv comedy television series \"Whose Line Is It Anyway?\" has familiarized American and British viewers with short-form.\n\nLongform improv performers create shows in which short scenes are often interrelated by story, characters, or themes. Longform shows may take the form of an existing type of theatre, for example a full-length play or Broadway-style musical such as Spontaneous Broadway. One of the better-known longform structures is the Harold, developed by ImprovOlympic co-founder Del Close. Many such longform structures now exist. Longform improvisation is especially performed in Chicago, New York City, Los Angeles; has a strong presence in Austin, Boston, Minneapolis, Phoenix, Philadelphia, San Francisco, Seattle, Detroit, Toronto, Vancouver, Washington, D.C.; and is building a growing following in Denver, Kansas City, Montreal, Columbus, New Orleans, Omaha, Rochester, and Hawaii. Outside the United States, longform improv has a growing presence in the United Kingdom, especially in cities such as London, Bristol, and at the Edinburgh Festival Fringe.\n\nOther forms of improvisational theatre training and performance techniques are experimental and avant-garde in nature and not necessarily intended to be comedic. These include Playback Theatre and Theatre of the Oppressed, the Poor Theatre, the Open Theatre, to name only a few.\n\nThe Open Theatre was founded in New York City by a group of former students of acting teacher Nola Chilton, and joined shortly thereafter by director Joseph Chaikin, formerly of The Living Theatre, and Peter Feldman. This avante-garde theatre group explored political, artistic, and social issues. The company, developing work through an improvisational process drawn from Chilton and Viola Spolin, created well-known exercises, such as \"sound and movement\" and \"transformations\", and originated radical forms and techniques that anticipated or were contemporaneous with Jerzy Grotowski's \"poor theater\" in Poland.[1] During the sixties Chaikin and the Open Theatre developed full theatrical productions with nothing but the actors, a few chairs and a bare stage, creating character, time and place through a series of transformations the actors physicalized and discovered through improvisations.\n\nLongform, dramatic, and narrative-based improvisation is well-established on the west coast with companies such as San Francisco's BATS Improv. This format allows for full-length plays and musicals to be created improvisationally.\n\nMany people who have studied improv have noted that the guiding principles of improv are useful, not just on stage, but in everyday life. For example, Stephen Colbert in a commencement address said,\n\nTina Fey in her book \"Bossypants\" lists several rules of improv that apply in the workplace. There has been much interest in bringing lessons from improv into the corporate world. In a New York Times article titled \"Can Executives Learn to Ignore the Script?\", Stanford professor and author, Patricia Ryan Madson notes, \"executives and engineers and people in transition are looking for support in saying yes to their own voice. Often, the systems we put in place to keep us secure are keeping us from our more creative selves.\"\n\nMany directors have made use of improvisation in the creation of both mainstream and experimental films. Many silent filmmakers such as Charlie Chaplin and Buster Keaton used improvisation in the making of their films, developing their gags while filming and altering the plot to fit. The Marx Brothers were notorious for deviating from the script they were given, their ad libs often becoming part of the standard routine and making their way into their films. Many people, however, make a distinction between ad-libbing and improvising.\n\nThe British director Mike Leigh makes extensive use of improvisation in the creation of his films, including improvising important moments in the characters' lives that will not even appear in the film. \"This Is Spinal Tap\" and other mockumentary films of director Christopher Guest are created with a mix of scripted and unscripted material and \"Blue in the Face\" is a 1995 comedy directed by Wayne Wang and Paul Auster created in part by the improvisations during the filming of \"Smoke\".\n\nSome of the best known American film directors who use improvisation in their work with actors are John Cassavetes, Robert Altman, Christopher Guest and Rob Reiner.\n\nImprov comedy techniques have also been used in hit television shows such as HBO's \"Curb Your Enthusiasm\" created by Larry David, the UK Channel 4 and ABC television series \"Whose Line Is It Anyway\" (and its spinoffs \"Drew Carey's Green Screen Show\" and \"Drew Carey's Improv-A-Ganza\"), Nick Cannon's improv comedy show \"Wild 'N Out\", and \"Thank God You're Here\". A very early American improv television program was the weekly half-hour “What Happens Now?” which premiered on New York's WOR-TV on October 15, 1949 and ran for 22 episodes. “The Improvisers” were six actors (including Larry Blyden, Ross Martin, and Jean Alexander – Jean Pugsley at the time) who improvised skits based on situations suggested by viewers. In Canada, the series \"Train 48\" was improvised from scripts which contained a minimal outline of each scene, and the comedy series \"This Sitcom Is...Not to Be Repeated\" incorporated dialogue drawn from a hat during the course of an episode. The American show \"Reno 911!\" also contained improvised dialogue based on a plot outline. \"Fast and Loose\" is an improvisational game show, much like \"Whose Line Is It Anyway?\". The BBC sitcoms \"Outnumbered\" and \"The Thick of It\" also had some improvised elements in them.\n\nIn the field of the psychology of consciousness, Eberhard Scheiffele explored the altered state of consciousness experienced by actors and improvisers in his scholarly paper \"Acting: an altered state of consciousness\". According to G. William Farthing in \"The Psychology of Consciousness\" comparative study, actors routinely enter into an altered state of consciousness (ASC). Acting is seen as altering most of the 14 dimensions of changed subjective experience which characterize ASCs according to Farthing, namely: attention, perception, imagery and fantasy, inner speech, memory, higher-level thought processes, meaning or significance of experiences, time experience, emotional feeling and expression, level of arousal, self-control, suggestibility, body image, and sense of personal identity.\n\nIn the growing field of Drama Therapy, psychodramatic improvisation, along with other techniques developed for Drama Therapy, are used extensively. The \"\"Yes, and\"\" rule has been compared to Milton Erickson's \"utilization\" process and to a variety of acceptance-based psychotherapies. Improv training has been recommended for couples therapy and therapist training, and it has been speculated that improv training may be helpful in some cases of social anxiety disorder.\n\nImprovisational theatre often allows an interactive relationship with the audience. Improv groups frequently solicit suggestions from the audience as a source of inspiration, a way of getting the audience involved, and as a means of proving that the performance is not scripted. That charge is sometimes aimed at the masters of the art, whose performances can seem so detailed that viewers may suspect the scenes are planned.\n\nIn order for an improvised scene to be successful, the improvisers involved must work together responsively to define the parameters and action of the scene, in a process of co-creation. With each spoken word or action in the scene, an improviser makes an \"offer\", meaning that he or she defines some element of the reality of the scene. This might include giving another character a name, identifying a relationship, location, or using mime to define the physical environment. These activities are also known as \"endowment\". It is the responsibility of the other improvisers to accept the offers that their fellow performers make; to not do so is known as blocking, negation, or denial, which usually prevents the scene from developing. Some performers may deliberately block (or otherwise break out of character) for comedic effect—this is known as \"gagging\"—but this generally prevents the scene from advancing and is frowned upon by many improvisers. Accepting an offer is usually accompanied by adding a new offer, often building on the earlier one; this is a process improvisers refer to as \"\"Yes, And...\"\" and is considered the cornerstone of improvisational technique. Every new piece of information added helps the improvisers to refine their characters and progress the action of the scene. The \"\"Yes, And...\"\" rule, however, applies to a scene's early stage since it is in this stage that a \"base (or shared) reality\" is established in order to be later redefined by applying the \"\"if (this is true), then (what else can also be true)\"\" practice progressing the scene into comedy, as explained in the 2013 manual by the \"Upright Citizens Brigade\" members.\n\nThe unscripted nature of improv also implies no predetermined knowledge about the props that might be useful in a scene. Improv companies may have at their disposal some number of readily accessible props that can be called upon at a moment's notice, but many improvisers eschew props in favor of the infinite possibilities available through mime. In improv, this is more commonly known as 'space object work' or 'space work', not 'mime', and the props and locations created by this technique, as 'space objects' created out of 'space substance,' developed as a technique by Viola Spolin. As with all improv \"offers\", improvisers are encouraged to respect the validity and continuity of the imaginary environment defined by themselves and their fellow performers; this means, for example, taking care not to walk through the table or \"miraculously\" survive multiple bullet wounds from another improviser's gun.\n\nBecause improvisers may be required to play a variety of roles without preparation, they need to be able to construct characters quickly with physicality, gestures, accents, voice changes, or other techniques as demanded by the situation. The improviser may be called upon to play a character of a different age or sex. Character motivations are an important part of successful improv scenes, and improvisers must therefore attempt to act according to the objectives that they believe their character seeks.\n\nMany theatre troupes are devoted to staging improvisational performances and growing the improv community through their training centers.\n\nIn addition to for-profit theatre troupes, there are many college-based improv groups in the United States and around the world.\n\nIn Europe the special contribution to the theatre of the abstract, the surreal, the irrational and the subconscious have been part of the stage tradition for centuries. From the 1990s onwards a growing number of European Improv groups have been set up specifically to explore the possibilities offered by the use of the abstract in improvised performance, including dance, movement, sound, music, mask work, lighting, and so on. These groups are not especially interested in comedy, either as a technique or as an effect, but rather in expanding the improv genre so as to incorporate techniques and approaches that have long been a legitimate part of European theatre.\n\nSome key figures in the development of improvisational theatre are Viola Spolin and her son Paul Sills, founder of Chicago's famed Second City troupe and originator of Theater Games, and Del Close, founder of ImprovOlympic (along with Charna Halpern) and creator of a popular longform improv format known as The Harold. Other luminaries include Keith Johnstone, the British teacher and writer–author of \"Impro\", who founded the Theatre Machine and whose teachings form the foundation of the popular shortform Theatresports format, Dick Chudnow, founder of ComedySportz which evolved its family-friendly show format from Johnstone's Theatersports, and Bill Johnson, creator/director of The Magic Meathands, who pioneered the concept of \"Commun-edy Outreach\" by tailoring performances to non-traditional audiences, such as the homeless and foster children.\n\nDavid Shepherd, with Paul Sills, founded The Compass Players in Chicago. Shepherd was intent on developing a true \"people's Theatre\", and hoped to bring political drama to the stockyards. The Compass went on to play in numerous forms and companies, in a number of cities including NY and Hyannis, after the founding of The Second City. A number of Compass members were also founding members of The Second City. In the 1970s, Shepherd began experimenting with group-created videos. He is the author of \"That Movie In Your Head\", about these efforts. In the 1970s, David Shepherd and Howard Jerome created the Improvisational Olympics, a format for competition based improv. The Improv Olympics were first demonstrated at Toronto's Homemade Theatre in 1976 and have been continued on as the Canadian Improv Games. In the United States, the Improv Olympics were later produced by Charna Halpern under the name \"ImprovOlympic\" and now as \"IO\"; IO operates training centers and theaters in Chicago and Los Angeles. At IO, Halpern combined Shepherd's \"Time Dash\" game with Del Close's \"Harold\" game; the revised format for the Harold became the fundamental structure for the development of modern longform improvisation.\n\nIn 1975 Jonathan Fox founded Playback Theatre, a form of improvised community theatre which is often not comedic and replays stories as shared by members of the audience.\n\nThe Groundlings is a popular and influential improv theatre and training center in Los Angeles, California. The late Gary Austin, founder of The Groundlings, taught improvisation around the country, focusing especially in Los Angeles. He was widely acclaimed as one of the greatest acting teachers in America. His work was grounded in the lessons he learned as an improviser at The Committee with Del Close, as well as in his experiences as founding director of The Groundlings. The Groundlings is often seen as the Los Angeles training ground for the \"second generation\" of improv luminaries and troupes. Stan Wells developed the \"Clap-In\" style of longform improvisation here, later using this as the basis for his own theatre, The Empty Stage which in turn bred multiple troupes utilizing this style.\n\nIn the late 1990s, Matt Besser, Amy Poehler, Ian Roberts, and Matt Walsh founded the Upright Citizens Brigade Theatre in New York and later they founded one in Los Angeles, each with an accompanying improv/sketch comedy school. In September 2011 the UCB opened a third theatre in New York City's East Village, known as UCBeast.\n\nIn 2015, The Free Association opened in London as a counterpart to American improv schools.\n\nGunter Lösel compared the existing improvisational theater theories (from Moreno, Spolin, Johnstone, Close...), structured them and wrote a general theory of improvisational theater.\n\nAlan Alda's book \"If I Understood You, Would I Have This Look on My Face?\" investigates the way in which improvisation improves communication in the sciences. The book is based on his work at Alan Alda Center for Communicating Science at Stony Brook University. The book has many examples of how improvisational theater games can increase communication skills and develop empathy.\n\nBULLET::::- Busking\nBULLET::::- Guerrilla theater\nBULLET::::- Improvisation\nBULLET::::- List of improvisational theatre companies\nBULLET::::- List of improvisational theater festivals\nBULLET::::- Playback Theatre\n\nBULLET::::- Abbott, John. 2007. \"The Improvisation Book\". London: Nick Hern Books. .\nBULLET::::- Besser, Matt; Ian Roberts, Matt Walsh. 2013. \"The Upright Citizens Brigade Comedy Improvisation Manual\", Comedy Council of Nicea,\nBULLET::::- Charna Halpern, Del Close, Kim Howard Johnson. 1994. \"The Truth in Comedy - The Manual for Improvisation\" Meriwether Pub Ltd.\nBULLET::::- Coleman, Janet. 1991. \"The Compass: The Improvisational Theatre that Revolutionized American Comedy\". Chicago: University Of Chicago Press.\nBULLET::::- Dudeck, Theresa Robbins. 2013. \"Keith Johnstone: A Critical Biography.\" London: Bloomsbury. .\nBULLET::::- Hauck, Ben. 2012. \"Long-Form Improv: The Complete Guide to Creating Characters, Sustaining Scenes, and Performing Extraordinary Harolds\". New York: Allworth Press, 2012. .\nBULLET::::- Johnstone, Keith. 1981. \"Impro: Improvisation and the Theatre\" Rev. ed. London: Methuen, 2007. .\nBULLET::::- Koppett, Kat. 2011. \"Training to imagine practical improvisational theatre techniques to enhance creativity, teamwork, leadership, and learn.\" Stylus Publishing.\nBULLET::::- Lösel, Gunter. 2013. \"Das Spiel mit dem Chaos - Zur Performativität des Improvisationstheaters\" transcript.\nBULLET::::- Ryan Madson, Patricia. 2005. \"Improv Wisdom: Don't Prepare, Just Show Up\" New York: Bell Tower.\nBULLET::::- Nachmanovitch, Stephen. 1990. New York: Penguin-Tarcher. .\nBULLET::::- Spolin, Viola. 1967. \"Improvisation for the Theater\". Third rev. ed. Evanston, Il.: Northwestern University Press, 1999. .\nBULLET::::- Weiner Wally. 2009. \"Improv: about gay sex\" Volume 1\n\nBULLET::::- Collection of improv games\nBULLET::::- How to improvise stand-up comedy, Wired UK, 11 April 2014\nBULLET::::- How To Be A Better Improviser, an essay by Daniel Gray Goldstein that lays out a foundation for improvising.\nBULLET::::- Improvisation: the Original Survival Tool, an essay by Brad Fortier linking evolution of humanity with ethics of improvisation.\nBULLET::::- Wiki about improvisational theatre (exercises and games)\nBULLET::::- Fundamental Improv Principles.\nBULLET::::- Improv Comedy for Anxiety.\nBULLET::::- Group Mind and Intuition with Improv.\nBULLET::::- Applied Improv Network - annual global conference on using improv off-stage, in business, education, and life\nBULLET::::- Academic Literature Review of Theatrical Improvisation Training in the Workplace\n"}
{"id": "15043", "url": "https://en.wikipedia.org/wiki?curid=15043", "title": "International Space Station", "text": "International Space Station\n\nThe International Space Station (ISS) is a space station (habitable artificial satellite) in low Earth orbit. The ISS programme is a joint project between five participating space agencies: NASA (United States), Roscosmos (Russia), JAXA (Japan), ESA (Europe), and CSA (Canada). The ownership and use of the space station is established by intergovernmental treaties and agreements.\n\nThe ISS serves as a microgravity and space environment research laboratory in which crew members conduct experiments in biology, human biology, physics, astronomy, meteorology, and other fields. The station is suited for the testing of spacecraft systems and equipment required for missions to the Moon and Mars. The ISS maintains an orbit with an average altitude of by means of reboost manoeuvres using the engines of the \"Zvezda\" module or visiting spacecraft. It circles the Earth in roughly 92 minutes and completes  orbits per day.\n\nThe station is divided into two sections, the Russian Orbital Segment (ROS), which is operated by Russia, and the United States Orbital Segment (USOS), which is shared by many nations. Roscosmos has endorsed the continued operation of ISS through 2024, but had previously proposed using elements of the Russian segment to construct a new Russian space station called OPSEK., the station is expected to operate until 2030.\n\nThe first ISS component was launched in 1998, with the first long-term residents arriving on 2 November 2000. Since then, the station has been continuously occupied for . This is the longest continuous human presence in low Earth orbit, having surpassed the previous record of held by \"Mir\". The latest major pressurised module was fitted in 2011, with an experimental inflatable space habitat added in 2016. Development and assembly of the station continues, with several major new Russian elements scheduled for launch starting in 2020. The ISS is the largest human-made body in low Earth orbit and can often be seen with the naked eye from Earth. The ISS consists of pressurised habitation modules, structural trusses, solar arrays, radiators, docking ports, experiment bays and robotic arms. Major ISS modules have been launched by Russian Proton and Soyuz rockets and US Space Shuttles.\n\nThe ISS is the ninth space station to be inhabited by crews, following the Soviet and later Russian \"Salyut\", \"Almaz\", and \"Mir\" stations as well as \"Skylab\" from the US. The station is serviced by a variety of visiting spacecraft: the Russian Soyuz and Progress, the US Dragon and Cygnus, the Japanese H-II Transfer Vehicle, and formerly the European Automated Transfer Vehicle. The Dragon spacecraft allows the return of pressurised cargo to Earth (downmass), which is used for example to repatriate scientific experiments for further analysis. The Soyuz return capsule has minimal downmass capability next to the astronauts.\n\nThe ISS has been visited by astronauts, cosmonauts and space tourists from 19 different nations. , 239 people from 19 countries had visited the space station, many of them multiple times. The United States sent 151 people, Russia sent 47, nine were Japanese, eight Canadian, five Italian, four French, three German, and one each from Belgium, Brazil, Denmark, Kazakhstan, Malaysia, the Netherlands, South Africa, South Korea, Spain, Sweden, the United Arab Emirates, and the United Kingdom.\n\nThe ISS was originally intended to be a laboratory, observatory, and factory while providing transportation, maintenance, and a low Earth orbit staging base for possible future missions to the Moon, Mars, and asteroids. However, not all of the uses envisioned in the initial Memorandum of Understanding between NASA and Roskosmos have come to fruition. In the 2010 United States National Space Policy, the ISS was given additional roles of serving commercial, diplomatic and educational purposes.\n\nThe ISS provides a platform to conduct scientific research, with power, data, cooling, and crew available to support experiments. Small uncrewed spacecraft can also provide platforms for experiments, especially those involving zero gravity and exposure to space, but space stations offer a long-term environment where studies can be performed potentially for decades, combined with ready access by human researchers.\n\nThe ISS simplifies individual experiments by allowing groups of experiments to share the same launches and crew time. Research is conducted in a wide variety of fields, including astrobiology, astronomy, physical sciences, materials science, space weather, meteorology, and human research including space medicine and the life sciences. Scientists on Earth have timely access to the data and can suggest experimental modifications to the crew. If follow-on experiments are necessary, the routinely scheduled launches of resupply craft allows new hardware to be launched with relative ease. Crews fly expeditions of several months' duration, providing approximately 160 person-hours per week of labour with a crew of 6. However, a considerable amount of crew time is taken up by station maintenance.\n\nPerhaps the most notable ISS experiment is the Alpha Magnetic Spectrometer (AMS), which is intended to detect dark matter and answer other fundamental questions about our universe and is as important as the Hubble Space Telescope according to NASA. Currently docked on station, it could not have been easily accommodated on a free flying satellite platform because of its power and bandwidth needs. On 3 April 2013, scientists reported that hints of dark matter may have been detected by the AMS. According to the scientists, \"The first results from the space-borne Alpha Magnetic Spectrometer confirm an unexplained excess of high-energy positrons in Earth-bound cosmic rays.\"\n\nThe space environment is hostile to life. Unprotected presence in space is characterised by an intense radiation field (consisting primarily of protons and other subatomic charged particles from the solar wind, in addition to cosmic rays), high vacuum, extreme temperatures, and microgravity. Some simple forms of life called extremophiles, as well as small invertebrates called tardigrades can survive in this environment in an extremely dry state through desiccation.\n\nMedical research improves knowledge about the effects of long-term space exposure on the human body, including muscle atrophy, bone loss, and fluid shift. This data will be used to determine whether high duration human spaceflight and space colonisation are feasible. , data on bone loss and muscular atrophy suggest that there would be a significant risk of fractures and movement problems if astronauts landed on a planet after a lengthy interplanetary cruise, such as the six-month interval required to travel to Mars.\n\nMedical studies are conducted aboard the ISS on behalf of the National Space Biomedical Research Institute (NSBRI). Prominent among these is the Advanced Diagnostic Ultrasound in Microgravity study in which astronauts perform ultrasound scans under the guidance of remote experts. The study considers the diagnosis and treatment of medical conditions in space. Usually, there is no physician on board the ISS and diagnosis of medical conditions is a challenge. It is anticipated that remotely guided ultrasound scans will have application on Earth in emergency and rural care situations where access to a trained physician is difficult.\n\nGravity at the altitude of the ISS is approximately 90% as strong as at Earth's surface, but objects in orbit are in a continuous state of freefall, resulting in an apparent state of weightlessness. This perceived weightlessness is disturbed by five separate effects:\nBULLET::::- Drag from the residual atmosphere.\nBULLET::::- Vibration from the movements of mechanical systems and the crew.\nBULLET::::- Actuation of the on-board attitude control moment gyroscopes.\nBULLET::::- Thruster firings for attitude or orbital changes.\nBULLET::::- Gravity-gradient effects, also known as tidal effects. Items at different locations within the ISS would, if not attached to the station, follow slightly different orbits. Being mechanically interconnected these items experience small forces that keep the station moving as a rigid body.\n\nResearchers are investigating the effect of the station's near-weightless environment on the evolution, development, growth and internal processes of plants and animals. In response to some of this data, NASA wants to investigate microgravity's effects on the growth of three-dimensional, human-like tissues, and the unusual protein crystals that can be formed in space.\n\nInvestigating the physics of fluids in microgravity will provide better models of the behaviour of fluids. Because fluids can be almost completely combined in microgravity, physicists investigate fluids that do not mix well on Earth. In addition, examining reactions that are slowed by low gravity and low temperatures will improve our understanding of superconductivity.\n\nThe study of materials science is an important ISS research activity, with the objective of reaping economic benefits through the improvement of techniques used on the ground. Other areas of interest include the effect of the low gravity environment on combustion, through the study of the efficiency of burning and control of emissions and pollutants. These findings may improve current knowledge about energy production, and lead to economic and environmental benefits. Future plans are for the researchers aboard the ISS to examine aerosols, ozone, water vapour, and oxides in Earth's atmosphere, as well as cosmic rays, cosmic dust, antimatter, and dark matter in the universe.\n\nThe ISS provides a location in the relative safety of Low Earth Orbit to test spacecraft systems that will be required for long-duration missions to the Moon and Mars. This provides experience in operations, maintenance as well as repair and replacement activities on-orbit, which will be essential skills in operating spacecraft farther from Earth, mission risks can be reduced and the capabilities of interplanetary spacecraft advanced. Referring to the MARS-500 experiment, ESA states that \"Whereas the ISS is essential for answering questions concerning the possible impact of weightlessness, radiation and other space-specific factors, aspects such as the effect of long-term isolation and confinement can be more appropriately addressed via ground-based simulations\". Sergey Krasnov, the head of human space flight programmes for Russia's space agency, Roscosmos, in 2011 suggested a \"shorter version\" of MARS-500 may be carried out on the ISS.\n\nIn 2009, noting the value of the partnership framework itself, Sergey Krasnov wrote, \"When compared with partners acting separately, partners developing complementary abilities and resources could give us much more assurance of the success and safety of space exploration. The ISS is helping further advance near-Earth space exploration and realisation of prospective programmes of research and exploration of the Solar system, including the Moon and Mars.\" A crewed mission to Mars may be a multinational effort involving space agencies and countries outside the current ISS partnership. In 2010, ESA Director-General Jean-Jacques Dordain stated his agency was ready to propose to the other four partners that China, India and South Korea be invited to join the ISS partnership. NASA chief Charlie Bolden stated in February 2011, \"Any mission to Mars is likely to be a global effort\". Currently, US federal legislation prevents NASA co-operation with China on space projects.\n\nThe ISS crew provides opportunities for students on Earth by running student-developed experiments, making educational demonstrations, allowing for student participation in classroom versions of ISS experiments, and directly engaging students using radio, videolink and email. ESA offers a wide range of free teaching materials that can be downloaded for use in classrooms. In one lesson, students can navigate a 3-D model of the interior and exterior of the ISS, and face spontaneous challenges to solve in real time.\n\nJAXA aims to inspire children to \"pursue craftsmanship\" and to heighten their \"awareness of the importance of life and their responsibilities in society.\" Through a series of education guides, a deeper understanding of the past and near-term future of crewed space flight, as well as that of Earth and life, will be learned. In the JAXA Seeds in Space experiments, the mutation effects of spaceflight on plant seeds aboard the ISS is explored. Students grow sunflower seeds which flew on the ISS for about nine months. In the first phase of \"Kibō\" utilisation from 2008 to mid-2010, researchers from more than a dozen Japanese universities conducted experiments in diverse fields.\n\nCultural activities are another major objective. Tetsuo Tanaka, director of JAXA's Space Environment and Utilization Center, says \"There is something about space that touches even people who are not interested in science.\"\n\nAmateur Radio on the ISS (ARISS) is a volunteer programme which encourages students worldwide to pursue careers in science, technology, engineering and mathematics through amateur radio communications opportunities with the ISS crew. ARISS is an international working group, consisting of delegations from nine countries including several countries in Europe as well as Japan, Russia, Canada, and the United States. In areas where radio equipment cannot be used, speakerphones connect students to ground stations which then connect the calls to the station.\n\n\"First Orbit\" is a feature-length documentary film about Vostok 1, the first crewed space flight around the Earth. By matching the orbit of the International Space Station to that of Vostok 1 as closely as possible, in terms of ground path and time of day, documentary filmmaker Christopher Riley and ESA astronaut Paolo Nespoli were able to film the view that Yuri Gagarin saw on his pioneering orbital space flight. This new footage was cut together with the original Vostok 1 mission audio recordings sourced from the Russian State Archive. Nespoli, during Expedition 26/27, filmed the majority of the footage for this documentary film, and as a result is credited as its director of photography. The film was streamed through the website firstorbit.org in a global YouTube premiere in 2011, under a free licence.\n\nIn May 2013, commander Chris Hadfield shot a music video of David Bowie's \"Space Oddity\" on board the station; the film was released on YouTube. It was the first music video ever to be filmed in space.\n\nIn November 2017, while participating in Expedition 52/53 on the ISS, Paolo Nespoli made two recordings (one in English the other in his native Italian) of his spoken voice, for use on Wikipedia articles. These were the first content made specifically for Wikipedia, in space.\n\nSince the International Space Station is a multi-national collaborative project, the components for in-orbit assembly were manufactured in various countries around the world. Beginning in the mid 1990s, the U.S. components \"Destiny\", \"Unity\", the Integrated Truss Structure, and the solar arrays were fabricated at the Marshall Space Flight Center and the Michoud Assembly Facility. These modules were delivered to the Operations and Checkout Building and the Space Station Processing Facility for final assembly and processing for launch.\n\nThe Russian modules, including \"Zarya\" and \"Zvezda\", were manufactured at the Khrunichev State Research and Production Space Center in Moscow. \"Zvezda\" was initially manufactured in 1985 as a component for \"Mir-2\", but was never launched and instead became the ISS Service Module.\n\nThe European Space Agency \"Columbus\" module was manufactured at the European Space Research and Technology Centre (ESTEC) in the Netherlands, along with many other contractors throughout Europe. The other ESA-built modules - \"Harmony\", \"Tranquility\", the Leonardo MPLM, and the \"Cupola\" - were initially manufactured at the Thales Alenia Space factory located at the Cannes Mandelieu Space Center. The structural steel hulls of the modules were transported by aircraft to the Kennedy Space Center SSPF for launch processing.\n\nThe Japanese Experiment Module \"Kibō\", was fabricated in various technology manufacturing facilities in Japan, at the NASDA (now JAXA) Tanegashima Space Center, and the Institute of Space and Astronautical Science. The \"Kibo\" module was transported by ship and flown by aircraft to the KSC Space Station Processing Facility.\n\nThe Mobile Servicing System, consisting of the Canadarm2 and the \"Dextre\" grapple fixture, was manufactured at various factories in Canada and the United States under contract by the Canadian Space Agency. The mobile base system, a connecting framework for Canadarm2 mounted on rails, was built by Northrop Grumman.\n\nThe assembly of the International Space Station, a major endeavour in space architecture, began in November 1998. Russian modules launched and docked robotically, with the exception of \"Rassvet\". All other modules were delivered by the Space Shuttle, which required installation by ISS and shuttle crewmembers using the Canadarm2 (SSRMS) and extra-vehicular activities (EVAs); , they had added 159 components during more than 1,000 hours of EVA (see List of ISS spacewalks). 127 of these spacewalks originated from the station, and the remaining 32 were launched from the airlocks of docked Space Shuttles. The beta angle of the station had to be considered at all times during construction.\n\nThe first module of the ISS, \"Zarya\", was launched on 20 November 1998 on an autonomous Russian Proton rocket. It provided propulsion, attitude control, communications, electrical power, but lacked long-term life support functions. Two weeks later, a passive NASA module \"Unity\" was launched aboard Space Shuttle flight STS-88 and attached to \"Zarya\" by astronauts during EVAs. This module has two Pressurised Mating Adapters (PMAs), one connects permanently to \"Zarya\", the other allowed the Space Shuttle to dock to the space station. At that time, the Russian station \"Mir\" was still inhabited, and the ISS remained uncrewed for two years. On 12 July 2000, \"Zvezda\" was launched into orbit. Preprogrammed commands on board deployed its solar arrays and communications antenna. It then became the passive target for a rendezvous with \"Zarya\" and \"Unity\": it maintained a station-keeping orbit while the \"Zarya\"-\"Unity\" vehicle performed the rendezvous and docking via ground control and the Russian automated rendezvous and docking system. \"Zarya\" computer transferred control of the station to \"Zvezda\" computer soon after docking. \"Zvezda\" added sleeping quarters, a toilet, kitchen, CO scrubbers, dehumidifier, oxygen generators, exercise equipment, plus data, voice and television communications with mission control. This enabled permanent habitation of the station.\n\nThe first resident crew, Expedition 1, arrived in November 2000 on Soyuz TM-31. At the end of the first day on the station, astronaut Bill Shepherd requested the use of the radio call sign \"\"Alpha\"\", which he and cosmonaut Krikalev preferred to the more cumbersome \"\"International Space Station\"\". The name \"\"Alpha\"\" had previously been used for the station in the early 1990s, and its use was authorised for the whole of Expedition 1. Shepherd had been advocating the use of a new name to project managers for some time. Referencing a naval tradition in a pre-launch news conference he had said: \"For thousands of years, humans have been going to sea in ships. People have designed and built these vessels, launched them with a good feeling that a name will bring good fortune to the crew and success to their voyage.\" Yuri Semenov, the President of Russian Space Corporation Energia at the time, disapproved of the name \"\"Alpha\"\" as he felt that \"Mir\" was the first modular space station, so the names \"\"Beta\"\" or \"\"Mir\" 2\" for the ISS would have been more fitting.\n\nExpedition 1 arrived midway between the flights of STS-92 and STS-97. These two Space Shuttle flights each added segments of the station's Integrated Truss Structure, which provided the station with Ku-band communication for US television, additional attitude support needed for the additional mass of the USOS, and substantial solar arrays supplementing the station's existing 4 solar arrays.\n\nOver the next two year, the station continued to expand. A Soyuz-U rocket delivered the \"Pirs\" docking compartment. The Space Shuttles \"Discovery\", \"Atlantis\", and \"Endeavour\" delivered the \"Destiny\" laboratory and \"Quest\" airlock, in addition to the station's main robot arm, the Canadarm2, and several more segments of the Integrated Truss Structure.\n\nThe expansion schedule was interrupted by the disaster in 2003 and a resulting hiatus in flights. The Space Shuttle was grounded until 2005 with STS-114 flown by \"Discovery\".\n\nAssembly resumed in 2006 with the arrival of STS-115 with \"Atlantis\", which delivered the station's second set of solar arrays. Several more truss segments and a third set of arrays were delivered on STS-116, STS-117, and STS-118. As a result of the major expansion of the station's power-generating capabilities, more pressurised modules could be accommodated, and the \"Harmony\" node and \"Columbus\" European laboratory were added. These were soon followed by the first two components of \"Kibō\". In March 2009, STS-119 completed the Integrated Truss Structure with the installation of the fourth and final set of solar arrays. The final section of \"Kibō\" was delivered in July 2009 on STS-127, followed by the Russian \"Poisk\" module. The third node, \"Tranquility\", was delivered in February 2010 during STS-130 by the Space Shuttle \"Endeavour\", alongside the Cupola, followed in May 2010 by the penultimate Russian module, \"Rassvet\". \"Rassvet\" was delivered by Space Shuttle \"Atlantis\" on STS-132 in exchange for the Russian Proton delivery of the US-funded \"Zarya\" module in 1998. The last pressurised module of the USOS, \"Leonardo\", was brought to the station in February 2011 on the final flight of \"Discovery\", STS-133. The Alpha Magnetic Spectrometer was delivered by \"Endeavour\" on STS-134 the same year.\n\n, the station consisted of 15 pressurised modules and the Integrated Truss Structure. Five modules are still to be launched, including the \"Nauka\" with the European Robotic Arm, the \"Prichal\" module, and two power modules called NEM-1 and NEM-2. , Russia's future primary research module \"Nauka\" is set to launch in the summer of 2020, along with the European Robotic Arm which will be able to relocate itself to different parts of the Russian modules of the station.\n\nThe gross mass of the station changes over time. The total launch mass of the modules on orbit is about (). The mass of experiments, spare parts, personal effects, crew, foodstuff, clothing, propellants, water supplies, gas supplies, docked spacecraft, and other items add to the total mass of the station. Hydrogen gas is constantly vented overboard by the oxygen generators.\n\nThe ISS is a third generation modular space station. Modular stations can allow modules to be added to or removed from the existing structure, allowing greater flexibility.\n\nBelow is a diagram of major station components. The blue areas are pressurised sections accessible by the crew without using spacesuits. The station's unpressurised superstructure is indicated in red. Other unpressurised components are yellow. The \"Unity\" node joins directly to the \"Destiny\" laboratory. For clarity, they are shown apart.\n\nZarya (), also known as the Functional Cargo Block or FGB (from the or \"ФГБ\"), is the first module of the ISS to be launched. The FGB provided electrical power, storage, propulsion, and guidance to the ISS during the initial stage of assembly. With the launch and assembly in orbit of other modules with more specialized functionality, \"Zarya \" is now primarily used for storage, both inside the pressurized section and in the externally mounted fuel tanks. The \"Zarya\" is a descendant of the TKS spacecraft designed for the Russian \"Salyut\" program. The name \"Zarya\", which means sunrise, was given to the FGB because it signified the dawn of a new era of international cooperation in space. Although it was built by a Russian company, it is owned by the United States.\n\n\"Zarya\" was built from December 1994 to January 1998 at the Khrunichev State Research and Production Space Center (KhSC) in Moscow.\n\n\"Zarya\" was launched on 20 November 1998 on a Russian Proton rocket from Baikonur Cosmodrome Site 81 in Kazakhstan to a high orbit with a designed lifetime of at least 15 years. After \"Zarya\" reached orbit, STS-88 launched on 4 December 1998 to attach the \"Unity\" module.\n\nThe Unity connecting module, also known as Node 1, is the first U.S.-built component of the ISS. It connects the Russian and United States segments of the station, and is where crew eat meals together.\n\nThe module is cylindrical in shape, with six berthing locations (forward, aft, port, starboard, zenith, and nadir) facilitating connections to other modules. \"Unity\" measures in diameter, is long, made of steel, and was built for NASA by Boeing in a manufacturing facility at the Marshall Space Flight Center in Huntsville, Alabama. \"Unity\" is the first of the three connecting modules; the other two are \"Harmony\" and \"Tranquility\".\n\n\"Unity\" was carried into orbit as the primary cargo of the on STS-88, the first Space Shuttle mission dedicated to assembly of the station. On 6 December 1998, the STS-88 crew mated the aft berthing port of \"Unity\" with the forward hatch of the already orbiting \"Zarya\" module. This was the first connection made between two station modules.\n\nZvezda (, meaning \"star\"), \"Salyut\" DOS-8, also known as the \"Zvezda\" Service Module, is a module of the ISS. It was the third module launched to the station, and provides all of the station's life support systems, some of which are supplemented in the USOS, as well as living quarters for two crew members. It is the structural and functional center of the Russian Orbital Segment, which is the Russian part of the ISS. Crew assemble here to deal with emergencies on the station.\n\nThe basic structural frame of \"Zvezda\", known as \"DOS-8\", was initially built in the mid-1980s to be the core of the \"Mir-2\" space station. This means that \"Zvezda\" is similar in layout to the core module (DOS-7) of the \"Mir\" space station. It was in fact labeled as \"Mir-2\" for quite some time in the factory. Its design lineage thus extends back to the original \"Salyut\" stations. The space frame was completed in February 1985 and major internal equipment was installed by October 1986.\n\nThe rocket used for launch to the ISS carried advertising; it was emblazoned with the logo of Pizza Hut restaurants, for which they are reported to have paid more than US$1 million. The money helped support Khrunichev State Research and Production Space Center and the Russian advertising agencies that orchestrated the event.\n\nOn 26 July 2000, \"Zvezda\" became the third component of the ISS when it docked at the aft port of \"Zarya\". (U.S. \"Unity\" module had already been attached to the \"Zarya\".) Later in July, the computers aboard \"Zarya\" handed over ISS commanding functions to computers on \"Zvezda\".\n\nThe Destiny module, also known as the U.S. Lab, is the primary operating facility for U.S. research payloads aboard the International Space Station (ISS). It was berthed to the \"Unity\" module and activated over a period of five days in February, 2001. \"Destiny\" is NASA's first permanent operating orbital research station since Skylab was vacated in February 1974.\n\nThe Boeing Company began construction of the research laboratory in 1995 at the Michoud Assembly Facility and then the Marshall Space Flight Center in Huntsville, Alabama. \"Destiny\" was shipped to the Kennedy Space Center in Florida in 1998, and was turned over to NASA for pre-launch preparations in August 2000. It launched on 7 February 2001 aboard the on STS-98.\n\nThe \"Quest\" Joint Airlock, previously known as the Joint Airlock Module, is the primary airlock for the ISS. \"Quest\" was designed to host spacewalks with both Extravehicular Mobility Unit (EMU) spacesuits and Orlan space suits. The airlock was launched on STS-104 on 14 July 2001. Before \"Quest\" was attached, Russian spacewalks using Orlan suits could only be done from the \"Zvezda\" service module, and American spacewalks using EMUs were only possible when a Space Shuttle was docked. The arrival of \"Pirs\" docking compartment on September 16, 2001 provided another airlock from which Orlan spacewalks can be conducted.\n\nPirs () and Poisk () are Russian airlock modules, each having 2 identical hatches. An outward-opening hatch on the \"Mir\" space station failed after it swung open too fast after unlatching, because of a small amount of air pressure remaining in the airlock. All EVA hatches on the ISS open inwards and are pressure-sealing. \"Pirs\" was used to store, service, and refurbish Russian Orlan suits and provided contingency entry for crew using the slightly bulkier American suits. The outermost docking ports on both airlocks allow docking of Soyuz and Progress spacecraft, and the automatic transfer of propellants to and from storage on the ROS.\n\n\"Pirs\" was launched on 14 September 2001, as ISS Assembly Mission 4R, on a Russian Soyuz-U rocket, using a modified Progress spacecraft, Progress M-SO1, as an upper stage. \"Poisk\" was launched on 10 November 2009 attached to a modified Progress spacecraft, called Progress M-MIM2, on a Soyuz-U rocket from Launch Pad 1 at the Baikonur Cosmodrome in Kazakhstan.\n\nHarmony, also known as Node 2, is the \"utility hub\" of the ISS. It connects the laboratory modules of the United States, Europe and Japan, as well as providing electrical power and electronic data. Sleeping cabins for four of the six crew are housed here.\n\n\"Harmony\" was successfully launched into space aboard Space Shuttle flight STS-120 on October 23, 2007. After temporarily being attached to the port side of the Unity node, it was moved to its permanent location on the forward end of the Destiny laboratory on November 14, 2007. \"Harmony\" added to the station's living volume, an increase of almost 20 percent, from to . Its successful installation meant that from NASA's perspective, the station was \"U.S. Core Complete\".\n\nTranquility, also known as Node 3, is a module of the ISS. It contains environmental control systems, life support systems, a toilet, exercise equipment, and an observation cupola.\n\nESA and the Italian Space Agency had \"Tranquility\" built by Thales Alenia Space. A ceremony on November 20, 2009 transferred ownership of the module to NASA. On February 8, 2010, NASA launched the module on the Space Shuttle's STS-130 mission.\n\nColumbus is a science laboratory that is part of the ISS and is the largest single contribution to the ISS made by the European Space Agency (ESA).\n\nLike the \"Harmony\" and \"Tranquility\" modules, the \"Columbus\" laboratory was constructed in Turin, Italy by Thales Alenia Space. The functional equipment and software of the lab was designed by EADS in Bremen, Germany. It was also integrated in Bremen before being flown to the Kennedy Space Center (KSC) in Florida in an Airbus Beluga. It was launched aboard on 7 February 2008 on flight STS-122. It is designed for ten years of operation. The module is controlled by the Columbus Control Centre, located at the German Space Operations Centre, part of the German Aerospace Center in Oberpfaffenhofen near Munich, Germany.\n\nThe European Space Agency has spent €1.4 billion (about US$2 billion) on building \"Columbus\", including the experiments that will fly in it and the ground control infrastructure necessary to operate them.\n\nThe Japanese Experiment Module (JEM), nicknamed , is a Japanese science module for the ISS developed by JAXA. It is the largest single ISS module, and is attached to the \"Harmony\" module. The first two pieces of the module were launched on Space Shuttle missions STS-123 and STS-124. The third and final components were launched on STS-127.\n\nThe Cupola is an ESA-built observatory module of the ISS. Its name derives from the Italian word \"\", which means \"dome\". Its seven windows are used to conduct experiments, dockings and observations of Earth. It was launched aboard Space Shuttle mission STS-130 on 8 February 2010 and attached to the \"Tranquility\" (Node 3) module. With the \"Cupola\" attached, ISS assembly reached 85 percent completion. The \"Cupola\" central window has a diameter of .\n\nRassvet (; lit. \"dawn\"), also known as the Mini-Research Module 1 (MRM-1) (, ) and formerly known as the Docking Cargo Module (DCM), is a component of the ISS. The module's design is similar to the Mir Docking Module launched on STS-74 in 1995. \"Rassvet\" is primarily used for cargo storage and as a docking port for visiting spacecraft. It was flown to the ISS aboard on the STS-132 mission on May 14, 2010, and was connected to the ISS on May 18. The hatch connecting \"Rassvet\" with the ISS was first opened on May 20. On 28 June 2010, the Soyuz TMA-19 spacecraft performed the first docking with the module.\n\nThe \"Leonardo\" Permanent Multipurpose Module (PMM) is a module of the ISS. It was flown into space aboard the Space Shuttle on STS-133 on 24 February 2011 and installed on 1 March. \"Leonardo\" is primarily used for storage of spares, supplies and waste on the ISS, which was until then stored in many different places within the space station. The \"Leonardo\" PMM was a Multi-Purpose Logistics Module (MPLM) before 2011, but was modified into its current configuration. It was formerly one of three MPLM used for bringing cargo to and from the ISS with the Space Shuttle. The module was named for Italian polymath Leonardo da Vinci.\n\nThe Bigelow Expandable Activity Module (BEAM) is an experimental expandable space station module developed by Bigelow Aerospace, under contract to NASA, for testing as a temporary module on the ISS from 2016 to at least 2020. It arrived at the ISS on 10 April 2016, was berthed to the station on 16 April, and was expanded and pressurized on 28 May 2016.\n\nThe International Docking Adapter (IDA) is a spacecraft docking system adapter developed to convert APAS-95 to the NASA Docking System (NDS)/International Docking System Standard (IDSS). An IDA is placed on each of the ISS' two open Pressurized Mating Adapters (PMAs), both of which are connected to the \"Harmony\" module.\n\nIDA-1 was lost during the launch failure of SpaceX CRS-7 on 28 June 2015.\n\nIDA-2 was launched on SpaceX CRS-9 on 18 July 2016. It was attached and connected to PMA-2 during a spacewalk on 19 August 2016. First docking was achieved with the arrival of Crew Dragon Demo-1 on 3 March 2019.\n\nIDA-3 was launched on the SpaceX CRS-18 mission in July 2019. IDA-3 is constructed mostly from spare parts to speed construction. It was attached and connected to PMA-3 during a spacewalk on 21 August 2019.\n\nThe ISS has a large number of external components that do not require pressurisation. The largest of these is the Integrated Truss Structure (ITS), to which the station's main solar arrays and thermal radiators are mounted. The ITS consists of ten separate segments forming a structure 108.5 m (356 ft) long.\n\nThe station was intended to have several smaller external components, such as six robotic arms, three External Stowage Platforms (ESPs) and four ExPRESS Logistics Carriers (ELCs). While these platforms allow experiments (including MISSE, the STP-H3 and the Robotic Refueling Mission) to be deployed and conducted in the vacuum of space by providing electricity and processing experimental data locally, their primary function is to store spare Orbital Replacement Units (ORUs). ORUs are parts that can be replaced when they fail or pass their design life, including pumps, storage tanks, antennas, and battery units. Such units are replaced either by astronauts during EVA or by robotic arms. Several shuttle missions were dedicated to the delivery of ORUs, including STS-129, STS-133 and STS-134. , only one other mode of transportation of ORUs had been utilised – the Japanese cargo vessel HTV-2 – which delivered an FHRC and CTC-2 via its Exposed Pallet (EP).\nThere are also smaller exposure facilities mounted directly to laboratory modules; the \"Kibō\" Exposed Facility serves as an external 'porch' for the \"Kibō\" complex, and a facility on the European \"Columbus\" laboratory provides power and data connections for experiments such as the European Technology Exposure Facility and the Atomic Clock Ensemble in Space. A remote sensing instrument, SAGE III-ISS, was delivered to the station in February 2017 aboard CRS-10, and the NICER experiment was delivered aboard CRS-11 in June 2017. The largest scientific payload externally mounted to the ISS is the Alpha Magnetic Spectrometer (AMS), a particle physics experiment launched on STS-134 in May 2011, and mounted externally on the ITS. The AMS measures cosmic rays to look for evidence of dark matter and antimatter.\n\nThe commercial \"Bartolomeo\" External Payload Hosting Platform, manufactured by Airbus, is due to launch in May 2019 aboard a commercial ISS resupply vehicle and be attached to the European \"Columbus\" module. It will provide a further 12 external payload slots, supplementing the eight on the ExPRESS Logistics Carriers, ten on \"Kibō\", and four on \"Columbus\". The system is designed to be robotically serviced and will require no astronaut intervention. It is named after Christopher Columbus's younger brother.\n\nThe Integrated Truss Structure serves as a base for the station's primary remote manipulator system, called the Mobile Servicing System (MSS), which is composed of three main components. Canadarm2, the largest robotic arm on the ISS, has a mass of and is used to dock and manipulate spacecraft and modules on the USOS, hold crew members and equipment in place during EVAs and move Dextre around to perform tasks. Dextre is a robotic manipulator with two arms, a rotating torso and has power tools, lights and video for replacing orbital replacement units (ORUs) and performing other tasks requiring fine control. The Mobile Base System (MBS) is a platform which rides on rails along the length of the station's main truss. It serves as a mobile base for Canadarm2 and Dextre, allowing the robotic arms to reach all parts of the USOS. To gain access to the Russian Segment a grapple fixture was added to \"Zarya\" on STS-134, so that Canadarm2 can inchworm itself onto the ROS. Also installed during STS-134 was the Orbiter Boom Sensor System (OBSS), which had been used to inspect heat shield tiles on Space Shuttle missions and can be used on station to increase the reach of the MSS. Staff on Earth or the station can operate the MSS components via remote control, performing work outside the station without space walks.\n\nJapan's Remote Manipulator System, which services the \"Kibō\" Exposed Facility, was launched on STS-124 and is attached to the \"Kibō\" Pressurised Module. The arm is similar to the Space Shuttle arm as it is permanently attached at one end and has a latching end effector for standard grapple fixtures at the other.\n\nThe European Robotic Arm, which will service the Russian Orbital Segment, will be launched alongside the Multipurpose Laboratory Module in 2017. The ROS does not require spacecraft or modules to be manipulated, as all spacecraft and modules dock automatically and may be discarded the same way. Crew use the two \"Strela\" (; lit. Arrow) cargo cranes during EVAs for moving crew and equipment around the ROS. Each Strela crane has a mass of .\n\nNauka (; lit. \"Science\"), also known as the Multipurpose Laboratory Module (MLM), (Russian: \"Многофункциональный лабораторный модуль\", or \"МЛМ\"), is a component of the ISS which has not yet been launched into space. The MLM is funded by the Roscosmos State Corporation. In the original ISS plans, \"Nauka\" was to use the location of the Docking and Stowage Module. Later, the DSM was replaced by the \"Rassvet\" module and it was moved to \"Zarya\"s nadir port. Planners anticipate \"Nauka\" will dock at \"Zvezda\"s nadir port, replacing \"Pirs\".\n\nThe launch of \"Nauka\", initially planned for 2007, has been repeatedly delayed for various reasons. , the launch to the ISS is assigned to no earlier than December 2020. After this date, the warranties of some of Nauka's systems will expire.\n\nPrichal, also known as \"Uzlovoy\" Module or UM (, \"Nodal Module Berth\"), is a ball-shaped module that will allow docking of two scientific and power modules during the final stage of the station assembly, and provide the Russian segment additional docking ports to receive Soyuz MS and Progress MS spacecraft. UM is due to be launched in 2022. It will be integrated with a special version of the Progress cargo ship and launched by a standard Soyuz rocket, docking to the nadir port of the \"Nauka\" module. One port is equipped with an active hybrid docking port, which enables docking with the MLM module. The remaining five ports are passive hybrids, enabling docking of Soyuz and Progress vehicles, as well as heavier modules and future spacecraft with modified docking systems. The node module was intended to serve as the only permanent element of the cancelled OPSEK.\n\nScience Power Module 1 (SPM-1, also known as NEM-1) and Science Power Module 2 (SPM-2, also known as NEM-2) are modules planned to arrive at the ISS in 2022. It is going to dock to the \"Prichal\" module, which is planned to be attached to the \"Nauka\" module. If \"Nauka\" is cancelled, then the \"Prichal\", SPM-1, and SPM-2 would dock at the zenith port of \"Zvezda\". SPM-1 and SPM-2 would also be required components for the OPSEK space station.\n\nThe NanoRacks Bishop Airlock Module is a commercially-funded airlock module intended to be launched to the ISS on SpaceX CRS-21 in August 2020. The module is being built by NanoRacks, Thales Alenia Space, and Boeing. It will be used to deploy CubeSats, small satellites, and other external payloads for NASA, CASIS, and other commercial and governmental customers.\n\nSeveral modules planned for the station were cancelled over the course of the ISS programme. Reasons include budgetary constraints, the modules becoming unnecessary, and station redesigns after the 2003 \"Columbia\" disaster. The US Centrifuge Accommodations Module would have hosted science experiments in varying levels of artificial gravity. The US Habitation Module would have served as the station's living quarters. Instead, the sleep stations are now spread throughout the station. The US Interim Control Module and ISS Propulsion Module would have replaced the functions of \"Zvezda\" in case of a launch failure. Two Russian Research Modules were planned for scientific research. They would have docked to a Russian Universal Docking Module. The Russian Science Power Platform would have supplied power to the Russian Orbital Segment independent of the ITS solar arrays.\n\nThe critical systems are the atmosphere control system, the water supply system, the food supply facilities, the sanitation and hygiene equipment, and fire detection and suppression equipment. The Russian Orbital Segment's life support systems are contained in the \"Zvezda\" service module. Some of these systems are supplemented by equipment in the USOS. The MLM \"Nauka\" laboratory has a complete set of life support systems.\n\nThe atmosphere on board the ISS is similar to the Earth's. Normal air pressure on the ISS is ; the same as at sea level on Earth. An Earth-like atmosphere offers benefits for crew comfort, and is much safer than a pure oxygen atmosphere, because of the increased risk of a fire such as that responsible for the deaths of the Apollo 1 crew. Earth-like atmospheric conditions have been maintained on all Russian and Soviet spacecraft.\n\nThe \"Elektron\" system aboard \"Zvezda\" and a similar system in \"Destiny\" generate oxygen aboard the station. The crew has a backup option in the form of bottled oxygen and Solid Fuel Oxygen Generation (SFOG) canisters, a chemical oxygen generator system. Carbon dioxide is removed from the air by the Vozdukh system in \"Zvezda\". Other by-products of human metabolism, such as methane from the intestines and ammonia from sweat, are removed by activated charcoal filters.\n\nPart of the ROS atmosphere control system is the oxygen supply. Triple-redundancy is provided by the Elektron unit, solid fuel generators, and stored oxygen. The primary supply of oxygen is the Elektron unit which produces and by electrolysis of water and vents overboard. The system uses approximately one litre of water per crew member per day. This water is either brought from Earth or recycled from other systems. \"Mir\" was the first spacecraft to use recycled water for oxygen production. The secondary oxygen supply is provided by burning -producing Vika cartridges (see also ISS ECLSS). Each 'candle' takes 5–20 minutes to decompose at , producing of . This unit is manually operated.\n\nThe US Orbital Segment has redundant supplies of oxygen, from a pressurised storage tank on the \"Quest\" airlock module delivered in 2001, supplemented ten years later by ESA-built Advanced Closed-Loop System (ACLS) in the \"Tranquility\" module (Node 3), which produces by electrolysis. Hydrogen produced is combined with carbon dioxide from the cabin atmosphere and converted to water and methane.\n\nDouble-sided solar arrays provide electrical power to the ISS. These bifacial cells collect direct sunlight on one side and light reflected off from the Earth on the other, and are more efficient and operate at a lower temperature than single-sided cells commonly used on Earth. \n\nThe Russian segment of the station, like most spacecraft, uses 28 volt low voltage DC from four rotating solar arrays mounted on \"Zarya\" and \"Zvezda\". The USOS uses 130–180 V DC from the USOS PV array, power is stabilised and distributed at 160 V DC and converted to the user-required 124 V DC. The higher distribution voltage allows smaller, lighter conductors, at the expense of crew safety. The two station segments share power with converters.\n\nThe USOS solar arrays are arranged as four wing pairs, for a total production of 75 to 90 kilowatts. These arrays normally track the sun to maximise power generation. Each array is about in area and long. In the complete configuration, the solar arrays track the sun by rotating the \"alpha gimbal\" once per orbit; the \"beta gimbal\" follows slower changes in the angle of the sun to the orbital plane. The Night Glider mode aligns the solar arrays parallel to the ground at night to reduce the significant aerodynamic drag at the station's relatively low orbital altitude.\n\nThe station originally used rechargeable nickel–hydrogen batteries () for continuous power during the 35 minutes of every 90-minute orbit that it is eclipsed by the Earth. The batteries are recharged on the day side of the orbit. They had a 6.5-year lifetime (over 37,000 charge/discharge cycles) and were regularly replaced over the anticipated 20-year life of the station. Starting in 2016, the nickel–hydrogen batteries were replaced by lithium-ion batteries, which are expected to last until the end of the ISS program.\n\nThe station's large solar panels generate a high potential voltage difference between the station and the ionosphere. This could cause arcing through insulating surfaces and sputtering of conductive surfaces as ions are accelerated by the spacecraft plasma sheath. To mitigate this, plasma contactor units (PCU)s create current paths between the station and the ambient plasma field.\nThe station's systems and experiments consume a large amount of electrical power, almost all of which is converted to heat. To keep the internal temperature within workable limits, a passive thermal control system (PTCS) is made of external surface materials, insulation such as MLI, and heat pipes. If the PTCS cannot keep up with the heat load, an External Active Thermal Control System (EATCS) maintains the temperature. The EATCS consists of an internal, non-toxic, water coolant loop used to cool and dehumidify the atmosphere, which transfers collected heat into an external liquid ammonia loop. From the heat exchangers, ammonia is pumped into external radiators that emit heat as infrared radiation, then back to the station. The EATCS provides cooling for all the US pressurised modules, including \"Kibō\" and \"Columbus\", as well as the main power distribution electronics of the S0, S1 and P1 trusses. It can reject up to 70 kW. This is much more than the 14 kW of the Early External Active Thermal Control System (EEATCS) via the Early Ammonia Servicer (EAS), which was launched on STS-105 and installed onto the P6 Truss.\n\nRadio communications provide telemetry and scientific data links between the station and Mission Control Centres. Radio links are also used during rendezvous and docking procedures and for audio and video communication between crew members, flight controllers and family members. As a result, the ISS is equipped with internal and external communication systems used for different purposes.\n\nThe Russian Orbital Segment communicates directly with the ground via the \"Lira\" antenna mounted to \"Zvezda\". The \"Lira\" antenna also has the capability to use the \"Luch\" data relay satellite system. This system fell into disrepair during the 1990s, and so was not used during the early years of the ISS, although two new \"Luch\" satellites—\"Luch\"-5A and \"Luch\"-5B—were launched in 2011 and 2012 respectively to restore the operational capability of the system. Another Russian communications system is the Voskhod-M, which enables internal telephone communications between \"Zvezda\", \"Zarya\", \"Pirs\", \"Poisk\", and the USOS and provides a VHF radio link to ground control centres via antennas on \"Zvezda\" exterior.\n\nThe US Orbital Segment (USOS) makes use of two separate radio links mounted in the Z1 truss structure: the S band (audio) and K band (audio, video and data) systems. These transmissions are routed via the United States Tracking and Data Relay Satellite System (TDRSS) in geostationary orbit, allowing for almost continuous real-time communications with NASA's Mission Control Center (MCC-H) in Houston. Data channels for the Canadarm2, European \"Columbus\" laboratory and Japanese \"Kibō\" modules were originally also routed via the S band and K band systems, with the European Data Relay System and a similar Japanese system intended to eventually complement the TDRSS in this role. Communications between modules are carried on an internal wireless network.\nUHF radio is used by astronauts and cosmonauts conducting EVAs and other spacecraft that dock to or undock from the station. Automated spacecraft are fitted with their own communications equipment; the ATV uses a laser attached to the spacecraft and the Proximity Communications Equipment attached to \"Zvezda\" to accurately dock with the station.\n\nThe ISS is equipped with about 100 IBM/Lenovo ThinkPad and HP ZBook 15 laptop computers. The laptops have run Windows 95, Windows 2000, Windows XP, Windows 7, Windows 10 and Linux operating systems. Each computer is a commercial off-the-shelf purchase which is then modified for safety and operation including updates to connectors, cooling and power to accommodate the station's 28V DC power system and weightless environment. Heat generated by the laptops does not rise but stagnates around the laptop, so additional forced ventilation is required. Laptops aboard the ISS are connected to the station's wireless LAN via Wi-Fi, which connects to the ground via K band. This provides speeds of 10 Mbit/s download and 3 Mbit/s upload from the station, comparable to home DSL connection speeds. Laptop hard drives occasionally fail and must be replaced. Other computer hardware failures include instances in 2001, 2007 and 2017; some of these failures have required EVAs to replace computer modules in externally mounted devices.\n\nThe operating system used for key station functions is the Debian Linux distribution. The migration from Microsoft Windows was made in May 2013 for reasons of reliability, stability and flexibility.\nIn 2017, an SG100 Cloud Computer was launched to the ISS as part of OA-7 mission. It was manufactured by NCSIST and designed in collaboration with Academia Sinica, and National Central University under contract for NASA.\n\nEach permanent crew is given an expedition number. Expeditions run up to six months, from launch until undocking, an 'increment' covers the same time period, but includes cargo ships and all activities. Expeditions 1 to 6 consisted of 3 person crews, Expeditions 7 to 12 were reduced to the safe minimum of two following the destruction of the NASA Shuttle Columbia. From Expedition 13 the crew gradually increased to 6 around 2010. With the arrival of the US Commercial Crew vehicles in the late 2010s, expedition size may be increased to seven crew members, the number ISS is designed for.\n\nGennady Padalka, member of Expeditions 9, 19/20, 31/32, and 43/44, and Commander of Expedition 11, has spent more time in space than anyone else, a total of 878 days, 11 hours, and 29 minutes. Peggy Whitson has spent the most time in space of any American, totalling 665 days, 22 hours, and 22 minutes during her time on Expeditions 5, 16, and 50/51/52.\n\nTravellers who pay for their own passage into space are termed spaceflight participants by Roscosmos and NASA, and are sometimes referred to as space tourists, a term they generally dislike. All seven were transported to the ISS on Russian Soyuz spacecraft. When professional crews change over in numbers not divisible by the three seats in a Soyuz, and a short-stay crewmember is not sent, the spare seat is sold by MirCorp through Space Adventures. When the space shuttle retired in 2011, and the station's crew size was reduced to 6, space tourism was halted, as the partners relied on Russian transport seats for access to the station. Soyuz flight schedules increase after 2013, allowing 5 Soyuz flights (15 seats) with only two expeditions (12 seats) required. The remaining seats are sold for around to members of the public who can pass a medical exam. ESA and NASA criticised private spaceflight at the beginning of the ISS, and NASA initially resisted training Dennis Tito, the first person to pay for his own passage to the ISS. \n\nAnousheh Ansari became the first Iranian in space and the first self-funded woman to fly to the station. Officials reported that her education and experience make her much more than a tourist, and her performance in training had been \"excellent.\" Ansari herself dismisses the idea that she is a tourist. She did Russian and European studies involving medicine and microbiology during her 10-day stay. The documentary \"Space Tourists\" follows her journey to the station, where she fulfilled \"an age-old dream of man: to leave our planet as a \"normal person\" and travel into outer space.\"\n\nIn 2008, spaceflight participant Richard Garriott placed a geocache aboard the ISS during his flight. This is currently the only non-terrestrial geocache in existence. At the same time, the Immortality Drive, an electronic record of eight digitised human DNA sequences, was placed aboard the ISS.\n\nThe ISS is maintained in a nearly circular orbit with a minimum mean altitude of and a maximum of , in the centre of the thermosphere, at an inclination of 51.6 degrees to Earth's equator. This orbit was selected because it is the lowest inclination that can be directly reached by Russian Soyuz and Progress spacecraft launched from Baikonur Cosmodrome at 46° N latitude without overflying China or dropping spent rocket stages in inhabited areas.\nIt travels at an average speed of , and completes  orbits per day (93 minutes per orbit). The station's altitude was allowed to fall around the time of each NASA shuttle flight to permit heavier loads to be transferred to the station. After the retirement of the shuttle, the nominal orbit of the space station was raised in altitude. Other, more frequent supply ships do not require this adjustment as they are substantially higher performance vehicles.\n\nOrbital boosting can be performed by the station's two main engines on the \"Zvezda\" service module, or Russian or European spacecraft docked to \"Zvezda\" aft port. The ATV is constructed with the possibility of adding a second docking port to its aft end, allowing other craft to dock and boost the station. It takes approximately two orbits (three hours) for the boost to a higher altitude to be completed. Maintaining ISS altitude uses about 7.5 tonnes of chemical fuel per annum at an annual cost of about $210 million.\n\nThe Russian Orbital Segment contains the Data Management System, which handles Guidance, Navigation and Control (ROS GNC) for the entire station. Initially, \"Zarya\", the first module of the station, controlled the station until a short time after the Russian service module \"Zvezda\" docked and was transferred control. \"Zvezda\" contains the ESA built DMS-R Data Management System. Using two fault-tolerant computers (FTC), \"Zvezda\" computes the station's position and orbital trajectory using redundant Earth horizon sensors, Solar horizon sensors as well as Sun and star trackers. The FTCs each contain three identical processing units working in parallel and provide advanced fault-masking by majority voting.\n\n\"Zvezda\" uses gyroscopes (reaction wheels) and thrusters to turn itself around. Gyroscopes do not require propellant, rather they use electricity to 'store' momentum in flywheels by turning in the opposite direction to the station's movement. The USOS has its own computer controlled gyroscopes to handle the extra mass of that section. When gyroscopes 'saturate', thrusters are used to cancel out the stored momentum. During Expedition 10, an incorrect command was sent to the station's computer, using about 14 kilograms of propellant before the fault was noticed and fixed. When attitude control computers in the ROS and USOS fail to communicate properly, it can result in a rare 'force fight' where the ROS GNC computer must ignore the USOS counterpart, which has no thrusters.\n\nDocked spacecraft can also be used to maintain station attitude, such as for troubleshooting or during the installation of the S3/S4 truss, which provides electrical power and data interfaces for the station's electronics.\n\nThe components of the ISS are operated and monitored by their respective space agencies at mission control centres across the globe, including:\nBULLET::::- Roscosmos's Mission Control Center at Korolyov, Moscow Oblast, controls the Russian Orbital Segment which handles Guidance, Navigation and Control for the entire Station, in addition to individual Soyuz and Progress missions.\nBULLET::::- ESA's ATV Control Centre, at the Toulouse Space Centre (CST) in Toulouse, France, controls flights of the uncrewed European Automated Transfer Vehicle.\nBULLET::::- JAXA's JEM Control Center and HTV Control Center at Tsukuba Space Center (TKSC) are responsible for operating the \"Kibō\" complex and all flights of the White Stork HTV Cargo spacecraft, respectively.\nBULLET::::- NASA's Mission Control Center at Lyndon B. Johnson Space Center in Houston, Texas, serves as the primary control facility for the United States segment of the ISS.\nBULLET::::- NASA's Payload Operations and Integration Center at Marshall Space Flight Center in Huntsville, Alabama, coordinates payload operations in the USOS.\nBULLET::::- ESA's \"Columbus\" Control Centre at the German Aerospace Center in Oberpfaffenhofen, Germany, manages the European \"Columbus\" research laboratory.\nBULLET::::- CSA's MSS Control at Saint-Hubert, Quebec, Canada, controls and monitors the Mobile Servicing System, or Canadarm2.\n\nOrbital Replacement Units (ORUs) are spare parts that can be readily replaced when a unit either passes its design life or fails. Examples of ORUs are pumps, storage tanks, controller boxes, antennas, and battery units. Some units can be replaced using robotic arms. Most are stored outside the station, either on small pallets called ExPRESS Logistics Carriers (ELCs) or share larger platforms called External Stowage Platforms which also hold science experiments. Both kinds of pallets provide electricity for many parts that could be damaged by the cold of space and require heating. The larger logistics carriers also have local area network (LAN) connections for telemetry to connect experiments. A heavy emphasis on stocking the USOS with ORU's occurred around 2011, before the end of the NASA shuttle programme, as its commercial replacements, Cygnus and Dragon, carry one tenth to one quarter the payload.\n\nUnexpected problems and failures have impacted the station's assembly time-line and work schedules leading to periods of reduced capabilities and, in some cases, could have forced abandonment of the station for safety reasons. Serious problems include an air leak from the USOS in 2004, the venting of fumes from an \"Elektron\" oxygen generator in 2006, and the failure of the computers in the ROS in 2007 during STS-117 that left the station without thruster, \"Elektron\", \"Vozdukh\" and other environmental control system operations. In the latter case, the root cause was found to be condensation inside electrical connectors leading to a short-circuit.\n\nDuring STS-120 in 2007 and following the relocation of the P6 truss and solar arrays, it was noted during the solar array had torn and was not deploying properly. An EVA was carried out by Scott Parazynski, assisted by Douglas Wheelock. Extra precautions were taken to reduce the risk of electric shock, as the repairs were carried out with the solar array exposed to sunlight. The issues with the array were followed in the same year by problems with the starboard Solar Alpha Rotary Joint (SARJ), which rotates the arrays on the starboard side of the station. Excessive vibration and high-current spikes in the array drive motor were noted, resulting in a decision to substantially curtail motion of the starboard SARJ until the cause was understood. Inspections during EVAs on STS-120 and STS-123 showed extensive contamination from metallic shavings and debris in the large drive gear and confirmed damage to the large metallic bearing surfaces, so the joint was locked to prevent further damage. Repairs to the joints were carried out during STS-126 with lubrication and the replacement of 11 out of 12 trundle bearings on the joint.\n\nIn September 2008, damage to the S1 radiator was first noticed in Soyuz imagery. The problem was initially not thought to be serious. The imagery showed that the surface of one sub-panel has peeled back from the underlying central structure, possibly because of micro-meteoroid or debris impact. On 15 May 2009 the damaged radiator panel's ammonia tubing was mechanically shut off from the rest of the cooling system by the computer-controlled closure of a valve. The same valve was then used to vent the ammonia from the damaged panel, eliminating the possibility of an ammonia leak. It is also known that a Service Module thruster cover struck the S1 radiator after being jettisoned during an EVA in 2008, but its effect, if any, has not been determined.\n\nEarly on 1 August 2010, a failure in cooling Loop A (starboard side), one of two external cooling loops, left the station with only half of its normal cooling capacity and zero redundancy in some systems. The problem appeared to be in the ammonia pump module that circulates the ammonia cooling fluid. Several subsystems, including two of the four CMGs, were shut down.\n\nPlanned operations on the ISS were interrupted through a series of EVAs to address the cooling system issue. A first EVA on 7 August 2010, to replace the failed pump module, was not fully completed because of an ammonia leak in one of four quick-disconnects. A second EVA on 11 August successfully removed the failed pump module. A third EVA was required to restore Loop A to normal functionality.\n\nThe USOS's cooling system is largely built by the US company Boeing, which is also the manufacturer of the failed pump.\n\nThe four Main Bus Switching Units (MBSUs, located in the S0 truss), control the routing of power from the four solar array wings to the rest of the ISS. Each MBSU has two power channels that feed 160V DC from the arrays to two DC-to-DC power converters (DDCUs) that supply the 124V power used in the station. In late 2011 MBSU-1 ceased responding to commands or sending data confirming its health. While still routing power correctly, it was scheduled to be swapped out at the next available EVA. A spare MBSU was already on board, but a 30 August 2012 EVA failed to be completed when a bolt being tightened to finish installation of the spare unit jammed before the electrical connection was secured. The loss of MBSU-1 limited the station to 75% of its normal power capacity, requiring minor limitations in normal operations until the problem could be addressed.\n\nOn 5 September 2012, in a second 6 hr EVA, astronauts Sunita Williams and Akihiko Hoshide successfully replaced MBSU-1 and restored the ISS to 100% power.\n\nOn 24 December 2013, astronauts installed a new ammonia pump for the station's cooling system. The faulty cooling system had failed earlier in the month, halting many of the station's science experiments. Astronauts had to brave a \"mini blizzard\" of ammonia while installing the new pump. It was only the second Christmas Eve spacewalk in NASA history.\n\nA wide variety of crewed and uncrewed spacecraft have supported the station's activities. Thirty-seven Space Shuttle ISS flights were conducted before retirement. 75 Progress resupply spacecraft (including the modified M-MIM2 and M-SO1 module transports), 59 crewed Soyuz spacecraft, 5 European ATV, 8 Japanese HTV 'Kounotori', 20 SpaceX Dragon, and 12 Orbital ATK Cygnus have flown to the ISS.\n\n\"See also the list of professional crew, private travellers, both or just uncrewed spaceflights.\"\n\nBULLET::::- Key\n\n! colspan=\"3\"  Spacecraft and mission\n! Location\n! Arrival (UTC)\n! Departure (planned)\n\nBULLET::::- All dates are UTC. Dates are the earliest possible dates and may change.\nBULLET::::- Forward ports are at the front of the station according to its normal direction of travel and orientation (attitude). Aft is at the rear of the station, used by spacecraft boosting the station's orbit. Nadir is closest the Earth, Zenith is on top.\n\nBULLET::::- Key\n\nAll Russian spacecraft and self-propelled modules are able to rendezvous and dock to the space station without human intervention using the Kurs radar docking system from over 200 kilometres away. The European ATV uses star sensors and GPS to determine its intercept course. When it catches up it uses laser equipment to optically recognize \"Zvezda\", along with the Kurs system for redundancy. Crew supervise these craft, but do not intervene except to send abort commands in emergencies. Progress and ATV supply craft can remain at the ISS for six months, allowing great flexibility in crew time for loading and unloading of supplies and trash.\n\nFrom the initial station programs, the Russians pursued an automated docking methodology that used the crew in override or monitoring roles. Although the initial development costs were high, the system has become very reliable with standardisations that provide significant cost benefits in repetitive operations. An automated approach could also allow assembly of modules orbiting other worlds prior to crew arrival.\n\nSoyuz spacecraft used for crew rotation also serve as lifeboats for emergency evacuation; they are replaced every six months and were used after the Columbia disaster to return stranded crew from the ISS. Expeditions require, on average, of supplies, and , crews had consumed a total of around . Soyuz crew rotation flights and Progress resupply flights visit the station on average two and three times respectively each year, with the ATV and HTV planned to visit annually from 2010 onwards.\n\nOther vehicles berth instead of docking. The Japanese H-II Transfer Vehicle parks itself in progressively closer orbits to the station, and then awaits 'approach' commands from the crew, until it is close enough for a robotic arm to grapple and berth the vehicle to the USOS. Berthed craft can transfer International Standard Payload Racks. Japanese spacecraft berth for one to two months. The berthing Cygnus and Dragon are contracted to fly cargo to the station under the Commercial Resupply Services program.\n\nFrom 26 February 2011 to 7 March 2011 four of the governmental partners (United States, ESA, Japan and Russia) had their spacecraft (NASA Shuttle, ATV, HTV, Progress and Soyuz) docked at the ISS, the only time this has happened to date. On 25 May 2012, SpaceX delivered the first commercial cargo with a Dragon spacecraft.\n\nPrior to a ship's docking to the ISS, navigation and attitude control (GNC) is handed over to the ground control of the ships' country of origin. GNC is set to allow the station to drift in space, rather than fire its thrusters or turn using gyroscopes. The solar panels of the station are turned edge-on to the incoming ships, so residue from its thrusters does not damage the cells. Before its retirement, Shuttle launches were often given priority over Soyuz, with occasional priority given to Soyuz arrivals carrying crew and time-critical cargoes, such as biological experiment materials.\n\nA typical day for the crew begins with a wake-up at 06:00, followed by post-sleep activities and a morning inspection of the station. The crew then eats breakfast and takes part in a daily planning conference with Mission Control before starting work at around 08:10. The first scheduled exercise of the day follows, after which the crew continues work until 13:05. Following a one-hour lunch break, the afternoon consists of more exercise and work before the crew carries out its pre-sleep activities beginning at 19:30, including dinner and a crew conference. The scheduled sleep period begins at 21:30. In general, the crew works ten hours per day on a weekday, and five hours on Saturdays, with the rest of the time their own for relaxation or work catch-up.\n\nThe time zone used aboard the ISS is Coordinated Universal Time (UTC). The windows are covered at night hours to give the impression of darkness because the station experiences 16 sunrises and sunsets per day. During visiting Space Shuttle missions, the ISS crew mostly follows the shuttle's Mission Elapsed Time (MET), which is a flexible time zone based on the launch time of the Space Shuttle mission.\n\nThe station provides crew quarters for each member of the expedition's crew, with two 'sleep stations' in the \"Zvezda\" and four more installed in \"Harmony\". The USOS quarters are private, approximately person-sized soundproof booths. The ROS crew quarters include a small window, but provide less ventilation and sound proofing. A crew member can sleep in a crew quarter in a tethered sleeping bag, listen to music, use a laptop, and store personal items in a large drawer or in nets attached to the module's walls. The module also provides a reading lamp, a shelf and a desktop. Visiting crews have no allocated sleep module, and attach a sleeping bag to an available space on a wall. It is possible to sleep floating freely through the station, but this is generally avoided because of the possibility of bumping into sensitive equipment. It is important that crew accommodations be well ventilated; otherwise, astronauts can wake up oxygen-deprived and gasping for air, because a bubble of their own exhaled carbon dioxide has formed around their heads. During various station activities and crew rest times, the lights in the ISS can be dimmed, switched off, and color temperatures adjusted.\n\nOn the USOS, most of the food aboard is vacuum sealed in plastic bags; cans are rare because they are heavy and expensive to transport. Preserved food is not highly regarded by the crew and taste is reduced in microgravity, so efforts are taken to make the food more palatable, including using more spices than in regular cooking. The crew looks forward to the arrival of any ships from Earth as they bring fresh fruit and vegetables. Care is taken that foods do not create crumbs, and liquid condiments are preferred over solid to avoid contaminating station equipment. Each crew member has individual food packages and cooks them using the on-board galley. The galley features two food warmers, a refrigerator which was added in November 2008, and a water dispenser that provides both heated and unheated water. Drinks are provided as dehydrated powder that is mixed with water before consumption. Drinks and soups are sipped from plastic bags with straws, while solid food is eaten with a knife and fork attached to a tray with magnets to prevent them from floating away. Any food that floats away, including crumbs, must be collected to prevent it from clogging the station's air filters and other equipment.\n\nShowers on space stations were introduced in the early 1970s on \"Skylab\" and \"Salyut\" 3. By \"Salyut\" 6, in the early 1980s, the crew complained of the complexity of showering in space, which was a monthly activity. The ISS does not feature a shower; instead, crewmembers wash using a water jet and wet wipes, with soap dispensed from a toothpaste tube-like container. Crews are also provided with rinseless shampoo and edible toothpaste to save water.\n\nThere are two space toilets on the ISS, both of Russian design, located in \"Zvezda\" and \"Tranquility\". These Waste and Hygiene Compartments use a fan-driven suction system similar to the Space Shuttle Waste Collection System. Astronauts first fasten themselves to the toilet seat, which is equipped with spring-loaded restraining bars to ensure a good seal. A lever operates a powerful fan and a suction hole slides open: the air stream carries the waste away. Solid waste is collected in individual bags which are stored in an aluminium container. Full containers are transferred to Progress spacecraft for disposal. Liquid waste is evacuated by a hose connected to the front of the toilet, with anatomically correct \"urine funnel adapters\" attached to the tube so that men and women can use the same toilet. The diverted urine is collected and transferred to the Water Recovery System, where it is recycled into drinking water.\n\nOn 12 April 2019, NASA reported medical results from the Astronaut Twin Study. One astronaut twin spent a year in space on the ISS, while the other twin spent the year on Earth. Several long-lasting changes were observed, including those related to alterations in DNA and cognition, when one twin was compared with the other.\n\nIn November 2019, researchers reported that astronauts experienced serious blood flow and clot problems while on board the International Space Station, based on a six-month study of 11 healthy astronauts. The results may influence long-term spaceflight, including a mission to the planet Mars, according to the researchers.\n\nThe ISS is partially protected from the space environment by Earth's magnetic field. From an average distance of about , depending on Solar activity, the magnetosphere begins to deflect solar wind around Earth and ISS. Solar flares are still a hazard to the crew, who may receive only a few minutes warning. In 2005, during the initial 'proton storm' of an X-3 class solar flare, the crew of Expedition 10 took shelter in a more heavily shielded part of the ROS designed for this purpose.\n\nSubatomic charged particles, primarily protons from cosmic rays and solar wind, are normally absorbed by Earth's atmosphere. When they interact in sufficient quantity, their effect is visible to the naked eye in a phenomenon called an aurora. Outside Earth's atmosphere, crews are exposed to about 1 millisievert each day, which is about a year of natural exposure on Earth, resulting in a higher risk of cancer. Radiation can penetrate living tissue and damage the DNA and chromosomes of lymphocytes. These cells are central to the immune system, and so any damage to them could contribute to the lower immunity experienced by astronauts. Radiation has also been linked to a higher incidence of cataracts in astronauts. Protective shielding and drugs may lower risks to an acceptable level.\n\nRadiation levels on the ISS are about five times greater than those experienced by airline passengers and crew, as Earth's electromagnetic field provides almost the same level of protection against solar and other radiation in low Earth orbit as in the stratosphere. For example, on a 12-hour flight an airline passenger would experience 0.1 millisieverts of radiation, or a rate of 0.2 millisieverts per day; only 1/5 the rate experienced by an astronaut in LEO. Additionally, airline passengers experience this level of radiation for a few hours of flight, while ISS crew are exposed for their whole stay.\n\nThere is considerable evidence that psychosocial stressors are among the most important impediments to optimal crew morale and performance. Cosmonaut Valery Ryumin wrote in his journal during a particularly difficult period on board the \"Salyut\" 6 space station: \"All the conditions necessary for murder are met if you shut two men in a cabin measuring 18 feet by 20 and leave them together for two months.\"\n\nNASA's interest in psychological stress caused by space travel, initially studied when their crewed missions began, was rekindled when astronauts joined cosmonauts on the Russian space station \"Mir\". Common sources of stress in early US missions included maintaining high performance under public scrutiny and isolation from peers and family. The latter is still often a cause of stress on the ISS, such as when the mother of NASA Astronaut Daniel Tani died in a car accident, and when Michael Fincke was forced to miss the birth of his second child.\n\nA study of the longest spaceflight concluded that the first three weeks are a critical period where attention is adversely affected because of the demand to adjust to the extreme change of environment. ISS crew flights typically last about five to six months.\n\nThe ISS working environment includes further stress caused by living and working in cramped conditions with people from very different cultures who speak a different language. First-generation space stations had crews who spoke a single language; second- and third-generation stations have crew from many cultures who speak many languages. Astronauts must speak English and Russian, and knowing additional languages is even better.\n\nDue to the lack of gravity, confusion often occurs. Even though there is no up and down in space, some crew members feel like they are oriented upside down. They may also have difficulty measuring distances. This can cause problems like getting lost inside the space station, pulling switches in the wrong direction or misjudging the speed of an approaching vehicle during docking.\n\nMedical effects of long-term weightlessness include muscle atrophy, deterioration of the skeleton (osteopenia), fluid redistribution, a slowing of the cardiovascular system, decreased production of red blood cells, balance disorders, and a weakening of the immune system. Lesser symptoms include loss of body mass, and puffiness of the face.\n\nSleep is disturbed on the ISS regularly because of mission demands, such as incoming or departing ships. Sound levels in the station are unavoidably high. Because the atmosphere is unable to thermosiphon, fans are required at all times to allow processing of the atmosphere which would stagnate in the freefall (zero-g) environment.\n\nTo prevent some of these adverse physiological effects, the station is equipped with two treadmills (including the COLBERT), and the aRED (advanced Resistive Exercise Device) which enables various weightlifting exercises which add muscle but do not compensate for or raise astronauts' reduced bone density, and a stationary bicycle; each astronaut spends at least two hours per day exercising on the equipment. Astronauts use bungee cords to strap themselves to the treadmill.\n\nHazardous moulds which can foul air and water filters may develop aboard space stations. They can produce acids which degrade metal, glass, and rubber. They can also be harmful for the crew's health. Microbiological hazards have led to a development of the LOCAD-PTS that can identify common bacteria and moulds faster than standard methods of culturing, which may require a sample to be sent back to Earth. , 76 types of unregulated micro-organisms have been detected on the ISS. Researchers in 2018 reported, after detecting the presence of five \"Enterobacter bugandensis\" bacterial strains on the ISS, none pathogenic to humans, that microorganisms on ISS should be carefully monitored to continue assuring a medically healthy environment for astronauts.\n\nReduced humidity, paint with mould-killing chemicals, and antiseptic solutions can be used to prevent contamination in space stations. All materials used in the ISS are tested for resistance against fungi.\n\nIn April 2019, NASA reported that a comprehensive study of microorganisms and fungi present on the International Space Station has been conducted. The results can be useful in improving health and safety conditions for astronauts.\n\nAn onboard fire or a toxic gas leak are other potential hazards. Ammonia is used in the external radiators of the station and could potentially leak into the pressurised modules.\n\nThe low altitudes at which the ISS orbits are also home to a variety of space debris, including spent rocket stages, defunct satellites, explosion fragments (including materials from anti-satellite weapon tests), paint flakes, slag from solid rocket motors, and coolant released by US-A nuclear-powered satellites. These objects, in addition to natural micrometeoroids, are a significant threat. Objects large enough to destroy the station can be tracked, and are not as dangerous as smaller debris. Objects too small to be detected by optical and radar instruments, from approximately 1 cm down to microscopic size, number in the trillions. Despite their small size, some of these objects are a threat because of their kinetic energy and direction in relation to the station. Spacewalking crew in spacesuits are also at risk of suit damage and consequent exposure to vacuum.\n\nBallistic panels, also called micrometeorite shielding, are incorporated into the station to protect pressurised sections and critical systems. The type and thickness of these panels depend on their predicted exposure to damage. The station's shields and structure have different designs on the ROS and the USOS. On the USOS, Whipple shields are used. The US segment modules consist of an inner layer made from 1.5 cm thick aluminum, a 10 cm thick intermediate layers of Kevlar and Nextel, and an outer layer of stainless steel, which causes objects to shatter into a cloud before hitting the hull, thereby spreading the energy of impact. On the ROS, a carbon plastic honeycomb screen is spaced from the hull, an aluminium honeycomb screen is spaced from that, with a screen-vacuum thermal insulation covering, and glass cloth over the top.\n\nSpace debris is tracked remotely from the ground, and the station crew can be notified. If necessary, thrusters on the Russian Orbital Segment can alter the station's orbital altitude, avoiding the debris. These Debris Avoidance Manoeuvres (DAMs) are not uncommon, taking place if computational models show the debris will approach within a certain threat distance. Ten DAMs had been performed by the end of 2009. Usually, an increase in orbital velocity of the order of 1 m/s is used to raise the orbit by one or two kilometres.If necessary, the altitude can also be lowered, although such a maneuver wastes propellant. If a threat from orbital debris is identified too late for a DAM to be safely conducted, the station crew close all the hatches aboard the station and retreat into their Soyuz spacecraft in order to be able to evacuate in the event the station was seriously damaged by the debris. This partial station evacuation has occurred on 13 March 2009, 28 June 2011, 24 March 2012 and 16 June 2015.\n\nAccording to the Outer Space Treaty, the United States and Russia are legally responsible for all modules they have launched. Natural orbital decay with random reentry (as with \"Skylab\"), boosting the station to a higher altitude (which would delay reentry), and a controlled targeted de-orbit to a remote ocean area were considered as ISS disposal options. As of late 2010, the preferred plan is to use a slightly modified Progress spacecraft to de-orbit the ISS. This plan was seen as the simplest, cheapest and with the highest margin.\n\nThe Orbital Piloted Assembly and Experiment Complex (OPSEK) was previously intended to be constructed of modules from the Russian Orbital Segment after the ISS is decommissioned. The modules under consideration for removal from the current ISS included the Multipurpose Laboratory Module (\"Nauka\"), planned to be launched in December 2020 , and the other new Russian modules that are proposed to be attached to \"Nauka\". These newly launched modules would still be well within their useful lives in 2020 or 2024.\n\nAt the end of 2011, the Exploration Gateway Platform concept also proposed using leftover USOS hardware and \"Zvezda 2\" as a refuelling depot and service station located at one of the Earth-Moon Lagrange points. However, the entire USOS was not designed for disassembly and will be discarded.\n\nIn February 2015, Roscosmos announced that it would remain a part of the ISS programme until 2024. Nine months earlier—in response to US sanctions against Russia over the annexation of Crimea—Russian Deputy Prime Minister Dmitry Rogozin had stated that Russia would reject a US request to prolong the orbiting station's use beyond 2020, and would only supply rocket engines to the US for non-military satellite launches.\n\nOn 28 March 2015, Russian sources announced that Roscosmos and NASA had agreed to collaborate on the development of a replacement for the current ISS. Igor Komarov, the head of Russia's Roscosmos, made the announcement with NASA administrator Charles Bolden at his side. In a statement provided to SpaceNews on 28 March, NASA spokesman David Weaver said the agency appreciated the Russian commitment to extending the ISS, but did not confirm any plans for a future space station.\n\nOn 30 September 2015, Boeing's contract with NASA as prime contractor for the ISS was extended to 30 September 2020. Part of Boeing's services under the contract will relate to extending the station's primary structural hardware past 2020 to the end of 2028.\n\nRegarding extending the ISS, on 15 November 2016 General Director Vladimir Solntsev of RSC Energia stated \"Maybe the ISS will receive continued resources. Today we discussed the possibility of using the station until 2028,\" with discussion to continue under the new presidential administration. There have also been suggestions that the station could be converted to commercial operations after it is retired by government entities.\n\nIn July 2018, the Space Frontier Act of 2018 was intended to extend operations of the ISS to 2030. This bill was unanimously approved in the Senate, but failed to pass in the U.S. House. In September 2018, the Leading Human Spaceflight Act was introduced with the intent to extend operations of the ISS to 2030, and was confirmed in December 2018.\n\nThe ISS has been described as the most expensive single item ever constructed. In 2010 the cost was expected to be $150 billion. This includes NASA's budget of $58.7 billion (inflation-unadjusted) for the station from 1985 to 2015 ($72.4 billion in 2010 dollars), Russia's $12 billion, Europe's $5 billion, Japan's $5 billion, Canada's $2 billion, and the cost of 36 shuttle flights to build the station; estimated at $1.4 billion each, or $50.4 billion in total. Assuming 20,000 person-days of use from 2000 to 2015 by two- to six-person crews, each person-day would cost $7.5 million, less than half the inflation-adjusted $19.6 million ($5.5 million before inflation) per person-day of \"Skylab\".\n\nBULLET::::- Participating countries\nThe ISS is visible to the naked eye as a slow-moving, bright white dot because of reflected sunlight, and can be seen in the hours after sunset and before sunrise, when the station remains sunlit but the ground and sky are dark. The ISS takes about 10 minutes to pass from one horizon to another, and will only be visible part of that time because of moving into or out of the Earth's shadow. Because of the size of its reflective surface area, the ISS is the brightest artificial object in the sky (excluding other satellite flares), with an approximate maximum magnitude of −4 when overhead (similar to Venus). The ISS, like many satellites including the Iridium constellation, can also produce flares of up to 8 or 16 times the brightness of Venus as sunlight glints off reflective surfaces. The ISS is also visible in broad daylight, albeit with a great deal more difficulty.\n\nTools are provided by a number of websites such as Heavens-Above (see \"Live viewing\" below) as well as smartphone applications that use orbital data and the observer's longitude and latitude to indicate when the ISS will be visible (weather permitting), where the station will appear to rise, the altitude above the horizon it will reach and the duration of the pass before the station disappears either by setting below the horizon or entering into Earth's shadow.\n\nIn November 2012 NASA launched its \"Spot the Station\" service, which sends people text and email alerts when the station is due to fly above their town. The station is visible from 95% of the inhabited land on Earth, but is not visible from extreme northern or southern latitudes.\n\nUsing a telescope-mounted camera to photograph the station is a popular hobby for astronomers, while using a mounted camera to photograph the Earth and stars is a popular hobby for crew. The use of a telescope or binoculars allows viewing of the ISS during daylight hours.\n\nSome amateur astronomers also use telescopic lenses to photograph the ISS while it transits the Sun, sometimes doing so during an eclipse (and so the Sun, Moon, and ISS are all positioned approximately in a single line). One example is during the 21 August solar eclipse, where at one location in Wyoming, images of the ISS were captured during the eclipse. Similar images were captured by NASA from a location in Washington.\n\nParisian engineer and astrophotographer Thierry Legault, known for his photos of spaceships transiting the Sun, travelled to Oman in 2011 to photograph the Sun, Moon and space station all lined up. Legault, who received the Marius Jacquemetton award from the Société astronomique de France in 1999, and other hobbyists, use websites that predict when the ISS will transit the Sun or Moon and from what location those passes will be visible.\n\nBULLET::::- \"A Beautiful Planet\" – 2016 IMAX documentary film showing scenes of Earth, as well as astronaut life aboard the ISS\nBULLET::::- Center for the Advancement of Science in Space – operates the US National Laboratory on the ISS\nBULLET::::- List of space stations\nBULLET::::- Origins of the International Space Station\nBULLET::::- List of spacecraft deployed from the International Space Station\nBULLET::::- Space architecture\nBULLET::::- \"Space Station 3D\" – 2002 Canadian documentary\n\n\nBULLET::::- ISS Location\n\nBULLET::::- Canadian Space Agency\nBULLET::::- European Space Agency\nBULLET::::- Centre national d'études spatiales (National Centre for Space Studies)\nBULLET::::- German Aerospace Center\nBULLET::::- Italian Space Agency\nBULLET::::- Japan Aerospace Exploration Agency\nBULLET::::- S.P. Korolev Rocket and Space Corporation Energia\nBULLET::::- Russian Federal Space Agency\nBULLET::::- National Aeronautics and Space Administration\n\nBULLET::::- NASA: Daily ISS Reports\nBULLET::::- NASA: Station Science\nBULLET::::- ESA: \"Columbus\"\nBULLET::::- RSC Energia: Science Research on ISS Russian Segment\n\nBULLET::::- Live ISS webcam by NASA at uStream.tv\nBULLET::::- Live HD ISS webcams by NASA HDEV at uStream.tv\nBULLET::::- Sighting opportunities at NASA.gov\nBULLET::::- Real-time position at Heavens-above.com\nBULLET::::- Real-time position at N2YO.com\n\nBULLET::::- Johnson Space Center image gallery at Flickr.com\nBULLET::::- ISS tour with Sunita Williams by NASA at YouTube.com\nBULLET::::- Journey to the ISS by ESA at YouTube.com\nBULLET::::- \"The Future of Hope\", \"Kibō\" module documentary by JAXA at YouTube.com\nBULLET::::- Seán Doran's compiled videos of orbital photography from the ISS: \"Orbit - Remastered\", \"Orbit: Uncut\"; \"The Four Seasons\" (see Flickr album for more)\n"}
{"id": "15044", "url": "https://en.wikipedia.org/wiki?curid=15044", "title": "Irish", "text": "Irish\n\nIrish most commonly refers to:\n\nBULLET::::- Someone or something of, from, or related to:\nBULLET::::- Ireland, an island situated off the north-western coast of continental Europe\nBULLET::::- Northern Ireland, a constituent unit of the United Kingdom of Great Britain and Northern Ireland\nBULLET::::- Republic of Ireland, a sovereign state\nBULLET::::- Irish language, a Celtic Goidelic language of the Indo-European language family spoken in Ireland\nBULLET::::- Irish people, people of Irish ethnicity, people born in Ireland and people who hold Irish citizenship\nIrish may also refer to:\nBULLET::::- Irish Creek (Kansas), a stream in Kansas\nBULLET::::- Irish Creek (South Dakota), a stream in South Dakota\nBULLET::::- Irish Lake, Watonwan County, Minnesota\nBULLET::::- Irish Sea, the body of water which separates the islands of Ireland and Great Britain\n\nBULLET::::- Irish (Junior Cert), a subject of the Junior Cycle examination in secondary schools in the Republic of Ireland\nBULLET::::- Irish (name), a given name or family name\nBULLET::::- Irish lace\nBULLET::::- Irish nationality law, law determining who can become an Irish citizen\nBULLET::::- Irish whiskey, a beverage originating in Ireland\nBULLET::::- Lace curtain and shanty Irish, terms that were commonly used in the 19th and 20th centuries to categorize Irish people by social class\n\nBULLET::::- List of Ireland-related topics\nBULLET::::- The Luck of the Irish (disambiguation)\n"}
{"id": "15045", "url": "https://en.wikipedia.org/wiki?curid=15045", "title": "Cosmicomics", "text": "Cosmicomics\n\nCosmicomics () is a collection of twelve short stories by Italo Calvino first published in Italian in 1965 and in English in 1968. The stories were originally published between 1964 and 1965 in the Italian periodicals \"Il Caffè\" and \"Il Giorno\". Each story takes a scientific \"fact\" (though sometimes a falsehood by today's understanding), and builds an imaginative story around it. An always-extant being called Qfwfq narrates all of the stories save two, each of which is a memory of an event in the history of the universe. Qfwfq also narrates some stories in Calvino's \"t zero\".\n\nAll of the stories in \"Cosmicomics\", together with those from \"t zero\" and other sources, are now available in a single volume collection, \"The Complete Cosmicomics\" (Penguin UK, 2009).\n\nThe first U.S. edition, translated by William Weaver, won the National Book Award in the Translation category.\n\nBULLET::::- \"The Distance of the Moon\", the first and probably the best known story. Calvino takes the fact that the Moon used to be much closer to the Earth, and builds a story about a love triangle among people who used to jump between the Earth and the Moon, in which lovers drift apart as the Moon recedes.\nBULLET::::- \"At Daybreak\" — Life before matter condenses.\nBULLET::::- \"A Sign in Space\" — The idea that the galaxy slowly revolves becomes a story about a being who is desperate to leave behind some unique sign of his existence. This story also is a direct illustration of one of the tenets of postmodern theory — that the sign is not the thing it signifies, nor can one claim to fully or properly describe a thing or an idea with a word or other symbol.\nBULLET::::- \"All at One Point\" — The fact that all matter and creation used to exist in a single point. \"Naturally, we were all there—old Qfwfq said—where else could we have been? Nobody knew then that there could be space. Or time either: what use did we have for time, packed in there like sardines?\"\nBULLET::::- \"Without Colors\" — Before there was an atmosphere, everything was the same shade of gray. As the atmosphere appears, so do colors. The novelty scares off Ayl, Qfwfq's love interest.\nBULLET::::- \"Games Without End\" — A galactic game of marbles back before the universe had formed much more than particles.\nBULLET::::- \"The Aquatic Uncle\" — A tale on the fact that at one stage in evolution animals left the sea and came to live on land. The story is about a family living on land that is a bit ashamed of their old uncle who still lives in the sea, refusing to come ashore like \"civilized\" people.\nBULLET::::- \"How Much Shall We Bet\" — A story about betting on the long term evolution of mankind.\nBULLET::::- \"The Dinosaurs\" — How some dinosaurs lived after most of them had become extinct, and how it felt to be that last existing dinosaur in an age where all the current mammals feared his kind as demons.\nBULLET::::- \"The Form of Space\" — As the unnamed narrator \"falls\" through space, he cannot help but notice that his trajectory is parallel to that of a beautiful woman, Ursula H'x, and that of lieutenant Fenimore, who is also in love with Ursula. The narrator dreams of the shape of space changing, so that he may touch Ursula (or fight with Fenimore).\nBULLET::::- \"The Light Years\" — The unnamed narrator looking at other galaxies, and spotting one with a sign pointed right at him saying \"I saw you.\" Given that there's a gulf of 100,000,000 light years, he checks his diary to find out what he had been doing that day, and finds out that it was something he wished to hide. Then he starts to worry.\nBULLET::::- \"The Spiral\" — A story about life as a mollusc, and the nature of love and writing.\n\nAll of the stories feature non-human characters which have been heavily anthropomorphized.\n\nBULLET::::- Italo Calvino’s Science Fiction Masterpiece, essay on Cosmicomics at The Millions, 25 July 2014\n"}
{"id": "15046", "url": "https://en.wikipedia.org/wiki?curid=15046", "title": "IA-32", "text": "IA-32\n\nIA-32 (short for \"Intel Architecture, 32-bit\", sometimes also called i386) is the 32-bit version of the x86 instruction set architecture, designed by Intel and first implemented in the 80386 microprocessor in 1985. IA-32 is the first incarnation of x86 that supports 32-bit computing; as a result, the \"IA-32\" term may be used as a metonym to refer to all x86 versions that support 32-bit computing.\n\nWithin various programming language directives, IA-32 is still sometimes referred to as the \"i386\" architecture. In some other contexts, certain iterations of the IA-32 ISA are sometimes labelled i486, i586 and i686, referring to the instruction supersets offered by the 80486, the P5 and the P6 microarchitectures respectively. These updates offered numerous additions alongside the base IA-32 set, i.e. floating-point capabilities and the MMX extensions.\n\nIntel was historically the largest manufacturer of IA-32 processors, with the second biggest supplier having been AMD. During the 1990s, VIA, Transmeta and other chip manufacturers also produced IA-32 compatible processors (e.g. WinChip). In the modern era, Intel still produces IA-32 processors under the Intel Quark microcontroller platform, however, since the 2000s, the majority of manufacturers (Intel included) moved almost exclusively to implementing CPUs based on the 64-bit variant of x86, x86-64. x86-64, by specification, offers legacy operating modes that operate on the IA-32 ISA for backwards compatibility. Even given the contemporary prevalence of x86-64, as of 2018, IA-32 protected mode versions of many modern operating systems are still maintained, e.g. Microsoft Windows and the Ubuntu Linux distribution. In spite of IA-32's name (and causing some potential confusion), the 64-bit evolution of x86 that originated out of AMD would not be known as \"IA-64\", that name instead belonging to Intel's Itanium architecture.\n\nThe primary defining characteristic of IA-32 is the availability of 32-bit general-purpose processor registers (for example, EAX and EBX), 32-bit integer arithmetic and logical operations, 32-bit offsets within a segment in protected mode, and the translation of segmented addresses to 32-bit linear addresses. The designers took the opportunity to make other improvements as well. Some of the most significant changes are described below.\n\nBULLET::::- 32-bit integer capability\nBULLET::::- More general addressing modes\nBULLET::::- Additional segment registers\nBULLET::::- Larger virtual address space\nBULLET::::- Demand paging\n\n! Operating mode\n! Operating system required\n! Type of code being run\n! Default address size\n! Default operand size\n! Typical GPR width\n\nBULLET::::- IA-64\nBULLET::::- List of former IA-32 compatible processor manufacturers\n"}
{"id": "15047", "url": "https://en.wikipedia.org/wiki?curid=15047", "title": "Internalism and externalism", "text": "Internalism and externalism\n\nInternalism and externalism are two opposing ways of explaining various subjects in several areas of philosophy. These include human motivation, knowledge, justification, meaning, and truth. The distinction arises in many areas of debate with similar but distinct meanings.\n\nInternalism is the thesis that no fact about the world can provide reasons for action independently of desires and beliefs. Externalism is the thesis that reasons are to be identified with objective features of the world.\n\nIn contemporary moral philosophy, motivational internalism (or moral internalism) is the view that moral convictions (which are not necessarily beliefs, e.g. feelings of moral approval or disapproval) are intrinsically motivating. That is, the motivational internalist believes that there is an internal, necessary connection between one's conviction that X ought to be done and one's motivation to do X. Conversely, the motivational externalist (or moral externalist) claims that there is no necessary internal connection between moral convictions and moral motives. That is, there is no necessary connection between the conviction that X is wrong and the motivational drive not to do X. (The use of these terms has roots in W.D. Falk's (1947) paper \"'Ought' and Motivation\").\n\nThese views in moral psychology have various implications. In particular, if motivational internalism is true, then an amoralist is unintelligible (and metaphysically impossible). An amoralist is not simply someone who is immoral, rather it is someone who knows what the moral things to do are, yet is not motivated to do them. Such an agent is unintelligible to the motivational internalist, because moral judgments about the right thing to do have built into them corresponding motivations to do those things that are judged by the agent to be the moral things to do. On the other hand, an amoralist is entirely intelligible to the motivational \"externalist\", because the motivational externalist thinks that moral judgments about the right thing to do not necessitate some motivation to do those things that are judged to be the right thing to do; rather, an independent desire—such as the desire to do the right thing—is required (Brink, 2003), (Rosati, 2006).\n\nThere is also a distinction in ethics and action theory, largely made popular by Bernard Williams (1979, reprinted in 1981), concerning internal and external reasons for action. An \"internal reason\" is, roughly, something that one has in light of one's own \"subjective motivational set\"—one's own commitments, desires (or wants), goals, etc. On the other hand, an \"external reason\" is something that one has independent of one's subjective motivational set. For example, suppose that Sally is going to drink a glass of poison, because she wants to commit suicide and believes that she can do so by drinking the poison. Sally has an internal reason to drink the poison, because she wants to commit suicide. However, one might say that she has an external reason not to drink the poison because, even though she wants to die, one ought not kill oneself no matter what—regardless of whether one wants to die.\n\nSome philosophers embrace the existence of both kinds of reason, while others deny the existence of one or the other. For example, Bernard Williams (1981) argues that there are really only internal reasons for action. Such a view is called \"internalism about reasons\" (or \"reasons internalism\"). \"Externalism about reasons\" (or \"reasons externalism\") is the denial of reasons internalism. It is the view that there are external reasons for action; that is, there are reasons for action that one can have even if the action is not part of one's subjective motivational set.\n\nConsider the following situation. Suppose that it's against the moral law to steal from the poor, and Sasha knows this. However, Sasha doesn't desire to follow the moral law, and there is currently a poor person next to him. Is it intelligible to say that Sasha has a reason to follow the moral law right now (to not steal from the poor person next to him), even though he doesn't care to do so? The reasons externalist answers in the affirmative (\"Yes, Sasha has a reason not to steal from that poor person.\"), since he believes that one can have reasons for action even if one does not have the relevant desire. Conversely, the reasons internalist answers the question in the negative (\"No, Sasha does not have a reason not to steal from that poor person, though others might.\"). The reasons internalist claims that external reasons are unintelligible; one has a reason for action only if one has the relevant desire (that is, only internal reasons can be reasons for action). The reasons internalist claims the following: the moral facts are a reason \"for Sasha's action\" not to steal from the poor person next to him only if he currently \"wants\" to follow the moral law (or if not stealing from the poor person is a way to satisfy his other current goals—that is, part of what Williams calls his \"subjective motivational set\"). In short, the reasoning behind reasons internalism, according to Williams, is that reasons for action must be able to explain one's action; and only internal reasons can do this.\n\nGenerally speaking, internalist conceptions of epistemic justification require that one's justification for a belief be internal to the believer in some way. Two main varieties of epistemic internalism about justification are access internalism and ontological internalism. Access internalists require that a believer must have internal access to the justifier(s) of her belief \"p\" in order to be justified in believing \"p\". For the access internalist, justification amounts to something like the believer being aware (or capable of being aware) of certain facts that make her belief in \"p\" rational, or her being able to give reasons for her belief in \"p\". At minimum, access internalism requires that the believer have some kind of reflective access or awareness to whatever justifies her belief. Ontological internalism is the view that justification for a belief is established by one's mental states. Ontological internalism can be distinct from access internalism, but the two are often thought to go together since we are generally considered to be capable of having reflective access to mental states.\n\nOne popular argument for internalism is known as the 'new evil demon problem'. The new evil demon problem indirectly supports internalism by challenging externalist views of justification, particularly reliabilism. The argument asks us to imagine a subject with beliefs and experiences identical to ours, but the subject is being systematically deceived by a malicious Cartesian demon so that all their beliefs turn out false. In spite of the subject's unfortunate deception, the argument goes, we do not think this subject ceases to be rational in taking things to be as they appear as we do. After all, it is possible that we could be radically deceived in the same way, yet we are still justified in holding most of our beliefs in spite of this possibility. Since reliabilism maintains that one's beliefs are justified via reliable belief-forming processes (where reliable means yielding true beliefs), the subject in the evil demon scenario would not likely have any justified beliefs according to reliabilism because all of their beliefs would be false. Since this result is supposed to clash with our intuitions that the subject is justified in their beliefs in spite of being systematically deceived, some take the new evil demon problem as a reason for rejecting externalist views of justification.\n\nExternalist views of justification emerged in epistemology during the late 20th century. Externalist conceptions of justification assert that facts external to the believer can serve as the justification for a belief. According to the externalist, a believer need not have any internal access or cognitive grasp of any reasons or facts which make her belief justified. The externalist's assessment of justification can be contrasted with access internalism, which demands that the believer have internal reflective access to reasons or facts which corroborate their belief in order to be justified in holding it. Externalism, on the other hand, maintains that the justification for someone's belief can come from facts that are entirely external to the agent's subjective awareness.\n\nAlvin Goldman, one of the most well-known proponents of externalism in epistemology, is known for developing a popular form of externalism called reliabilism. In his paper, “What is Justified Belief?” Goldman characterizes the reliabilist conception of justification as such:\n\n\"If S’s believing \"p\" at \"t\" results from a reliable cognitive belief-forming process (or set of processes), then S’s belief in \"p\" at \"t\" is justified.”\n\nGoldman notes that a reliable belief-forming process is one which generally produces true beliefs.\n\nA unique consequence of reliabilism (and other forms of externalism) is that one can have a justified belief without knowing one is justified (this is not possible under most forms of epistemic internalism). In addition, we do not yet know which cognitive processes are in fact reliable, so anyone who embraces reliabilism must concede that we do not always know whether some of our beliefs are justified (even though there is a fact of the matter).\n\nIn responding to skepticism, Hilary Putnam (1982) claims that semantic externalism yields \"an argument we can give that shows we are not brains in a vat (BIV). (See also DeRose, 1999.) If semantic externalism is true, then the meaning of a word or sentence is not wholly determined by what individuals think those words mean. For example, semantic externalists maintain that the word \"water\" referred to the substance whose chemical composition is HO even before scientists had discovered that chemical composition. The fact that the substance out in the world we were calling \"water\" actually had that composition at least partially determined the meaning of the word. One way to use this in a response to skepticism is to apply the same strategy to the terms used in a skeptical argument in the following way (DeRose, 1999):\n\nTo clarify how this argument is supposed to work: Imagine that there is brain in a vat, and a whole world is being simulated for it. Call the individual who is being deceived \"Steve.\" When Steve is given an experience of walking through a park, semantic externalism allows for his thought, \"I am walking through a park\" to be true so long as the simulated reality is one in which he is walking through a park. Similarly, what it takes for his thought, \"I am a brain in a vat,\" to be true is for the simulated reality to be one where he is a brain in a vat. But in the simulated reality, he is not a brain in a vat.\n\nApart from disputes over the success of the argument or the plausibility of the specific type of semantic externalism required for it to work, there is question as to what is gained by defeating the skeptical worry with this strategy. Skeptics can give new skeptical cases that wouldn't be subject to the same response (e.g., one where the person was very recently turned into a brain in a vat, so that their words \"brain\" and \"vat\" still pick out real brains and vats, rather than simulated ones). Further, if even brains in vats can correctly believe \"I am not a brain in a vat,\" then the skeptic can still press us on how we know we are not in that situation (though the externalist will point out that it may be difficult for the skeptic to describe that situation).\n\nAnother attempt to use externalism to refute skepticism is done by Brueckner and Warfield. It involves the claim that our thoughts are \"about\" things, unlike a BIV's thoughts, which cannot be \"about\" things (DeRose, 1999).\n\nSemantic externalism comes in two varieties, depending on whether meaning is construed cognitively or linguistically. On a cognitive construal, externalism is the thesis that what concepts (or contents) are available to a thinker is determined by their environment, or their relation to their environment. On a linguistic construal, externalism is the thesis that the meaning of a word is environmentally determined. Likewise, one can construe semantic internalism in two ways, as a denial of either of these two theses.\n\nExternalism and internalism in semantics is closely tied to the distinction in philosophy of mind concerning mental content, since the contents of one's thoughts (specifically, intentional mental states) are usually taken to be semantic objects that are truth-evaluable.\n\nSee also:\nBULLET::::- Linguistic turn and cognitive turn for more about the two construals of meaning\nBULLET::::- Swamp man thought experiment\nBULLET::::- Twin Earth thought experiment\n\nWithin the context of the philosophy of mind, externalism is the theory that the contents of at least some of one's mental states are dependent in part on their relationship to the external world or one's environment.\n\nThe traditional discussion on externalism was centered around the semantic aspect of mental content. This is by no means the only meaning of externalism now. Externalism is now a broad collection of philosophical views considering all aspects of mental content and activity. There are various forms of externalism that consider either the content or the vehicles of the mind or both. Furthermore, externalism could be limited to cognition, or it could address broader issues of consciousness.\n\nAs to the traditional discussion on semantic externalism (often dubbed \"content externalism\"), some mental states, such as believing that water is wet, and fearing that the Queen has been insulted, have contents we can capture using 'that' clauses. The content externalist often appeal to observations found as early as Hilary Putnam's seminal essay, \"The Meaning of 'Meaning',\" (1975). Putnam stated that we can easily imagine pairs of individuals that are microphysical duplicates embedded in different surroundings who use the same words but mean different things when using them.\n\nFor example, suppose that Ike and Tina's mothers are identical twins and that Ike and Tina are raised in isolation from one another in indistinguishable environments. When Ike says, \"I want my mommy,\" he expresses a want satisfied only if he is brought to his mommy. If we brought Tina's mommy, Ike might not notice the difference, but he doesn't get what he wants. It seems that what he wants and what he says when he says, \"I want my mommy,\" will be different from what Tina wants and what she says she wants when she says, \"I want my mommy.\"\n\nExternalists say that if we assume competent speakers know what they think, and say what they think, the difference in what these two speakers mean corresponds to a difference in the thoughts of the two speakers that is not (necessarily) reflected by a difference in the internal make up of the speakers or thinkers. They urge us to move from externalism about meaning of the sort Putnam defended to externalism about contentful states of mind. The example pertains to singular terms, but has been extended to cover kind terms as well such as natural kinds (e.g., 'water') and for kinds of artifacts (e.g., 'espresso maker'). There is no general agreement amongst content externalists as to the scope of the thesis.\n\nPhilosophers now tend to distinguish between \"wide content\" (externalist mental content) and \"narrow content\" (anti-externalist mental content). Some, then, align themselves as endorsing one view of content exclusively, or both. For example, Jerry Fodor (1980) argues for narrow content (although he comes to reject that view in his 1995), while David Chalmers (2002) argues for a two dimensional semantics according to which the contents of mental states can have both wide and narrow content.\n\nCritics of the view have questioned the original thought experiments saying that the lessons that Putnam and later writers such as Tyler Burge (1979, 1982) have urged us to draw can be resisted. Frank Jackson and John Searle, for example, have defended internalist accounts of thought content according to which the contents of our thoughts are fixed by descriptions that pick out the individuals and kinds that our thoughts intuitively pertain to the sorts of things that we take them to. In the Ike/Tina example, one might agree that Ike's thoughts pertain to Ike's mother and that Tina's thoughts pertain to Tina's but insist that this is because Ike thinks of that woman as his mother and we can capture this by saying that he thinks of her as 'the mother of the speaker'. This descriptive phrase will pick out one unique woman. Externalists claim this is implausible, as we would have to ascribe to Ike knowledge he wouldn't need to successfully think about or refer to his mother.\n\nCritics have also claimed that content externalists are committed to epistemological absurdities. Suppose that a speaker can have the concept of water we do only if the speaker lives in a world that contains HO. It seems this speaker could know a priori that they think that water is wet. This is the thesis of privileged access. It also seems that they could know on the basis of simple thought experiments that they can only think that water is wet if they live in a world that contains water. What would prevent her from putting these together and coming to know a priori that the world contains water? If we should say that no one could possibly know whether water exists a priori, it seems either we cannot know content externalism to be true on the basis of thought experiments or we cannot know what we are thinking without first looking into the world to see what it is like.\n\nAs mentioned, content externalism (limited to the semantic aspects) is only one among many other options offered by externalism by and large.\n\nSee also:\nBULLET::::- Twin Earth thought experiment\nBULLET::::- Extended cognition\n\nInternalism in the historiography of science claims that science is completely distinct from social influences and pure natural science can exist in any society and at any time given the intellectual capacity. Imre Lakatos is a notable proponent of historiographical internalism.\n\nExternalism in the historiography of science is the view that the history of science is due to its social context – the socio-political climate and the surrounding economy determines scientific progress. Thomas Kuhn is a notable proponent of historiographical externalism.\n\nBULLET::::- Anti-psychologism\nBULLET::::- Dream argument\nBULLET::::- Emic and etic\nBULLET::::- Foundationalism\nBULLET::::- Relativism\nBULLET::::- Simulated reality\n\nBULLET::::- Brink, David (1989) \"Moral Realism and the Foundations of Ethics\", New York: Cambridge University Press, Ch. 3, pp. 37–80.\nBULLET::::- Brown, Curtis (2007) \"Narrow Mental Content\", \"The Stanford Encyclopedia of Philosophy\" (Spring 2007 Edition), Edward N. Zalta (ed.). (link)\nBULLET::::- Burge, Tyler (1979) \"Individualism and the Mental\", in French, Uehling, and Wettstein (eds.) \"Midwest Studies in Philosophy\" IV, Minneapolis: University of Minnesota Press, pp. 73–121.\nBULLET::::- Burge, Tyler (1982) \"Other Bodies\", in Woodfield, Andrew, ed., Thought and Object. New York: Oxford.\nBULLET::::- Chalmers, David (2002) \"The Components of Content\", in Chalmers (ed.) \"Philosophy of Mind: Classical and Contemporary Readings\", Oxford: Oxford University Press. Preprint available online\nBULLET::::- Cohen, Stewart (1984) \"Justification and Truth\", \"Philosophical Studies\" 46, pp. 279–296.\nBULLET::::- DeRose, Keith (1999) \"Responding to Skepticism\", \"Skepticism: A Contemporary Reader\".\nBULLET::::- Falk, W. D. (1947) \"'Ought' and Motivation\", \"Proceedings of the Aristotelian Society\", 48: 492–510\nBULLET::::- Finlay, Stephen & Schroeder, Mark (2008). \"Reasons for Action: Internal vs. External\". \"The Stanford Encyclopedia of Philosophy\", Edward N. Zalta (ed.). (link)\nBULLET::::- Fodor, Jerry (1980) \"Methodological Solipsism Considered as a Research Strategy in Cognitive Psychology\", \"Behavioral and Brain Sciences\" 3:1.\nBULLET::::- Fodor, Jerry (1995) \"The Elm and the Expert: Mentalese and its Semantics\", Cambridge: MIT Press.\nBULLET::::- Kornblith, Hilary (ed.) (2001) \"Epistemology: Internalism and Externalism\", Blackwell Press.\nBULLET::::- Lau, Joe (2004) \"Externalism About Mental Content\", \"The Stanford Encyclopedia of Philosophy\" (Fall 2004 Edition), Edward N. Zalta (ed.). (link)\nBULLET::::- Le Morvan, Pierre (2005) \"A Metaphilosophical Dilemma for Epistemic Externalism\", \"Metaphilosophy\" 36(5), pp. 688–707.\nBULLET::::- Pappas, George (2005) \"Internalist vs. Externalist Conceptions of Epistemic Justification\", \"The Stanford Encyclopedia of Philosophy\" (Spring 2005 Edition), Edward N. Zalta (ed.). (link)\nBULLET::::- Putnam, Hilary (1975) \"The Meaning of 'Meaning'\", in Keith Gunderson (ed.) \"Language, Mind and Knowledge\", Minneapolis: University of Minnesota Press, pp. 131–93 (reprinted in Putnam (1975), \"Mind, Language and Reality: Philosophical Papers Volume 2\", Cambridge: Cambridge University Press). (link)\nBULLET::::- Putnam, Hilary (1982) \"Brains in a Vat\", in \"Reason, Truth, and History\", Cambridge University Press. (link)\nBULLET::::- Rosati, Connie S. (2006). \"Moral Motivation\", \"The Stanford Encyclopedia of Philosophy\" Edward N. Zalta (ed.). (link)\nBULLET::::- Smith, Basil (2013). \"Internalism and Externalism in the Philosophy of Mind and Language,\" 'The Internet Encyclopedia of Philosophy,' P. Saka (ed.). (link)\nBULLET::::- Sosa, Ernest (1991) \"Reliabilism and Intellectual Virtue,\" in E. Sosa, \"Knowledge In Perspective\", Cambridge Press, pp. 131–145.\nBULLET::::- Williams, Bernard (1981) \"Internal and External Reasons\", in Williams's \"Moral Luck\", Cambridge: Cambridge University Press, pp. 101–13.\n\nBULLET::::- Internalism and Externalism – (draft) by Alberto Voltolini.\nBULLET::::- Internalist Explorations of Meaning reading group at Harvard University, autumn 2007.\n"}
{"id": "15048", "url": "https://en.wikipedia.org/wiki?curid=15048", "title": "Isolationism", "text": "Isolationism\n\nIsolationism is a category of foreign policies institutionalized by leaders who assert that nations' best interests are best served by keeping the affairs of other countries at a distance. One possible motivation for limiting international involvement is to avoid being drawn into dangerous and otherwise undesirable conflicts. There may also be a perceived benefit from avoiding international trade agreements or other mutual assistance pacts.\n\nIsolationism has been defined as:\n\nBefore 1999, Bhutan had banned television and the Internet in order to preserve its culture, environment, identity etc. Eventually, Jigme Singye Wangchuck lifted the ban on television and the Internet. His son, Jigme Khesar Namgyel Wangchuck, was elected as Druk Gyalpo of Bhutan, which helped forge the Bhutanese democracy. Bhutan has subsequently undergone a transition from an absolute monarchy to a multi-party democracy. The development of \"Bhutanese democracy\" has been marked by the active encouragement and participation of reigning Bhutanese monarchs since the 1950s, beginning with legal reforms such as the abolition of slavery, and culminating in the enactment of Bhutan's Constitution \n\nAfter Zheng He's voyages in the 15th century, the foreign policy of the Ming dynasty in China became increasingly isolationist. The Hongwu Emperor was the first to propose the policy to ban all maritime shipping in 1390. The Qing dynasty that came after the Ming dynasty often continued the Ming dynasty's isolationist policies. Wokou, which literally translates to \"Japanese pirates\" or \"dwarf pirates\", were pirates who raided the coastlines of China, Japan, and Korea, and were one of the key primary concerns, although the maritime ban was not without some control.\n\nFrom 1641 to 1853, the Tokugawa shogunate of Japan enforced a policy which it called \"kaikin\". The policy prohibited foreign contact with most outside countries. The commonly held idea that Japan was entirely closed, however, is misleading. In fact, Japan maintained limited-scale trade and diplomatic relations with China, Korea and Ryukyu Islands, as well as the Dutch Republic as the only Western trading partner of Japan for much of the period.\n\nThe culture of Japan developed with limited influence from the outside world and had one of the longest stretches of peace in history. During this period, Japan developed thriving cities, castle towns, increasing commodification of agriculture and domestic trade, wage labor, increasing literacy and concomitant print culture, laying the groundwork for modernization even as the shogunate itself grew weak.\n\nIn 1863, Emperor Gojong took the throne of the Joseon Dynasty when he was a child. His father, Regent Heungseon Daewongun, ruled for him until Gojong reached adulthood. During the mid-1860s he was the main proponent of isolationism and the principal instrument of the persecution of both native and foreign Catholics.\n\nFollowing the division of the peninsula after independence from Japan in 1945–48, Kim il-Sung inaugurated an isolationist totalitarian regime in the North, which has been continued by his son and grandson to the present day. North Korea is often referred to as \"The Hermit Kingdom\".\n\nJust after independence was achieved, Paraguay was governed from 1814 by the dictator José Gaspar Rodríguez de Francia, who closed the country's borders and prohibited trade or any relation with the outside world until his death in 1840. The Spanish settlers who had arrived just before independence had to intermarry with either the old colonists or with the native Guarani, in order to create a single Paraguayan people.\n\nFrancia had a particular dislike of foreigners and any who came to Paraguay during his rule (which would have been very difficult) were not allowed to leave for the rest of their lives. An independent character, he hated European influences and the Catholic Church, turning church courtyards into artillery parks and confession boxes into border sentry posts, in an attempt to keep foreigners at bay.\n\nWhile some scholars, such as Robert J. Art, believe that the United States has an isolationist history, other scholars dispute this by describing the United States as following a strategy of unilateralism or non-interventionism instead. Robert Art makes his argument in \"A Grand Strategy for America\" (2003). Books that have made the argument that the United States followed unilaterism instead of isolationism include Walter A. McDougall's \"Promised Land, Crusader State\" (1997), John Lewis Gaddis's \"Surprise, Security, and the American Experience\" (2004), and Bradley F. Podliska's \"Acting Alone\" (2010). Both sides claim policy prescriptions from George Washington's Farewell Address as evidence for their argument. Bear F. Braumoeller argues that even the best case for isolationism, the United States in the interwar period, has been widely misunderstood and that Americans proved willing to fight as soon as they believed a genuine threat existed.\nEvents during and after the Revolution related to the treaty of alliance with France, as well as difficulties arising over the neutrality policy pursued during the French revolutionary wars and the Napoleonic wars, encouraged another perspective. A desire for separateness and unilateral freedom of action merged with national pride and a sense of continental safety to foster the policy of isolation. Although the United States maintained diplomatic relations and economic contacts abroad, it sought to restrict these as narrowly as possible in order to retain its independence. The Department of State continually rejected proposals for joint cooperation, a policy made explicit in the Monroe Doctrine's emphasis on unilateral action. Not until 1863 did an American delegate attend an international conference.\n\nBULLET::::- Autarky\nBULLET::::- Civil society\nBULLET::::- Country neutrality\nBULLET::::- Economic nationalism\nBULLET::::- Global issue\nBULLET::::- Globalism\nBULLET::::- Globalization\nBULLET::::- Imperium\nBULLET::::- Interconnectivity\nBULLET::::- International isolation\nBULLET::::- Monroe Doctrine\nBULLET::::- Non-interventionism\nBULLET::::- Sakoku\nBULLET::::- Splendid isolation\nBULLET::::- United States non-interventionism\nBULLET::::- Unilateralism in the United States\nBULLET::::- Sovereignty\nBULLET::::- Barry, Tom. \"A Global Affairs Commentary: The Terms of Power,\" \"Foreign Policy in Focus\", November 6, 2002, University Press.\nBULLET::::- Berry, Mary Elizabeth. (2006). \"Japan in Print: Information and Nation in the Early Modern Period.\" Berkeley: University of California Press. ;\nBULLET::::- Chalberg, John C. (1995). \"Isolationism: Opposing Viewpoints.\" San Diego: Greenhaven Press. ;\nBULLET::::- Craig, Albert. (1961). \"Chōshū in the Meiji Restoration.\" Cambridge: Harvard University Press. ;\nBULLET::::- Glahn, Richard Von. (1996). \"Fountain of Fortune: Money and Monetary Policy in China, 1000–1700.\" Berkeley: University of California Press. ;\nBULLET::::- Graebner, Norman A. (1956). \"The New Isolationism; a Study in Politics and Foreign Policy Since 1950.\" New York: Ronald Press.\nBULLET::::- Jansen, Marius B. (1961). \"Sakamoto Ryoma and the Meiji Restoration.\" Princeton: Princeton University Press.\nBULLET::::- Nichols, Christopher McKnight (2011). \"Promise and Peril: America at the Dawn of a Global Age.\" Cambridge, Massachusetts: Harvard University Press, 2011.\nBULLET::::- Nordlinger, Eric A. (1995). \"Isolationism Reconfigured: American Foreign Policy for a New Century.\" Princeton: Princeton University Press. ;\nBULLET::::- Smith, Thomas C. (1959). \"The Agrarian Origins of Modern Japan.\" Stanford: Stanford University Press.\nBULLET::::- Sullivan, Michael P. \"Isolationism.\" World Book Deluxe 2001. CD-ROM.\nBULLET::::- Toby, Ronald P. (1984). \"State and Diplomacy in Early Modern Japan: Asia in the Development of the Tokugawa Bakufu.\" Princeton: Princeton University Press. ;\n\nBULLET::::- Washington, George \"Washington's Farewell Address 1796.\" \"Yale Law School Avalon Project, 2008\". Web. 12 Sept 2013.\n"}
{"id": "15049", "url": "https://en.wikipedia.org/wiki?curid=15049", "title": "Indianapolis Colts", "text": "Indianapolis Colts\n\nThe Indianapolis Colts are an American football team based in Indianapolis, Indiana. The Colts compete in the National Football League (NFL) as a member club of the league's American Football Conference (AFC) South division. Since the 2008 season, the Colts have played their games in Lucas Oil Stadium. Previously, the team had played for over two decades (1984–2007) at the RCA Dome. Since 1987, the Colts have been the host team for the NFL Scouting Combine.\n\nThe Colts have been a member club of the NFL since their founding in Baltimore in 1953. They were one of three NFL teams to join those of the American Football League (AFL) to form the AFC following the 1970 merger. While in Baltimore, the team advanced to the playoffs 10 times and won three NFL Championship games in 1958, 1959, and 1968. The Colts played in two Super Bowls while they were based in Baltimore, losing to the New York Jets in Super Bowl III and defeating the Dallas Cowboys in Super Bowl V. The Colts relocated to Indianapolis in 1984 and have since appeared in the playoffs 16 times, won two conference championships, and won one Super Bowl, in which they defeated the Chicago Bears in Super Bowl XLI.\n\nFollowing World War II, a competing professional football league was organized known as the All America Football Conference which began to play in the 1946 season. In its second year the franchise assigned to the Miami Seahawks was relocated to Maryland's major commercial and manufacturing city of Baltimore. After a fan contest the team was renamed the Baltimore Colts and used the team colors of silver and green. The Colts played for the next three seasons in the old AAFC. until they agreed to merge with the old National Football League (of 1920–1922 to 1950) when the NFL was reorganized. The Baltimore Colts were one of the three former AAFC powerhouse teams (known by the designation \"AAFC\" or \"1947–50\") to merge with the NFL at that time, the others being the San Francisco 49ers and the Cleveland Browns. This new Colts team, now in the \"big league\" of professional American football for the first time, although with shaky financing and ownership, played only in the 1950 season of the reorganized \"third\" NFL, and was later disbanded and moved.\n\nTwo years later, in 1953, a new Baltimore-based group, heavily supported by the City's municipal government and with a large subscription-base of fan-purchased season tickets, led by local owner Carroll Rosenbloom won the rights to a new Baltimore NFL franchise. Rosenbloom was awarded the remains of the former Dallas Texans team, who themselves had a long and winding history starting as the Boston Yanks in 1944, merging later with the Brooklyn Tigers, and who were previously known as the Dayton Triangles, one of the original old NFL teams established even before the League itself, in 1913. With the organization in 1920 of the original \"American Professional Football Conference\" [APFC], (soon renamed the \"American Professional Football Association\", [APF.]), then two years later in 1922, renamed a second time, now permanently as the \"National Football League\". That team later became the New York Yanks in 1950, and many of the players from the New York Yankees of the former competing All-America Football Conference (1946–49) were added to the team to begin playing in the newly merged League for the 1950 season. The Yanks then moved to Dallas in Texas after the 1951 season having competed for two seasons, but played their final two \"home\" games of the 1952 season as a so-called \"road team\" at the Rubber Bowl football stadium in Akron, Ohio. The NFL considers the Texans and Colts to be separate teams, although many of the earlier teams shared the same colors of blue and white. Thus, the Indianapolis Colts are legally considered to be a 1953 expansion team.\nThe third (and current) version of the Colts football team played their first season in Baltimore in 1953, where the team compiled a 3–9 record under first-year head coach Keith Molesworth. The franchise struggled during the first few years in Baltimore, with the team not achieving their first winning record until the 1957 season. However, under head coach Weeb Ewbank and the leadership of quarterback Johnny Unitas, the Colts went on to a 9–3 record during the 1958 season and reached the NFL Championship Game for the first time in their history by winning the NFL Western Conference. The Colts faced the New York Giants in the 1958 NFL Championship Game, which is considered to be among the greatest contests in professional football history. The Colts defeated the Giants 23–17 in the first game ever to utilize the overtime rule, a game seen by 45 million people.\n\nFollowing the Colts first NFL championship, the team posted a 9–3 record during the 1959 season and once again defeated the Giants in the NFL Championship Game to claim their second title in back to back fashion. Following the two championships in 1958 and 1959, the Colts did not return to the NFL Championship for four seasons and replaced the head coach Ewbank with the young Don Shula in 1963. In Shula's second season the Colts compiled a 12–2 record, but lost to the Cleveland Browns in the NFL Championship. However, in 1968 the Colts returned with the continued leadership of Unitas and Shula and went on to win the Colts' third NFL Championship and made an appearance in Super Bowl III.\nLeading up to the Super Bowl and following the 34–0 trouncing of the Cleveland Browns in the NFL Championship, many were calling the 1968 Colts team one of the \"greatest pro football teams of all time\" and were favored by 18 points against their counterparts from the American Football League, the New York Jets. The Colts, however, were stunned by the Jets, who won the game 16–7 in the first Super Bowl victory for the young AFL. The result of the game surprised many in the sports media as Joe Namath and Matt Snell led the Jets to the Super Bowl victory under head coach Weeb Ewbank, who had previously won two NFL Championships with the Colts.\n\nRosenbloom of the Colts, Art Modell of the Browns, and Art Rooney of the Pittsburgh Steelers agreed to have their teams join the ten AFL teams in the American Football Conference as part of the AFL–NFL merger in 1970. The Colts immediately went on a rampage in the new league, as new head coach Don McCafferty led the 1970 team to an 11–2–1 regular season record, winning the AFC East title. In the first round of the NFL Playoffs, the Colts beat the Cincinnati Bengals 17–0; one week later in the first ever AFC Championship Game, they beat the Oakland Raiders 27–17. Baltimore went on to win the first post-merger Super Bowl (Super Bowl V), defeating the National Football Conference's Dallas Cowboys 16–13 on a Jim O'Brien field goal with five seconds left to play. The victory gave the Colts their fourth NFL championship and first Super Bowl victory. Following the championship, the Colts returned to the playoffs in 1971 and defeated the Cleveland Browns in the first round, but lost to the Miami Dolphins in the AFC Championship.\n\nCiting friction with the City of Baltimore and the local press, Rosenbloom traded the Colts franchise to Robert Irsay on July 13, 1972 and received the Los Angeles Rams in return. Under the new ownership, the Colts did not reach the postseason for three consecutive seasons after 1971, and after the 1972 season, starting quarterback and legend Johnny Unitas was traded to the San Diego Chargers. Following Unitas' departure, the Colts made the playoffs three consecutive seasons from 1975 to 1977, losing in the divisional round each time. The Colts 1977 playoff loss in double overtime against the Oakland Raiders was famous for the fact that it was the last playoff game for the Colts in Baltimore and is also known for the Ghost to the Post play. These consecutive championship teams featured 1976 NFL Most Valuable Player Bert Jones at quarterback and an outstanding defensive line, nicknamed the \"Sack Pack.\"\n\nFollowing the 1970s success, the team endured nine consecutive losing seasons beginning in 1978. In 1981, the Colts defense allowed an NFL-record 533 points, set an all-time record for fewest sacks (13), and also set a modern record for fewest punt returns (12). The following year, the offense collapsed, including a game against the Buffalo Bills where the Colts' offense did not cross mid-field the entire game. The Colts finished 0–8–1 in the strike-shortened 1982 season, thereby earning the right to select Stanford quarterback John Elway with the first overall pick. Elway, however, refused to play for Baltimore, and using leverage as a draftee of the New York Yankees baseball club, forced a trade to Denver. Behind an improved defense the team finished 7–9 in 1983, but that would be their last season in Baltimore.\n\nThe Baltimore Colts played their final home game in Baltimore on December 18, 1983, against the then Houston Oilers. Irsay continued to request upgrades to Memorial Stadium or construction of a new stadium. As a result of the poor performance on the field and the stadium issues, fan attendance and team revenue continued to dwindle. City officials were precluded from using tax-payer funds for the building of a new stadium, and the modest proposals that were offered by the city were not acceptable to either the Colts or the city's MLB franchise the Orioles. However, all sides continued to negotiate. Relations between Irsay and the city of Baltimore deteriorated. Although Irsay assured fans that his ultimate desire was to stay in Baltimore, he nevertheless began discussions with several other cities willing to build new football stadiums, eventually narrowing the list of cities to two: Phoenix and Indianapolis. Under the administration of mayors Richard Lugar and then William Hudnut, Indianapolis had undertaken an ambitious effort to reinvent itself into a 'Great American City'. The Hoosier Dome, which was later renamed the RCA Dome, had been built specifically for, and was ready to host, an NFL expansion team.\n\nMeanwhile, in Baltimore, the situation worsened. The Maryland General Assembly intervened when a bill was introduced to give the city of Baltimore the right to seize ownership of the team by eminent domain. As a result, Irsay began serious negotiations with Indianapolis Mayor William Hudnut in order to move the team before the Maryland legislature could pass the law. Indianapolis offered loans as well as the Hoosier Dome and a training complex. After the deal was reached, moving vans from Indianapolis-based Mayflower Transit were dispatched overnight to the team's Maryland training complex, arriving on the morning of March 29, 1984. Once in Maryland, workers loaded all of the team's belongings, and by midday the trucks departed for Indianapolis, leaving nothing of the Colts organization that could be seized by Baltimore. The Baltimore Colts' Marching Band had to scramble to retrieve their equipment and uniforms before they were shipped to Indianapolis as well.\n\nThe move triggered a flurry of legal activity that ended when representatives of the city of Baltimore and the Colts organization reached a settlement in March 1986. Under the agreement, all lawsuits regarding the relocation were dismissed, and the Colts agreed to endorse a new NFL team for Baltimore.\n\nUpon the Colts' arrival in Indianapolis over 143,000 requests for season tickets were received in just two weeks. The move to Indianapolis, however, did not change the recent fortune of the Colts, with the team appearing in the postseason only once in the first eleven seasons in Indianapolis. During the 1984 season, the first in Indianapolis, the team went 4–12 and accounted for the lowest offensive yardage in the league that season. The 1985 and 1986 teams combined for only eight wins, including an 0–13 start in 1986 which prompted the firing of head coach Rod Dowhower, who was replaced by Ron Meyer. The Colts, however, did receive eventual Hall of Fame running back Eric Dickerson as a result of a trade during the 1987 season, and went on to compile a 9–6 record, thereby winning the AFC East and advancing to the postseason for the first time in Indianapolis; they lost that game to the Cleveland Browns.\n\nFollowing 1987, the Colts did not see any real success for quite some time, with the team missing the postseason for seven consecutive seasons. The struggles came to a climax in 1991 when the team went 1–15 and was just one point away from the first \"imperfect\" season in the history of a 16-game schedule. The season resulted in the firing of head coach Ron Meyer and the return of former head coach Ted Marchibroda to the organization in 1992; he had coached the team from 1975 to 1979. The team continued to struggle under Marchibroda and Jim Irsay, son of Robert Irsay and general manager at the time. It was in 1994 that Robert Irsay brought in Bill Tobin to become the general manager of the Indianapolis Colts.\n\nUnder Tobin, the Colts drafted running back Marshall Faulk with the second overall pick in the 1994 and acquired quarterback Jim Harbaugh as well. These moves along with others saw the Colts begin to turn their fortunes around with playoff appearances in 1995 and 1996. The Colts won their first postseason game as the Indianapolis Colts in 1995 and advanced to the AFC Championship Game against the Pittsburgh Steelers, coming just a Hail Mary pass reception away from a trip to Super Bowl XXX.\n\nMarchibroda retired following the 1995 season and was replaced by Lindy Infante in 1996. After two consecutive playoff appearances, the Colts regressed and went 3–13 during the 1997 season. Along with the disappointing season, the principal owner and man who moved the team to Indianapolis, Robert Irsay, died in January 1997 after years of declining health. Jim Irsay, Robert Irsay's son, entered the role of principal owner following his father's death and quickly began to change the organization. Irsay replaced general manager Tobin with Bill Polian in 1997 as the team decided to build through their number one overall pick in the 1998 draft.\n\nJim Irsay began to shape the Colts one year after assuming control from his father by firing head coach Lindy Infante and hiring Bill Polian as the general manager of the organization. Polian in turn hired Jim Mora to become the next head coach of the team and drafted Tennessee Volunteer quarterback Peyton Manning, the son of New Orleans Saints legend Archie Manning, with the first overall pick in the 1998 NFL Draft.\n\nThe team and Manning struggled during the 1998 season, winning only three games; Manning threw a league high 28 interceptions. However, Manning did pass for 3,739 yards and threw 26 touchdown passes and was named to the NFL All-Rookie First Team. The Colts began to improve towards the end of the 1998 season and showed continued growth in 1999. Indianapolis drafted Edgerrin James in 1999 and continued to improve their roster heading into the upcoming season. The Colts went 13–3 in 1999 and finished first in the AFC East, their first division title since 1987. Indianapolis lost to the eventual AFC champion Tennessee Titans in the divisional playoffs.\n\nThe 2000 and 2001 Colts teams were considerably less successful compared to the 1999 team, and pressure began to mount on team administration and the coaching staff following a 6–10 season in 2001. Head coach Jim Mora was fired at the end of the season and was replaced by former Tampa Bay Buccaneers head coach Tony Dungy. Dungy and the team quickly changed the atmosphere of the organization and returned to the playoffs in 2002 with a 10–6 record. The Colts also returned to the playoffs in 2003 and 2004 with 12–4 records and AFC South championships. The Colts lost to the New England Patriots and Tom Brady in the 2003 AFC Championship Game and in the 2004 divisional playoffs, thereby beginning a rivalry between the two teams, and between Manning and Brady. Following two consecutive playoff losses to the Patriots, the Colts began the 2005 season with a 13–0 record, including a regular season victory over the Patriots, the first in the Manning era. During the season Manning and Marvin Harrison broke the NFL record for touchdowns by a quarterback and receiver tandem. Indianapolis finished the 2005 season with a 14–2 record, the best record in the league that year and the best in a 16 games season for the franchise, but lost to the Pittsburgh Steelers in the divisional round, a disappointing end to the season.\n\nIndianapolis entered the 2006 season with a veteran quarterback, receivers, and defenders, and chose running back Joseph Addai in the 2006 draft. As in the previous season, the Colts began the season undefeated and went 9–0 before losing their first game against the Dallas Cowboys. Indianapolis finished the season with a 12–4 record and entered the playoffs for the fifth consecutive year, this time as the number three seed in the AFC. The Colts won their first two playoff games against the Kansas City Chiefs and the Baltimore Ravens to return to the AFC Championship Game for the first time since the 2003 playoffs, where they faced their rivals, the New England Patriots. In a classic game, the Colts overcame a 21–3 first half deficit to win the game 38–34 and earned a trip to Super Bowl XLI, the franchise's first Super Bowl appearance since 1970 and for the first as Indianapolis. The Colts faced the Chicago Bears in the Super Bowl, winning the game 29–17 and giving Manning, Polian, Irsay, and Dungy, as well as the city of Indianapolis, their first Super Bowl title.\n\nFollowing their Super Bowl championship, the Colts compiled a 13–3 record during the 2007 season; they lost to the San Diego Chargers in the divisional playoffs, in what was the final game the Colts played at the RCA Dome before moving into Lucas Oil Stadium in 2008. The 2008 season began with Manning being sidelined for most of the pre-season due to surgery. Indianapolis began the season with a 3–4 record, but then won nine consecutive games to end the season at 12–4 and make in into the playoffs as a wild card team, eventually losing to the Chargers in the wild card round. Following the season, Tony Dungy announced his retirement after seven seasons as head coach, having compiled an overall record of 92–33 with the team.<ref name=\"Dungy Retires/Caldwell Hired\"></ref>\n\nJim Caldwell was hired as head coach of the team following Dungy, and led the team during the 2009 season. The Colts went 14–0 during the season to finish with an overall record of 14–2 after controversially benching their starters during the last two games. The Colts for the second time in the Manning era entered the playoffs with the best record in the AFC. The Colts managed victories over the Baltimore Ravens and New York Jets to advance to Super Bowl XLIV against the New Orleans Saints, but lost to the Saints 31–17 to end the season in disappointment.\n\nAt the completion of the 2009 season, the Colts had finished the first decade of the 2000s (2000–2009) with the most regular season wins (115) and highest winning percentage (.719) of any team in the NFL during that span.\n\nThe 2010 team compiled a 10–6 record, the first time the Colts did not win 12 games since 2002, and lost to the New York Jets in the wild card round of the playoffs. The loss to the Jets was the last game for Peyton Manning as a Colt.\n\nAfter missing the preseason, Manning was ruled out for the Colts' opening game in Houston and eventually the entire 2011 season. Taking over as starter was veteran quarterback Kerry Collins, who had been signed to the team after dissatisfaction with backup quarterback Curtis Painter and Dan Orlovsky. However, even with a veteran quarterback, the Colts lost their first 13 games and finished the season with a 2–14 record, enough to receive the first overall pick in the 2012 draft. Immediately following the season, team president Bill Polian was fired, ending his 14-year tenure with the team. The change built the anticipation of the organization's decision regarding Manning's future with the team. The Peyton Manning era came to an end on March 8, 2012 when Jim Irsay announced that Manning was being released from the roster after 13 seasons.\n\nDuring the 2012 off-season owner Jim Irsay hired Ryan Grigson to be the General Manager. Grigson decided to let Head Coach Jim Caldwell go and Chuck Pagano was hired as the new Head Coach shortly thereafter. The Colts also began to release some higher paid and oft-injured veteran players, including Joseph Addai, Dallas Clark, and Gary Brackett. The Colts used their number one overall draft pick in 2012 to draft Stanford Cardinal quarterback Andrew Luck and also drafted his teammate Coby Fleener in the second round. The team also switched to a 3–4 defensive scheme.\n\nWith productive seasons from both Luck and veteran receiver Reggie Wayne, the Colts rebounded from the 2–14 season of 2011 with a 2012 season record of 11–5. The franchise, team, and fan base rallied behind Head Coach Chuck Pagano during his fight with leukemia. Clinching an unexpected playoff spot in the 2012–13 NFL playoffs, the 14th playoff berth for the club since 1995. The season ended in a 24–9 playoff loss to the eventual Super Bowl Champion Baltimore Ravens.\n\nTwo weeks into the 2013 season, the Colts traded their first round selection in the 2014 NFL Draft to the Cleveland Browns for running back Trent Richardson. In Week 7, Luck led the Colts to a 39–33 win over his predecessor, Peyton Manning, and the undefeated Broncos. Luck went on to lead the Colts to a 15th division championship later that season. In the first round of the 2013 NFL playoffs, Andrew Luck led the Colts to a 45–44 victory over Kansas City, outscoring the Chiefs 35–13 in the second half in the 2nd biggest comeback in NFL playoff history.\n\nDuring the 2014 season, Luck led the Colts to the AFC Championship game for the first time in his career after breaking the Colts' single season passing yardage record previously held by Manning.\n\nAfter finishing 8-8 in both the 2015 and 2016 seasons and missing the playoffs in back-to-back seasons for the first time since 1997-98, Grigson was fired as general manager. Just three of his previous 18 draft picks remained on the team at the time of his firing. On January 30, 2017 the team hired Chris Ballard, who served as the Kansas City Chiefs Director of Football Operations, to replace Grigson.\n\nOn December 31, 2017, after winning the final game of the season and a final record of 4-12, the Colts parted ways with Pagano.\n\nIn the weeks following, after two interviews, it was widely reported that the Colts would hire Josh McDaniels, offensive coordinator of the New England Patriots, to replace him, after McDaniels fulfilled his obligations to the Patriots in Super Bowl LII. On February 8, 2018, the Colts announced McDaniels as their new head coach. Hours later, however, McDaniels rescinded his decision to be the head coach, and he returned to the Patriots.\n\nOn February 11, 2018, the Colts announced Frank Reich, then offensive coordinator of the Philadelphia Eagles, as their new head coach.\n\nOn August 24, 2019, Luck informed the Colts that he would be retiring from the NFL after not attending training camp. On November 17, 2019, the Colts defeated the Jacksonville Jaguars for the team's 300th win in the Indianapolis era, with a record of 300-267.\n\nThe Colts' helmets in 1953 were white with a blue stripe. In 1954–55 they were blue with a white stripe and a pair of horseshoes at the rear of the helmet. For 1956, the colors were reversed, white helmet, blue stripe and horseshoes at the rear. In 1957 the horseshoes moved to their current location, one on each side of the helmet. The blue jerseys have white shoulder stripes and the white jerseys have blue stripes. The team also wears white pants with blue stripes down the sides. \n\nFor much of the team's history, the Colts wore blue socks, accenting them with two or three white stripes for much of their history in Baltimore and during the 2004 and 2005 seasons. From 1982 to 1987, the blue socks featured gray stripes. For a period lasting 1955 to 1958 and again from 1988 to 1992, the Colts wore white socks with either two or three blue stripes. \n\nFrom 1982 through 1986, the Colts wore gray pants with their blue jerseys. The gray pants featured a horseshoe on the top of the sides with the player's number inside the horseshoe. The Colts continued to wear white pants with their white jerseys throughout this period, and in 1987, the gray pants were retired. \n\nThe Colts wore blue pants with their white jerseys for the first three games of the 1995 season (pairing them with white socks), but then returned to white pants with both the blue and white jerseys. The team made some minor uniform adjustments before the start of the 2004 season, including reverting from blue to the traditional gray face masks, darkening their blue colors from a royal blue to speed blue, as well as adding two white stripes to the socks. In 2006, the stripes were removed from the socks.\n\nIn 2002, the Colts made a minor striping pattern change on their jerseys, having the stripes only on top of the shoulders then stop completely. Previously, the stripes used to go around to underneath the jersey sleeves. This was done because the Colts, like many other football teams, were beginning to manufacture the jerseys to be tighter to reduce holding calls and reduce the size of the sleeves. Although the white jerseys of the Minnesota Vikings at the time also had a similar striping pattern and continued as such (as well as the throwbacks the New England Patriots wore in the Thanksgiving game against the Detroit Lions in 2002, though the Patriots later wore the same throwbacks in 2009 with truncated stripes and in 2010 became their official alternate uniform), the Colts and most college teams with this striping pattern did not make this adjustment.\n\nIn 2017, the Colts brought back the blue pants but paired them with the blue jerseys as part of the NFL Color Rush program.\n\nAfter 24 years of playing at the RCA Dome, the Colts moved to their new home Lucas Oil Stadium in the fall of 2008. In December 2004, the City of Indianapolis and Jim Irsay agreed to a new stadium deal at an estimated cost of $1 billion (including the Indiana Convention Center upgrades). In a deal estimated at $122 million, Lucas Oil Products won the naming rights to the stadium for 20 years.\n\nLucas Oil Stadium is a seven-level stadium which seats 63,000 for football. It can be reconfigured to seat 70,000 or more for NCAA basketball and football and concerts. It covers . The stadium features a retractable roof allowing the Colts to play home games outdoors for the first time since arriving in Indianapolis. Using FieldTurf, the playing surface is roughly below ground level. In addition to being larger than the RCA Dome, the new stadium features: 58 permanent concession stands, 90 portable concession stands, 13 escalators, 11 passenger elevators, 800 restrooms, high definition video displays from Daktronics and replay monitors and 142 luxury suites. The stadium also features a retractable roof, with electrification technology developed by VAHLE, Inc. Other than being the home of the Colts, the stadium will host games in both the Men's and Women's NCAA Basketball Tournaments and will serve as the back up host for all NCAA Final Four Tournaments. The stadium hosted the Super Bowl for the 2011 season (Super Bowl XLVI) and has a potential economic impact estimated at $286 million. Lucas Oil Stadium has also hosted the Drum Corps International World Championships since 2009.\n\nAs a transplant from the AFC East into the AFC South upon the realignment of the NFL's divisions in , the Colts merely share loose rivalries with the other three teams in its division, namely the Houston Texans, Jacksonville Jaguars, and Tennessee Titans (formerly the Houston Oilers). They have dominated the AFC South for much of the division's history under quarterbacks Peyton Manning and Andrew Luck, but have faced competition for divisional supremacy in recent years from the Texans.\n\nThe rivalry between the Indianapolis Colts and New England Patriots is one of the NFL's newest rivalries. The rivalry is fueled by the quarterback comparison between Peyton Manning and Tom Brady. The Patriots owned the beginning of the series, defeating the Colts in six consecutive contests including the 2003 AFC Championship game and a 2004 AFC Divisional game. The Colts won the next three matches, notching two regular season victories and a win in the 2006 AFC Championship game on the way to their win in Super Bowl XLI. On November 4, 2007 the Patriots defeated the Colts 24–20; in the next matchup on November 2, 2008, the Colts won 18–15 in a game that was one of the reasons the Patriots failed to make the playoffs; in the 2009 meeting, the Colts staged a spirited comeback to beat the Patriots 35–34; in 2010 the Colts almost staged another comeback, pulling within 31–28 after trailing 31–14 in the fourth quarter, but fell short due to a Patriots interception of a Manning pass late in the game; it turned out to be Manning's final meeting against the Patriots as a member of the Colts. After a dismal 2011 season that included a 31–24 loss to the Patriots, the Colts drafted Andrew Luck and in November of 2012 the two teams met with identical 6–3 records; the Patriots erased a 14–7 gap to win 59–24. The nature of this rivalry is ironic because the Colts and Patriots were division rivals from 1970 to 2001, but it did not become prominent in league circles until after Indianapolis was relocated to the AFC South. On November 16, 2014, the New England Patriots traveled at 7–2 to play the 6–3 Colts at Lucas Oil Stadium. After a stellar four touchdown performance by New England running back Jonas Gray, the Patriots defeated the Colts 42–20. The Patriots followed up with a 45–7 defeat of the Colts in the 2014 AFC Championship Game.\n\nIn the years 1953–66, the Colts played in the NFL Western Conference (also known as division), but did not have significant rivalries with other franchises in that alignment, as they were the eastern-most team and the rest of the division included the Great Lakes franchises Green Bay, Detroit Lions, Chicago Bears, and after 1961, the Minnesota Vikings, along with the league's two West Coast teams in San Francisco and Los Angeles. The closest team to Baltimore was the Washington Redskins, but they were not in the same division and not very competitive during most years at that time.\n\nIn 1958, Baltimore played its first NFL Championship Game against the 10–3 New York Giants. The Giants qualified for the championship after a tie-breaking playoff against the Cleveland Browns. Having already been defeated by the Giants in the regular season, Baltimore was not favored to win, yet proceeded to take the title in sudden death overtime. The Colts then repeated the feat by posting an identical record and routing the Giants in the 1959 final. Up until the Colts' back-to-back titles, the Giants had been the premier club in the NFL, and continued to be post-season stalwarts the next decade, losing three straight finals. The situation was reversed by the end of the decade, with Baltimore winning the 1968 NFL title and New York compiling less impressive results. In recent years, the Colts and Giants featured brothers as their starting quarterbacks (Peyton and Eli Manning respectively), leading to their occasional match-up being referred to as the \"Manning Bowl\".\n\nSuper Bowl III became the most famous upset in professional sports history as the American Football League's New York Jets won 16–7 over the overwhelmingly favored Colts. With the merger of the AFL and NFL the Colts and Jets were placed in the new AFC East. The two teams met twice a year (interrupted in 1982 by a player strike) 1970–2001; with the move of the Colts to the AFC South the two teams' rivalry actually escalated, as they met three times in the playoffs in the South's first nine seasons of existence; the Jets crushed the Colts 41–0 in the 2002 Wild Card playoff round; the Colts then defeated the Jets 30–17 in the 2009 AFC Championship Game; but the next year in the Wild Card round the Jets pulled off another playoff upset of the Colts, winning 17–16; it was Peyton Manning's final game with the Colts. The Jets defeated the Colts 35–9 in 2012 in Andrew Luck's debut season; after two straight losses Luck led a 45–10 rout of the Jets in 2016.\n\nJoe Namath and Johnny Unitas were the focal point of the rivalry at its beginning, but they did not meet for a full game until September 24, 1972. Namath erupted with six touchdowns and 496 passing yards despite only 28 throws and 15 completions. Unitas threw for 376 yards and two scores but was sacked six times as the Jets won 44–34; the game was considered one of the top ten passing duels in NFL history.\n\nBaltimore's post NFL-AFL merger passage to the AFC saw them thrust into a new environment with little in common with its fellow divisional teams: the Jets, Miami Dolphins, Buffalo Bills, and Boston Patriots. One angle where the two clubs did have something in common, however, lay in new Miami coach Don Shula. Shula had coached the Colts the previous seven pre-merger seasons (1963–69) and was signed by Joe Robbie after the merger was consummated; because the signing came after the merger the NFL's rules on tampering came into play, and the Dolphins had to give up their first-round pick to the Colts.\n\nPowered by QB Earl Morrall Baltimore was the first non-AFL franchise to win a division title in the conference, outlasting the Miami Dolphins by one game, and leading the division since Week 3 of 1970. The two franchises were denied a playoff confrontation by Miami's first-round defeat to the Oakland Raiders, whereas Baltimore won its first Super Bowl title that year.\n\nYet in 1971, the teams were engaged in a heated race that went down to the final week of the season, where Miami won its first division title with a 10–3–1 title compared to the 10–4 Baltimore record after the Colts won the Week 13 matchup between them at home, but proceeded to lose the last game of the season to Boston. In the playoffs Baltimore advanced to the AFC title game after a 20–3 rout of the Cleveland Browns, whereas Miami survived a double-overtime nailbiter against the Kansas City Chiefs. This set up a title game that was favored for the defending league champion Colts. Yet Miami won the AFC championship with a 21–0 shutout and advanced to lose Super Bowl VI to Dallas. In 1975 Baltimore and Miami tied with 10–4 records, yet the Colts advanced to the playoffs based on a head-to-head sweep of their series. In 1977 Baltimore tied for first for the third straight year (in 1976 they tied with the now-New England Patriots) with Miami, and this time advanced to the playoffs on even slimmer pretenses, with a conference record of 9–3 compared to Miami's 8–4, as they had split the season series. The rivalry in the following years was virtually negated by very poor play of the Colts; the Colts won just 117 games in the twenty-one seasons (1978–98) that bracketed their 1977 playoff loss to the Oakland Raiders and the 1999 trade of star running back Marshall Faulk; this included a 0–8–1 record during the NFL's strike shortened 1982 season.\n\nIn 1995, now as Indianapolis, the two both posted borderline 9–7 records to tie for second against Buffalo, yet the Colts once again reached the post-season having swept the season series. The following season they edged out Miami by posting a 9–7 record and winning the ordinarily meaningless 3rd-place position, but qualifying for the wild card. The two clubs' 1999 meetings were dramatic affairs between Hall Of Fame-bound Dan Marino and up-and-coming star Peyton Manning. Marino led a 25-point fourth quarter comeback for a 34–31 Dolphins win at the RCA Dome, and then in Miami Marino led another comeback to tie the game 34–34 with 36 seconds remaining; Manning, however, drove the Colts in range for a 53-yard field goal as time expired (37–34 Colts win).\n\nThe last truly meaningful matchup between the two franchises was in the 2000 season, when Miami edged out Indianapolis with an 11–5 record for the division championship. The two then met in the wild-card round where the Dolphins won 23–17 before being blown out by Oakland 27–0 (the Colts themselves had suffered a bitter loss to the Raiders in Week 2 of the season when the Raiders erased a 24–7 gap to win 38–31). In 2002 the Colts moved to the newly created AFC South division; the two clubs met at the RCA Dome on September 15 where the Dolphins edged the Colts 21–13 after stopping a late Colts drive. The rivalry was effectively retired after this; the two clubs did meet in a memorable \"Monday Night Football\" matchup in 2009 where the Colts, despite having the ball for only 15 minutes, defeated the Dolphins 27–23.\n\nThe rivalry saw a rekindling after the 2012 NFL Draft brought new quarterbacks to both teams in Ryan Tannehill and Luck. The two met during the 2012 season with Luck breaking the rookie record for passing yards in a game in a 23–20 win over the Dolphins, but Tannehill and the Dolphins beat the Colts 24–20 the next season. The Dolphins win began a slump for Luck and the Colts against AFC East teams (eight straight losses by the Colts) that ended in December 2016 against the Jets, when they defeated them by a score of 41-10.\n\n colspan=\"5\" style=\";\"Indianapolis Colts retired numbers\n! width=40px style=\";\"No.\n! width=150px style=\";\"Player\n! width=40px style=\";\"Position\n! width=100px style=\";\"Years played\n! width=150px style=\";\"Retired\n\n! colspan=\"10\" style=;\"Baltimore/Indianapolis Colts Hall of Famers\n! colspan=\"10\" style=\";\"Players\n! No.\n! Name\n! Positions\n! Seasons\n! style=\"border-right:3px solid #002C5F\"Inducted\n! No.\n! Name\n! Positions\n! Seasons\n! Inducted\n! colspan=\"10\" style=\";\"Coaches and executives\n! colspan=\"2\"Name\n! Positions\n! Tenure\n! Inducted\n! colspan=\"5\"Notes\n\nThe Ring of Honor was established on September 23, 1996. There have been 15 inductees.\n\nThis is a partial list of the Colts' last five completed seasons. For the full season-by-season franchise results, see List of Indianapolis Colts seasons.\n\n\"Note: The Finish, Wins, Losses, and Ties columns list regular season results and exclude any postseason play.\"\n\n! rowspan=\"2\" style=\";\"Season\n! rowspan=\"2\" style=\";\"Team\n! rowspan=\"2\" style=\";\"League\n! rowspan=\"2\" style=\";\"Conference\n! rowspan=\"2\" style=\";\"Division\n! colspan=\"4\" style=\";\"Regular season\n! rowspan=\"2\" style=\";\"Postseason results\n! rowspan=\"2\" style=\";\"Awards\n! style=\";\"Finish\n! style=\";\"Wins\n! style=\";\"Losses\n! style=\";\"Ties\n!  2013\n!  2014\n!  2015\n!  2016\n!  2017\n!  2018\n\n! colspan=\"5\" style=\";\"All-time Colts leaders\n! style=\";\"Leader  style=\";\"Player  style=\";\"Record  style=\";\"Years with Colts\n\nThe Colts' flagship radio station since relocating from Baltimore in 1984 to 1998 and again starting in the 2007 season is WIBC 1070 AM (later renamed WFNI as of December 26, 2007); under the new contract, games are also simulcast on WLHK at 97.1 FM. From 1998 through 2006, the Colts' flagship radio station was WFBQ at 94.7 FM (with additional programming on station WNDE at 1260 AM). Bob Lamey was the team's play-by-play announcer, holding that title from 1984 to 1991 and again from 1995 to 2018. Former Colts offensive lineman, Will Wolford serves as the \"color commentator\". Ted Marchibroda, who had been the head coach of the Colts in both Baltimore and Indianapolis and who served as color commentator from 1999 to 2006, is now an analyst on the Colts pre-game show. Mike Jansen serves as the public address announcer at all Colts home games. Jansen has been the public address announcer since the 1998 season.\n\nUntil 2011, WTTV (channel 4/29) carried the team's preseason games, when WNDY-TV (channel 23) began to carry them as part of an agreement with sister station WISH-TV (channel 8) to become the team's official station; WISH had carried most of the team's games through the NFL on CBS since the 1998 season. Indiana University's \"Hoosiers\" announcer Don Fischer provides play-by-play. \"Monday Night Football\" broadcasts are usually carried by ABC affiliate WRTV (channel 6).\n\nThe team's carriage rights were shaken up in mid-2014 when WTTV's owner Tribune Media came to terms with CBS to become the network's Indianapolis affiliate as of January 1, 2015, leaving WISH with the market's affiliation with The CW. With the deal, both Tribune Media stations, including WXIN (channel 59) will carry the bulk of the team's regular season games starting with the 2015 NFL season (WXIN will carry a minimum of two home games against NFC opponents under the NFL on Fox deal, along with flex-scheduled Sunday games no matter the division matchup), with the team's Wild Card playoff game against the Cincinnati Bengals on January 4, 2015 on WTTV rather than new CW affiliate WISH. Also as of the 2015 season, WTTV and WXIN became the official Colts stations and air the team's preseason games, along with official team programming and coach's shows, and have a signage presence along the fascia of Lucas Oil Stadium.\n\n! scope=\"col\" style=\";\"City \n! scope=\"col\" style=\";\"Call Sign \n! scope=\"col\" style=\";\"Frequency\n\n! scope=\"col\" style=\";\"City \n! scope=\"col\" style=\";\"Call Sign \n! scope=\"col\" style=\";\"Frequency\n\n! scope=\"col\" style=\";\"City \n! scope=\"col\" style=\";\"Call Sign \n! scope=\"col\" style=\";\"Frequency\n\nBULLET::::- Indianapolis Colts Radio Affiliates\n\nBefore the third regular season game of 2017, against the Cleveland Browns, more than ten Indianapolis Colts players kneeled on one knee as opposed to the tradition of standing during the playing of \"The Star-Spangled Banner\", while thousands of fans booed and others posted responses to social media. The following day, then Colts head coach Chuck Pagano commented, “I’m proud of our players and their commitment and their compassion toward the game and the [horse] shoe and each community. We are a unified group,” and former head coach, Tony Dungy was quoted saying \"A group of our family got attacked, and called names ... and said they should be fired for what we feel is demonstrating our first amendment right\".\n\nBefore the fourth regular season game of 2017, against the Seattle Seahawks, the Colts stood during \"The Star-Spangled Banner\", however the entire team, including quarterback Andrew Luck locked arms in protest, instead of the customary holding of the right hand over the heart. Ratings for this \"NBC Sunday Night Football\" game was down five percent from the prior week's game in the same time slot.\n\nBefore the fifth regular season game of 2017, against the San Francisco 49ers, the entire Colts team as in the Week 4 game, stood during \"The Star-Spangled Banner\", however with locking of arms, instead of the customary holding of the right hand over the heart. In addition to the Colts response, more than 20 members of the opposing team, the San Francisco 49ers, kneeled for \"The Star-Spangled Banner\". In attendance within the stadium, was then Vice President of the United States and former Governor of Indiana, Mike Pence who responded to these protests by leaving the stadium. This was a heavily attended home game for the halftime retirement of the #18 jersey of former quarterback and 2-time Super Bowl winner, Peyton Manning.\n\nDuring warmups prior to the sixth regular game of the 2017 season, a \"Monday Night Football\" game between the Colts and the Tennessee Titans, the Colts wore black T-shirts with the words \"We will\" on the front and \"Stand for equality, justice, unity, respect, dialogue, opportunity\" on the back for the third straight week. The Colts plays stood with their arms locked during the playing of \"The Star-Spangled Banner\" instead of the customary holding of the right hand over the heart.\n"}
{"id": "15051", "url": "https://en.wikipedia.org/wiki?curid=15051", "title": "Immigration to the United States", "text": "Immigration to the United States\n\nImmigration to the United States is the international movement of non-U.S. nationals in order to reside permanently in the country. Immigration has been a major source of population growth and cultural change throughout much of the U.S. history. Because the United States is a settler colonial society, all Americans, with the exception of the small percentage of Native Americans, can trace their ancestry to immigrants from other nations around the world.\n\nIn absolute numbers, the United States has a larger immigrant population than any other country, with 47 million immigrants as of 2015. This represents 19.1% of the 244 million international migrants worldwide, and 14.4% of the U.S. population. Some other countries have larger proportions of immigrants, such as Switzerland with 24.9% and Canada with 21.9%.\n\nAccording to the 2016 Yearbook of Immigration Statistics, the United States admitted a total of 1.18 million legal immigrants in 2016. Of these, 48% were the immediate relatives of U.S. citizens, 20% were family-sponsored, 13% were refugees and/or asylum seekers, 12% were employment-based preferences, 4.2% were part of the Diversity Immigrant Visa program, 1.4% who were victims of a crime (U1) or their family members (U2 to U5), and 1.0% who were granted the Special Immigrant Visa (SIV) for Iraqis and Afghans employed by U.S. Government. The remaining 0.4% included small numbers from several other categories, including 0.2% who were granted suspension of deportation as an immediate relative of a citizen (Z13); persons admitted under the Nicaraguan and Central American Relief Act; children born subsequent to the issuance of a parent's visa; and certain parolees from the former Soviet Union, Cambodia, Laos, and Vietnam who were denied refugee status.\n\nThe economic, social, and political aspects of immigration have caused controversy regarding such issues as maintaining ethnic homogeneity, workers for employers versus jobs for non-immigrants, settlement patterns, impact on upward social mobility, crime, and voting behavior.\n\nBetween 1921 and 1965, policies such as the national origins formula limited immigration and naturalization opportunities for people from areas outside Western Europe. Exclusion laws enacted as early as the 1880s generally prohibited or severely restricted immigration from Asia, and quota laws enacted in the 1920s curtailed Eastern European immigration. The civil rights movement led to the replacement of these ethnic quotas with per-country limits for family-sponsored and employment-based preference visas. Since then, the number of first-generation immigrants living in the United States has quadrupled.\n\nResearch suggests that immigration to the United States is beneficial to the U.S. economy. With few exceptions, the evidence suggests that on average, immigration has positive economic effects on the native population, but it is mixed as to whether low-skilled immigration adversely affects low-skilled natives. Studies also show that immigrants have lower crime rates than natives in the United States.\n\nAmerican immigration history can be viewed in four epochs: the colonial period, the mid-19th century, the start of the 20th century, and post-1965. Each period brought distinct national groups, races and ethnicities to the United States.\n\nDuring the 17th century, approximately 400,000 English people migrated to Colonial America. However, only half stayed permanently. They comprised 85-90% of white immigrants. From 1700 to 1775 between 350-500,000 Europeans immigrated: the estimates vary in the sources. Only 52,000 English supposedly immigrated in the period 1701 to 1775., a figure questioned as too low. The rest, 400-450,000 were Scots, Scots-Irish from Ulster, Germans and Swiss, French Huguenots, and involuntarily 300,000 Africans. Over half of all European immigrants to Colonial America during the 17th and 18th centuries arrived as indentured servants. They numbered 350,000. On the eve of the War for Independence 1770 to 1775 7,000 English, 15,00 Scots, 13,200 Scots-Irish, 5,200 Germans, and 3,900 Irish Catholics arrived Fully half the English immigrants were young single men, well-skilled, trained artisans like the Huguenots The European populations of the Middle Colonies of New York, New Jersey, Pennsylvania and Delaware were ethnically very mixed, the English constituting only 30% in Pennsylvania, 40-45% in New Jersey, to 18% in New York numbered 22,000. The mid-19th century saw an influx mainly from northern Europe from the same major ethnic groups as for the Colonial Period but with large numbers of Catholic Irish and Scandinavians added to the mix; the late 19th and early 20th-century immigrants were mainly from Southern and Eastern Europe, but there were also several million immigrants from Canada; post-1965 most came from Latin America and Asia.\n\nHistorians estimate that fewer than 1 million immigrants moved to the United States from Europe between 1600 and 1799. By comparison, in the first federal census, in 1790, the population of the United States was enumerated to be 3,929,214.\n\nThe Naturalization Act of 1790 limited naturalization to \"free white persons\"; it was expanded to include blacks in the 1860s and Asians only in the 1950s. This made the United States an outlier, since laws that made racial distinctions were uncommon in the world in the 18th Century.\n\nIn the early years of the United States, immigration was fewer than 8,000 people a year, including French refugees from the slave revolt in Haiti. After 1820, immigration gradually increased. From 1836 to 1914, over 30 million Europeans migrated to the United States. The death rate on these transatlantic voyages was high, during which one in seven travelers died. In 1875, the nation passed its first immigration law, the Page Act of 1875.\n\nAfter an initial wave of immigration from China following the California Gold Rush, Congress passed a series of laws culminating in the Chinese Exclusion Act of 1882, banning virtually all immigration from China until the law's repeal in 1943. In the late 1800s, immigration from other Asian countries, especially to the West Coast, became more common.\n\nThe peak year of European immigration was in 1907, when 1,285,349 persons entered the country. By 1910, 13.5 million immigrants were living in the United States.\n\nWhile the Chinese Exclusion Act of 1882 had already excluded immigrants from China, the immigration of people from Asian countries in addition to China was banned by the sweeping Immigration Act of 1917, also known as the Asiatic Barred Zone Act, which also banned homosexuals, people with intellectual disability, and people with an anarchist worldview. The Emergency Quota Act was enacted in 1921, followed by the Immigration Act of 1924. The 1924 Act was aimed at further restricting immigrants from Southern and Eastern Europe, particularly Jews, Italians, and Slavs, who had begun to enter the country in large numbers beginning in the 1890s, and consolidated the prohibition of Asian immigration. \n\nImmigration patterns of the 1930s were affected by the Great Depression. In the final prosperous year, 1929, there were 279,678 immigrants recorded, but in 1933, only 23,068 moved to the U.S. In the early 1930s, more people emigrated from the United States than to it. The U.S. government sponsored a Mexican Repatriation program which was intended to encourage people to voluntarily move to Mexico, but thousands were deported against their will. Altogether about 400,000 Mexicans were repatriated half of them US citizens. Most of the Jewish refugees fleeing the Nazis and World War II were barred from coming to the United States. In the post-war era, the Justice Department launched Operation Wetback, under which 1,075,168 Mexicans were deported in 1954.\n\nThe Immigration and Nationality Act of 1965, also known as the Hart-Cellar Act, abolished the system of national-origin quotas. By equalizing immigration policies, the act resulted in new immigration from non-European nations, which changed the ethnic make-up of the United States. In 1970, 60% of immigrants were from Europe; this decreased to 15% by 2000. In 1990, George H. W. Bush signed the Immigration Act of 1990, which increased legal immigration to the United States by 40%. In 1991, Bush signed the Armed Forces Immigration Adjustment Act 1991, allowing foreign service members who had serve 12 or more years in the US Armed Forces to qualify for permanent residency and, in some cases, citizenship.\n\nIn November 1994, California voters passed Proposition 187 amending the state constitution, denying state financial aid to illegal immigrants. The federal courts voided this change, ruling that it violated the federal constitution.\n\nAppointed by Bill Clinton, the U.S. Commission on Immigration Reform recommended reducing legal immigration from about 800,000 people per year to approximately 550,000. While an influx of new residents from different cultures presents some challenges, \"the United States has always been energized by its immigrant populations,\" said President Bill Clinton in 1998. \"America has constantly drawn strength and spirit from wave after wave of immigrants ... They have proved to be the most restless, the most adventurous, the most innovative, the most industrious of people.\"\n\nIn 2001, President George W. Bush discussed an accord with Mexican President Vincente Fox. Possible accord was derailed by the September 11 attacks. From 2005 to 2013, the US Congress discussed various ways of controlling immigration. The Senate and House were unable to reach an agreement.\n\nNearly 14 million immigrants entered the United States from 2000 to 2010, and over one million persons were naturalized as U.S. citizens in 2008. The per-country limit applies the same maximum on the number of visas to all countries regardless of their population and has therefore had the effect of significantly restricting immigration of persons born in populous nations such as Mexico, China, India, and the Philippines—the leading countries of origin for legally admitted immigrants to the United States in 2013; nevertheless, China, India, and Mexico were the leading countries of origin for immigrants overall to the United States in 2013, regardless of legal status, according to a U.S. Census Bureau study. , 66% of legal immigrants were admitted on the basis of family ties, along with 13% admitted for their employment skills and 17% for humanitarian reasons.\n\nNearly 8 million people immigrated to the United States from 2000 to 2005; 3.7 million of them entered without papers. In 1986 president Ronald Reagan signed immigration reform that gave amnesty to 3 million undocumented immigrants in the country. Hispanic immigrants suffered job losses during the late-2000s recession, but since the recession's end in June 2009, immigrants posted a net gain of 656,000 jobs. Over 1 million immigrants were granted legal residence in 2011.\n\nFor those who enter the US illegally across the Mexico–United States border and elsewhere, migration is difficult, expensive and dangerous. Virtually all undocumented immigrants have no avenues for legal entry to the United States due to the restrictive legal limits on green cards, and lack of immigrant visas for low-skilled workers. Participants in debates on immigration in the early twenty-first century called for increasing enforcement of existing laws governing illegal immigration to the United States, building a barrier along some or all of the Mexico-U.S. border, or creating a new guest worker program. Through much of 2006 the country and Congress was immersed in a debate about these proposals. few of these proposals had become law, though a partial border fence had been approved and subsequently canceled.\n\nAccording to a report released by ICE, during the fiscal year of 2016 ICE removed 240,255 immigrants. During the fiscal year of 2018, ICE removed 256,085 immigrants. There has been a significant increase in the removal of immigrants since President Trump took office. The reason for the increase in removals is due to the policies that the Trump administrations have put in place. \n\nIn January 2017, U.S. President Donald Trump signed an executive order temporarily suspending entry to the United States by nationals of seven Muslim-majority countries. It was replaced by another executive order in March 2017 and by a presidential proclamation in September 2017, with various changes to the list of countries and exemptions. The orders were temporarily suspended by federal courts but later allowed to proceed by the Supreme Court, pending a definite ruling on their legality. Another executive order called for the immediate construction of a wall across the U.S.–Mexico border, the hiring of 5,000 new border patrol agents and 10,000 new immigration officers, and federal funding penalties for sanctuary cities.\n\nThe most recent Trump policy to affect immigration to the United States was his ‘zero tolerance policy’. The ‘zero tolerance’ policy was put in place by President Trump in 2018, Attorney General Jeff Sessions made a formal statement putting in place the ‘zero tolerance’ policy, this policy legally allows children to be separated from adults unlawfully entering the United States. This is justified by labeling all adults that enter unlawfully as criminals thus subjecting them to criminal prosecution . The policy has faced a lot of criticism and backlash and was reportedly stopped in June 2018. “The United Nations condemned the USA government’s Zero Tolerance policy as ‘The Trump administration’s practice of separating children from migrant families entering the United States violates their rights and international law’” . Only after the stopping the ‘zero tolerance policy’ did the Trump administration uncover that there were no official plans in place to reunite families; resulting in further separation. Learn more about the Trump administration family separation policy.\n\nThe Trump Administration has continued their promise of a heavy hand on immigration and is now making it harder for asylum seekers. Most recent policies are attacking what is means for an asylum seeker to claim credible fear, these policies are changing the ways in which asylum officers assess an asylee’s circumstance, “A passage has been altered on individuals’ ‘demeanor, candor, and responsiveness’ as a factor in their credibility. Both the 2017 and 2014 versions note that migrants’ demeanor is often affected by cultural factors, including being detained in a foreign land and perhaps not speaking the language, as well as by trauma sustained at home of on the journey to the US. But the new version removes guidance that said these factors shouldn't be ‘significant factors’ in determining someone’s credibility — essentially allowing asylum officers to consider signs of stress as a reason to doubt someone’s credibility” . To further decrease the amount of asylum seekers into the United States, Attorney Jeff Sessions released a decision that restricts those fleeing gang violence and domestic abuse as ‘private crime’, therefore making their claims ineligible for asylum, “The 31-page decision narrows the ground for asylum for victims of ‘private crime’ and will cut off an avenue to refuge for women fleeing to the United States from Central America. ‘Generally, claims by aliens pertaining to domestic violence or gang violence perpetrated by non-governmental actors will not qualify for asylum,’ Sessions said in the opinion” . These new policies that have been put in place are putting many lives at risk, to the point that the ACLU has officially sued Jeff Sessions along with other members of the Trump Administration. The ACLU claims that the polices that are currently being put in place by this Presidential Administration is undermining the fundamental human rights of those immigrating into the United States, specifically women. They also claim that these policies violate decades of settle asylum law (. \n\nSince the Trump Administration took office, it remained true to its hard stance on immigration. Trump and his administration almost immediately looked to remove the DACA program that was put in place by the Obama Administration. A policy was passed to stop granting citizenship requests. If you go to the DACA page on the United States Citizenship and Immigration Services a warning appears that states: “Important information about DACA requests: Due to federal court orders, USCIS has resumed accepting requests to renew a grant of deferred action under DACA. USCIS is not accepting requests from individuals who have never before been granted deferred action under DACA. Until further notice, and unless otherwise provided in this guidance, the DACA policy will be operated on the terms in place before it was rescinded on Sept. 5, 2017” . The Trump administration ordered federal courts to no longer grant citizenship to DACA requestors, making the process to citizenship for young children brought to the country illegally by their parents almost non-existent.\n\nAccording to the Department of State, in the 2016 fiscal year 84,988 refugees were accepted into the US from around the world. In the fiscal year of 2017, 53,691 refugees were accepted to the US. There was a significant decrease after Trump took office and it continues in the fiscal year of 2018 when only 22,405 refugees were accepted into the US. This is displays a massive drop in acceptance of refugees since the Trump Administration has been in place. \n!Period!!Refugee Programme \nBULLET::::- Origins of the U.S. immigrant population, 1960–2016\n\n+ % of foreign-born population residing in the U.S. who were born in ...\n! cyrus=\"col\"  \n! cyrus=\"col\"  1960\n! scope=\"col\"  1970\n! cyrus=\"col\"  1980\n! cyrus=\"col\"  1990\n! scope=\"col\"  2000\n! cyrus=\"col\"  2010\n! cyrus=\"col\"  2011\n! scope=\"col\"  2012\n! cyrus=\"col\"  2013\n! cyrus=\"col\"  2014\n! scope=\"col\"  2015\n! scope=\"col\"  2016\n\nNote: \"Other Latin America\" includes Central America, South America and the Caribbean.\n\nBULLET::::- Persons obtaining legal permanent resident status by fiscal year\n\n! scope=\"col\" style=\"width:50px;\"Year\n! scope=\"col\" style=\"width:80px;\"\n! scope=\"col\" style=\"width:50px;\"Year\n! scope=\"col\" style=\"width:80px;\"\n! scope=\"col\" style=\"width:50px;\"Year\n! scope=\"col\" style=\"width:80px;\"\n! scope=\"col\" style=\"width:50px;\"Year\n! scope=\"col\" style=\"width:80px;\"\n! scope=\"col\" style=\"width:50px;\"Year\n! scope=\"col\" style=\"width:80px;\"\n! scope=\"col\" style=\"width:50px;\"Year\n! scope=\"col\" style=\"width:80px;\"\n! scope=\"col\" style=\"width:50px;\"Year\n! scope=\"col\" style=\"width:80px;\"\n\n! scope=\"col\" style=\"width:80px;\"Decade\n! scope=\"col\" style=\"width:160px;\"Average per year\n\nApproximately half of immigrants living in the United States are from Mexico and other Latin American countries. Many Central Americans are fleeing because of desperate social and economic circumstances created in part by U.S. foreign policy in Central America over many decades. The large number of Central American refugees arriving in the U.S. have been explained as \"blowback\" to policies such as U.S. military interventions and covert operations that installed or maintained in power authoritarian leaders allied with wealthy land owners and multinational corporations who crush family farming and democratic efforts, which have caused drastically sharp social inequality, wide scale poverty and rampant crime. Economic austerity dictated by neoliberal policies imposed by the International Monetary Fund and its ally, the U.S., has also been cited as a driver of the dire social and economic conditions, as has the U.S. \"War on Drugs,\" which has been understood as fueling murderous gang violence in the region. “The current debate … is almost totally about what to do about immigrants when they get here. But the 800-pound gorilla that’s missing from the table is what we have been doing there that brings them here, that drives them here,\" according to Jeff Faux, an economist who is a distinguished fellow at the Economic Policy Institute.\n\nUntil the 1930s most legal immigrants were male. By the 1990s women accounted for just over half of all legal immigrants. Contemporary immigrants tend to be younger than the native population of the United States, with people between the ages of 15 and 34 substantially overrepresented. Immigrants are also more likely to be married and less likely to be divorced than native-born Americans of the same age.\n\nImmigrants are likely to move to and live in areas populated by people with similar backgrounds. This phenomenon has held true throughout the history of immigration to the United States. Seven out of ten immigrants surveyed by Public Agenda in 2009 said they intended to make the U.S. their permanent home, and 71% said if they could do it over again they would still come to the US. In the same study, 76% of immigrants say the government has become stricter on enforcing immigration laws since the September 11, 2001 attacks (\"9/11\"), and 24% report that they personally have experienced some or a great deal of discrimination.\n\nPublic attitudes about immigration in the U.S. were heavily influenced in the aftermath of the 9/11 attacks. After the attacks, 52% of Americans believed that immigration was a good thing overall for the U.S., down from 62% the year before, according to a 2009 Gallup poll. A 2008 Public Agenda survey found that half of Americans said tighter controls on immigration would do \"a great deal\" to enhance U.S. national security. Harvard political scientist and historian Samuel P. Huntington argued in his 2004 book \"Who Are We? The Challenges to America's National Identity\" that a potential future consequence of continuing massive immigration from Latin America, especially Mexico, could lead to the bifurcation of the United States.\n\nThe estimated population of illegal Mexican immigrants in the US fell from approximately 7 million in 2007 to 6.1 million in 2011 Commentators link the reversal of the immigration trend to the economic downturn that started in 2008 and which meant fewer available jobs, and to the introduction of tough immigration laws in many states. According to the Pew Hispanic Center the net immigration of Mexican born persons had stagnated in 2010, and tended toward going into negative figures.\n\nMore than 80 cities in the United States, including Washington D.C., New York City, Los Angeles, Chicago, San Francisco, San Diego, San Jose, Salt Lake City, Phoenix, Dallas, Fort Worth, Houston, Detroit, Jersey City, Minneapolis, Denver, Baltimore, Seattle, Portland, Oregon and Portland, Maine, have sanctuary policies, which vary locally.\n\nBULLET::::- Inflow of New Legal Permanent Residents by region, in the recent years.\n\n! scope=\"col\" style=\"width:120px;\"Region\n! scope=\"col\" style=\"width:80px;\" 2015\n! scope=\"col\" style=\"width:80px;\" \n! scope=\"col\" style=\"width:80px;\" 2016\n! scope=\"col\" style=\"width:80px;\" \n! scope=\"col\" style=\"width:80px;\" 2017\n! scope=\"col\" style=\"width:80px;\" \n! scope=\"col\" style=\"width:80px;\" \n\nSource: US Department of Homeland Security, Office of Immigration Statistics\n\nTop 10 sending countries in the recent years:\n! scope=\"col\" style=\"width:120px;\"Country\n! scope=\"col\" style=\"width:80px;\"2015\n! scope=\"col\" style=\"width:80px;\"2016\n! scope=\"col\" style=\"width:80px;\"2017\n\n!Year\n!Number of foreign-born\n!Percentforeign-born\n1850\n2,244,602\n9.7\n1860\n4,138,697\n13.2\n1870\n5,567,229\n14.4\n1880\n6,679,943\n13.3\n1890\n9,249,547\n14.8\n1900\n10,341,276\n13.6\n1910\n13,515,886\n14.7\n1920\n13,920,692\n13.2\n1930\n14,204,149\n11.6\n1940\n11,594,896\n8.8\n1950\n10,347,395\n6.9\n1960\n9,738,091\n5.4\n1970\n9,619,302\n4.7\n1980\n14,079,906\n6.2\n1990\n19,767,316\n7.9\n2000\n31,107,889\n11.1\n2010\n39,956,000\n12.9\n2015\n43,290,000\n13.4\n\nThe United States admitted more legal immigrants from 1991 to 2000, between ten and eleven million, than in any previous decade. In the most recent decade, the 10 million legal immigrants that settled in the U.S. represent roughly one third of the annual growth, as the U.S. population grew by 32 million (from 249 million to 281 million). By comparison, the highest previous decade was the 1900s, when 8.8 million people arrived, increasing the total U.S. population by one percent every year. Specifically, \"nearly 15% of Americans were foreign-born in 1910, while in 1999, only about 10% were foreign-born.\"\n\nBy 1970, immigrants accounted for 4.7 percent of the US population and rising to 6.2 percent in 1980, with an estimated 12.5 percent in 2009. , 25% of US residents under age 18 were first- or second-generation immigrants. Eight percent of all babies born in the U.S. in 2008 belonged to illegal immigrant parents, according to a recent analysis of U.S. Census Bureau data by the Pew Hispanic Center.\n\nLegal immigration to the U.S. increased from 250,000 in the 1930s, to 2.5 million in the 1950s, to 4.5 million in the 1970s, and to 7.3 million in the 1980s, before resting at about 10 million in the 1990s. Since 2000, legal immigrants to the United States number approximately 1,000,000 per year, of whom about 600,000 are \"Change of Status\" who already are in the U.S. Legal immigrants to the United States now are at their highest level ever, at just over 37,000,000 legal immigrants. In reports in 2005-2006, estimates of illegal immigration ranged from 700,000 to 1,500,000 per year. Immigration led to a 57.4% increase in foreign born population from 1990 to 2000.\n\nForeign-born immigration has caused the U.S. population to continue its rapid increase with the foreign-born population doubling from almost 20 million in 1990 to over 47 million in 2015. In 2018, there were almost 90 million immigrants and U.S.-born children of immigrants (second-generation Americans) in the United States, accounting for 28% of the overall U.S. population.\n\nWhile immigration has increased drastically over the last century, the foreign born share of the population is, at 13.4, only somewhat below what it was at its peak in 1910 at 14.7%. A number of factors may be attributed to the decrease in the representation of foreign born residents in the United States. Most significant has been the change in the composition of immigrants; prior to 1890, 82% of immigrants came from North and Western Europe. From 1891 to 1920, that number dropped to 25%, with a rise in immigrants from East, Central, and South Europe, summing up to 64%. Animosity towards these different and foreign immigrants rose in the United States, resulting in much legislation to limit immigration.\n\nContemporary immigrants settle predominantly in seven states, California, New York, Florida, Texas, Pennsylvania, New Jersey and Illinois, comprising about 44% of the U.S. population as a whole. The combined total immigrant population of these seven states was 70% of the total foreign-born population in 2000.\n\nBULLET::::- Foreign-born population of the United States in 2016 and number of immigrants between 1986 and 2016, by country of birth\n\n! style=\"text-align:center\"Country of birth\n! style=\"text-align:center\"Population (2016)\n! style=\"text-align:center\"Immigrants (1986–2016)\n\nThe Census Bureau estimates the US population will grow from 317 million in 2014 to 417 million in 2060 with immigration, when nearly 20% will be foreign born. A 2015 report from the Pew Research Center projects that by 2065, non-Hispanic whites will account for 46% of the population, down from the 2005 figure of 67%. Non-Hispanic whites made up 85% of the population in 1960. It also foresees the Hispanic population rising from 17% in 2014 to 29% by 2060. The Asian population is expected to nearly double in 2060. Overall, the Pew Report predicts the population of the United States will rise from 296 million in 2005 to 441 million in 2065, but only to 338 million with no immigration.\n\nIn 35 of the country's 50 largest cities, non-Hispanic whites were at the last census or are predicted to be in the minority. In California, non-Hispanic whites slipped from 80% of the state's population in 1970 to 42% in 2001 and 39% in 2013.\n\nImmigrant segregation declined in the first half of the 20th century, but has been rising over the past few decades. This has caused questioning of the correctness of describing the United States as a melting pot. One explanation is that groups with lower socioeconomic status concentrate in more densely populated area that have access to public transit while groups with higher socioeconomic status move to suburban areas. Another is that some recent immigrant groups are more culturally and linguistically different from earlier groups and prefer to live together due to factors such as communication costs. Another explanation for increased segregation is white flight.\n\nBULLET::::- Place of birth for the foreign-born population in the United States\n\n! scope=\"col\" style=\"width:130px;\"Top ten countries\n! scope=\"col\" style=\"width:80px;\" 2017\n! scope=\"col\" style=\"width:80px;\" 2010\n! scope=\"col\" style=\"width:80px;\" 2000\n! scope=\"col\" style=\"width:80px;\" 1990\n\nSource: 1990, 2000 and 2010 decennial Census and 2017 American Community Survey\n\nA survey of leading economists shows a consensus behind the view that high-skilled immigration makes the average American better off. A survey of the same economists also shows strong support behind the notion that low-skilled immigration makes the average American better off. According to David Card, Christian Dustmann, and Ian Preston, \"most existing studies of the economic impacts of immigration suggest these impacts are small, and on average benefit the native population\". In a survey of the existing literature, Örn B Bodvarsson and Hendrik Van den Berg write, \"a comparison of the evidence from all the studies ... makes it clear that, with very few exceptions, there is no strong statistical support for the view held by many members of the public, namely that immigration has an adverse effect on native-born workers in the destination country.\"\n\nWhereas the impact on the average native tends to be small and positive, studies show more mixed results for low-skilled natives, but whether the effects are positive or negative, they tend to be small either way.\n\nImmigrants may often do types of work that natives are largely unwilling to do, contributing to greater economic prosperity for the economy as a whole: for instance, Mexican migrant workers taking up manual farm work in the United States has close to zero effect on native employment in that occupation, which means that the effect of Mexican workers on U.S. employment outside farm work was therefore most likely positive, since they raised overall economic productivity. Research indicates that immigrants are more likely to work in risky jobs than U.S.-born workers, partly due to differences in average characteristics, such as immigrants' lower English language ability and educational attainment. Further, some studies indicate that higher ethnic concentration in metropolitan areas is positively related to the probability of self-employment of immigrants.\n\nResearch also suggests that diversity has a net positive effect on productivity and economic prosperity. A study by Nathan Nunn, Nancy Qian and Sandra Sequeira found that the Age of Mass Migration (1850–1920) has had substantially beneficial long-term effects on U.S. economic prosperity: \"locations with more historical immigration today have higher incomes, less poverty, less unemployment, higher rates of urbanization, and greater educational attainment. The long-run effects appear to arise from the persistence of sizeable short-run benefits, including earlier and more intensive industrialization, increased agricultural productivity, and more innovation.\" The authors also find that the immigration had short-term benefits: \"that there is no evidence that these long-run benefits come at short-run costs. In fact, immigration immediately led to economic benefits that took the form of higher incomes, higher productivity, more innovation, and more industrialization.\"\n\nResearch also finds that migration leads to greater trade in goods and services. Using 130 years of data on historical migrations to the United States, one study finds \"that a doubling of the number of residents with ancestry from a given foreign country relative to the mean increases by 4.2 percentage points the probability that at least one local firm invests in that country, and increases by 31% the number of employees at domestic recipients of FDI from that country. The size of these effects increases with the ethnic diversity of the local population, the geographic distance to the origin country, and the ethno-linguistic fractionalization of the origin country.\"\n\nSome research suggests that immigration can offset some of the adverse effects of automation on native labor outcomes in the United States. By increasing overall demand, immigrants could push natives out of low-skilled manual labor into better paying occupations. A 2018 study in the \"American Economic Review\" found that the Bracero program (which allowed almost half a million Mexican workers to do seasonal farm labor in the United States) did not have any adverse impact on the labor market outcomes of American-born farm workers.\n\nA 2011 literature review of the economic impacts of immigration found that the net fiscal impact of migrants varies across studies but that the most credible analyses typically find small and positive fiscal effects on average. According to the authors, \"the net social impact of an immigrant over his or her lifetime depends substantially and in predictable ways on the immigrant's age at arrival, education, reason for migration, and similar\".\n\nA 2016 report by the National Academies of Sciences, Engineering, and Medicine concluded that over a 75-year time horizon, \"the fiscal impacts of immigrants are generally positive at the federal level and generally negative at the state and local level.\" The reason for the costs to state and local governments is that the cost of educating the immigrants' children falls on state and local governments. According to a 2007 literature review by the Congressional Budget Office, \"Over the past two decades, most efforts to estimate the fiscal impact of immigration in the United States have concluded that, in aggregate and over the long term, tax revenues of all types generated by immigrants—both legal and unauthorized—exceed the cost of the services they use.\"\n\nAccording to James Smith, a senior economist at Santa Monica-based RAND Corporation and lead author of the United States National Research Council's study \"\"\"\", immigrants contribute as much as $10 billion to the U.S. economy each year. The NRC report found that although immigrants, especially those from Latin America, caused a net loss in terms of taxes paid versus social services received, immigration can provide an overall gain to the domestic economy due to an increase in pay for higher-skilled workers, lower prices for goods and services produced by immigrant labor, and more efficiency and lower wages for some owners of capital. The report also notes that although immigrant workers compete with domestic workers for low-skilled jobs, some immigrants specialize in activities that otherwise would not exist in an area, and thus can be beneficial for all domestic residents.\n\nImmigration and foreign labor documentation fees increased over 80% in 2007, with over 90% of funding for USCIS derived from immigration application fees, creating many USCIS jobs involving immigration to US, such as immigration interview officials, finger print processor, Department of Homeland Security, etc.\n\nOverall immigration has not had much effect on native wage inequality but low-skill immigration has been linked to greater income inequality in the native population.\n\nResearch on the economic effects of undocumented immigrants is scant but existing peer-reviewed studies suggest that the effects are positive for the native population and public coffers. A 2015 study shows that \"increasing deportation rates and tightening border control weakens low-skilled labor markets, increasing unemployment of native low-skilled workers. Legalization, instead, decreases the unemployment rate of low-skilled natives and increases income per native.\" Studies show that legalization of undocumented immigrants would boost the U.S. economy; a 2013 study found that granting legal status to undocumented immigrants would raise their incomes by a quarter (increasing U.S. GDP by approximately $1.4 trillion over a ten-year period), and 2016 study found that \"legalization would increase the economic contribution of the unauthorized population by about 20%, to 3.6% of private-sector GDP.\"\n\nA 2007 literature by the Congressional Budget Office found that estimating the fiscal effects of undocumented immigrants has proven difficult: \"currently available estimates have significant limitations; therefore, using them to determine an aggregate effect across all states would be difficult and prone to considerable error\". The impact of undocumented immigrants differs on federal levels than state and local levels, with research suggesting modest fiscal costs at the state and local levels but with substantial fiscal gains at the federal level.\n\nIn 2009, a study by the Cato Institute, a free market think tank, found that legalization of low-skilled illegal resident workers in the US would result in a net increase in US GDP of $180 billion over ten years. The Cato Institute study did not examine the impact on per capita income for most Americans. Jason Riley notes that because of progressive income taxation, in which the top 1% of earners pay 37% of federal income taxes (even though they actually pay a lower tax percentage based on their income), 60% of Americans collect more in government services than they pay in, which also reflects on immigrants. In any event, the typical immigrant and his children will pay a net $80,000 more in their lifetime than they collect in government services according to the NAS. Legal immigration policy is set to maximize net taxation. Illegal immigrants even after an amnesty tend to be recipients of more services than they pay in taxes. In 2010, an econometrics study by a Rutgers economist found that immigration helped increase bilateral trade when the incoming people were connected via networks to their country of origin, particularly boosting trade of final goods as opposed to intermediate goods, but that the trade benefit weakened when the immigrants became assimilated into American culture.\n\nAccording to NPR in 2005, about 3% of illegal immigrants were working in agriculture. The H-2A visa allows U.S. employers to bring foreign nationals to the United States to fill temporary agricultural jobs. The passing of tough immigration laws in several states from around 2009 provides a number of practical case studies. The state of Georgia passed immigration law HB 87 in 2011; this led, according to the coalition of top Kansas businesses, to 50% of its agricultural produce being left to rot in the fields, at a cost to the state of more than $400 million. Overall losses caused by the act were $1 billion; it was estimated that the figure would become over $20 billion if all the estimated 325,000 unauthorized workers left Georgia. The cost to Alabama of its crackdown in June 2011 has been estimated at almost $11 billion, with up to 80,000 unauthorized immigrant workers leaving the state.\n\nStudies of refugees' impact on native welfare are scant but the existing literature shows a positive fiscal impact and mixed results (negative, positive and no significant effects) on native welfare. A 2017 paper by Evans and Fitzgerald found that refugees to the United States pay \"$21,000 more in taxes than they receive in benefits over their first 20 years in the U.S.\" An internal study by the Department of Health and Human Services under the Trump administration, which was suppressed and not shown to the public, found that refugees to the United States brought in $63 billion more in government revenues than they cost the government. According to labor economist Giovanni Peri, the existing literature suggests that there are no economic reasons why the American labor market could not easily absorb 100,000 Syrian refugees in a year. Refugees integrate more slowly into host countries' labor markets than labor migrants, in part due to the loss and depreciation of human capital and credentials during the asylum procedure. \n\nAccording to one survey of the existing economic literature, \"much of the existing research points towards positive net contributions by immigrant entrepreneurs.\" Areas where immigrant are more prevalent in the United States have substantially more innovation (as measured by patenting and citations). Immigrants to the United States create businesses at higher rates than natives. According to a 2018 paper, \"first-generation immigrants create about 25% of new firms in the United States, but this share exceeds 40% in some states.\" Another 2018 paper links H-1B visa holders to innovation.\n\nImmigrants have been linked to greater invention and innovation in the US. According to one report, \"immigrants have started more than half (44 of 87) of America's startup companies valued at $1 billion or more and are key members of management or product development teams in over 70 percent (62 of 87) of these companies.\" Foreign doctoral students are a major source of innovation in the American economy. In the United States, immigrant workers hold a disproportionate share of jobs in science, technology, engineering, and math (STEM): \"In 2013, foreign-born workers accounted for 19.2 percent of STEM workers with a bachelor's degree, 40.7 percent of those with a master's degree, and more than half—54.5 percent—of those with a Ph.D.\"\n\nThe Kauffman Foundation's index of entrepreneurial activity is nearly 40% higher for immigrants than for natives. Immigrants were involved in the founding of many prominent American high-tech companies, such as Google, Yahoo, YouTube, Sun Microsystems, and eBay.\n\nIrish immigration was opposed in the 1850s by the nativist Know Nothing movement, originating in New York in 1843. It was engendered by popular fears that the country was being overwhelmed by Irish Catholic immigrants. On March 14, 1891, a lynch mob stormed a local jail and lynched several Italians following the acquittal of several Sicilian immigrants alleged to be involved in the murder of New Orleans police chief David Hennessy. The Congress passed the Emergency Quota Act in 1921, followed by the Immigration Act of 1924. The Immigration Act of 1924 was aimed at limiting immigration overall, and making sure that the nationalities of new arrivals matched the overall national profile.\n\nA 2014 meta-analysis of racial discrimination in product markets found extensive evidence of minority applicants being quoted higher prices for products. A 1995 study found that car dealers \"quoted significantly lower prices to white males than to black or female test buyers using identical, scripted bargaining strategies.\" A 2013 study found that eBay sellers of iPods received 21 percent more offers if a white hand held the iPod in the photo than a black hand.\n\nResearch suggests that police practices, such as racial profiling, over-policing in areas populated by minorities and in-group bias may result in disproportionately high numbers of racial minorities among crime suspects. Research also suggests that there may be possible discrimination by the judicial system, which contributes to a higher number of convictions for racial minorities. A 2012 study found that \"(i) juries formed from all-white jury pools convict black defendants significantly (16 percentage points) more often than white defendants, and (ii) this gap in conviction rates is entirely eliminated when the jury pool includes at least one black member.\" Research has found evidence of in-group bias, where \"black (white) juveniles who are randomly assigned to black (white) judges are more likely to get incarcerated (as opposed to being placed on probation), and they receive longer sentences.\" In-group bias has also been observed when it comes to traffic citations, as black and white cops are more likely to cite out-groups.\n\nA 2015 study using correspondence tests \"found that when considering requests from prospective students seeking mentoring in the future, faculty were significantly more responsive to White males than to all other categories of students, collectively, particularly in higher-paying disciplines and private institutions.\" Through affirmative action, there is reason to believe that elite colleges favor minority applicants.\n\nA 2014 meta-analysis found extensive evidence of racial discrimination in the American housing market. Minority applicants for housing needed to make many more enquiries to view properties. Geographical steering of African-Americans in US housing remained significant. A 2003 study finds \"evidence that agents interpret an initial housing request as an indication of a customer's preferences, but also are more likely to withhold a house from all customers when it is in an integrated suburban neighborhood (redlining). Moreover, agents' marketing efforts increase with asking price for white, but not for black, customers; blacks are more likely than whites to see houses in suburban, integrated areas (steering); and the houses agents show are more likely to deviate from the initial request when the customer is black than when the customer is white. These three findings are consistent with the possibility that agents act upon the belief that some types of transactions are relatively unlikely for black customers (statistical discrimination).\"\n\nA report by the federal Department of Housing and Urban Development where the department sent African-Americans and whites to look at apartments found that African-Americans were shown fewer apartments to rent and houses for sale.\n\nSeveral meta-analyses find extensive evidence of ethnic and racial discrimination in hiring in the American labor market. A 2016 meta-analysis of 738 correspondence tests—tests where identical CVs for stereotypically black and white names were sent to employers—in 43 separate studies conducted in OECD countries between 1990 and 2015 finds that there is extensive racial discrimination in hiring decisions in Europe and North-America. These correspondence tests showed that equivalent minority candidates need to send around 50% more applications to be invited for an interview than majority candidates. A study that examine the job applications of actual people provided with identical résumés and similar interview training showed that African-American applicants with no criminal record were offered jobs at a rate as low as white applicants who had criminal records.\n\nRacist thinking among and between minority groups does occur; examples of this are conflicts between blacks and Korean immigrants, notably in the 1992 Los Angeles Riots, and between African Americans and non-white Latino immigrants. There has been a long running racial tension between African American and Mexican prison gangs, as well as significant riots in California prisons where they have targeted each other, for ethnic reasons. There have been reports of racially motivated attacks against African Americans who have moved into neighborhoods occupied mostly by people of Mexican origin, and vice versa. There has also been an increase in violence between non-Hispanic whites and Latino immigrants, and between African immigrants and African Americans.\n\nA 2018 study in the \"American Sociological Review\" found that within racial groups, most immigrants to the United States had fully assimilated within a span of 20 years. Immigrants arriving in the United States after 1994 assimilate more rapidly than immigrants who arrived in previous periods. Measuring assimilation can be difficult due to \"ethnic attrition\", which refers to when descendants of migrants cease to self-identify with the nationality or ethnicity of their ancestors. This means that successful cases of assimilation will be underestimated. Research shows that ethnic attrition is sizable in Hispanic and Asian immigrant groups in the United States. By taking account of ethnic attrition, the assimilation rate of Hispanics in the United States improves significantly. A 2016 paper challenges the view that cultural differences are necessarily an obstacle to long-run economic performance of migrants. It finds that \"first generation migrants seem to be less likely to success the more culturally distant they are, but this effect vanishes as time spent in the USA increases.\"\n\nImmigration from South Asia and elsewhere has contributed to enlarging the religious composition of the United States. Islam in the United States is growing mainly due to immigration. Hinduism in the United States, Buddhism in the United States, and Sikhism in the United States are other examples. Whereas non-Christians together constitute only 4% of the U.S. population, they made up 20% of the 2003 cohort of new immigrants.\n\nSince 1992, an estimated 1.7 million Muslims, approximately 1 million Hindus, and approximately 1 million Buddhists have immigrated legally to the United States.\n\nConversely, non-religious are underrepresented in the immigrant populations. Although \"other\" non-Christian religions are also slightly more common among immigrants than among U.S. adults—1.9% compared with 1.0%—those professing no religion are slightly under-represented among new immigrants. Whereas 12% of immigrants said they had no religion, the figure was 15% for adult Americans. This lack of representation for non-religious could be related to stigmas around atheists and agnostics or could relate to the need for identity when entering a new country.\n\nThe American Federation of Labor (AFL), a coalition of labor unions formed in the 1880s, vigorously opposed unrestricted immigration from Europe for moral, cultural, and racial reasons. The issue unified the workers who feared that an influx of new workers would flood the labor market and lower wages. Nativism was not a factor because upwards of half the union members were themselves immigrants or the sons of immigrants from Ireland, Germany and Britain. However, nativism was a factor when the AFL even more strenuously opposed all immigration from Asia because it represented (to its Euro-American members) an alien culture that could not be assimilated into American society. The AFL intensified its opposition after 1906 and was instrumental in passing immigration restriction bills from the 1890s to the 1920s, such as the 1921 Emergency Quota Act and the Immigration Act of 1924, and seeing that they were strictly enforced.\n\nMink (1986) concludes that the link between the AFL and the Democratic Party rested in part on immigration issues, noting the large corporations, which supported the Republicans, wanted more immigration to augment their labor force.\n\nUnited Farm Workers during Cesar Chavez tenure was committed to restricting immigration. Chavez and Dolores Huerta, cofounder and president of the UFW, fought the Bracero Program that existed from 1942 to 1964. Their opposition stemmed from their belief that the program undermined U.S. workers and exploited the migrant workers. Since the Bracero Program ensured a constant supply of cheap immigrant labor for growers, immigrants could not protest any infringement of their rights, lest they be fired and replaced. Their efforts contributed to Congress ending the Bracero Program in 1964. In 1973, the UFW was one of the first labor unions to oppose proposed employer sanctions that would have prohibited hiring illegal immigrants.\n\nOn a few occasions, concerns that illegal immigrant labor would undermine UFW strike campaigns led to a number of controversial events, which the UFW describes as anti-strikebreaking events, but which have also been interpreted as being anti-immigrant. In 1973, Chavez and members of the UFW marched through the Imperial and Coachella Valleys to the border of Mexico to protest growers' use of illegal immigrants as strikebreakers. Joining him on the march were Reverend Ralph Abernathy and U.S. Senator Walter Mondale. In its early years, the UFW and Chavez went so far as to report illegal immigrants who served as strikebreaking replacement workers (as well as those who refused to unionize) to the Immigration and Naturalization Service.\n\nIn 1973, the United Farm Workers set up a \"wet line\" along the United States-Mexico border to prevent Mexican immigrants from entering the United States illegally and potentially undermining the UFW's unionization efforts. During one such event, in which Chavez was not involved, some UFW members, under the guidance of Chavez's cousin Manuel, physically attacked the strikebreakers after peaceful attempts to persuade them not to cross the border failed.\n\nA \"Boston Globe\" article attributed Barack Obama's win in the 2008 U.S. Presidential election to a marked reduction over the preceding decades in the percentage of whites in the American electorate, attributing this demographic change to the Immigration Act of 1965. The article quoted Simon Rosenberg, president and founder of the New Democrat Network, as having said that the Act is \"the most important piece of legislation that no one's ever heard of,\" and that it \"set America on a very different demographic course than the previous 300 years.\"\n\nImmigrants differ on their political views; however, the Democratic Party is considered to be in a far stronger position among immigrants overall. Research shows that religious affiliation can also significantly impact both their social values and voting patterns of immigrants, as well as the broader American population. Hispanic evangelicals, for example, are more strongly conservative than non-Hispanic evangelicals. This trend is often similar for Hispanics or others strongly identifying with the Catholic Church, a religion that strongly opposes abortion and gay marriage.\n\nThe key interests groups that lobby on immigration are religious, ethnic and business groups, together with some liberals and some conservative public policy organizations. Both the pro- and anti- groups affect policy.\n\nStudies have suggested that some special interest group lobby for less immigration for their own group and more immigration for other groups since they see effects of immigration, such as increased labor competition, as detrimental when affecting their own group but beneficial when affecting other groups.\n\nA 2011 paper found that both pro- and anti-immigration special interest groups play a role in migration policy. \"Barriers to migration are lower in sectors in which business lobbies incur larger lobbying expenditures and higher in sectors where labor unions are more important.\" A 2011 study examining the voting of US representatives on migration policy suggests that \"representatives from more skilled labor abundant districts are more likely to support an open immigration policy towards the unskilled, whereas the opposite is true for representatives from more unskilled labor abundant districts.\"\n\nAfter the 2010 election, Gary Segura of Latino Decisions stated that Hispanic voters influenced the outcome and \"may have saved the Senate for Democrats\". Several ethnic lobbies support immigration reforms that would allow illegal immigrants that have succeeded in entering to gain citizenship. They may also lobby for special arrangements for their own group. The Chairman for the Irish Lobby for Immigration Reform has stated that \"the Irish Lobby will push for any special arrangement it can get—'as will every other ethnic group in the country.'\" The irredentist and ethnic separatist movements for Reconquista and Aztlán see immigration from Mexico as strengthening their cause.\n\nThe book \"Ethnic Lobbies and US Foreign Policy\" (2009) states that several ethnic special interest groups are involved in pro-immigration lobbying. Ethnic lobbies also influence foreign policy. The authors write that \"Increasingly, ethnic tensions surface in electoral races, with House, Senate, and gubernatorial contests serving as proxy battlegrounds for antagonistic ethnoracial groups and communities. In addition, ethnic politics affect party politics as well, as groups compete for relative political power within a party\". However, the authors argue that currently ethnic interest groups, in general, do not have too much power in foreign policy and can balance other special interest groups.\n\nIn a 2012 news story, \"Reuters\" reported, \"Strong support from Hispanics, the fastest-growing demographic in the United States, helped tip President Barack Obama's fortunes as he secured a second term in the White House, according to Election Day polling.\"\n\nLately, there is talk among several Republican leaders, such as governors Bobby Jindal and Susana Martinez, of taking a new, friendlier approach to immigration. Former US Secretary of Commerce Carlos Gutierrez is promoting the creation of Republicans for Immigration Reform.\n\nBernie Sanders opposes guest worker programs and is also skeptical about skilled immigrant (H-1B) visas, saying, \"Last year, the top 10 employers of H-1B guest workers were all offshore outsourcing companies. These firms are responsible for shipping large numbers of American information technology jobs to India and other countries.\" In an interview with \"Vox\" he stated his opposition to an open borders immigration policy, describing it as:\n\n... a right-wing proposal, which says essentially there is no United States ... you're doing away with the concept of a nation-state. What right-wing people in this country would love is an open-border policy. Bring in all kinds of people, work for $2 or $3 an hour, that would be great for them. I don't believe in that. I think we have to raise wages in this country, I think we have to do everything we can to create millions of jobs.\n\nApril 2018, Trump calls for National Guard at the border to secure the ongoing attempts at a border wall along the United States-Mexico border. According to the LAtimes, \"Defense Secretary James N. Mattis has signed an order to send up to 4,000 National Guard troops to the U.S.-Mexico border but barred them from interacting with migrants detained by the Border Patrol in most circumstances\".\n\nThe caravan of migrants from Central America have reached the United States to seek asylum. The last of the caravan have arrived and are processing as of May 4, 2018. Remarks by Attorney General Sessions have expressed hesitation with asylum seekers. Sessions has stated, \"The system is being gamed, there's no doubt about it\". This statement implied asylum seekers were attempting to immigrate to the United States for work or various other reasons rather than seeking refuge.\n\nThe issue of the health of immigrants and the associated cost to the public has been largely discussed. On average, per capita health care spending is lower for immigrants than it is for native-born Americans. The non-emergency use of emergency rooms ostensibly indicates an incapacity to pay, yet some studies allege disproportionately lower access to unpaid health care by immigrants. For this and other reasons, there have been various disputes about how much immigration is costing the United States public health system. University of Maryland economist and Cato Institute scholar Julian Lincoln Simon concluded in 1995 that while immigrants probably pay more into the health system than they take out, this is not the case for elderly immigrants and refugees, who are more dependent on public services for survival. Immigration itself may impact women's health. A 2017 study found that Latino women suffer higher rates of intimate partner violence (IPV) than native US women. Migration may worsen IPV rates and outcomes. Migration itself may not cause IPV, but it may make it more difficult for women to get help. According to Kim et al., the IPV is usually the result of unequal family structures rather than the process of migration.\n\nImmigration from areas of high incidences of disease is thought to have fueled the resurgence of tuberculosis (TB), chagas, and hepatitis in areas of low incidence. According to Centers for Disease Control and Prevention (CDC), TB cases among foreign-born individuals remain disproportionately high, at nearly nine times the rate of U.S.-born persons. To reduce the risk of diseases in low-incidence areas, the main countermeasure has been the screening of immigrants on arrival. HIV/AIDS entered the United States in around 1969, likely through a single infected immigrant from\nHaiti. Conversely, many new HIV infections in Mexico can be traced back to the United States. People infected with HIV were banned from entering the United States in 1987 by executive order, but the 1993 statute supporting the ban was lifted in 2009. The executive branch is expected to administratively remove HIV from the list of infectious diseases barring immigration, but immigrants generally would need to show that they would not be a burden on public welfare. Researchers have also found what is known as the \"healthy immigrant effect\", in which immigrants in general tend to be healthier than individuals born in the U.S. Immigrants are more likely than native-born Americans to have a medical visit labeled uncompensated care.\n\nThere is no empirical evidence that either legal or illegal immigration increases crime in the United States. In fact, a majority of studies in the U.S. have found lower crime rates among immigrants than among non-immigrants, and that higher concentrations of immigrants are associated with lower crime rates. Explanations proposed to account for this relationship have included ethnic enclaves, self-selection, and the hypothesis that immigrants revitalize communities to which they emigrate.\n\nSome research even suggests that increases in immigration may partly explain the reduction in the U.S. crime rate. A 2005 study showed that immigration to large U.S. metropolitan areas does not increase, and in some cases decreases, crime rates there. A 2009 study found that recent immigration was not associated with homicide in Austin, Texas. The low crime rates of immigrants to the United States despite having lower levels of education, lower levels of income and residing in urban areas (factors that should lead to higher crime rates) may be due to lower rates of antisocial behavior among immigrants. A 2015 study found that Mexican immigration to the United States was associated with an increase in aggravated assaults and a decrease in property crimes. A 2016 study finds no link between immigrant populations and violent crime, although there is a small but significant association between undocumented immigrants and drug-related crime.\n\nA 2018 study found that undocumented immigration to the United States did not increase violent crime. Research finds that Secure Communities, an immigration enforcement program which led to a quarter of a million of detentions (when the study was published; November 2014), had no observable impact on the crime rate. A 2015 study found that the 1986 Immigration Reform and Control Act, which legalized almost 3 million immigrants, led to \"decreases in crime of 3–5 percent, primarily due to decline in property crimes, equivalent to 120,000-180,000 fewer violent and property crimes committed each year due to legalization\".\n\nAccording to one study, sanctuary cities—which adopt policies designed to not prosecute people solely for being an illegal immigrant—have no statistically meaningful effect on crime.\n\nOne of the first political analyses in the U.S. of the relationship between immigration and crime was performed in the beginning of the 20th century by the Dillingham Commission, which found a relationship especially for immigrants from non-Northern European countries, resulting in the sweeping 1920s immigration reduction acts, including the Emergency Quota Act of 1921, which favored immigration from northern and western Europe. Recent research is skeptical of the conclusion drawn by the Dillingham Commission. One study finds that \"major government commissions on immigration and crime in the early twentieth century relied on evidence that suffered from aggregation bias and the absence of accurate population data, which led them to present partial and sometimes misleading views of the immigrant-native criminality comparison. With improved data and methods, we find that in 1904, prison commitment rates for more serious crimes were quite similar by nativity for all ages except ages 18 and 19, for which the commitment rate for immigrants was higher than for the native-born. By 1930, immigrants were less likely than natives to be committed to prisons at all ages 20 and older, but this advantage disappears when one looks at commitments for violent offenses.\"\n\nFor the early twentieth century, one study found that immigrants had \"quite similar\" imprisonment rates for major crimes as natives in 1904 but lower for major crimes (except violent offenses; the rate was similar) in 1930. Contemporary commissions used dubious data and interpreted it in questionable ways.\n\nResearch suggests that police practices, such as racial profiling, over-policing in areas populated by minorities and in-group bias may result in disproportionately high numbers of immigrants among crime suspects. Research also suggests that there may be possible discrimination by the judicial system, which contributes to a higher number of convictions for immigrants.\n\nCrimmigration has emerged as a field in which critical immigration scholars conceptualize the current immigration law enforcement system. Crimmigration is broadly defined as the convergence of the criminal justice system and immigration enforcement, where immigration law enforcement has adopted the \"criminal\" law enforcement approach. This frames undocumented immigrants as \"criminal\" deviants and security risks. Crime and migration control have become completely intertwined, so much so that both undocumented and documented individuals suspected of being a noncitizen may be targeted.\n\nUsing a \"crimmigration\" point of thought, César Cuauhtémoc García Hernández explains the criminalization of undocumented immigrants began in the aftermath of the civil rights movement. Michelle Alexander explores how the U.S. criminal justice system is made of \"colorblind\" policies and law enforcement practices that have shaped the mass incarceration of people of color into an era of \"The New Jim Crow\" As Alexander and García Hernández state, overt racism and racist laws became culturally scorned, and covert racism became the norm. This new form of racism focuses on penalizing criminal activity and promoting \"neutral\" rhetoric.\n\n\"Crimmigration\" recognizes how laws and policies throughout different states contribute to the convergence of criminal law enforcement and immigration law. For example, states are implementing a variety of immigration related criminal offenses that are punishable by imprisonment. California, Oregon, and Wyoming criminalize the use of fraudulent immigration or citizenship documents. Arizona allows judges to confine witnesses in certain \"criminal\" cases if they are suspected of being in the U.S. without documentation. The most common violations of immigration law on the federal level are unauthorized entry (a federal misdemeanor) and unauthorized reentry (a federal felony). These \"offenses\" deemed as \"crimes\" under immigration law set the tone of \"crimmigration\" and for what García Hernández refers to as the \"removal pipeline\" of immigrants.\n\nSome scholars focus on the organization of \"crimmigration\" as it relates to the mass removal of certain immigrants. Jennifer Chacón finds that immigration law enforcement is being decentralized. Customs and Border Patrol (CBP), Immigration and Customs Enforcement (ICE), and the Department of Homeland Security (DHS) are the central law enforcement agencies in control of enforcing immigration law. However, other federal, state and local law enforcement agencies, such as sheriff's offices, municipal police departments, the Federal Bureau of Investigation (FBI), and the Drug and Enforcement Agency (DEA), aid in immigrant removal. In 1996, Congress expanded power to state and local law enforcement agencies to enforce federal immigration law. These agencies keep people locked up in jails or prison when they receive an \"immigration detainer\" from ICE, and therefore aid in interior enforcement. In addition, some agencies participate in the State Criminal Alien Assistance Program (\"SCAAP\"), which gives these agencies financial incentives to cooperate with ICE in identifying immigrants in their custody.\n\nScientific laboratories and startup internet opportunities have been a powerful American magnet. By 2000, 23% of scientists with a PhD in the U.S. were immigrants, including 40% of those in engineering and computers. Roughly a third of the United States' college and universities graduate students in STEM fields are foreign nationals—in some states it is well over half of their graduate students. On Ash Wednesday, March 5, 2014, the presidents of 28 Catholic and Jesuit colleges and universities, joined the \"Fast for Families\" movement. The \"Fast for Families\" movement reignited the immigration debate in the fall of 2013 when the movement's leaders, supported by many members of Congress and the President, fasted for twenty-two days on the National Mall in Washington, D.C.\n\nA study on public schools in California found that white enrollment declined in response to increases in the number of Spanish-speaking Limited English Proficient and Hispanic students. This white flight was greater for schools with relatively larger proportions of Spanish-speaking Limited English Proficient.\n\nA North Carolina study found that the presence of Latin American children in schools had no significant negative effects on peers, but that students with limited English skills had slight negative effects on peers.\n\nIn the United States, a significant proportion of scientists and engineers are foreign-born, as well as students in science and engineering programs. However, this is not unique to the US since foreigners make up significant amounts of scientists and engineers in other countries. As of 2011, 28% of graduate students in science, engineering, and health are foreign. The number of science and engineering (S&E) bachelor's degrees has risen steadily over the past 15 years, reaching a new peak of about half a million in 2009. Since 2000, foreign born students in the United States have consistently earned a small share (3%-4%) of S&E degrees at the bachelor's level. Foreign students make up a much higher proportion of S&E master's degree recipients than of bachelor's or associate degree recipients. In 2009, foreign students earned 27% of S&E master's degrees and 33% in doctorate degrees. Significant numbers of foreign born students in science and engineering are not unique to America since foreign students now account for nearly 60% of graduate students in mathematics, computer sciences, and engineering globally. In Switzerland and the United Kingdom, more than 40% of doctoral students are foreign. A number of other countries, including New Zealand, Austria, Australia, Belgium, Canada, and the United States, have relatively high percentages (more than 20%) of doctoral students who are foreign. Foreign student enrollment in the United Kingdom has been increasing. In 2008, foreign students made up 47% of all graduate students studying S&E in the United Kingdom (an increase from 32% in 1998). Top destinations for international students include the United Kingdom (12%), Germany (9%), and France (9%). Together with the U.S., these countries receive more than half of all internationally mobile students worldwide. Although the United States continues to attract the largest number and fraction of foreign students worldwide, its share of foreign students has decreased in recent years.\n\n55% of Ph.D. students in engineering in the United States are foreign born (2004). Between 1980 and 2000, the percentage of Ph.D. scientists and engineers employed in the United States who were born abroad has increased from 24% to 37%. 45% of Ph.D. physicists working in the United States are foreign born in 2004. 80% of total post-doctoral chemical and materials engineering in the United States are foreign-born (1988).\n\nAt the undergraduate level, US-born engineering students constitute upwards of 90-95% of the student population (most foreign born candidates for engineering graduate schools are trained in their home countries). However, the pool of BS engineering graduates with US citizenship is much larger than the number who apply to engineering graduate schools. The proportion of foreign-born engineers among assistant professors younger than 35 years has increased from 10% in 1972 to 50%-55% in 1983-1985, illustrating a dramatic increase on US dependence on foreign-born students in the US college system. The increase in non-citizen assistant professors of engineering is the result of the fact that, in recent years, foreign-born engineers received close to 50 percent of newly awarded engineering doctorates (naturalized citizens accounted for about 4 percent) and, furthermore, they entered academe in disproportionately large numbers. 33% of all U.S. Ph.D.s in science and engineering are now awarded to foreign born graduate students as of 2004.\n\nIn 1982, foreign-born engineers constituted about 3.6% of all engineers employed in the United States, 13.9% of which were naturalized; and foreign-born Phds in Engineering constituted 15% and 20% were naturalized. In 1985, foreign-born Phds represented almost 33% of the engineering post-doctorate researchers in US universities. Foreign-born Phd engineers often accept postdoctoral position because other employment is unavailable until green card is obtained. A system that further incentivising replacement of US-citizens in the upper echelons of academic and private sector engineering firms due to higher educational attainment relative to native-born engineer who for the most part do train beyond undergraduate level.\n\nIn recent years, the number of applicants for faculty openings at research universities have increased dramatically. Numbers of 50 to 200 applications for a single faculty opening have become typical, yet even with such high numbers of applicants, the foreign-born component is in excess of 50%. 60% of the top science students and 65 percent of the top math students in the United States are the children of immigrants. In addition, foreign-born high school students make up 50 percent of the 2004 U.S.Math Olympiad's top scorers, 38 percent of the U.S. Physics Team, and 25 percent of the Intel Science Talent Search finalists—the United States' most prestigious awards for young scientists and mathematicians.\n\nAmong 1985 foreign-born engineering doctorate holders, about 40% expected to work in the United States after graduating. An additional 17 percent planned to stay on as post-doctorates, and most of these are likely to remain permanently in the United States. Thus, almost 60% of foreign-born engineering doctorate holders are likely to become part of the US engineering labor force within a few years after graduating. The other approximately 40% of foreign born engineering PhDs mostly likely find employment working for multinational corporations outside of the US.\n\nIn the 2004 Intel Science Talent Search, more children (18) have parents who entered the country on H-1B (professional) visas than parents born in the United States (16). To place this finding in perspective, note that new H-1B visa holders each year represent less than 0.04 percent of the U.S. population. Foreign born faculty now account for over 50% of faculty in engineering (1994).\n\n27 out the 87 (more than 30%) American Nobel Prize winners in Medicine and Physiology between 1901 and 2005 were born outside the US.\n\n1993 median salaries of U.S. recipients of Ph.D.s in Science and Engineering foreign-born vs. native-born were as follows:\n!Years Since Earning Degree\n!Foreign-Born\n!Native-Born\n\nMajor American corporations spent $345 million lobbying for just three pro-immigration bills between 2006 and 2008.\n\nThe two most prominent groups lobbying for more restrictive immigration policies for the United States are NumbersUSA and the Federation for American Immigration Reform (FAIR); additionally, the Center for Immigration Studies think tank produces policy analysis supportive of a more restrictive stance.\n\nThe ambivalent feeling of Americans toward immigrants is shown by a positive attitude toward groups that have been visible for a century or more, and much more negative attitude toward recent arrivals. For example, a 1982 national poll by the Roper Center at the University of Connecticut showed respondents a card listing a number of groups and asked, \"Thinking both of what they have contributed to this country and have gotten from this country, for each one tell me whether you think, on balance, they've been a good or a bad thing for this country,\" which produced the results shown in the table. \"By high margins, Americans are telling pollsters it was a very good thing that Poles, Italians, and Jews immigrated to America. Once again, it's the newcomers who are viewed with suspicion. This time, it's the Mexicans, the Filipinos, and the people from the Caribbean who make Americans nervous.\"\n\nIn a 2002 study, which took place soon after the September 11 attacks, 55% of Americans favored decreasing legal immigration, 27% favored keeping it at the same level, and 15% favored increasing it.\n\nIn 2006, the immigration-reduction advocacy think tank the Center for Immigration Studies released a poll that found that 68% of Americans think U.S. immigration levels are too high, and just 2% said they are too low. They also found that 70% said they are less likely to vote for candidates that favor increasing legal immigration. In 2004, 55% of Americans believed legal immigration should remain at the current level or increased and 41% said it should be decreased. The less contact a native-born American has with immigrants, the more likely one would have a negative view of immigrants.\n\nOne of the most important factors regarding public opinion about immigration is the level of unemployment; anti-immigrant sentiment is where unemployment is highest, and vice versa.\n\nSurveys indicate that the U.S. public consistently makes a sharp distinction between legal and illegal immigrants, and generally views those perceived as \"playing by the rules\" with more sympathy than immigrants that have entered the country illegally.\n\nAccording to a Gallup poll in July 2015, immigration is the fourth most important problem facing the United States and seven percent of Americans said it was the most important problem facing America today. In March 2015, another Gallup poll provided insight into American public opinion on immigration; the poll revealed that 39% of people worried about immigration \"a great deal.\" A January poll showed that only 33% of Americans were satisfied with the current state of immigration in America. As an issue that is very important to Americans, polling reveals change in sentiment over time and diverse opinions regarding how to handle immigration.\n\nBefore 2012, majority of Americans supported securing United States borders compared to dealing with illegal immigrants in the United States. In 2013, that trend has reversed and 55% of people polled by Gallup revealed that they would choose \"developing a plan to deal with immigrants who are currently in the U.S. illegally.\" Changes regarding border control are consistent across party lines, with Republicans saying that \"securing U.S. borders to halt flow of illegal immigrants\" is extremely important decreasing from 68% in 2011 to 56% in 2014. Meanwhile, Democrats who chose extremely important shifted from 42% in 2011 to 31% in 2014. In July 2013, 87% of Americans said they would vote in support of a law that would \"allow immigrants already in the country to become U.S. citizens if they meet certain requirements including paying taxes, having a criminal background check and learning English.\" However, in the same survey, 83% also said they would support the tightening of U.S. border security.\n\nDonald Trump's campaign for Presidency focused on a rhetoric of reducing illegal immigration and toughening border security. In July 2015, 48% of Americans thought that Donald Trump would do a poor job of handling immigration problems. In November 2016, 55% of Trump's voters thought that he would do the right thing in regards to illegal immigration. In general, Trump supporters are not united upon how to handle immigration. In December 2016, Trump voters were polled and 60% said that \"undocumented immigrants in the U.S. who meet certain requirements should be allowed to stay legally.\"\n\nAmerican opinion regarding how immigrants affect the country and how the government should respond to illegal immigration have changed over time. In 2006, out of all U.S. adults surveyed, 28% declared that they believed the growing number of immigrants helped American workers and 55% believed that it hurt American workers. In 2016, those views had changed, with 42% believing that they helped and 45% believing that they hurt. The PRRI 2015 American Values Atlas showed that between 46% and 53% of Americans believed that \"the growing number of newcomers from other countries ... strengthens American society.\" In the same year, 57% and 66% of Americans chose that the U.S. should \"allow [immigrants living in the U.S. illegally] a way to become citizens provided they meet certain requirements.\" \n\nIn February 2017, the American Enterprise Institute released a report on recent surveys about immigration issues. In July 2016, 63% of Americans favored the temporary bans of immigrants from areas with high levels of terrorism and 53% said the U.S. should allow fewer refugees to enter the country. In November 2016, 55% of Americans were opposed to building a border wall with Mexico. Since 1994, Pew Research center has tracked a change from 63% of Americans saying that immigrants are a burden on the country to 27%.\n\nPublic response to the Trump Administration’s ‘zero tolerance’ policy: What really stood out to the public in regards to the issue were the ways in which children were treated during their detainment. For example in Southern Texas, children were held in an old warehouse. Hundreds of children waited in cages made of metal fencing. One of the cages had 20 children inside, looking throughout the facility bottles of water and bags of chips can be found scattered about. Children were also forced to use large foil sheets as blankets . This created massive amounts of outrage, the campaign ‘Close the Camps’ was created and the facilities that were detaining young children were compared to concentration camps and internment camps. Many have begun to recognize the ways in which these policies are negatively impacting the children that have been separated from their families. There has been research showing that this has had psychological impacts on these young children, many of them have been diagnosed with post-traumatic stress disorder, these children are still distraught from the stressors of living their home countries and from the journey. Due to this, when these children are separated they showed more feelings of fear, abandonment and post-traumatic stress symptoms than children who were not separated from their families . \n\nReligious figures in the United States have put forth their views on the topic of immigration as informed by their religious traditions.\n\nBULLET::::- Catholicism – In 2018, Catholic leaders stated that asylum-limiting laws proposed by the Trump administration were immoral. Some bishops considered imposing sanctions (known as \"canonical penalties\") on church members who have participated in enforcing such policies.\nBULLET::::- Judaism – American Jewish rabbis from various denominations have stated that their understanding of Judaism is that immigrants and refugees should be welcomed, and even assisted. The exception would be if there is significant economic hardship or security issues faced by the host country or community, in which case immigration may be limited, discouraged or even prohibited altogether. Some liberal denominations place greater emphasis on the welcoming of immigrants while Conservative, Orthodox and Independent rabbis also weigh economic and security concerns. Some provide moral arguments for both the right of country to enforce immigration standards as well as for providing some sort of amnesty for illegal migrants.\n\nLaws concerning immigration and naturalization include:\nBULLET::::- the Immigration Act of 1990 (IMMACT), which limits the annual number of immigrants to 700,000. It emphasizes that family reunification is the main immigration criterion, in addition to employment-related immigration.\nBULLET::::- the Antiterrorism and Effective Death Penalty Act (AEDPA)\nBULLET::::- the Illegal Immigration Reform and Immigrant Responsibility Act (IIRIRA)\n\nAEDPA and IIRARA exemplify many categories of criminal activity for which immigrants, including green card holders, can be deported and have imposed mandatory detention for certain types of cases.\n\nIn contrast to economic migrants, who generally do not gain legal admission, refugees, as defined by international law, can gain legal status through a process of seeking and receiving asylum, either by being designated a refugee while abroad, or by physically entering the United States and requesting asylum status thereafter. A specified number of legally defined refugees, who either apply for asylum overseas or after arriving in the U.S., are admitted annually. Refugees compose about one-tenth of the total annual immigration to the United States, though some large refugee populations are very prominent. In the year 2014, the number of asylum seekers accepted into the U.S. was about 120,000. This compared with about 31,000 in the UK and 13,500 in Canada. Japan accepted just 41 refugees for resettlement in 2007.\n\nSince 1975, more than 1.3 million refugees from Asia have been resettled in the United States. Since 2000 the main refugee-sending regions have been Somalia, Liberia, Sudan, and Ethiopia. The ceiling for refugee resettlement for fiscal year 2008 was 80,000 refugees. The United States expected to admit a minimum of 17,000 Iraqi refugees during fiscal year 2009. The U.S. has resettled more than 42,000 Bhutanese refugees from Nepal since 2008.\n\nIn fiscal year 2008, the Office of Refugee Resettlement (ORR) appropriated over $655 million for long-term services provided to refugees after their arrival in the US. The Obama administration has kept to about the same level.\n\nA common problem in the current system for asylum seekers is the lack of resources. Asylum offices in the United States receive more applications for asylum than they can process every month and every year. These continuous applications pile onto the backlog.\n\nIn removal proceedings in front of an immigration judge, cancellation of removal is a form of relief that is available for certain long-time residents of the United States. It allows a person being faced with the threat of removal to obtain permanent residence if that person has been physically present in the U.S. for at least ten years, has had good moral character during that period, has not been convicted of certain crimes, and can show that removal would result in exceptional and extremely unusual hardship to his or her U.S. citizen or permanent resident spouse, children, or parent. This form of relief is only available when a person is served with a Notice to Appear to appear in the proceedings in the court.\n\nMembers of Congress may submit private bills granting residency to specific named individuals. A special committee vets the requests, which require extensive documentation. The Central Intelligence Agency has the statutory authority to admit up to one hundred people a year outside of normal immigration procedures, and to provide for their settlement and support. The program is called \"PL110\", named after the legislation that created the agency, Public Law 110, the Central Intelligence Agency Act.\n\nThe illegal immigrant population of the United States is estimated to be between 11 and 12 million. The population of unauthorized immigrants peaked in 2007 and has declined since that time. The majority of the U.S. unauthorized immigrants are from Mexico, but \"their numbers (and share of the total) have been declining\" and as of 2016 Mexicans no longer make up a clear majority of unauthorized immigrants, as they did in the past. Unauthorized immigrants made up about 5% of the total U.S. civilian labor force in 2014. By the 2010s, an increasing share of U.S. unauthorized immigrants were long-term residents; in 2015, 66% of adult unauthorized residents had lived in the country for at least ten years, while only 14% had lived in the U.S. for less than five years.\n\nIn June 2012, President Obama issued a memorandum instructing officers of the federal government to defer deporting young undocumented immigrants who were brought to the U.S. as children as part of the Deferred Action for Childhood Arrivals (DACA) program. Under the program, eligible recipients who applied and were granted DACA status were granted a two-year deferral from deportation and temporary eligibility to work legally in the country. Among other criteria, in order to be eligible a youth applicant must (1) be between age 15 and 31; (2) have come to the United States before the age of 16; (3) have lived in the U.S. continuously for at least five years; (4) be a current student, or have earned a high school diploma or equivalent, or have received an honorable discharge from the U.S. armed services; and (5) must not \"have not been convicted of a felony, significant misdemeanor, or three or more misdemeanors, and do not otherwise pose a threat to public safety or national security.\" The Migration Policy Institution estimated that as of 2016, about 1.3 million unauthorized young adults ages 15 and older were \"immediately eligible for DACA\"; of this eligible population, 63% had applied as of March 2016.\n\nChildren of legal migrants will not qualify as Dreamers under DACA protection because they entered the country legally. This is highlighted as the biggest contradiction in US immigration policy by many advocates of legal immigrants.\n\nIn 2014, President Obama announced a set of executive actions, the Deferred Action for Parents of Americans and Lawful Permanent Residents. Under this program, \"unauthorized immigrants who are parents of U.S. citizens or lawful permanent residents (LPRs) would qualify for deferred action for three years if they meet certain other requirements.\" A February 2016 Migration Policy Institute/Urban Institute report found that about 3.6 million people were potentially eligible for DAPA and \"more than 10 million people live in households with at least one potentially DAPA-eligible adult, including some 4.3 million children under age 18 - an estimated 85 percent of whom are U.S. citizens.\" The report also found that \"the potentially DAPA eligible are well settled with strong U.S. roots, with 69 percent having lived in the United States ten years or more, and 25 percent at least 20 years.\"\n\nAlthough not without precedent under prior presidents, President Obama's authority to create DAPA and expand DACA were challenged in the federal courts by Texas and 25 other states. In November 2015, the U.S. Court of Appeals for the Fifth Circuit, in a 2-1 decision in \"United States v. Texas\", upheld a preliminary injunction blocking the programs from going forward. The case was heard by the U.S. Supreme Court, which in June 2016 deadlocked 4-4, thus affirming the ruling of the Fifth Circuit but setting no nationally binding precedent.\n\nOn November 15, 2013, the United States Citizenship and Immigration Services announced that they would be issuing a new policy memorandum called \"parole in place.\" Parole in place would offer green cards to immigrant parents, spouses and children of active military duty personnel. Prior to this law relatives of military personnel – excluding husbands and wives – were forced to leave the United States and apply for green cards in their home countries. The law allows for family members to avoid the possible ten-year bar from the United States and remain in the United States while applying for lawful permanent residence. The parole status, given in one year terms, will be subject to the family member being \"absent a criminal conviction or other serious adverse factors.\"\n\nMilitary children born in foreign countries are considered American from birth assuming both parents were American citizens at the time of birth. Children born to American citizens will have to process Conciliary Reports of Birth Abroad. This report of birth abroad is the equivalent of a birth certificate and the child will use the report in place of a Birth Certificate for documentation. However, children born in foreign countries to United States servicemembers before they have gained citizenship could only gain citizenship through the naturalization process.\n\nMost immigration proceedings are civil matters, including deportation proceedings, asylum cases, employment without authorization, and visa overstay. People who evade border enforcement (such as by crossing outside any official border checkpoint), who commit fraud to gain entry, or who commit identity theft to gain employment, may face criminal charges. People entering illegally were seldom charged with this crime until Operation Streamline in 2005. Conviction of this crime generally leads to a prison term, after which the person is deported if they are not eligible to remain in the country.\n\nThe guarantees under the Sixth Amendment to the United States Constitution, such as the right to counsel, and the right to a jury trial, have not been held to apply to civil immigration proceedings. As a result, people generally represent themselves in asylum and deportation cases unless they can afford an immigration lawyer or receive assistance from a legal charity. In contrast, the Due Process Clause of the Fifth Amendment \"has\" been applied to immigration proceedings. Because the right to confrontation in the Sixth Amendment does not apply, people can be ordered deported \"in absentia\" - without being present at the immigration proceeding.\n\nRemoval proceedings are considered administrative proceedings under the authority of the United States Attorney General, acting through the Executive Office for Immigration Review, part of the Justice Department. Immigration judges are employees of the Justice Department, and thus part of the executive branch rather than the judicial branch of government. Appeals are heard within the EOIR by the Board of Immigration Appeals, and the Attorney General may intervene in individual cases, within the bounds of due process.\n\nAfter various actions by Attorney General Jeff Sessions pressuring judges to speed up deportations, the National Association of Immigration Judges and \"The Boston Globe\" editorial board called for moving immigration courts to the judicial branch, to prevent abuse by strengthening separation of powers.\n\nWhether people who are awaiting a decision on their deportation are detained or released to live in the United States in the meantime (possibly paying bail) is a matter of both law and discretion of the Justice Department. The policy has varied over time and differs for those with crimes (including entry outside an official checkpoint) versus civil infractions.\n\nThe 2001 Supreme Court case \"Zadvydas v. Davis\" held that immigrants who cannot be deported because no country will accept them cannot be detained indefinitely.\n\nThe history of immigration to the United States is the history of the country itself, and the journey from beyond the sea is an element found in American folklore, appearing over and over again in everything from \"The Godfather\" to \"Gangs of New York\" to \"The Song of Myself\" to Neil Diamond's \"America\" to the animated feature \"An American Tail\".\n\nFrom the 1880s to the 1910s, vaudeville dominated the popular image of immigrants, with very popular caricature portrayals of ethnic groups. The specific features of these caricatures became widely accepted as accurate portrayals.\n\nIn \"The Melting Pot\" (1908), playwright Israel Zangwill (1864–1926) explored issues that dominated Progressive Era debates about immigration policies. Zangwill's theme of the positive benefits of the American melting pot resonated widely in popular culture and literary and academic circles in the 20th century; his cultural symbolism – in which he situated immigration issues – likewise informed American cultural imagining of immigrants for decades, as exemplified by Hollywood films.\nThe popular culture's image of ethnic celebrities often includes stereotypes about immigrant groups. For example, Frank Sinatra's public image as a superstar contained important elements of the \"American Dream\" while simultaneously incorporating stereotypes about Italian Americans that were based in nativist and Progressive responses to immigration.\n\nThe process of assimilation has been a common theme of popular culture. For example, \"lace-curtain Irish\" refers to middle-class Irish Americans desiring assimilation into mainstream society in counterpoint to the older, more raffish \"shanty Irish\". The occasional malapropisms and left-footed social blunders of these upward mobiles were gleefully lampooned in vaudeville, popular song, and the comic strips of the day such as \"Bringing Up Father\", starring Maggie and Jiggs, which ran in daily newspapers for 87 years (1913 to 2000). In \"The Departed\" (2006), Staff Sergeant Dignam regularly points out the dichotomy between the lace curtain Irish lifestyle Billy Costigan enjoyed with his mother, and the shanty Irish lifestyle of Costigan's father. In recent years the popular culture has paid special attention to Mexican immigration and the film \"Spanglish\" (2004) tells of a friendship of a Mexican housemaid (Paz Vega) and her boss played by Adam Sandler.\n\nNovelists and writers have captured much of the color and challenge in their immigrant lives through their writings.\n\nRegarding Irish women in the 19th century, there were numerous novels and short stories by Harvey O'Higgins, Peter McCorry, Bernard O'Reilly and Sarah Orne Jewett that emphasize emancipation from Old World controls, new opportunities and expansiveness of the immigrant experience.\n\nOn the other hand, Hladnik studies three popular novels of the late 19th century that warned Slovenes not to immigrate to the dangerous new world of the United States.\n\nJewish American writer Anzia Yezierska wrote her novel \"Bread Givers\" (1925) to explore such themes as Russian-Jewish immigration in the early 20th century, the tension between Old and New World Yiddish culture, and women's experience of immigration. A well established author Yezierska focused on the Jewish struggle to escape the ghetto and enter middle- and upper-class America. In the novel, the heroine, Sara Smolinsky, escape from New York City's \"down-town ghetto\" by breaking tradition. She quits her job at the family store and soon becomes engaged to a rich real-estate magnate. She graduates college and takes a high-prestige job teaching public school. Finally Sara restores her broken links to family and religion.\nThe Swedish author Vilhelm Moberg in the mid-20th century wrote a series of four novels describing one Swedish family's migration from Småland to Minnesota in the late 19th century, a destiny shared by almost one million people. The author emphasizes the authenticity of the experiences as depicted (although he did change names). These novels have been translated into English (\"The Emigrants\", 1951, \"Unto a Good Land\", 1954, \"The Settlers\", 1961, \"The Last Letter Home\", 1961). The musical Kristina från Duvemåla by ex-ABBA members Björn Ulvaeus and Benny Andersson is based on this story.\n\n\"The Immigrant\" is a musical by Steven Alper, Sarah Knapp, and Mark Harelik. The show is based on the story of Harelik's grandparents, Matleh and Haskell Harelik, who traveled to Galveston, Texas in 1909.\n\nIn their documentary \"\", filmmakers Shari Robertson and Michael Camerini examine the American political system through the lens of immigration reform from 2001 to 2007. Since the debut of the first five films, the series has become an important resource for advocates, policy-makers and educators.\n\nThat film series premiered nearly a decade after the filmmakers' landmark documentary film \"Well-Founded Fear\" which provided a behind-the-scenes look at the process for seeking asylum in the United States. That film still marks the only time that a film-crew was privy to the private proceedings at the U.S. Immigration and Naturalization Service (INS), where individual asylum officers ponder the often life-or-death fate of immigrants seeking asylum.\n\nUniversity of North Carolina law professor Hiroshi Motomura has identified three approaches the United States has taken to the legal status of immigrants in his book \"Americans in Waiting: The Lost Story of Immigration and Citizenship in the United States\". The first, dominant in the 19th century, treated immigrants as in transition; in other words, as prospective citizens. As soon as people declared their intention to become citizens, they received multiple low-cost benefits, including the eligibility for free homesteads in the Homestead Act of 1869, and in many states, the right to vote. The goal was to make the country more attractive, so large numbers of farmers and skilled craftsmen would settle new lands. By the 1880s, a second approach took over, treating newcomers as \"immigrants by contract\". An implicit deal existed where immigrants who were literate and could earn their own living were permitted in restricted numbers. Once in the United States, they would have limited legal rights, but were not allowed to vote until they became citizens, and would not be eligible for the New Deal government benefits available in the 1930s. The third and more recent policy is \"immigration by affiliation\", which Motomura argues is the treatment which depends on how deeply rooted people have become in the country. An immigrant who applies for citizenship as soon as permitted, has a long history of working in the United States, and has significant family ties, is more deeply affiliated and can expect better treatment.\nThe American Dream is the belief that through hard work and determination, any United States immigrant can achieve a better life, usually in terms of financial prosperity and enhanced personal freedom of choice. According to historians, the rapid economic and industrial expansion of the U.S. is not simply a function of being a resource rich, hard working, and inventive country, but the belief that anybody could get a share of the country's wealth if he or she was willing to work hard. This dream has been a major factor in attracting immigrants to the United States.\n\nBULLET::::- Demographics of the United States\nBULLET::::- Emigration from the United States\nBULLET::::- European colonization of the Americas\nBULLET::::- History of laws concerning immigration and naturalization in the United States\nBULLET::::- Illegal immigration to the United States\nBULLET::::- Immigration policies of American labor unions\nBULLET::::- Inequality within immigrant families (United States)\nBULLET::::- Nativism (politics), opposition to immigration\nBULLET::::- Opposition to immigration\nBULLET::::- United States immigration statistics\nBULLET::::- Immigrant benefits urban legend, a hoax regarding benefits comparison\n\nBULLET::::- Anbinder, Tyler. \"City of Dreams: The 400-Year Epic History of Immigrant New York\" (Houghton Mifflin Harcourt, 2016). 766 pp.\nBULLET::::- Archdeacon, Thomas J. \"Becoming American: An Ethnic History\" (1984)\nBULLET::::- Bankston, Carl L. III and Danielle Antoinette Hidalgo, eds. \"Immigration in U.S. History\" Salem Press, (2006)\nBULLET::::- short scholarly biographies With bibliographies; 448 pp.\nBULLET::::- Bodnar, John. \"The Transplanted: A History of Immigrants in Urban America\" Indiana University Press, (1985)\nBULLET::::- Daniels, Roger. \"Asian America: Chinese and Japanese in the United States since 1850\" University of Washington Press, (1988)\nBULLET::::- Daniels, Roger. \"Coming to America\" 2nd ed. (2005)\nBULLET::::- Daniels, Roger. \"Guarding the Golden Door : American Immigration Policy and Immigrants since 1882\" (2005)\nBULLET::::- Diner, Hasia. \"The Jews of the United States, 1654 to 2000\" (2004)\nBULLET::::- Dinnerstein, Leonard, and David M. Reimers. \"Ethnic Americans: a history of immigration\" (1999) online\nBULLET::::- Gerber, David A. \"American Immigration: A Very Short Introduction\" (2011).\nBULLET::::- Gjerde, Jon, ed. \"Major Problems in American Immigration and Ethnic History\" (1998).\nBULLET::::- Glazier, Michael, ed. \"The Encyclopedia of the Irish in America\" (1999).\nBULLET::::- Jones, Maldwyn A. \"American immigration\" (1960) online\nBULLET::::- Joselit, Jenna Weissman. \"Immigration and American religion\" (2001) online\nBULLET::::- Parker, Kunal M. \"Making Foreigners: Immigration and Citizenship Law in America, 1600–2000.\" New York: Cambridge University Press, 2015.\nBULLET::::- Sowell, Thomas. \"Ethnic America: A History\" (1981).\nBULLET::::- Thernstrom, Stephan, ed. \"Harvard Encyclopedia of American Ethnic Groups\" (1980).\n\nBULLET::::- Alexander, June Granatir. \"Daily Life in Immigrant America, 1870–1920: How the Second Great Wave of Immigrants Made Their Way in America\" (Chicago: Ivan R. Dee, 2007. xvi, 332 pp.)\nBULLET::::- Berthoff, Rowland Tappan. \"British Immigrants in Industrial America, 1790–1950\" (1953).\nBULLET::::- Briggs, John. \"An Italian Passage: Immigrants to Three American Cities, 1890–1930\" Yale University Press, (1978)\nBULLET::::- Diner, Hasia. \"Hungering for America: Italian, Irish, and Jewish Foodways in the Age of Migration\" (2003)\nBULLET::::- Dudley, William, ed. \"Illegal immigration: opposing viewpoints\" (2002) online\nBULLET::::- Eltis, David; \"Coerced and Free Migration: Global Perspectives\" (2002) emphasis on migration to Americas before 1800\nBULLET::::- Greene, Victor R. \"A Singing Ambivalence: American Immigrants Between Old World and New, 1830–1930\" (2004), covering musical traditions\nBULLET::::- Isaac Aaronovich Hourwich. \"Immigration and Labor: The Economic Aspects of European Immigration to the United States\" (1912) (full text online)\nBULLET::::- Joseph, Samuel; \"Jewish Immigration to the United States from 1881 to 1910\" Columbia University Press, (1914)\nBULLET::::- Kulikoff, Allan; \"From British Peasants to Colonial American Farmers\" (2000), details on colonial immigration\nBULLET::::- Meagher, Timothy J. \"The Columbia Guide to Irish American History\". (2005)\nBULLET::::- Miller, Kerby M. \"Emigrants and Exiles\" (1985), influential scholarly interpretation of Irish immigration\nBULLET::::- Motomura, Hiroshi. \"Americans in Waiting: The Lost Story of Immigration and Citizenship in the United States\" (2006), legal history\nBULLET::::- Pochmann, Henry A. and Arthur R. Schultz; \"German Culture in America, 1600–1900: Philosophical and Literary Influences\" (1957)\nBULLET::::- Waters, Tony. \"Crime and Immigrant Youth\" Sage Publications (1999), a sociological analysis.\nBULLET::::- U.S. Immigration Commission, \"Abstracts of Reports,\" 2 vols. (1911); the full 42-volume report is summarized (with additional information) in Jeremiah W. Jenks and W. Jett Lauck, \"The Immigrant Problem\" (1912; 6th ed. 1926)\nBULLET::::- Wittke, Carl. \"We Who Built America: The Saga of the Immigrant\" (1939), covers all major groups\nBULLET::::- Yans-McLaughlin, Virginia ed. \"Immigration Reconsidered: History, Sociology, and Politics\" Oxford University Press. (1990)\n\nBULLET::::- Beasley, Vanessa B. ed. \"Who Belongs in America?: Presidents, Rhetoric, And Immigration\" (2006)\nBULLET::::- Bogen, Elizabeth. \"Immigration in New York\" (1987)\nBULLET::::- Bommes, Michael and Andrew Geddes. \"Immigration and Welfare: Challenging the Borders of the Welfare State\" (2000)\nBULLET::::- Borjas, George J. ed. \"Issues in the Economics of Immigration\" (National Bureau of Economic Research Conference Report) (2000).\nBULLET::::- Borjas, George. \"Friends or Strangers\" (1990)\nBULLET::::- Briggs, Vernon M., Jr. \"Immigration Policy and the America Labor Force.\" Baltimore, MD: Johns Hopkins University Press, 1984.\nBULLET::::- Briggs, Vernon M., Jr. \"Mass Immigration and the National Interest\" (1992)\nBULLET::::- Cooper, Mark A. \"Moving to the United States of America and Immigration.\" 2008.\nBULLET::::- Egendorf, Laura K., ed. \"Illegal immigration : an opposing viewpoints guide\" (2007) online\nBULLET::::- Fawcett, James T., and Benjamin V. Carino. \"Pacific Bridges: The New Immigration from Asia and the Pacific Islands\". New York: Center for Migration Studies, 1987.\nBULLET::::- Foner, Nancy. \"In A New Land: A Comparative View Of Immigration\" (2005)\nBULLET::::- Garland, Libby. \"After They Closed the Gate: Jewish Illegal Immigration to the United States, 1921–1965.\" Chicago: University of Chicago Press, 2014.\nBULLET::::- Levinson, David and Melvin Ember, eds. \"American Immigrant Cultures\" 2 vol (1997).\nBULLET::::- Lowe, Lisa. \"Immigrant Acts: On Asian American Cultural Politics\" (1996)\nBULLET::::- Meier, Matt S. and Gutierrez, Margo, eds. \"The Mexican American Experience : An Encyclopedia\" (2003) ()\nBULLET::::- Mohl, Raymond A. \"Latinization in the Heart of Dixie: Hispanics in Late-twentieth-century Alabama\" \"Alabama Review\" 2002 55(4): 243–74.\nBULLET::::- Portes, Alejandro, and Robert L. Bach. \"Latin Journey: Cuban and Mexican Immigrants in the United States.\" Berkeley, CA: University of California Press, 1985.\nBULLET::::- Portes, Alejandro, and József Böröcz. \"Contemporary Immigration: Theoretical Perspectives on Its Determinants and Modes of Incorporation.\" \"International Migration Review\" 23 (1989): 606–30.\nBULLET::::- Portes, Alejandro, and Rubén Rumbaut. \"Immigrant America.\" Berkeley, CA: University of California Press, 1990.\nBULLET::::- Reimers, David. \"Still the Golden Door: The Third World Comes to America.\" New York: Columbia University Press, (1985).\nBULLET::::- Smith, James P., and Barry Edmonston, eds. \"The Immigration Debate: Studies on the Economic, Demographic, and Fiscal Effects of Immigration\" (1998), online version\nBULLET::::- Waters, Tony. \"Crime and Immigrant Youth\" Thousand Oaks: Sage 1999.\nBULLET::::- Zhou, Min and Carl L. Bankston III. \"\" Russell Sage Foundation. (1998)\nBULLET::::- Borjas, George J. \"Heaven's Door: Immigration Policy and the American Economy\". Princeton, N.J.: Princeton University Press, 1999. xvii, 263 pp.\nBULLET::::- Lamm, Richard D., and Gary Imhoff. \"The Immigration Time Bomb: the Fragmenting of America\", in series, \"Truman Talley Books\". First ed. New York: E.P. Dutton, 1985. xiii, 271 pp.\n\nBULLET::::- Immigrant Servants Database\nBULLET::::- Asian-Nation: Early Asian Immigration to the U.S.\nBULLET::::- Irish Catholic Immigration to America\nBULLET::::- Scotch-Irish Immigration to Colonial America\nBULLET::::- Immigration Archives of Historical Documents, Articles, and Immigrants\nBULLET::::- Maurer, Elizabeth. \"New Beginnings: Immigrant Women and the American Experience\". National Women's History Museum. 2014.\n\nBULLET::::- Immigration policy reports from the Brookings Institution\nBULLET::::- Immigration policy reports from the Urban Institute\nBULLET::::- Permanent Legal Immigration to the United States: Policy Overview Congressional Research Service (May 2018)\nBULLET::::- A Primer on U.S. Immigration Policy Congressional Research Service (November 2017)\n\nBULLET::::- U.S. Citizenship and Immigration Services\nBULLET::::- U.S. Immigration and Customs Enforcement\nBULLET::::- Cornell University's Legal Information Institute: Immigration\nBULLET::::- Yearbook of Immigration Statistics – United States Department of Homeland Security, Office of Immigration Statistics 2004, 2005 editions available.\nBULLET::::- \"Estimates of the Unauthorized Immigrant Population Residing in the United States: January 2005\" M. Hoefer, N. Rytina, C. Campbell (2006) \"Population Estimates (August). U.S. Department of Homeland Security, Office of Immigration Statistics.\n"}
{"id": "15052", "url": "https://en.wikipedia.org/wiki?curid=15052", "title": "Image and Scanner Interface Specification", "text": "Image and Scanner Interface Specification\n\nImage and Scanner Interface Specification (ISIS) is an industry standard interface for image scanning technologies, developed by Pixel Translations in 1990 (which became EMC Corporation's Captiva Software and later acquired by OpenText).\n\nISIS is an open standard for scanner control and a complete image-processing framework. It is currently supported by a number of application and scanner vendors.\n\nThe modular design allows the scanner to be accessed both directly or with built-in routines to handle most situations automatically.\n\nA message-based interface with tags is used so that features, operations, and formats not yet supported by ISIS can be added as desired without waiting for a new version of the specification.\n\nThe standard addresses all of the issues that an application using a scanner needs to be concerned with. Functions include but are not limited to selecting, installing, and configuring a new scanner; setting scanner-specific parameters; scanning, reading and writing files, and fast image scaling, rotating, displaying, and printing. Drivers have been written to dynamically process data for operations such as converting grayscale to binary image data.\n\nAn ISIS interface can run scanners at or above their rated speed by linking drivers together in a pipe so that data flows from a scanner driver to compression driver, to packaging driver, to a file, viewer, or printer in a continuous stream, usually without the need to buffer more than a small portion of the full image. As a result of using the piping method, each driver can be optimised to perform one function well. Drivers are typically small and modular in order to make it simple to add new functionality to an existing application.\n\nBULLET::::- Scanner Access Now Easy\nBULLET::::- TWAIN\nBULLET::::- Windows Image Acquisition\n\nBULLET::::- EMC Captiva\nBULLET::::- Official portal for ISIS developers\n"}
{"id": "15053", "url": "https://en.wikipedia.org/wiki?curid=15053", "title": "Ivo Caprino", "text": "Ivo Caprino\n\nIvo Caprino (17 February 1920 – 8 February 2001) was a Norwegian film director and writer, best known for his puppet films. His most famous film is \"Flåklypa Grand Prix\" (\"Pinchcliffe Grand Prix\"), made in 1975.\n\nIn the mid-1940s, Caprino helped his mother design puppets for a puppet theatre, which inspired him to try making a film using his mother's designs. The result of their collaboration was \"Tim og Tøffe\", an 8-minute film released in 1949. \n\nSeveral films followed in the next couple of years, including two 15-minute shorts that are still shown regularly in Norway today, \"Veslefrikk med Fela\" (Little Freddy and his Fiddle), based on a Norwegian folk tale, and \"Karius og Baktus\", a story by Thorbjørn Egner of two little trolls, representing Caries and Bacterium, living in a boy's teeth. Ingeborg Gude made the puppets for these films as well, as she would continue to do up until her death in the mid sixties.\n\nWhen making \"Tim og Tøffe\", Caprino invented an ingenious method for controlling the puppet's movements in real time. The technique can be described as a primitive, mechanical version of animatronics.\n\nCaprino's films received rave reviews, and he quickly became a celebrity in Norway. In particular, the public were fascinated with the secret technology used to make his films. When he switched to traditional stop motion, Caprino tried to maintain the impression that he was still using some kind of \"magic\" technology to make the puppets move, even though all his later films were made with traditional stop motion techniques.\n\nIn addition to the short films, Caprino produced dozens of advertising films with puppets. In 1959, he directed a live action feature film, \"Ugler i Mosen\", which also contained stop motion sequences. He then embarked on his most ambitious project, a feature film about Peter Christen Asbjørnsen, who travelled around Norway in the 19th century collecting traditional folk tales. \n\nThe plan was to use live action for the sequences showing Asbjørnsen, and then to realise the folk tales using stop motion. Unfortunately, Caprino was unable to secure funding for the project, so he ended up making the planned folk tale sequences as separate 16-minute puppet films, bookended by live action sequences showing Asbjørnsen.\n\nIn 1970, Caprino and his small team of collaborators, started work on a 25 minutes TV special, which would eventually become \"The Pinchcliffe Grand Prix\". Based on a series of books by Norwegian cartoonist and author Kjell Aukrust, it featured a group of eccentric characters all living in the small village of Pinchcliffe. The TV special was a collection of sketches based on Aukrust's books, with no real story line. After 1.5 years of work, it was decided that it didn't really work as a whole, so production on the TV special was stopped (with the exception of some very short clips, no material from it has ever been seen by the public), and Caprino and Aukrust instead wrote a screenplay for a feature film using the characters and environments that had already been built.\n\nThe result was \"The Pinchcliffe Grand Prix\", which stars Theodore Rimspoke (No. Reodor Felgen) and his two assistants, Sonny Duckworth (No. Solan Gundersen), a cheerful and optimistic bird, and Lambert (No. Ludvig), a nervous, pessimistic and melancholic hedgehog. Theodore works as a bicycle repairman, though he spends most of his time inventing weird Rube Goldberg-like contraptions. One day, the trio discover that one of Theodore's former assistants, Rudolph Gore-Slimey (), has stolen his design for a race car engine, and has become a world champion Formula One driver.\n\nSonny secures funding from an Arab oil sheik who happens to be vacationing in Pinchcliffe, and the trio then build a gigantic racing car, \"Il Tempo Gigante\" – a fabulous construction with two engines, radar and its own blood bank. Theodore then enters a race, and ends up winning, beating Gore-Slimey despite his attempts at sabotage.\n\nThe film was made in 3.5 years by a team of approximately 5 people. Caprino directed and animated, Bjarne Sandemose (Caprino's principal collaborator throughout his career) built the sets and the cars, and was in charge of the technical side, Ingeborg Riiser modeled the puppets and Gerd Alfsen made the costumes and props.\n\nWhen it came out in 1975, The Pinchcliffe Grand Prix was an enormous success in Norway, selling 1 million tickets in its first year of release. It remains the biggest box office hit of all time in Norway (Caprino Studios claim it has sold 5.5 million tickets to date) and was also released in many other countries.\n\nTo help promote the film abroad, Caprino and Sandemose built a full-scale replica of Il Tempo Gigante that is legal for public roads, but is usually exposited at Hunderfossen Familiepark.\n\nExcept for some TV work in the late 1970s, Caprino made no more puppet films, focusing instead on creating attractions for the \"Hunderfossen\" theme park outside Lillehammer based on his folk tale movies, and making tourist films using a custom built multi camera setup of his own design that shoots 280 degrees panorama movies.\n\nCaprino was the son of Italian furniture designer Mario Caprino and the artist Ingeborg Gude, who was a granddaughter of the painter Hans Gude. He was born and died in Oslo, but lived all of his life at Snarøya in Bærum. He died in 2001 after having lived several years with a cancer diagnosis. Since Caprino's death, his son Remo has had great success developing a computer game based on \"Flåklypa Grand Prix\".\n\nBULLET::::- 1975 – \"Flåklypa Grand Prix\"\nBULLET::::- 1967 – \"Gutten som kappåt med trollet\"\nBULLET::::- 1966 – \"Sjuende far i huset\"\nBULLET::::- 1963 – \"Papirdragen\nBULLET::::- 1962 – \"Reveenka\"\nBULLET::::- 1961 – \"Askeladden og de gode hjelperne\nBULLET::::- 1959 – \"Ugler i mosen\"\nBULLET::::- 1958 – \"Et hundeliv med meg\"\nBULLET::::- 1955 – \"Den standhaftige tinnsoldat\"\nBULLET::::- 1955 – \"Klatremus i knipe\"\nBULLET::::- 1954 – \"Karius og Baktus\"\nBULLET::::- 1952 – \"Veslefrikk med fela\"\nBULLET::::- 1950 – \"Musikk på loftet/En dukkedrøm\"\nBULLET::::- 1949 – \"Tim og Tøffe\"\n\nBULLET::::- Ivo Caprinos Supervideograf\n\nBULLET::::- Caprino Studios – Official page\n"}
{"id": "15054", "url": "https://en.wikipedia.org/wiki?curid=15054", "title": "Intel 80286", "text": "Intel 80286\n\nThe Intel 80286 (also marketed as the iAPX 286 and often called Intel 286) is a 16-bit microprocessor that was introduced on February 1, 1982. It was the first 8086-based CPU with separate, non-multiplexed address and data buses and also the first with memory management and wide protection abilities. The 80286 used approximately 134,000 transistors in its original nMOS (HMOS) incarnation and, just like the contemporary 80186, it could correctly execute most software written for the earlier Intel 8086 and 8088 processors.\n\nThe 80286 was employed for the IBM PC/AT, introduced in 1984, and then widely used in most PC/AT compatible computers until the early 1990s.\n\nIntel's first 80286 chips were specified for a maximum clockrate of 4, 6 or 8 MHz and later releases for 12.5 MHz. AMD and Harris later produced 16 MHz, 20 MHz and 25 MHz parts, respectively. Intersil and Fujitsu also designed fully static CMOS versions of Intel's original depletion-load nMOS implementation, largely aimed at battery-powered devices.\n\nOn average, the 80286 was reportedly measured to have a speed of about 0.21 instructions per clock on \"typical\" programs, although it could be significantly faster on optimized code and in tight loops, as many instructions could execute in 2 clock cycles each. The 6 MHz, 10 MHz and 12 MHz models were reportedly measured to operate at 0.9 MIPS, 1.5 MIPS and 2.66 MIPS respectively.\n\nThe later E-stepping level of the 80286 was free of the several significant errata that caused problems for programmers and operating-system writers in the earlier B-step and C-step CPUs (common in the AT and AT clones).\n\nThe 80286 was designed for multi-user systems with multitasking applications, including communications (such as automated PBXs) and real-time process control. It had 134,000 transistors and consisted of four independent units: address unit, bus unit, instruction unit and execution unit, organized into a loosely coupled (buffered) pipeline just as in the 8086. The significantly increased performance over the 8086 was primarily due to the non-multiplexed address and data buses, more address-calculation hardware (most importantly, a dedicated adder) and a faster (more hardware-based) multiplier. It was produced in a 68-pin package, including PLCC (plastic leaded chip carrier), LCC (leadless chip carrier) and PGA (pin grid array) packages.\n\nThe performance increase of the 80286 over the 8086 (or 8088) could be more than 100% per clock cycle in many programs (i.e., a doubled performance at the same clock speed). This was a large increase, fully comparable to the speed improvements around a decade later when the i486 (1989) or the original Pentium (1993) were introduced. This was partly due to the non-multiplexed address and data buses, but mainly to the fact that address calculations (such as base+index) were less expensive. They were performed by a dedicated unit in the 80286, while the older 8086 had to do effective address computation using its general ALU, consuming several extra clock cycles in many cases. Also, the 80286 was more efficient in the prefetch of instructions, buffering, execution of jumps, and in complex microcoded numerical operations such as MUL/DIV than its predecessor.\n\nThe 80286 included, in addition to all of the 8086 instructions, all of the new instructions of the 80186: ENTER, LEAVE, BOUND, INS, OUTS, PUSHA, POPA, PUSH immediate, IMUL immediate, and immediate shifts and rotates. The 80286 also added new instructions for protected mode: ARPL, CLTS, LAR, LGDT, LIDT, LLDT, LMSW, LSL, LTR, SGDT, SIDT, SLDT, SMSW, STR, VERR, and VERW. Some of the instructions for protected mode can (or must) be used in real mode to set up and switch to protected mode, and a few (such as SMSW and LMSW) are useful for real mode itself.\n\nThe Intel 80286 had a 24-bit address bus and was able to address up to 16 MB of RAM, compared to the 1 MB addressability of its predecessor. However, memory cost and the initial rarity of software using the memory above 1 MB meant that 80286 computers were rarely shipped with more than one megabyte of RAM. Additionally, there was a performance penalty involved in accessing extended memory from real mode (in which DOS, the dominant PC operating system until the mid-1990s, ran), as noted below.\n\nThe 286 was the first of the x86 CPU family to support \"protected virtual-address mode\", commonly called \"protected mode\". In addition, it was the first commercially available microprocessor with on-chip MMU capabilities (systems using the contemporaneous Motorola 68010 and NS320xx could be equipped with an optional MMU controller). This would allow IBM compatibles to have advanced multitasking OSes for the first time and compete in the Unix-dominated server/workstation market.\n\nSeveral additional instructions were introduced in protected mode of 80286, which are helpful for multitasking operating systems.\n\nAnother important feature of 80286 is prevention of unauthorized access. This is achieved by:\nBULLET::::- Forming different segments for data, code, and stack, and preventing their overlapping.\nBULLET::::- Assigning privilege levels to each segment. Segments with lower privilege levels cannot access segments with higher privilege levels.\n\nIn 80286 (and in its co-processor Intel 80287), arithmetic operations can be performed on the following different types of numbers:\nBULLET::::- unsigned packed decimal,\nBULLET::::- unsigned binary,\nBULLET::::- unsigned unpacked decimal,\nBULLET::::- signed binary,\nBULLET::::- floating-point numbers (only with an 80287).\n\nBy design, the 286 could not revert from protected mode to the basic 8086-compatible \"real address mode\" (\"real mode\") without a hardware-initiated reset. In the PC/AT introduced in 1984, IBM added external circuitry, as well as specialized code in the ROM BIOS and the 8042 peripheral microcontroller to enable software to cause the reset, allowing real-mode reentry while retaining active memory and returning control to the program that initiated the reset. (The BIOS is necessarily involved because it obtains control directly whenever the CPU resets.) Though it worked correctly, the method imposed a huge performance penalty.\n\nIn theory, real-mode applications could be directly executed in 16-bit protected mode if certain rules (newly proposed with the introduction of the 80286) were followed; however, as many DOS programs did not conform to those rules, protected mode was not widely used until the appearance of its successor, the 32-bit Intel 80386, which was designed to go back and forth between modes easily and to provide an emulation of real mode within protected mode. When Intel designed the 286, it was not designed to be able to multitask real-mode applications; real mode was intended to be a simple way for a bootstrap loader to prepare the system and then switch to protected mode; essentially, in protected mode the 80286 was designed to be a new processor with many similarities to its predecessors, while real mode on the 80286 was offered for smaller-scale systems that could benefit from a more advanced version of the 80186 CPU core, with advantages such as higher clock rates, faster instruction execution (measured in clock cycles), and unmultiplexed buses, but not the 24-bit (16 MB) memory space.\n\nTo support protected mode, new instructions have been added: ARPL, VERR, VERW, LAR, LSL, SMSW, SGDT, SIDT, SLDT, STR, LMSW, LGDT, LIDT, LLDT, LTR, CLTS. There are also new exceptions (internal interrupts): invalid opcode, coprocessor not available, double fault, coprocessor segment overrun, stack fault, segment overrun/general protection fault, and others only for protected mode.\n\nThe protected mode of the 80286 was not utilized until many years after its release, in part because of the high cost of adding extended memory to a PC, but also because of the need for software to support the large user base of 8086 PCs. For example, in 1986 the only program that made use of it was VDISK, a RAM disk driver included with PC DOS 3.0 and 3.1. A DOS could utilize the additional RAM available in protected mode (extended memory) either via a BIOS call (INT 15h, AH=87h), as a RAM disk, or as emulation of expanded memory. The difficulty lay in the incompatibility of older real-mode DOS programs with protected mode. They simply could not natively run in this new mode without significant modification. In protected mode, memory management and interrupt handling were done differently than in real mode. In addition, DOS programs typically would directly access data and code segments that did not belong to them, as real mode allowed them to do without restriction; in contrast, the design intent of protected mode was to prevent programs from accessing any segments other than their own unless special access was explicitly allowed. While it was possible to set up a protected-mode environment that allowed all programs access to all segments (by putting all segment descriptors into the GDT and assigning them all the same privilege level), this undermined nearly all of the advantages of protected mode except the extended (24-bit) address space. The choice that OS developers faced was either to start from scratch and create an OS that would not run the vast majority of the old programs, or to come up with a version of DOS that was slow and ugly (i.e., ugly from an internal technical viewpoint) but would still run a majority of the old programs. Protected mode also did not provide a significant enough performance advantage over the 8086-compatible real mode to justify supporting its capabilities; actually, except for task switches when multitasking, it actually yielded only a performance disadvantage, by slowing down many instructions through a litany of added privilege checks. In protected mode, registers were still 16-bit, and the programmer was still forced to use a memory map composed of 64 kB segments, just like in real mode.\n\nIn January 1985, Digital Research previewed the Concurrent DOS 286 1.0 operating system developed in cooperation with Intel. The product would function strictly as an 80286 native-mode (i.e. protected-mode) operating system, allowing users to take full advantage of the protected mode to perform multi-user, multitasking operations while running 8086 emulation. This worked on the B-1 prototype step of the chip, but Digital Research discovered problems with the emulation on the production level C-1 step in May, which would not allow Concurrent DOS 286 to run 8086 software in protected mode. The release of Concurrent DOS 286 was delayed until Intel would develop a new version of the chip. In August, after extensive testing on E-1 step samples of the 80286, Digital Research acknowledged that Intel corrected all documented 286 errata, but said that there were still undocumented chip performance problems with the prerelease version of Concurrent DOS 286 running on the E-1 step. Intel said that the approach Digital Research wished to take in emulating 8086 software in protected mode differed from the original specifications. Nevertheless, in the E-2 step, they implemented minor changes in the microcode that would allow Digital Research to run emulation mode much faster. Named IBM 4680 OS, IBM originally chose DR Concurrent DOS 286 as the basis of their IBM 4680 computer for IBM Plant System products and point-of-sale terminals in 1986. Digital Research's FlexOS 286 version 1.3, a derivation of Concurrent DOS 286, was developed in 1986, introduced in January 1987, and later adopted by IBM for their IBM 4690 OS, but the same limitations affected it.\n\nThe problems led to Bill Gates famously referring to the 80286 as a \"brain-dead chip\", since it was clear that the new Microsoft Windows environment would not be able to run multiple MS-DOS applications with the 286. It was arguably responsible for the split between Microsoft and IBM, since IBM insisted that OS/2, originally a joint venture between IBM and Microsoft, would run on a 286 (and in text mode).\n\nOther operating systems that used the protected mode of the 286 were Microsoft Xenix (around 1984), Coherent, and Minix. These were less hindered by the limitations of the 80286 protected mode because they did not aim to run MS-DOS applications or other real-mode programs. In its successor 80386 chip, Intel enhanced the protected mode to address more memory and also added the separate virtual 8086 mode, a mode within protected mode with much better MS-DOS compatibility, in order to satisfy the diverging needs of the market.\n\nBULLET::::- U80601 – Almost identical copy of the 80286 manufactured 1989/90 in East Germany. In the Soviet Union a clone of the 80286 was designated KR1847VM286 ().\nBULLET::::- LOADALL – Undocumented 80286/80386 instruction that could be used to gain access to all available memory in real mode.\nBULLET::::- Windows/286\nBULLET::::- iAPX, for the iAPX name\n\nBULLET::::- Intel Datasheets\nBULLET::::- Intel 80286 and 80287 Programmer's Reference Manual at bitsavers.org\nBULLET::::- Intel 80286 Programmer's Reference Manual 1987 (txt). Hint: use e.g. \"Hebrew (IBM-862)\" encoding.\nBULLET::::- Linux on 286 laptops and notebooks\nBULLET::::- Intel 80286 images and descriptions at cpu-collection.de\nBULLET::::- CPU-INFO: 80286, in-depth processor history\nBULLET::::- Overview of all 286 compatible chips\nBULLET::::- Intel 80286 CPU Information, including chip errata and undocumented behaviour\n"}
{"id": "15055", "url": "https://en.wikipedia.org/wiki?curid=15055", "title": "Ivanhoe", "text": "Ivanhoe\n\nIvanhoe () is a historical novel by Sir Walter Scott, first published in late 1819 in three volumes and subtitled \"A Romance\". At the time it was written it represented a shift by Scott away from fairly realistic novels set in Scotland in the comparatively recent past, to a somewhat fanciful depiction of medieval England. It has proved to be one of the best known and most influential of Scott's novels.\n\n\"Ivanhoe\" is set in 12th-century England with colourful descriptions of a tournament, outlaws, a witch trial and divisions between Jews and Christians. It has been credited for increasing interest in romance and medievalism; John Henry Newman claimed Scott \"had first turned men's minds in the direction of the Middle Ages\", while Thomas Carlyle and John Ruskin made similar assertions of Scott's overwhelming influence over the revival, based primarily on the publication of this novel. It has also had an important influence on popular perceptions of Richard the Lionheart, King John and Robin Hood.\n\nThere have been several adaptations for stage, film and television.\n\nIn June 1819, Scott was still suffering from the severe stomach pains that had forced him to dictate the last part of \"The Bride of Lammermoor\" and most of \"A Legend of the Wars of Montrose\", finishing at the end of May. But by the beginning of July at the latest he had started dictating his new novel \"Ivanhoe\", again with John Ballantyne and William Laidlaw as amanuenses. He was able to take up the pen himself for the second half of the novel and completed it in early November.\n\nFor detailed information about the middle ages Scott drew on three works by the antiquarian Joseph Strutt: \"Horda Angel-cynnan or a Compleat View of the Manners, Customs, Arms, Habits etc. of the Inhabitants of England\" (1775–76), \"Dress and Habits of the People of England\" (1796–99), and \"Sports and Pastimes of the People of England\" (1801). Two historians gave him a solid grounding in the period: Robert Henry with his \"The History of Great Britain\" (1771–93), and Sharon Turner with \"The History of the Anglo-Saxons from the Earliest Period to the Norman Conquest\" (1799–1805). His clearest debt to an original medieval source involved the Templar Rule, reproduced in \"The Theatre of Honour and Knight-Hood\" (1623) translated from the French of André Favine. Scott was happy to introduce details from the later middle ages, and Chaucer was particularly helpful, as (in a different way) was the fourteenth-century romance \"Richard Coeur de Lion\".\n\n\"Ivanhoe\" was published by Archibald Constable in Edinburgh. All first editions carry the date of 1820, but it was released on 20 December 1819 and issued in London on the 29th. As with all of the Waverley novels before 1827, publication was anonymous. It is possible that Scott was involved in minor changes to the text during the early 1820s but his main revision was carried out in 1829 for the 'Magnum' edition where the novel appeared in Volumes 16 and 17 in September and October 1830. The standard modern edition, by Graham Tulloch, appeared as Volume 8 of the Edinburgh Edition of the Waverley Novels in 1998: this is based on the first edition with emendations principally from Scott's manuscript in the second half of the work; the new Magnum material is included in Volume 25b.\n\n\"Ivanhoe\" is the story of one of the remaining Anglo-Saxon noble families at a time when the nobility in England was overwhelmingly Norman. It follows the Saxon protagonist, Sir Wilfred of Ivanhoe, who is out of favour with his father for his allegiance to the Norman king Richard the Lionheart. The story is set in 1194, after the failure of the Third Crusade, when many of the Crusaders were still returning to their homes in Europe. King Richard, who had been captured by Leopold of Austria on his return journey to England, was believed to still be in captivity.\n\nProtagonist Wilfred of Ivanhoe is disinherited by his father Cedric of Rotherwood for supporting the Norman King Richard and for falling in love with the Lady Rowena, a ward of Cedric and descendant of the Saxon Kings of England. Cedric planned to have Rowena marry the powerful Lord Athelstane, a pretender to the Crown of England by his descent from the last Saxon King, Harold Godwinson. Ivanhoe accompanies King Richard on the Crusades, where he is said to have played a notable role in the Siege of Acre; and tends to Louis of Thuringia, who suffers from malaria.\n\nThe book opens with a scene of Norman knights and prelates seeking the hospitality of Cedric. They are guided there by a pilgrim, known at that time as a palmer. Also returning from the Holy Land that same night, Isaac the Count of York, a Jewish moneylender, seeks refuge at Rotherwood. Following the night's meal, the palmer observes one of the Normans, the Templar Brian de Bois-Guilbert, issue orders to his Saracen soldiers to capture Isaac.\n\nThe palmer then assists in Isaac's escape from Rotherwood, with the additional aid of the swineherd Gurth.\n\nIsaac of York offers to repay his debt to the palmer with a suit of armour and a war horse to participate in the tournament at Ashby-de-la-Zouch Castle, on his inference that the palmer was secretly a knight. The palmer is taken by surprise, but accepts the offer.\n\nThe tournament is presided over by Prince John. Also in attendance are Cedric, Athelstane, Lady Rowena, Isaac of York, his daughter Rebecca, Robin of Locksley and his men, Prince John's advisor Waldemar Fitzurse, and numerous Norman knights.\n\nOn the first day of the tournament, in a bout of individual jousting, a mysterious knight, identifying himself only as \"Desdichado\" (described in the book as Spanish, taken by the Saxons to mean Disinherited), defeats Bois-Guilbert. The masked knight declines to reveal himself despite Prince John's request, but is nevertheless declared the champion of the day and is permitted to choose the Queen of the Tournament. He bestows this honour upon Lady Rowena.\n\nOn the second day, at a melee, Desdichado is the leader of one party, opposed by his former adversaries. Desdichado's side is soon hard pressed and he himself beset by multiple foes until rescued by a knight nicknamed 'Le Noir Faineant' (\"the Black Sluggard\"), who thereafter departs in secret. When forced to unmask himself to receive his coronet (the sign of championship), Desdichado is identified as Wilfred of Ivanhoe, returned from the Crusades. This causes much consternation to Prince John and his court who now fear the imminent return of King Richard.\n\nIvanhoe is severely wounded in the competition yet his father does not move quickly to tend to him. Instead, Rebecca, a skilled healer, tends to him while they are lodged near the tournament and then convinces her father to take Ivanhoe with them to their home in York, when he is fit for that trip. The conclusion of the tournament includes feats of archery by Locksley, such as splitting a willow reed with his arrow. Prince John’s dinner for the local Saxons ends in insults.\n\nIn the forests between Ashby and York, Isaac, Rebecca and the wounded Ivanhoe are abandoned by their guards, who fear bandits and take all of Isaac’s horses. Cedric, Athelstane and the Lady Rowena meet them and agree to travel together. The party is captured by de Bracy and his companions and taken to Torquilstone, the castle of Front-de-Boeuf. The swineherd Gurth and Wamba the jester manage to escape, and then encounter Locksley, who plans a rescue.\n\nThe Black Knight, having taken refuge for the night in the hut of local friar, the Holy Clerk of Copmanhurst, volunteers his assistance on learning about the captives from Robin of Locksley. They then besiege the Castle of Torquilstone with Robin's own men, including the friar and assorted Saxon yeomen. Inside Torquilstone, de Bracy expresses his love for the Lady Rowena but is refused. Brian de Bois-Guilbert tries to seduce Rebecca and is rebuffed. Front-de-Boeuf tries to wring a hefty ransom from Isaac of York, but Isaac refuses to pay unless his daughter is freed.\n\nWhen the besiegers deliver a note to yield up the captives, their Norman captors demand a priest to administer the Final Sacrament to Cedric; whereupon Cedric's jester Wamba slips in disguised as a priest, and takes the place of Cedric, who escapes and brings important information to the besiegers on the strength of the garrison and its layout. The besiegers storm the castle. The castle is set aflame during the assault by Ulrica, the daughter of the original lord of the castle, Lord Torquilstone, as revenge for her father's death. Front-de-Boeuf is killed in the fire while de Bracy surrenders to the Black Knight, who identifies himself as King Richard and releases de Bracy. Bois-Guilbert escapes with Rebecca while Isaac is rescued by the Clerk of Copmanhurst. The Lady Rowena is saved by Cedric, while the still-wounded Ivanhoe is rescued from the burning castle by King Richard. In the fighting, Athelstane is wounded and presumed dead while attempting to rescue Rebecca, whom he mistakes for Rowena.\n\nFollowing the battle, Locksley plays host to King Richard. Word is conveyed by de Bracy to Prince John of the King's return and the fall of Torquilstone. In the meantime, Bois-Guilbert rushes with his captive to the nearest Templar Preceptory, where Lucas de Beaumanoir, the Grand Master of the Templars, takes umbrage at Bois-Guilbert's infatuation and subjects Rebecca to a trial for witchcraft. At Bois-Guilbert's secret request, she claims the right to trial by combat; and Bois-Guilbert, who had hoped for the position, is devastated when the Grand-Master orders him to fight against Rebecca's champion. Rebecca then writes to her father to procure a champion for her. Cedric organises Athelstane's funeral at Coningsburgh, in the midst of which the Black Knight arrives with a companion. Cedric, who had not been present at Locksley's carousal, is ill-disposed towards the knight upon learning his true identity; but Richard calms Cedric and reconciles him with his son. During this conversation, Athelstane emerges – not dead, but laid in his coffin alive by monks desirous of the funeral money. Over Cedric's renewed protests, Athelstane pledges his homage to the Norman King Richard and urges Cedric to marry Rowena to Ivanhoe; to which Cedric finally agrees.\n\nSoon after this reconciliation, Ivanhoe receives word from Isaac beseeching him to fight on Rebecca's behalf. Ivanhoe, riding day and night, arrives in time for the trial by combat, but horse and man are exhausted, with little chance of victory. The two knights make one charge at each other with lances, Bois-Guilbert appearing to have the advantage. However, Bois-Guilbert, a man trying to have it all without offering to marry Rebecca, dies of natural causes in the saddle before the combat can continue.\n\nFearing further persecution, Rebecca and her father plan to leave England for Granada. Before leaving, Rebecca comes to bid Rowena a fond farewell on her wedding day. Ivanhoe and Rowena marry and live a long and happy life together. Ivanhoe's military service ends with the death of King Richard.\n\n\"(principal characters in bold)\"\n\nBULLET::::- Cedric the Saxon, of Rotherwood\nBULLET::::- Wilfred of Ivanhoe, his son\nBULLET::::- Rowena, his ward\nBULLET::::- Athelstane, his kinsman\nBULLET::::- Gurth, his swineherd\nBULLET::::- Wamba, his jester\nBULLET::::- Oswald, his cup-bearer\nBULLET::::- Elgitha, Rowena's waiting-woman\nBULLET::::- Albert Malvoisin, Preceptor of Templestowe\nBULLET::::- Philip Malvoisin, his brother\nBULLET::::- Hubert, Philip's forester\nBULLET::::- The Prior of Aymer, Abbot of Jorvaulx\nBULLET::::- Ambrose, a monk attending him\nBULLET::::- Brian de Bois-Guilbert, a Templar\nBULLET::::- Baldwin, his squire\nBULLET::::- Isaac of York, a money-lender\nBULLET::::- Rebecca, his daughter\nBULLET::::- Nathan, a rabbi and physician\nBULLET::::- King Richard ('the Black Knight')\nBULLET::::- Prince John, his brother\nBULLET::::- Locksley, alias Robin Hood, an outlaw\nBULLET::::- Reginald Front-de-Bœuf, a Templar\nBULLET::::- Maurice de Bracy, a Templar\nBULLET::::- Hugh de Grantmesnil, a Templar\nBULLET::::- Ralph de Vipont, a Templar\nBULLET::::- Friar Tuck, of Copmanhurst\nBULLET::::- Ulrica, of Torquilstone, alias Urfried\nBULLET::::- Lucas de Beaumanoir, grand-master of the Templars\nBULLET::::- Conrade Mountfitchet, his attendant knight\nBULLET::::- Higg, a peasant\nBULLET::::- Kirjath Jairam of Leicester, a rich Jew\nBULLET::::- Hubert, a forester\nBULLET::::- Alan-a-Dale, a minstrel\n\nDedicatory Epistle: An imaginary letter from the Rev. Dr Dryasdust from Laurence Templeton who has found the materials for the following tale mostly in the Anglo-Norman Wardour Manuscript. He wishes to provide an English counterpart to the preceding Waverley novels, in spite of various difficulties arising from the chronologically remote setting made necessary by the earlier progress of civilisation south of the Border.\n\nCh. 1: Historical sketch. Gurth the swineherd and Wamba the jester discuss life under Norman rule.\n\nCh. 2: Wamba and Gurth wilfully misdirect a group of horsemen headed by Prior Aymer and Brian de Bois-Guilbert seeking shelter at Cedric's Rotherwood. Aymer and Bois-Guilbert discuss the beauty of Cedric's ward Rowena and are redirected, this time correctly, by a palmer [Ivanhoe in disguise].\n\nCh. 3: Cedric anxiously awaits the return of Gurth and the pigs. Aymer and Bois-Guilbert arrive.\n\nCh. 4: Bois-Guilbert admires Rowena as she enters for the evening feast.\n\nCh. 5: During the feast: Isaac enters and is befriended by the palmer; Cedric laments the decay of the Saxon language; the palmer refutes Bois-Guilbert's assertion of Templar supremacy in a tournament in Palestine, where Ivanhoe defeated him; the palmer and Rowena give a pledge for a return match; and Isaac is thunderstruck by Bois-Guilbert's denial of his assertion of poverty.\n\nCh. 6: On the road to Sheffield, the palmer tells Rowena that Ivanhoe will soon be home. In the morning he offers to protect Isaac from Bois-Guilbert, whom he has overheard giving instructions for his capture. Isaac mentions a source of horse and armour of which he guesses the palmer has need.\n\nCh. 7: As the audience for a tournament at Ashby assembles, Prince John amuses himself by making fun of Athelstane and Isaac.\n\nCh. 8: After a series of Saxon defeats in the tournament the 'Disinherited Knight' [Ivanhoe] triumphs over Bois-Guilbert.\n\nCh. 9: The Disinherited Knight nominates Rowena as Queen of the Tournament.\n\nCh. 10: The Disinherited Knight refuses to ransom Bois-Guilbert's armour, declaring that their business is not concluded. He instructs his attendant, Gurth in disguise, to convey money to Isaac to repay him for arranging the provision of his horse and armour. Gurth does so, but Rebecca secretly refunds the money.\n\nCh. 11: Gurth is assailed by a band of outlaws, but they spare him on hearing his story and after he has defeated one of their number, a miller, at quarter-staves.\n\nCh. 12: The Disinherited Knight's party triumph at the tournament, with the aid of a knight in black [Richard in disguise]; he is revealed as Ivanhoe and faints as a result of the wounds he has incurred.\n\nCh. 13: John encourages De Bracy to court Rowena and receives a warning from France that Richard has escaped. Locksley [Robin Hood] triumphs in an archery contest.\n\nCh. 14: At the tournament banquet Cedric continues to disown his son (who has been associating with the Normans) but drinks to the health of Richard, rather than John, as the noblest of that race.\n\nCh. 1 (15): De Bracy (disguised as a forester) tells Fitzurse of his plan to capture Rowena and then 'rescue' her in his own person.\n\nCh. 2 (16): The Black Knight is entertained by a hermit [Friar Tuck] at Copmanhurst.\n\nCh. 3 (17): The Black Knight and the hermit exchange songs.\n\nCh. 4 (18): (Retrospect: Before going to the banquet Cedric learned that Ivanhoe had been removed by unknown carers; Gurth was recognised and captured by Cedric's cupbearer Oswald.) Cedric finds Athelstane unresponsive to his attempts to interest him in Rowena, who is herself only attracted by Ivanhoe.\n\nCh. 5 (19): Rowena persuades Cedric to escort Isaac and Rebecca, who have been abandoned (along with a sick man [Ivanhoe] in their care) by their hired protectors. Wamba helps Gurth to escape again. De Bracy mounts his attack, during which Wamba escapes. He meets up with Gurth and they encounter Locksley who, after investigation, advises against a counter-attack, the captives not being in immediate danger.\n\nCh. 6 (20): Locksley sends two of his men to watch De Bracy. At Copmanhurst he meets the Black Knight who agrees to join in the rescue.\n\nCh. 7 (21): De Bracy tells Bois-Guilbert he has decided to abandon his 'rescue' plan, mistrusting his companion though the Templar says it is Rebecca he is interested in. On arrival at Torquilstone castle Cedric laments its decline.\n\nCh. 8 (22): Under threat of torture Front-de-Bœuf forces Isaac to agree to pay him a thousand pounds, but only if Rebecca is released.\n\nCh. 9 (23): De Bracy uses Ivanhoe's danger from Front-de-Bœuf to put pressure on Rowena, but he is moved by her resulting distress. The narrator refers the reader to historical instances of baronial oppression in medieval England.\n\nCh. 10 (24): A hag Urfried [Ulrica] warns Rebecca of her forthcoming fate. Rebecca impresses Bois-Guilbert by her spirited resistance to his advances.\n\nCh. 11 (25): Front-de-Bœuf rejects a written challenge from Gurth and Wamba. Wamba offers to spy out the castle posing as a confessor. \n\nCh. 12 (26): Entering the castle, Wamba exchanges clothes with Cedric who encounters Rebecca and Urfried.\n\nCh. 13 (27): Urfried recognises Cedric as a Saxon and, revealing herself as Ulrica, tells her story which involves Front-de-Bœuf murdering his father, who had killed her father and seven brothers when taking the castle, and had become her detested lover. She says she will give a signal when the time is ripe for storming the castle. Front-de-Bœuf sends the presumed friar with a message to summon reinforcements. Athelstane defies him, claiming that Rowena is his fiancée. The monk Ambrose arrives seeking help for Aymer who has been captured by Locksley's men.\n\nCh. 14 (28): (Retrospective chapter detailing Rebecca's care for Ivanhoe from the tournament to the assault on Torquilstone.)\n\nCh. 15 (29): Rebecca describes the assault on Torquilstone to the wounded Ivanhoe, disagreeing with his exalted view of chivalry.\n\nCh. 16 (30): Front-de-Bœuf and De Bracy discuss how best to repel the besiegers. Ulrica sets fire to the castle and Front-de-Bœuf dies in the flames.\n\nCh. 1 (31): (The chapter opens with a retrospective account of the attackers' plans and the taking of the barbican.) The Black Knight defeats De Bracy, making himself known to him as Richard, and rescues Ivanhoe. Bois-Guilbert rescues Rebecca, striking down Athelstane who thinks she is Rowena. Ulrica perishes in the flames after singing a wild pagan hymn.\n\nCh. 2 (32): Locksley supervises the orderly division of the spoil. Friar Tuck brings Isaac whom he has rescued and made captive, and engages in good-natured buffeting with the Black Knight.\n\nCh. 3 (33): Locksley arranges ransom terms for Isaac and Aymer.\n\nCh. 4 (34): De Bracy informs John that Richard is in England. Together with Fitzurse he threatens to desert John, but the prince responds cunningly.\n\nCh. 5 (35): At York, Nathan is horrified by Isaac's determination to seek Rebecca at Templestowe. At the priory Beaumanoir tells Mountfitchet that he intends to take a hard line with Templar irregularities. Arriving, Isaac shows him a letter from Aymer to Bois-Guilbert referring to Rebecca.\n\nCh. 6 (36): Beaumanoir tells Albert Malvoisin of his outrage at Rebecca's presence in the preceptory. Albert insists to Bois-Guilbert that her trial for sorcery must proceed. Mountfichet says he will seek evidence against her.\n\nCh. 7 (37): Rebecca is tried and found guilty. At Bois-Guilbert's secret prompting she demands that a champion defend her in trial by combat.\n\nCh. 8 (38): Rebecca's demand is accepted, Bois-Guilbert being appointed champion for the prosecution. Bearing a message to her father, Higg meets him and Nathan on their way to the preceptory, and Isaac goes in search of Ivanhoe.\n\nCh. 9 (39): Rebecca rejects Bois-Guilbert's offer to fail to appear for the combat in return for her love. Albert persuades him that it is in his interest to appear.\n\nCh. 10 (40): The Black Knight leaves Ivanhoe to travel to Coningsburgh castle for Athelstane's funeral, and Ivanhoe follows him the next day. The Black Knight is rescued by Locksley from an attack carried out by Fitzurse on John's orders, and reveals his identity as Richard to his companions, prompting Locksley to identify himself as Robin Hood.\n\nCh. 11 (41): Richard talks to Ivanhoe and dines with the outlaws before Robin arranges a false alarm to put an end to the delay. The party arrive at Coningsburgh.\n\nCh. 12 (42): Richard procures Ivanhoe's pardon from his father. Athelstane appears, not dead, giving his allegiance to Richard and surrendering Rowena to Ivanhoe.\n\nCh. 13 (43): Ivanhoe appears as Rebecca's champion, and Bois-Guilbert dies the victim of his contending passions.\n\nCh. 14 (44): Beaumanoir and his Templars leave Richard defiantly. Cedric agrees to the marriage of Ivanhoe and Rowena. Rebecca takes her leave of Rowena before her father and she leave England to make a new life under the tolerant King of Grenada.\n\nCritics of the novel have treated it as a romance intended mainly to entertain boys. \"Ivanhoe\" maintains many of the elements of the Romance genre, including the quest, a chivalric setting, and the overthrowing of a corrupt social order to bring on a time of happiness. Other critics assert that the novel creates a realistic and vibrant story, idealising neither the past nor its main character.\n\nScott treats themes similar to those of some of his earlier novels, like \"Rob Roy\" and \"The Heart of Midlothian\", examining the conflict between heroic ideals and modern society. In the latter novels, industrial society becomes the centre of this conflict as the backward Scottish nationalists and the \"advanced\" English have to arise from chaos to create unity. Similarly, the Normans in \"Ivanhoe\", who represent a more sophisticated culture, and the Saxons, who are poor, disenfranchised, and resentful of Norman rule, band together and begin to mould themselves into one people. The conflict between the Saxons and Normans focuses on the losses both groups must experience before they can be reconciled and thus forge a united England. The particular loss is in the extremes of their own cultural values, which must be disavowed in order for the society to function. For the Saxons, this value is the final admission of the hopelessness of the Saxon cause. The Normans must learn to overcome the materialism and violence in their own codes of chivalry. Ivanhoe and Richard represent the hope of reconciliation for a unified future.\n\nIvanhoe, though of a more noble lineage than some of the other characters, represents a middling individual in the medieval class system who is not exceptionally outstanding in his abilities, as is expected of other quasi-historical fictional characters, such as the Greek heroes. Critic György Lukács points to middling main characters like Ivanhoe in Sir Walter Scott's other novels as one of the primary reasons Scott's historical novels depart from previous historical works, and better explore social and cultural history.\n\nThe location of the novel is centred upon southern Yorkshire and northern Nottinghamshire in England. Castles mentioned within the story include Ashby de la Zouch Castle (now a ruin in the care of English Heritage), York (though the mention of Clifford's Tower, likewise an extant English Heritage property, is anachronistic, it not having been called that until later after various rebuilds) and 'Coningsburgh', which is based upon Conisbrough Castle, in the ancient town of Conisbrough near Doncaster (the castle also being a popular English Heritage site). Reference is made within the story to York Minster, where the climactic wedding takes place, and to the Bishop of Sheffield, although the Diocese of Sheffield did not exist at either the time of the novel or the time Scott wrote the novel and was not founded until 1914. Such references suggest that Robin Hood lived or travelled in the region.\n\nConisbrough is so dedicated to the story of \"Ivanhoe\" that many of its streets, schools, and public buildings are named after characters from the book.\n\nThe modern conception of Robin Hood as a cheerful, decent, patriotic rebel owes much to \"Ivanhoe\".\n\n\"Locksley\" becomes Robin Hood's title in the Scott novel, and it has been used ever since to refer to the legendary outlaw. Scott appears to have taken the name from an anonymous manuscript – written in 1600 – that employs \"Locksley\" as an epithet for Robin Hood. Owing to Scott's decision to make use of the manuscript, Robin Hood from Locksley has been transformed for all time into \"Robin of Locksley\", alias Robin Hood. (There is, incidentally, a village called Loxley in Yorkshire.)\n\nScott makes the 12th-century's Saxon-Norman conflict a major theme in his novel. The original medieval ballads about Robin Hood did not mention any conflict between Saxons and Normans; it was Scott who introduced this theme into the legend. The characters in \"Ivanhoe\" refer to Prince John and King Richard I as \"Normans\"; contemporary medieval documents from this period do not refer to either of these two rulers as Normans. Recent re-tellings of the story retain Scott emphasis on the Norman-Saxon conflict. Scott also shunned the late 16th-century depiction of Robin as a dispossessed nobleman (the Earl of Huntingdon). This, however, has not prevented Scott from making an important contribution to the noble-hero strand of the legend, too, because some subsequent motion picture treatments of Robin Hood's adventures give Robin traits that are characteristic of Ivanhoe as well. The most notable Robin Hood films are the lavish Douglas Fairbanks 1922 silent film, the 1938 triple Academy Award-winning \"Adventures of Robin Hood\" with Errol Flynn as Robin (which contemporary reviewer Frank Nugent links specifically with \"Ivanhoe\"), and the 1991 box-office success \"\" with Kevin Costner). There is also the Mel Brooks spoof \"\". In most versions of Robin Hood, both Ivanhoe and Robin, for instance, are returning Crusaders. They have quarrelled with their respective fathers, they are proud to be Saxons, they display a highly evolved sense of justice, they support the rightful king even though he is of Norman-French ancestry, they are adept with weapons, and they each fall in love with a \"fair maid\" (Rowena and Marian, respectively).\n\nThis particular time-frame was popularised by Scott. He borrowed it from the writings of the 16th-century chronicler John Mair or a 17th-century ballad presumably to make the plot of his novel more gripping. Medieval balladeers had generally placed Robin about two centuries later in the reign of Edward I, II or III.\n\nRobin's familiar feat of splitting his competitor's arrow in an archery contest appears for the first time in \"Ivanhoe\".\n\nThe general political events depicted in the novel are relatively accurate; the novel tells of the period just after King Richard's imprisonment in Austria following the Crusade and of his return to England after a ransom is paid. Yet the story is also heavily fictionalised. Scott himself acknowledged that he had taken liberties with history in his \"Dedicatory Epistle\" to \"Ivanhoe\". Modern readers are cautioned to understand that Scott's aim was to create a compelling novel set in a historical period, not to provide a book of history.\n\nThere has been criticism of Scott's portrayal of the bitter extent of the \"enmity of Saxon and Norman, represented as persisting in the days of Richard\" as \"unsupported by the evidence of contemporary records that forms the basis of the story.\" Historian E. A. Freeman criticised Scott's novel, stating its depiction of a Saxon-Norman conflict in late twelfth-century England was unhistorical. Freeman cited medieval writer Walter Map, who claimed that tension between the Saxons and Normans had declined by the reign of Henry I. Freeman also cited the late twelfth-century book \"Dialogus de Scaccario\" by Richard FitzNeal. This book claimed that the Saxons and Normans had so merged together through intermarriage and cultural assimilation that (outside the aristocracy) it was impossible to tell \"one from the other.\" Finally, Freeman ended his critique of Scott by saying that by the end of the twelfth century, the descendants of both Saxons and Normans in England referred to themselves as \"English\", not \"Saxon\" or \"Norman\". However, Scott may have intended to suggest parallels between the Norman conquest of England, about 130 years previously, and the prevailing situation in Scott's native Scotland (Scotland's union with England in 1707 – about the same length of time had elapsed before Scott's writing and the resurgence in his time of Scottish nationalism evidenced by the cult of Robert Burns, the famous poet who deliberately chose to work in Scots vernacular though he was an educated man and spoke modern English eloquently). Indeed, some experts suggest that Scott deliberately used \"Ivanhoe\" to illustrate his own combination of Scottish patriotism and pro-British Unionism.\n\nThe novel generated a new name in English – Cedric. The original Saxon name had been \"Cerdic\" but Sir Walter misspelled it – an example of metathesis. \"It is not a name but a misspelling\" said satirist H. H. Munro.\n\nIn England in 1194, it would have been unlikely for Rebecca to face the threat of being burned at the stake on charges of witchcraft. It is thought that it was shortly afterwards, from the 1250s, that the Church began to undertake the finding and punishment of witches and death did not become the usual penalty until the 15th century. Even then, the form of execution used for witches in England was hanging, burning being reserved for those also convicted of treason. There are various minor errors, e.g. the description of the tournament at Ashby owes more to the 14th century, most of the coins mentioned by Scott are exotic, William Rufus is said to have been John Lackland's grandfather, but he was actually his great-great-uncle, and Wamba (disguised as a monk) says \"I am a poor brother of the Order of St Francis\", but St. Francis of Assisi only began his preaching ten years after the death of Richard I.\n\n\"For a writer whose early novels were prized for their historical accuracy, Scott was remarkably loose with the facts when he wrote \"Ivanhoe\"... But it is crucial to remember that \"Ivanhoe\", unlike the Waverly books, is entirely a romance. It is meant to please, not to instruct, and is more an act of imagination than one of research. Despite this fancifulness, however, \"Ivanhoe\" does make some prescient historical points. The novel is occasionally quite critical of King Richard, who seems to love adventure more than he loves the well-being of his subjects. This criticism did not match the typical idealised, romantic view of Richard the Lion-Hearted that was popular when Scott wrote the book, and yet it accurately echoes the way King Richard is often judged by historians today.\"\n\nRebecca may be based on Rebecca Gratz, a Philadelphia teacher and philanthropist and the first Jewish female college student in America. Scott's attention had been drawn to Gratz's character by novelist Washington Irving, who was a close friend of the Gratz family. The assertion has been disputed, but it has been supported by \"The Original of Rebecca in Ivanhoe\", in \"The Century Magazine\" in 1882. The two Jewish characters, the moneylender Isaac of York and his beautiful daughter Rebecca, feature as main characters; the book was written and published during a period of increasing struggle for the emancipation of the Jews in England, and there are frequent references to injustices against them.\n\nMost of the original reviewers gave \"Ivanhoe\" an enthusiastic or broadly favourable reception.\nAs usual Scott's descriptive powers and his ability to present the matters of the past were generally praised. More than one reviewer found the work notably poetic. Several of them found themselves transported imaginatively to the remote period of the novel, although some problems were recognised: the combining of features from the early and late middle ages; an awkwardly created language for the dialogue; and antiquarian overload. The author's excursion into England was generally judged a success, the forest outlaws and the creation of 'merry England' attracting particular praise. Rebecca was almost unanimously admired, especially in her farewell scene. The plot was either criticised for its weakness, or just regarded as of less importance than the scenes and characters. The scenes at Torquilstone were judged horrible by several critics, with special focus on Ulrica. Athelstane's resurrection found no favour, the kindest response being that of Francis Jeffrey in \"The Edinburgh Review\" who suggested (writing anonymously, like all the reviewers) that it was 'introduced out of the very wantonness of merriment'.\n\nThe Eglinton Tournament of 1839 held by the 13th Earl of Eglinton at Eglinton Castle in Ayrshire was inspired by and modelled on \"Ivanhoe\".\n\nOn November 5, 2019, the \"BBC News\" listed \"Ivanhoe\" on its list of the 100 most influential novels.\n\nBULLET::::- In 1850, novelist William Makepeace Thackeray wrote a spoof sequel to Ivanhoe called \"Rebecca and Rowena\".\nBULLET::::- Edward Eager's book \"Knight's Castle\" (1956) magically transports four children into the story of Ivanhoe.\nBULLET::::- Simon Hawke uses the story as the basis for \"The Ivanhoe Gambit\" (1984) the first novel in his time travel adventure series TimeWars.\nBULLET::::- Pierre Efratas wrote a sequel called \"Le Destin d'Ivanhoe\" (2003), published by Éditions Charles Corlet.\nBULLET::::- Christopher Vogler wrote a sequel called \"Ravenskull\" (2006), published by Seven Seas Publishing.\n\nThe novel has been the basis for several motion pictures:\n\nBULLET::::- \"Ivanhoe\", United States 1911, directed by J. Stuart Blackton\nBULLET::::- \"Ivanhoe\" United States 1913, directed by Herbert Brenon; with King Baggot, Leah Baird, and Brenon. Filmed on location in England\nBULLET::::- \"Ivanhoe\" Wales 1913, directed by Leedham Bantock;\nBULLET::::- \"Ye Olden Days\" United States 1933, directed by Burt Gillett\nBULLET::::- \"Ivanhoe\", Wales 1913, directed by Leedham Bantock, filmed at Chepstow Castle\nBULLET::::- \"Ivanhoe\", 1952, directed by Richard Thorpe, starring Robert Taylor, Elizabeth Taylor, Joan Fontaine and George Sanders; nominated for three Oscars.\nBULLET::::- \"The Revenge of Ivanhoe\" (1965) starred Rik Battaglia (an Italian \"peplum\")\nBULLET::::- \"Ivanhoe, the Norman Swordsman\" (1971) aka \"La spada normanna\", directed by Roberto Mauri (an Italian \"peplum\")\nBULLET::::- \"The Ballad of the Valiant Knight Ivanhoe\" (Баллада о доблестном рыцаре Айвенго), USSR 1983, directed by Sergey Tarasov, with songs of Vladimir Vysotsky, starring Peteris Gaudins as Ivanhoe.\n\nThere have also been many television adaptations of the novel, including:\nBULLET::::- 1958: A television series based on the character of Ivanhoe starring Roger Moore as Ivanhoe\nBULLET::::- 1970: A TV miniseries starring Eric Flynn as Ivanhoe.\nBULLET::::- 1982: \"Ivanhoe\", a television movie starring Anthony Andrews as Ivanhoe.\nBULLET::::- 1986: \"Ivanhoe\", a 1986 animated telemovie produced by Burbank Films in Australia.\nBULLET::::- 1995: \"Young Ivanhoe\", a 1995 television movie directed by Ralph L. Thomas and starring Kristen Holden-Ried as Ivanhoe, Rachel Blanchard as Rowena, Stacy Keach as Pembrooke, Margot Kidder as Lady Margarite, Nick Mancuso as Bourget, and Matthew Daniels as Tuck.\nBULLET::::- 1997: \"Ivanhoe the King's Knight\" a televised cartoon series produced by CINAR and France Animation. General retelling of classic tale.\nBULLET::::- 1997: \"Ivanhoe\", a 6-part, 5-hour TV miniseries, a co-production of A&E and the BBC. It stars Steven Waddington as Ivanhoe, Ciarán Hinds as Bois-Guilbert, Susan Lynch as Rebecca, Ralph Brown as Prince John and Victoria Smurfit as Rowena.\nBULLET::::- 1999: \"The Legend of Ivanhoe\", a Columbia TriStar International Television production dubbed into English starring John Haverson as Ivanhoe and Rita Shaver as Rowena.\nBULLET::::- 2005: A Channel 5 adaptation entitled \"Dark Knight\" attempted to adapt \"Ivanhoe\" for an ongoing series. Ben Pullen played Ivanhoe and Charlotte Comer played Rebecca.\n\nVictor Sieg's dramatic cantata \"Ivanhoé\" won the Prix de Rome in 1864 and premiered in Paris the same year. An operatic adaptation of the novel by Sir Arthur Sullivan (entitled \"Ivanhoe\") ran for over 150 consecutive performances in 1891. Other operas based on the novel have been composed by Gioachino Rossini (\"Ivanhoé\"), Thomas Sari (\"Ivanhoé\"), Bartolomeo Pisani (\"Rebecca\"), A. Castagnier (\"Rébecca\"), Otto Nicolai (\"Il Templario\"), and Heinrich Marschner (\"Der Templer und die Jüdin\"). Rossini's opera is a \"pasticcio\" (an opera in which the music for a new text is chosen from pre-existent music by one or more composers). Scott attended a performance of it and recorded in his journal, \"It was an opera, and, of course, the story sadly mangled and the dialogue, in part nonsense.\"\n\nThe railway running through Ashby-de-la-Zouch was known as the Ivanhoe line between 1993 and 2005, in reference to the book's setting in the locality.\n\nBULLET::::- Norman yoke\nBULLET::::- Trysting Tree – several references are made to these trees as agreed gathering places.\nBULLET::::- Ivanhoe, New South Wales, a small outback town in the Australian state of New South Wales, named circa 1869 by a pioneering Scottish-born settler.\nBULLET::::- Ivanhoe, California, a small town founded in Tulare County, United States is named after the novel.\nBULLET::::- Ivanhoe, North Carolina, a village in eastern North Carolina\nBULLET::::- Ivanhoe, Victoria, a suburb in the Australian state of Victoria.\n\nBULLET::::- Online edition at eBooks@Adelaide\nBULLET::::- Full text online in HTML at dustylibrary.com, by chapter\nBULLET::::- First edition at Google Books: Volume 1, Volume 2, Volume 3\nBULLET::::- \"Ivanhoe\" Map\nBULLET::::- Page on \"Ivanhoe\" at the Walter Scott Digital Archive\n"}
{"id": "15056", "url": "https://en.wikipedia.org/wiki?curid=15056", "title": "Isoelectric point", "text": "Isoelectric point\n\nThe isoelectric point (pI, pH(I), IEP), is the pH at which a molecule carries no net electrical charge or is electrically neutral in the statistical mean. The standard nomenclature to represent the isoelectric point is pH(I), although pI is also commonly seen, and is used in this article for brevity. The net charge on the molecule is affected by pH of its surrounding environment and can become more positively or negatively charged due to the gain or loss, respectively, of protons (H).\n\nSurfaces naturally charge to form a double layer. In the common case when the surface charge-determining ions are H/OH, the net surface charge is affected by the pH of the liquid in which the solid is submerged.\n\nThe pI value can affect the solubility of a molecule at a given pH. Such molecules have minimum solubility in water or salt solutions at the pH that corresponds to their pI and often precipitate out of solution. Biological amphoteric molecules such as proteins contain both acidic and basic functional groups. Amino acids that make up proteins may be positive, negative, neutral, or polar in nature, and together give a protein its overall charge. At a pH below their pI, proteins carry a net positive charge; above their pI they carry a net negative charge. Proteins can, thus, be separated by net charge in a polyacrylamide gel using either preparative gel electrophoresis, which uses a constant pH to separate proteins or isoelectric focusing, which uses a pH gradient to separate proteins. Isoelectric focusing is also the first step in 2-D gel polyacrylamide gel electrophoresis.\n\nIn biomolecules, proteins can be separated by ion exchange chromatography. Biological proteins are made up of zwitterionic amino acid compounds; the net charge of these proteins can be positive or negative depending on the pH of the environment. The specific pI of the target protein can be used to model the process around and the compound can then be purified from the rest of the mixture. Buffers of various pH can be used for this purification process to change the pH of the environment. When a mixture containing a target protein is loaded into an ion exchanger, the stationary matrix can be either positively-charged (for mobile anions) or negatively-charged (for mobile cations). At low pH values, the net charge of most proteins in the mixture is positive - in cation exchangers, these positively-charged proteins bind to the negatively-charged matrix. At high pH values, the net charge of most proteins is negative, where they bind to the positively-charged matrix in anion exchangers. When the environment is at a pH value equal to the protein's pI, the net charge is zero, and the protein is not bound to any exchanger, and therefore, can be eluted out.\n\nFor an amino acid with only one amine and one carboxyl group, the pI can be calculated from the mean of the pKas of this molecule.\n\nThe pH of an electrophoretic gel is determined by the buffer used for that gel. If the pH of the buffer is above the pI of the protein being run, the protein will migrate to the positive pole (negative charge is attracted to a positive pole). If the pH of the buffer is below the pI of the protein being run, the protein will migrate to the negative pole of the gel (positive charge is attracted to the negative pole). If the protein is run with a buffer pH that is equal to the pI, it will not migrate at all. This is also true for individual amino acids.\n\nalign=centerglycine pK = 2.72, 9.60\nalign=centeradenosine monophosphate pK = 0.9, 3.8, 6.1\nIn the two examples (on the right) the isoelectric point is shown by the green vertical line. In glycine the pK values are separated by nearly 7 units so the concentration of the neutral species, glycine (GlyH), is effectively 100% of the analytical glycine concentration. Glycine may exist as a zwitterion at the isoelectric point, but the equilibrium constant for the isomerization reaction in solution\nis not known.\n\nThe other example, adenosine monophosphate is shown to illustrate the fact that a third species may, in principle, be involved. In fact the concentration of (AMP)H is negligible at the isoelectric point in this case.\nIf the pI is greater than the pH, the molecule will have a positive charge.\n\nA number of algorithms for estimating isoelectric points of peptides and proteins have been developed. Most of them use Henderson–Hasselbalch equation with different pK values. For instance, within the model proposed by Bjellqvist and co-workers the pK's were determined between closely related immobilines, by focusing the same sample in overlapping pH gradients. Some improvements in the methodology (especially in the determination of the pK values for modified amino acids) have been also proposed. More advanced methods take into account the effect of adjacent amino acids ±3 residues away from a charged aspartic or glutamic acid, the effects on free C terminus, as well as they apply a correction term to the corresponding pK values using genetic algorithm. Other recent approaches are based on a support vector machine algorithm and pKa optimization against experimentally known protein/peptide isoelectric points.\n\nMoreover, experimentally measured isoelectric point of proteins were aggregated into the databases. Recently, a database of isoelectric points for all proteins predicted using most of the available methods had been also developed.\n\nThe isoelectric points (IEP) of metal oxide ceramics are used extensively in material science in various aqueous processing steps (synthesis, modification, etc.). In the absence of chemisorbed or physisorbed species particle surfaces in aqueous suspension are generally assumed to be covered with surface hydroxyl species, M-OH (where M is a metal such as Al, Si, etc.). At pH values above the IEP, the predominate surface species is M-O, while at pH values below the IEP, M-OH species predominate. Some approximate values of common ceramics are listed below:\n! Material\n! IEP\n! Material\n! IEP\n! Material\n! IEP\n! Material\n! IEP\n! Material\n! IEP\n! Material\n! IEP\n\"Note: The following list gives the isoelectric point at 25 °C for selected materials in water. The exact value can vary widely, depending on material factors such as purity and phase as well as physical parameters such as temperature. Moreover, the precise measurement of isoelectric points can be difficult, thus many sources often cite differing values for isoelectric points of these materials.\"\n\nMixed oxides may exhibit isoelectric point values that are intermediate to those of the corresponding pure oxides. For example, a synthetically prepared amorphous aluminosilicate (AlO-SiO) was initially measured as having IEP of 4.5 (the electrokinetic behavior of the surface was dominated by surface Si-OH species, thus explaining the relatively low IEP value). Significantly higher IEP values (pH 6 to 8) have been reported for 3AlO-2SiO by others. Similarly, also IEP of barium titanate, BaTiO was reported in the range 5-6 while others got a value of 3. Mixtures of titania (TiO) and zirconia (ZrO) were studied and found to have an isoelectric point between 5.3-6.9, varying non-linearly with %(ZrO). The surface charge of the mixed oxides was correlated with acidity. Greater titania content led to increased Lewis acidity, whereas zirconia-rich oxides displayed Br::onsted acidity. The different types of acidities produced differences in ion adsorption rates and capacities.\n\nThe terms isoelectric point (IEP) and point of zero charge (PZC) are often used interchangeably, although under certain circumstances, it may be productive to make the distinction.\n\nIn systems in which H/OH are the interface potential-determining ions, the point of zero charge is given in terms of pH. The pH at which the surface exhibits a neutral net electrical charge is the point of zero charge at the surface. Electrokinetic phenomena generally measure zeta potential, and a zero zeta potential is interpreted as the point of zero net charge at the shear plane. This is termed the isoelectric point. Thus, the isoelectric point is the value of pH at which the colloidal particle remains stationary in an electrical field. The isoelectric point is expected to be somewhat different than the point of zero charge at the particle surface, but this difference is often ignored in practice for so-called pristine surfaces, i.e., surfaces with no specifically adsorbed positive or negative charges. In this context, specific adsorption is understood as adsorption occurring in a Stern layer or chemisorption. Thus, point of zero charge at the surface is taken as equal to isoelectric point in the absence of specific adsorption on that surface.\n\nAccording to Jolivet, in the absence of positive or negative charges, the surface is best described by the point of zero charge. If positive and negative charges are both present in equal amounts, then this is the isoelectric point. Thus, the PZC refers to the absence of any type of surface charge, while the IEP refers to a state of neutral net surface charge. The difference between the two, therefore, is the quantity of charged sites at the point of net zero charge. Jolivet uses the intrinsic surface equilibrium constants, p\"K\" and p\"K\" to define the two conditions in terms of the relative number of charged sites:\n\nFor large Δp\"K\" (>4 according to Jolivet), the predominant species is MOH while there are relatively few charged species - so the PZC is relevant. For small values of Δp\"K\", there are many charged species in approximately equal numbers, so one speaks of the IEP.\n\nBULLET::::- Henderson-Hasselbalch equation\nBULLET::::- Isoelectric focusing\nBULLET::::- Zeta potential\nBULLET::::- Electrophoretic deposition\nBULLET::::- Isoionic point\nBULLET::::- pK acid dissociation constant\nBULLET::::- QPNC-PAGE\n\nBULLET::::- Nelson DL, Cox MM (2004). \"Lehninger Principles of Biochemistry\". W. H. Freeman; 4th edition (Hardcover).\nBULLET::::- Kosmulski M. (2009). \"Surface Charging and Points of Zero Charge\". CRC Press; 1st edition (Hardcover).\n\nBULLET::::- IPC – Isoelectric Point Calculator — calculate protein isoelectric point using over 15 methods\nBULLET::::- prot pi - protein isoelectric point — an online program for calculating pI of proteins (include multiple subunits and posttranslational modifications)\nBULLET::::- CurTiPot — a suite of spreadsheets for computing acid-base equilibria (charge versus pH plot of amphoteric molecules e.g., amino acids)\nBULLET::::- pICalculax — Isoelectric point (pI) predictor for chemically modified peptides and proteins\nBULLET::::- SWISS-2DPAGE — a database of isoelectric points coming from two-dimensional polyacrylamide gel electrophoresis (~ 2,000 proteins)\nBULLET::::- PIP-DB — a Protein Isoelectric Point database (~ 5,000 proteins)\nBULLET::::- \"Proteome-pI\" — a proteome isoelectric point database (predicted isoelectric point for all proteins)\n"}
{"id": "15058", "url": "https://en.wikipedia.org/wiki?curid=15058", "title": "International reply coupon", "text": "International reply coupon\n\nAn international reply coupon (IRC) is a coupon that can be exchanged for one or more postage stamps representing the minimum postage for an unregistered priority airmail letter of up to twenty grams sent to another Universal Postal Union (UPU) member country. IRCs are accepted by all UPU member countries.\n\nUPU member postal services are obliged to exchange an IRC for postage, but are not obliged to sell them.\n\nThe purpose of the IRC is to allow a person to send someone in another country a letter, along with the cost of postage for a reply. If the addressee is within the same country, there is no need for an IRC because a self-addressed stamped envelope (SASE) or return postcard will suffice; but if the addressee is in another country an IRC removes the necessity of acquiring foreign postage or sending appropriate currency.\n\nInternational reply coupons (in French, \"Coupons-Reponse Internationaux\") are printed in blue ink on paper that has the letters “UPU” in large characters in the watermark. The front of each coupon is printed in French. The reverse side of the coupon, which has text relating to its use, is printed in German, English, Arabic, Chinese, Spanish, and Russian. Under Universal Postal Union's regulations, participating member countries are not required to place a control stamp or postmark on the international reply coupons that they sell. Therefore, some foreign issue reply coupons that are tendered for redemption may bear the name of the issuing country (generally in French) rather than the optional control stamp or postmark.\n\nThe Nairobi Model was an international reply coupon printed by the Universal Postal Union which is approximately 3.75 inches by 6 inches and had an expiration date of December 31, 2013. This model was designed by Rob Van Goor, a graphic artist from the Luxembourg Post. It was selected from among 10 designs presented by Universal Postal Union member countries. Van Goor interpreted the theme of the contest – \"The Postage Stamp: A Vehicle for Exchange\" – by depicting the world being cradled by a hand and the perforated outline of a postage stamp.\n\nThe Doha Model is named for the 25th UPU congress held in Doha, Qatar, in 2012. The Doha model, designed by Czech artist and graphic designer Michal Sindelar, shows cupped hands catching a stream of water, to celebrate the theme of Water for Life. It expires after December 31, 2017.\n\nThe Istanbul Model was designed by graphic artist Nguyen Du's and features a pair of hands and a dove against an Arctic backdrop to represent sustainable development in the postal sector. Ten countries participated in the competition which was held Oct. 7, 2016, during the UPU congress in Istanbul, Turkey. It expires after December 31, 2021.\n\nThe IRC was introduced in 1906 at a Universal Postal Union congress in Rome. At the time an IRC could be exchanged for a single-rate, ordinary postage stamp for surface delivery to a foreign country, as this was before the introduction of airmail services. An IRC is exchangeable in a UPU member country for the minimum postage of a priority or unregistered airmail letter to a foreign country.\n\nThe current IRC, which features the theme \"the Post and sustainable development\", was designed by Vietnamese artist Nguyen Du for 2017-2021 and was adopted in Istanbul in 2016, it is known also as the \"Istanbul model\" for this reason. The previous design, \"Water for Life\" by Czech artist and graphic designer Michal Sindelar, was issued in 2013 and was valid until 31 December 2017.\n\nIRCs are ordered from the UPU headquarters in Bern, Switzerland by postal authorities. They are generally available at large post offices; in the U.S., they were requisitioned along with regular domestic stamps by any post office that had sufficient demand for them.\n\nPrices for IRCs vary by country. In the United States in November 2012, the purchase price was $2.20 USD; however, the US Postal Service discontinued sales of IRCs on 27 January 2013 due to declining demand. Britain's Royal Mail also stopped selling IRCs on 18 February 2012, citing minimal sales and claiming that the average post office sold less than one IRC per year. IRCs purchased in foreign countries may be used in the United States toward the purchase of postage stamps and embossed stamped envelopes at the current one-ounce First Class International rate (US$1.05 as of April 2012) per coupon.\n\nIRCs are often used by amateur radio operators sending QSL cards to each other; it has traditionally been considered good practice and common courtesy to include an IRC when writing to a foreign operator and expecting a reply by mail. If the operator's home country does not sell IRCs, then a foreign IRC may be used.\n\nPrevious editions of the IRC, the \"Beijing\" model and all subsequent versions, bear an expiration date. Consequently, a new IRC will be issued every three years.\n\nInternational reply coupons are sold by the HongKong Post for 19 HKD as of 2018-10-19.\n\nInternational reply coupons are sold by the Swiss Post in packs of 10 for 25 CHF.\n\nThe Royal Mail stopped selling IRCs on 31 December 2011 due to a lack of demand.\n\nThe United States Postal Service stopped selling international reply coupons on January 27, 2013.\n\nIn 1920, Charles Ponzi made use of the idea that profit could be made by taking advantage of the differing postal rates in different countries to buy IRCs cheaply in one country and exchange them for stamps of a higher value in another country. This subsequently became the fraudulent Ponzi scheme. In practice, the overhead on buying and selling large numbers of the very low-value IRCs precluded any profitability.\n\nThe selling price and exchange value in stamps in each country have been adjusted to some extent to remove some of the potential for profit, but ongoing fluctuations in currency value and exchange rates make it impossible to achieve this completely, as long as stamps represent a specific currency value, instead of acting as vouchers granting specific postal services, devoid of currency nomination.\n\nBULLET::::- Heys Collection\n\nBULLET::::- UPU IRC info\nBULLET::::- UPU list of countries selling IRCs\nBULLET::::- IRC info\nBULLET::::- Some IRC illustrations and exchange guidelines\nBULLET::::- International Reply Coupons and Ham Radio\n"}
{"id": "15059", "url": "https://en.wikipedia.org/wiki?curid=15059", "title": "Isaac Bonewits", "text": "Isaac Bonewits\n\nPhillip Emmons Isaac Bonewits (October 1, 1949 – August 12, 2010) was an American Neo-Druid who published a number of books on the subject of Neopaganism and magic. He was a public speaker, liturgist, singer and songwriter, and founder of the Neopagan organizations Ár nDraíocht Féin and the Aquarian Anti-Defamation League. Born in Royal Oak, Michigan, Bonewits had been heavily involved in occultism since the 1960s.\n\nBonewits was born on October 1, 1949 in Royal Oak, Michigan, as the fourth of five children. His mother and father were Roman Catholics. Spending much of his childhood in Ferndale, he was moved at age 12 to San Clemente, California, where he spent a short time in a Catholic high school before he went back to public school to graduate from high school a year early. He enrolled at UC Berkeley in 1966; he graduated from the university in 1970 with a Bachelor of Arts in Magic, perhaps becoming the first and only person known to have ever received any kind of academic degree in Magic from an accredited university.\n\nIn 1966, while enrolled at UC Berkeley, Bonewits joined the Reformed Druids of North America (RDNA). Bonewits was ordained as a Neo-druid priest in 1969. During this period, the 18-year-old Bonewits was also recruited by the Church of Satan, but left due to political and philosophical conflicts with Anton LaVey. During his stint in the Church of Satan, Bonewits appeared in some scenes of the 1970 documentary \"Satanis: The Devil's Mass\". Bonewits, in his article \"My Satanic Adventure\", asserts that the rituals in \"Satanis\" were staged for the movie at the behest of the filmmakers and were not authentic ceremonies.\n\nHis first book, \"Real Magic\", was published in 1972. Between 1973 and 1975 Bonewits was employed as the editor of \"Gnostica\" magazine in Minnesota (published by Llewellyn Publications). He established an offshoot group of the Reformed Druids of North America (RDNA) called the Schismatic Druids of North America, and helped create a group called the Hasidic Druids of North America (despite, in his words, his \"lifelong status as a gentile\"). He also founded the short-lived Aquarian Anti-Defamation League (AADL), an early Pagan civil rights group.\n\nIn 1976, Bonewits moved back to Berkeley and rejoined his original grove there, now part of the New Reformed Druids of North America (NRDNA). He was later elected Archdruid of the Berkeley Grove.\n\nThroughout his life Bonewits had varying degrees of involvement with occult groups including Gardnerian Wicca and the New Reformed Orthodox Order of the Golden Dawn (a Wiccan organization not to be confused with the Hermetic Order of the Golden Dawn). Bonewits was a regular presenter at Neopagan conferences and festivals all over the US, as well as attending gaming conventions in the Bay Area. He promoted his book 'Authentic Thaumaturgy' to gamers as a way of organizing Dungeons and Dragons games and to give a background to games of .\n\nIn 1983, Bonewits founded Ár nDraíocht Féin (also known as \"A Druid Fellowship\" or ADF), which was incorporated in 1990 in the state of Delaware as a U.S. 501(c)3 non-profit organization. Although illness curtailed many of his activities and travels for a time, he remained Archdruid of ADF until 1996. In that year, he resigned from the position of Archdruid but retained the lifelong title of ADF Archdruid Emeritus.\n\nA songwriter, singer, and recording artist, he produced two CDs of pagan music and numerous recorded lectures and panel discussions, produced and distributed by the Association for Consciousness Exploration. He lived in Rockland County, New York, and was a member of the Covenant of Unitarian Universalist Pagans (CUUPS).\n\nBonewits encouraged charity programs to help Neopagan seniors, and in January 2006 was the keynote speaker at the Conference On Current Pagan Studies at the Claremont Graduate University in Claremont, CA.\n\nBonewits was married five times. He was married to Rusty Elliot from 1973 to 1976. His second wife was Selene Kumin Vega, followed by marriage to Sally Eaton (1980 to 1985). His fourth wife was author Deborah Lipp, from 1988 to 1998. On July 23, 2004, he was married in a handfasting ceremony to a former vice-president of the Covenant of Unitarian Universalist Pagans, Phaedra Heyman Bonewits. At the time of the handfasting, the marriage was not yet legal because he had not yet been legally divorced from Lipp, although they had been separated for several years. Paperwork and legalities caught up on December 31, 2007, making them legally married.\n\nBonewits' only child, Arthur Shaffrey Lipp-Bonewits, was born to Deborah Lipp in 1990.\n\nIn 1990, Bonewits was diagnosed with Eosinophilia-myalgia syndrome. The illness was a factor in his eventual resignation from the position of Archdruid of the ADF.\n\nOn October 25, 2009, Bonewits was diagnosed with a rare form of colon cancer, for which he underwent treatment. He died at home, on August 12, 2010, surrounded by his family.\n\nIn his book \"Real Magic\" (1971), Bonewits proposed his \"Laws of Magic.\" These \"laws\" are synthesized from a multitude of belief systems from around the world to explain and categorize magical beliefs within a cohesive framework. Many interrelationships exist, and some belief systems are subsets of others. This work was chosen by Dennis Wheatley in the 1970s to be part of his publishing project 'Library of the Occult'.\n\nBonewits also coined much of the modern terminology used to articulate the themes and issues that affect the North American Neopagan community.\nBULLET::::- Pioneered the modern usage of the terms \"thealogy,\" \"Paleo-Paganism,\" \"Meso-Paganism,\" and numerous other retronyms.\nBULLET::::- Possibly coined the term \"Pagan Reconstructionism,\" though the communities in question would later diverge from his initial meaning.\nBULLET::::- Founded Ar nDraiocht Fein, which was incorporated in 1990 in the state of Delaware as a U.S. 501(c)3 non-profit organization.\nBULLET::::- Developed the \"Advanced Bonewits Cult Danger Evaluation Frame\" (ABCDEF).\nBULLET::::- Coined the phrase \"Never Again the Burning.\"\nBULLET::::- Critiqued the Burning Times / Old Religion Murray thesis (in \"Bonewits's Essential Guide to Witchcraft and Wicca\").\n\nBULLET::::- \"Real Magic: An Introductory Treatise on the Basic Principles of Yellow Magic\". (1972, 1979, 1989) Weiser Books\nBULLET::::- \"The Druid Chronicles (Evolved)\". (1976 Drunemeton Press, 2005 Drynemetum Press) (With Selene Kumin Vega, Rusty Elliot, and Arlynde d'Loughlan)\nBULLET::::- \"Authentic Thaumaturgy\". (With others) (1978, 1998) Steve Jackson Games\nBULLET::::- \"Rites of Worship: A Neopagan Approach\". (2003) Earth Religions Press OP\nBULLET::::- \"Witchcraft: A Concise Guide or Which Witch Is Which?\". (2003) Earth Religions Press\nBULLET::::- \"The Pagan Man: Priests, Warriors, Hunters, and Drummers\". (2005) Citadel ,\nBULLET::::- \"Bonewits's Essential Guide to Witchcraft and Wicca\". (2006) Citadel ,\nBULLET::::- \"Bonewits's Essential Guide to Druidism\". (2006) Citadel ,\nBULLET::::- \"Real Energy: Systems, Spirits, And Substances to Heal, Change, And Grow\". (2007) New Leaf , . Co-authored with Phaedra Bonewits.\nBULLET::::- \"Neopagan Rites: A Guide to Creating Public Rituals that Work\". (2007) Llewellyn ,\n\nBULLET::::- \"Be Pagan Once Again!\" – Isaac Bonewits & Friends (including Ian Corrigan, Victoria Ganger, and Todd Alan) (CD) (ACE/ADF)\nBULLET::::- \"Avalon is Rising!\" – Real Magic (CD)(ACE/ADF)\n\nBULLET::::- \"The Structure of Craft Ritual\" (ACE)\nBULLET::::- \"A Magician Prepares\" (ACE)\nBULLET::::- \"Programming Magical Ritual: Top-Down Liturgical Design\" (ACE)\nBULLET::::- \"Druidism: Ancient & Modern\" (ACE)\nBULLET::::- \"How Does Magic Work?\" (ACE)\nBULLET::::- \"Rituals That Work\" (ACE)\nBULLET::::- \"Sexual Magic & Magical Sex\" (with Deborah Lipp) (ACE)\nBULLET::::- \"Making Fun of Religion\" (with Deborah Lipp) (ACE)\n\nBULLET::::- \"The Magickal Movement: Present & Future\" (with Margot Adler, Selena Fox, and Robert Anton Wilson) (ACE)\nBULLET::::- \"Magick Changing the World, the World Changing Magick\" (with AmyLee, Selena Fox, Jeff Rosenbaum and Robert Anton Wilson) (ACE)\n\n\nBULLET::::- Neopagan Net (formerly \"Isaac Bonewits' Homepage\")\nBULLET::::- The laws\nBULLET::::- A Reformed Druid Anthology includes \"The Druid Chronicles (Evolved)\"\nBULLET::::- \"An Open Letter to Selena Fox\"\nBULLET::::- \"Advanced Bonewits Cult Danger Evaluation Frame\"\n"}
{"id": "15062", "url": "https://en.wikipedia.org/wiki?curid=15062", "title": "Intel 8080", "text": "Intel 8080\n\nThe Intel 8080 (\"\"eighty-eighty\"\") was the second 8-bit microprocessor designed and manufactured by Intel and was released in April 1974. It is an extended and enhanced variant of the earlier 8008 design, although without binary compatibility. The initial specified clock rate or frequency limit was 2 MHz, and with common instructions using 4, 5, 7, 10, or 11 cycles this meant that it operated at a typical speed of a few hundred thousand instructions per second. A faster variant 8080A-1 (Sometimes called the 8080B) became available later with clock frequency limit up to 3.125 MHz.\n\nThe 8080 needs two support chips to function in most applications, the i8224 clock generator/driver and the i8228 bus controller, and it is implemented in N-type metal-oxide-semiconductor logic (NMOS) using non-saturated enhancement mode transistors as loads thus demanding a +12 V and a −5 V voltage in addition to the main transistor–transistor logic (TTL) compatible +5 V.\n\nAlthough earlier microprocessors were used for calculators, cash registers, computer terminals, industrial robots, and other applications, the 8080 became one of the first widespread microprocessors. Several factors contributed to its popularity: its 40-pin package made it easier to interface than the 18-pin 8008, and also made its data bus more efficient; its NMOS implementation gave it faster transistors than those of the P-type metal-oxide-semiconductor logic (PMOS) 8008, while also simplifying interfacing by making it TTL-compatible; a wider variety of support chips was available; its instruction set was enhanced over the 8008; and its full 16-bit address bus (versus the 14-bit one of the 8008) enabled it to access 64 KB of memory, four times more than the 8008's range of 16 KB. It became the engine of the Altair 8800, and subsequent S-100 bus personal computers, until it was replaced by the Z80 in this role, and was the original target CPU for CP/M operating systems developed by Gary Kildall.\n\nThe 8080 was successful enough that compatibility at the assembly language level became a design requirement for the Intel 8086 when its design began in 1976, and led to the 8080 directly influencing all later variants of the ubiquitous 32-bit and 64-bit x86 architectures.\n\n+ Intel 8080 registers\ncolspan=\"17\"  Main registers \ncolspan=\"17\"  Index registers \ncolspan=\"17\"  Program counter \ncolspan=\"17\"  Status register\n\nThe Intel 8080 is the successor to the 8008. It uses the same basic instruction set and register model as the 8008 (developed by Computer Terminal Corporation), even though it is not source code compatible nor binary code compatible with its predecessor. Every instruction in the 8008 has an equivalent instruction in the 8080 (even though the opcodes differ between the two CPUs). The 8080 also adds a few 16-bit operations in its instruction set. Whereas the 8008 required the use of the HL register pair to indirectly access its 14-bit memory space, the 8080 added addressing modes to allow direct access to its full 16-bit memory space. In addition, the internal 7-level push-down call stack of the 8008 was replaced by a dedicated 16-bit stack-pointer (SP) register. The 8080's large 40-pin DIP packaging permits it to provide a 16-bit address bus and an 8-bit data bus, allowing easy access to 64 KB of memory.\n\nThe processor has seven 8-bit registers (A, B, C, D, E, H, and L), where A is the primary 8-bit accumulator, and the other six registers can be used as either individual 8-bit registers or as three 16-bit register pairs (BC, DE, and HL, referred to as B, D and H in Intel documents) depending on the particular instruction. Some instructions also enable the HL register pair to be used as a (limited) 16-bit accumulator, and a pseudo-register M can be used almost anywhere that any other register can be used, referring to the memory address pointed to by the HL pair. It also has a 16-bit stack pointer to memory (replacing the 8008's internal stack), and a 16-bit program counter.\n\nThe processor maintains internal flag bits (a status register), which indicate the results of arithmetic and logical instructions. Only certain instructions affect the flags. The flags are:\nBULLET::::- Sign (S), set if the result is negative.\nBULLET::::- Zero (Z), set if the result is zero.\nBULLET::::- Parity (P), set if the number of 1 bits in the result is even.\nBULLET::::- Carry (C), set if the last addition operation resulted in a carry or if the last subtraction operation required a borrow\nBULLET::::- Auxiliary carry (AC or H), used for binary-coded decimal arithmetic (BCD).\n\nThe carry bit can be set or complemented by specific instructions. Conditional-branch instructions test the various flag status bits. The flags can be copied as a group to the accumulator. The A accumulator and the flags together are called the PSW register, or program status word.\n\nAs with many other 8-bit processors, all instructions are encoded in one byte (including register numbers, but excluding immediate data), for simplicity. Some of them are followed by one or two bytes of data, which can be an immediate operand, a memory address, or a port number. Like larger processors, it has automatic CALL and RET instructions for multi-level procedure calls and returns (which can even be conditionally executed, like jumps) and instructions to save and restore any 16-bit register pair on the machine stack. There are also eight one-byte call instructions () for subroutines located at the fixed addresses 00h, 08h, 10h, ..., 38h. These were intended to be supplied by external hardware in order to invoke a corresponding interrupt service routine, but were also often employed as fast system calls. The most sophisticated command is , which is used for exchanging the register pair HL with the value stored at the address indicated by the stack pointer.\n\nMost 8-bit operations can only be performed on the 8-bit accumulator (the A register). For 8-bit operations with two operands, the other operand can be either an immediate value, another 8-bit register, or a memory byte addressed by the 16-bit register pair HL. Direct copying is supported between any two 8-bit registers and between any 8-bit register and an HL-addressed memory byte. Due to the regular encoding of the instruction (using a quarter of available opcode space), there are redundant codes to copy a register into itself (, for instance), which were of little use, except for delays. However, what would have been a copy from the HL-addressed cell into itself (i.e., ) is instead used to encode the halt () instruction, halting execution until an external reset or interrupt occurs.\n\nAlthough the 8080 is generally an 8-bit processor, it also has limited abilities to perform 16-bit operations: Any of the three 16-bit register pairs (BC, DE, or HL, referred to as B, D, H in Intel documents) or SP can be loaded with an immediate 16-bit value (using ), incremented or decremented (using and ), or added to HL (using ). The instruction exchanges the values of the HL and DE register pairs. By adding HL to itself, it is possible to achieve the same result as a 16-bit arithmetical left shift with one instruction. The only 16-bit instructions that affect any flag are , which set the CY (carry) flag in order to allow for programmed 24-bit or 32-bit arithmetic (or larger), needed to implement floating-point arithmetics, for instance.\n\nThe 8080 supports up to 256 input/output (I/O) ports, accessed via dedicated I/O instructions taking port addresses as operands. This I/O mapping scheme was regarded as an advantage, as it freed up the processor's limited address space. Many CPU architectures instead use so-called memory-mapped I/O (MMIO), in which a common address space is used for both RAM and peripheral chips. This removes the need for dedicated I/O instructions, although a drawback in such designs may be that special hardware must be used to insert wait states, as peripherals are often slower than memory. However, in some simple 8080 computers, I/O was indeed addressed as if they were memory cells, \"memory-mapped\", leaving the I/O commands unused. I/O addressing could also sometimes employ the fact that the processor would output the same 8-bit port address to both the lower and the higher address byte (i.e., would put the address 0505h on the 16-bit address bus). Similar I/O-port schemes were used in the backward-compatible Zilog Z80 and Intel 8085, and the closely related x86 microprocessor families.\n\nOne of the bits in the processor state word (see below) indicates that the processor is accessing data from the stack. Using this signal, it is possible to implement a separate stack memory space. However, this feature was seldom used.\n\nFor more advanced systems, during one phase of its working loop, the processor set its \"internal state byte\" on the data bus. This byte contains flags that determine whether the memory or I/O port is accessed and whether it is necessary to handle an interrupt.\n\nThe interrupt system state (enabled or disabled) is also output on a separate pin. For simple systems, where the interrupts are not used, it is possible to find cases where this pin is used as an additional single-bit output port (the popular Radio-86RK computer made in the Soviet Union, for instance).\n\nThe following 8080/8085 assembler source code is for a subroutine named codice_1 that copies a block of data bytes of a given size from one location to another. The data block is copied one byte at a time, and the data movement and looping logic utilizes 16-bit operations.\n\nThe address bus has its own 16 pins, and the data bus has 8 pins that are usable without any multiplexing. Using the two additional pins (read and write signals), it is possible to assemble simple microprocessor devices very easily. Only the separate IO space, interrupts, and DMA need added chips to decode the processor pin signals. However, the processor load capacity is limited, and even simple computers often contained bus amplifiers.\n\nThe processor needs three power sources (−5, +5, and +12 V) and two non-overlapping high-amplitude synchronizing signals. However, at least the late Soviet version КР580ВМ80А was able to work with a single +5 V power source, the +12 V pin being connected to +5 V and the −5 V pin to ground. The processor consumes about 1.3 W of power.\n\nThe pin-out table, from the chip's accompanying documentation, describes the pins as follows:\nBULLET::::- D0 reading interrupt command. In response to the interrupt signal, the processor is reading and executing a single arbitrary command with this flag raised. Normally the supporting chips provide the subroutine call command (CALL or RST), transferring control to the interrupt handling code.\nBULLET::::- D1 reading (low level means writing)\nBULLET::::- D2 accessing stack (probably a separate stack memory space was initially planned)\nBULLET::::- D3 doing nothing, has been halted by the HLT instruction\nBULLET::::- D4 writing data to an output port\nBULLET::::- D5 reading the first byte of an executable instruction\nBULLET::::- D6 reading data from an input port\nBULLET::::- D7 reading data from memory\n  4  D5\n\nA key factor in the success of the 8080 was the broad range of support chips available, providing serial communications, counter/timing, input/output, direct memory access, and programmable interrupt control amongst other functions:\nBULLET::::- 8238 – System controller and bus driver\nBULLET::::- 8251 – Communication controller\nBULLET::::- 8253 – Programmable interval timer\nBULLET::::- 8255 – Programmable peripheral interface\nBULLET::::- 8257 – DMA controller\nBULLET::::- 8259 – Programmable interrupt controller\n\nThe 8080 integrated circuit uses non-saturated enhancement-load nMOS gates, demanding extra voltages (for the load-gate bias). It was manufactured in a silicon gate process using a minimal feature size of 6 µm. A single layer of metal is used to interconnect the approximately 6,000 transistors in the design, but the higher resistance polysilicon layer, which required higher voltage for some interconnects, is implemented with transistor gates. The die size was approximately 20 mm.\n\nThe 8080 is used in many early microcomputers, such as the MITS Altair 8800 Computer, Processor Technology SOL-20 Terminal Computer and IMSAI 8080 Microcomputer, forming the basis for machines running the CP/M operating system (the later, almost fully compatible and more able, Zilog Z80 processor would capitalize on this, with Z80 & CP/M becoming the dominant CPU and OS combination of the period circa 1976 to 1983 much as did the x86 & DOS for the PC a decade later).\n\nEven in 1979 after introduction of the Z80 and 8085 processors, five manufacturers of the 8080 were selling an estimated 500,000 units per month at a price around $3 to $4 each.\n\nThe first single-board microcomputers, such as MYCRO-1 and the \"dyna-micro\" were based on the Intel 8080. One of the early uses of the 8080 was made in the late 1970s by Cubic-Western Data of San Diego, CA in its Automated Fare Collection Systems custom designed for mass transit systems around the world. An early industrial use of the 8080 is as the \"brain\" of the DatagraphiX Auto-COM (Computer Output Microfiche) line of products which takes large amounts of user data from reel-to-reel tape and images it onto microfiche. The Auto-COM instruments also include an entire automated film cutting, processing, washing, and drying sub-system – quite a feat, both then and in the 21st century, to all be accomplished successfully with only an 8-bit microprocessor running at a clock speed of less than 1 MHz with a 64 KB memory limit. Also, several early video arcade games were built around the 8080 microprocessor, including \"Space Invaders\", one of the most popular arcade games ever made.\n\nShortly after the launch of the 8080, the Motorola 6800 competing design was introduced, and after that, the MOS Technology 6502 derivative of the 6800.\n\nZilog introduced the Z80, which has a compatible machine language instruction set and initially used the same assembly language as the 8080, but for legal reasons, Zilog developed a syntactically-different (but code compatible) alternative assembly language for the Z80. At Intel, the 8080 was followed by the compatible and electrically more elegant 8085.\n\nLater Intel issued the assembly-language compatible (but not binary-compatible) 16-bit 8086 and then the 8/16-bit 8088, which was selected by IBM for its new PC to be launched in 1981. Later NEC made the NEC V20 (an 8088 clone with Intel 80186 instruction set compatibility) which also supports an 8080 emulation mode. This is also supported by NEC's V30 (a similarly enhanced 8086 clone). Thus, the 8080, via its instruction set architecture (ISA), made a lasting impact on computer history.\n\nA number of processors compatible with the Intel 8080A were manufactured in the Eastern Bloc: the KR580VM80A (initially marked as KP580ИK80) in the Soviet Union, the MCY7880 made by Unitra CEMI in Poland, the MHB8080A made by TESLA in Czechoslovakia, the 8080APC made by Tungsram / MEV in Hungary, and the MMN8080 made by Microelectronica Bucharest in Romania.\n\n, the 8080 is still in production at Lansdale Semiconductors.\n\nThe 8080 also changed how computers were created. When the 8080 was introduced, computer systems were usually created by computer manufacturers such as Digital Equipment Corporation, Hewlett Packard, or IBM. A manufacturer would produce the whole computer, including processor, terminals, and system software such as compilers and operating system. The 8080 was designed for almost any application \"except\" a complete computer system. Hewlett Packard developed the HP 2640 series of smart terminals around the 8080. The HP 2647 is a terminal which runs the programming language BASIC on the 8080. Microsoft would market as its founding product the first popular language for the 8080, and would later acquire DOS for the IBM PC.\n\nThe 8080 and 8085 gave rise to the 8086, which was designed as a source code compatible (although not binary compatible) extension of the 8085. This design, in turn, later spawned the x86 family of chips, the basis for most CPUs in use today. Many of the 8080's core machine instructions and concepts, for example, registers named \"A\", \"B\", \"C\", and \"D\", and many of the flags used to control conditional jumps, are still in use in the widespread x86 platform. 8080 assembly code can still be directly translated into x86 instructions; all of its core elements are still present.\n\nPCs based upon the 8086 design and its successors evolved into workstations and servers of 16, 32 and 64 bits, with advanced memory protection, segmentation, and multiprocessing features, blurring the difference between small and large computers (the 80286 and 80386's protected mode were important in doing so). The size of chips has grown so that the size and power of large x86 chips is not much different from high end architecture chips, and a common strategy to produce a very large computer is to network many x86 processors.\n\nFederico Faggin, the originator of the 8080 architecture in early 1972, proposed it to Intel's management and pushed for its implementation. He finally got the permission to develop it six months later. Faggin hired Masatoshi Shima from Japan in November 1972, who did the detailed design under his direction, using the design methodology for random logic with silicon gate that Faggin had created for the 4000 family. Stanley Mazor contributed a couple of instructions to the instruction set.\n\nShima finished the layout in August 1973. After the regulation of NMOS fabrication, a prototype of the 8080 was completed in January 1974. It had a flaw, in that driving with standard TTL devices increased the ground voltage because high current flowed into the narrow line. However, Intel had already produced 40,000 units of the 8080 at the direction of the sales section before Shima characterized the prototype. It was released as requiring Low-power Schottky TTL (LS TTL) devices. The 8080A fixed this flaw.\n\nBULLET::::- Asteroid 8080 Intel is named as a pun and praise on the name of Intel 8080.\nBULLET::::- Microsoft's published phone number, 425-882-8080, was chosen because so much early work was on this chip.\nBULLET::::- Many of Intel's main phone numbers also take a similar form: xxx-xxx-8080\n\nBULLET::::- CP/M – operating system\nBULLET::::- S-100 bus\nBULLET::::- MPT8080\n\nBULLET::::- \"8080A/8085 Assembly Language Programming\"; 1st Ed; Lance Leventhal; Adam Osborne & Associates; 495 pages; 1978.\nBULLET::::- \"8080/Z80 Assembly Language - Techniques for Improved Programming\"; 1st Ed; Alan Miller; John Wiley & Sons; 332 pages; 1981; .\nBULLET::::- \"Microprocessor Interfacing Techniques\"; 3rd Ed; Rodnay Zaks and Austin Lesea; Sybex; 466 pages; 1979; .\nBULLET::::- \"Z80 and 8080 Assembly Language Programming\"; 1st Ed; Kathe Spracklen; Hayden; 180 pages; 1979; .\n\nBULLET::::- Intel and other manufacturers' 8080 CPU images and descriptions at cpu-collection.de\nBULLET::::- Scan of the Intel 8080 data book at DataSheetArchive.com\nBULLET::::- Microcomputer Design, Second Edition, 1976\nBULLET::::- 8080 Emulator written in JavaScript\nBULLET::::- Intel 8080/KR580VM80A emulator in JavaScript\nBULLET::::- Intel 8080 Microcomputer Systems User's Manual (September 1975, 262 pages)\nBULLET::::- Intel 8080 Microcomputer Systems User's Manual (September 1975, 234 pages)\n"}
{"id": "15063", "url": "https://en.wikipedia.org/wiki?curid=15063", "title": "Intel 8086", "text": "Intel 8086\n\nThe 8086 (also called iAPX 86) is a 16-bit microprocessor chip designed by Intel between early 1976 and June 8, 1978, when it was released. The Intel 8088, released July 1, 1979, is a slightly modified chip with an external 8-bit data bus (allowing the use of cheaper and fewer supporting ICs), and is notable as the processor used in the original IBM PC design.\n\nThe 8086 gave rise to the x86 architecture, which eventually became Intel's most successful line of processors. On June 5, 2018, Intel released a limited-edition CPU celebrating the anniversary of the Intel 8086, called the Intel Core i7-8086K.\n\nIn 1972, Intel launched the 8008, the first 8-bit microprocessor. It implemented an instruction set designed by Datapoint corporation with programmable CRT terminals in mind, which also proved to be fairly general-purpose. The device needed several additional ICs to produce a functional computer, in part due to it being packaged in a small 18-pin \"memory package\", which ruled out the use of a separate address bus (Intel was primarily a DRAM manufacturer at the time).\n\nTwo years later, Intel launched the 8080, employing the new 40-pin DIL packages originally developed for calculator ICs to enable a separate address bus. It has an extended instruction set that is source-compatible (not binary compatible) with the 8008 and also includes some 16-bit instructions to make programming easier. The 8080 device, was eventually replaced by the depletion-load-based 8085 (1977), which sufficed with a single +5 V power supply instead of the three different operating voltages of earlier chips. Other well known 8-bit microprocessors that emerged during these years are Motorola 6800 (1974), General Instrument PIC16X (1975), MOS Technology 6502 (1975), Zilog Z80 (1976), and Motorola 6809 (1978).\n\nThe 8086 project started in May 1976 and was originally intended as a temporary substitute for the ambitious and delayed iAPX 432 project. It was an attempt to draw attention from the less-delayed 16- and 32-bit processors of other manufacturers (such as Motorola, Zilog, and National Semiconductor) and at the same time to counter the threat from the Zilog Z80 (designed by former Intel employees), which became very successful. Both the architecture and the physical chip were therefore developed rather quickly by a small group of people, and using the same basic microarchitecture elements and physical implementation techniques as employed for the slightly older 8085 (and for which the 8086 also would function as a continuation).\n\nMarketed as source compatible, the 8086 was designed to allow assembly language for the 8008, 8080, or 8085 to be automatically converted into equivalent (suboptimal) 8086 source code, with little or no hand-editing. The programming model and instruction set is (loosely) based on the 8080 in order to make this possible. However, the 8086 design was expanded to support full 16-bit processing, instead of the fairly limited 16-bit capabilities of the 8080 and 8085.\n\nNew kinds of instructions were added as well; full support for signed integers, base+offset addressing, and self-repeating operations were akin to the Z80 design but were all made slightly more general in the 8086. Instructions directly supporting nested ALGOL-family languages such as Pascal and PL/M were also added. According to principal architect Stephen P. Morse, this was a result of a more software-centric approach than in the design of earlier Intel processors (the designers had experience working with compiler implementations). Other enhancements included microcoded multiply and divide instructions and a bus structure better adapted to future coprocessors (such as 8087 and 8089) and multiprocessor systems.\n\nThe first revision of the instruction set and high level architecture was ready after about three months, and as almost no CAD tools were used, four engineers and 12 layout people were simultaneously working on the chip. The 8086 took a little more than two years from idea to working product, which was considered rather fast for a complex design in 1976–1978.\n\nThe 8086 was sequenced using a mixture of random logic and microcode and was implemented using depletion-load nMOS circuitry with approximately 20,000 active transistors (29,000 counting all ROM and PLA sites). It was soon moved to a new refined nMOS manufacturing process called HMOS (for High performance MOS) that Intel originally developed for manufacturing of fast static RAM products. This was followed by HMOS-II, HMOS-III versions, and, eventually, a fully static CMOS version for battery powered devices, manufactured using Intel's CHMOS processes. The original chip measured 33 mm² and minimum feature size was 3.2 μm.\n\nThe architecture was defined by Stephen P. Morse with some help and assistance by Bruce Ravenel (the architect of the 8087) in refining the final revisions. Logic designer Jim McKevitt and John Bayliss were the lead engineers of the hardware-level development team and Bill Pohlman the manager for the project. The legacy of the 8086 is enduring in the basic instruction set of today's personal computers and servers; the 8086 also lent its last two digits to later extended versions of the design, such as the Intel 286 and the Intel 386, all of which eventually became known as the x86 family. (Another reference is that the PCI Vendor ID for Intel devices is 8086.)\n\nAll internal registers, as well as internal and external data buses, are 16 bits wide, which firmly established the \"16-bit microprocessor\" identity of the 8086. A 20-bit external address bus provides a 1 MB physical address space (2 = 1,048,576). This address space is addressed by means of internal memory \"segmentation\". The data bus is multiplexed with the address bus in order to fit all of the control lines into a standard 40-pin dual in-line package. It provides a 16-bit I/O address bus, supporting 64 KB of separate I/O space. The maximum linear address space is limited to 64 KB, simply because internal address/index registers are only 16 bits wide. Programming over 64 KB memory boundaries involves adjusting the segment registers (see below); this difficulty existed until the 80386 architecture introduced wider (32-bit) registers (the memory management hardware in the 80286 did not help in this regard, as its registers are still only 16 bits wide).\n\nSome of the control pins, which carry essential signals for all external operations, have more than one function depending upon whether the device is operated in \"min\" or \"max\" mode. The former mode is intended for small single-processor systems, while the latter is for medium or large systems using more than one processor (a kind of multiprocessor mode). Maximum mode is required when using an 8087 or 8089 coprocessor. The voltage on pin 33 (MN/) determine the mode. Changing the state of pin 33 changes the function of certain other pins, most of which have to do with how the CPU handles the (local) bus. The mode is usually hardwired into the circuit and therefore cannot be changed by software. The workings of these modes are described in terms of timing diagrams in Intel datasheets and manuals. In minimum mode, all control signals are generated by the 8086 itself.\n\nalign=\"center\" \"Intel 8086 registers\"\n\ncolspan=\"21\"  Main registers \ncolspan=\"21\"  Index registers \ncolspan=\"21\"  Program counter \ncolspan=\"21\"  Segment registers \ncolspan=\"21\"  Status register\nThe 8086 has eight more or less general 16-bit registers (including the stack pointer but excluding the instruction pointer, flag register and segment registers). Four of them, AX, BX, CX, DX, can also be accessed as twice as many 8-bit registers (see figure) while the other four, SI, DI, BP, SP, are 16-bit only.\n\nDue to a compact encoding inspired by 8-bit processors, most instructions are one-address or two-address operations, which means that the result is stored in one of the operands. At most one of the operands can be in memory, but this memory operand can also be the \"destination\", while the other operand, the \"source\", can be either \"register\" or \"immediate\". A single memory location can also often be used as both \"source\" and \"destination\" which, among other factors, further contributes to a code density comparable to (and often better than) most eight-bit machines at the time.\n\nThe degree of generality of most registers are much greater than in the 8080 or 8085. However, 8086 registers were more specialized than in most contemporary minicomputers and are also used implicitly by some instructions. While perfectly sensible for the assembly programmer, this makes register allocation for compilers more complicated compared to more orthogonal 16-bit and 32-bit processors of the time such as the PDP-11, VAX, 68000, 32016 etc. On the other hand, being more regular than the rather minimalistic but ubiquitous 8-bit microprocessors such as the 6502, 6800, 6809, 8085, MCS-48, 8051, and other contemporary accumulator-based machines, it is significantly easier to construct an efficient code generator for the 8086 architecture.\n\nAnother factor for this is that the 8086 also introduced some new instructions (not present in the 8080 and 8085) to better support stack-based high-level programming languages such as Pascal and PL/M; some of the more useful instructions are push \"mem-op\", and ret \"size\", supporting the \"Pascal calling convention\" directly. (Several others, such as push \"immed\" and enter, were added in the subsequent 80186, 80286, and 80386 processors.)\n\nA 64 KB (one segment) stack growing towards lower addresses is supported in hardware; 16-bit words are pushed onto the stack, and the top of the stack is pointed to by SS:SP. There are 256 interrupts, which can be invoked by both hardware and software. The interrupts can cascade, using the stack to store the return addresses.\n\nThe 8086 has 64 K of 8-bit (or alternatively 32 K of 16-bit word) I/O port space.\n\nThe 8086 has a 16-bit flags register. Nine of these condition code flags are active, and indicate the current state of the processor: Carry flag (CF), Parity flag (PF), Auxiliary carry flag (AF), Zero flag (ZF), Sign flag (SF), Trap flag (TF), Interrupt flag (IF), Direction flag (DF), and Overflow flag (OF).\nAlso referred to as the status word, the layout of the flags register is as follows:\n! Bit\n! Flag\n\nThere are also three 16-bit segment registers (see figure) that allow the 8086 CPU to access one megabyte of memory in an unusual way. Rather than concatenating the segment register with the address register, as in most processors whose address space exceeds their register size, the 8086 shifts the 16-bit segment only four bits left before adding it to the 16-bit offset (16×segment + offset), therefore producing a 20-bit external (or effective or physical) address from the 32-bit segment:offset pair. As a result, each external address can be referred to by 2 = 4096 different segment:offset pairs.\n\nAlthough considered complicated and cumbersome by many programmers, this scheme also has advantages; a small program (less than 64 KB) can be loaded starting at a fixed offset (such as 0000) in its own segment, avoiding the need for relocation, with at most 15 bytes of alignment waste.\n\nCompilers for the 8086 family commonly support two types of pointer, \"near\" and \"far\". Near pointers are 16-bit offsets implicitly associated with the program's code or data segment and so can be used only within parts of a program small enough to fit in one segment. Far pointers are 32-bit segment:offset pairs resolving to 20-bit external addresses. Some compilers also support \"huge\" pointers, which are like far pointers except that pointer arithmetic on a huge pointer treats it as a linear 20-bit pointer, while pointer arithmetic on a far pointer wraps around within its 16-bit offset without touching the segment part of the address.\n\nTo avoid the need to specify \"near\" and \"far\" on numerous pointers, data structures, and functions, compilers also support \"memory models\" which specify default pointer sizes. The \"tiny\" (max 64K), \"small\" (max 128K), \"compact\" (data > 64K), \"medium\" (code > 64K), \"large\" (code,data > 64K), and \"huge\" (individual arrays > 64K) models cover practical combinations of near, far, and huge pointers for code and data. The \"tiny\" model means that code and data are shared in a single segment, just as in most 8-bit based processors, and can be used to build \".com\" files for instance. Precompiled libraries often come in several versions compiled for different memory models.\n\nAccording to Morse et al.. the designers actually contemplated using an 8-bit shift (instead of 4-bit), in order to create a 16 MB physical address space. However, as this would have forced segments to begin on 256-byte boundaries, and 1 MB was considered very large for a microprocessor around 1976, the idea was dismissed. Also, there were not enough pins available on a low cost 40-pin package for the additional four address bus pins.\n\nIn principle, the address space of the x86 series \"could\" have been extended in later processors by increasing the shift value, as long as applications obtained their segments from the operating system and did not make assumptions about the equivalence of different segment:offset pairs. In practice the use of \"huge\" pointers and similar mechanisms was widespread and the flat 32-bit addressing made possible with the 32-bit offset registers in the 80386 eventually extended the limited addressing range in a more general way (see below).\n\nIntel could have decided to implement memory in 16 bit words (which would have eliminated the signal along with much of the address bus complexities already described). This would mean that all instruction object codes and data would have to be accessed in 16-bit units. Users of the 8080 long ago realized, in hindsight, that the processor makes very efficient use of its memory. By having a large number of 8-bit object codes, the 8080 produces object code as compact as some of the most powerful minicomputers on the market at the time.\n\nIf the 8086 is to retain 8-bit object codes and hence the efficient memory use of the 8080, then it cannot guarantee that (16-bit) opcodes and data will lie on an even-odd byte address boundary. The first 8-bit opcode will shift the next 8-bit instruction to an odd byte or a 16-bit instruction to an odd-even byte boundary. By implementing the signal and the extra logic needed, the 8086 allows instructions to exist as 1-byte, 3-byte or any other odd byte object codes.\n\nSimply put: this is a trade off. If memory addressing is simplified so that memory is only accessed in 16-bit units, memory will be used less efficiently. Intel decided to make the logic more complicated, but memory use more efficient. This was at a time when memory size was considerably smaller, and at a premium, than that which users are used to today.\n\nSmall programs could ignore the segmentation and just use plain 16-bit addressing. This allows 8-bit software to be quite easily ported to the 8086. The authors of most DOS implementations took advantage of this by providing an Application Programming Interface very similar to CP/M as well as including the simple \".com\" executable file format, identical to CP/M. This was important when the 8086 and MS-DOS were new, because it allowed many existing CP/M (and other) applications to be quickly made available, greatly easing acceptance of the new platform.\n\nThe following 8086/8088 assembler source code is for a subroutine named codice_1 that copies a block of data bytes of a given size from one location to another. The data block is copied one byte at a time, and the data movement and looping logic utilizes 16-bit operations.\n\nThe code above uses the BP (base pointer) register to establish a call frame, an area on the stack that contains all of the parameters and local variables for the execution of the subroutine. This kind of calling convention supports reentrant and recursive code, and has been used by most ALGOL-like languages since the late 1950s.\n\nThe above routine is a rather cumbersome way to copy blocks of data. The 8086 provides dedicated instructions for copying strings of bytes. These instructions assume that the source data is stored at DS:SI, the destination data is stored at ES:DI, and that the number of elements to copy is stored in CX. The above routine requires the source and the destination block to be in the same segment, therefore DS is copied to ES. The loop section of the above can be replaced by:\n\nThis copies the block of data one byte at a time. The codice_2 instruction causes the following codice_3 to repeat until CX is zero, automatically incrementing SI and DI and decrementing CX as it repeats. Alternatively the codice_4 instruction can be used to copy 16-bit words (double bytes) at a time (in which case CX counts the number of words copied instead of the number of bytes). Most assemblers will properly recognize the codice_2 instruction if used as an in-line prefix to the codice_3 instruction, as in codice_7.\n\nThis routine will operate correctly if interrupted, because the program counter will continue to point to the codice_8 instruction until the block copy is completed. The copy will therefore continue from where it left off when the interrupt service routine returns control.\n\nAlthough partly shadowed by other design choices in this particular chip, the multiplexed address and data buses limit performance slightly; transfers of 16-bit or 8-bit quantities are done in a four-clock memory access cycle, which is faster on 16-bit, although slower on 8-bit quantities, compared to many contemporary 8-bit based CPUs. As instructions vary from one to six bytes, fetch and execution are made concurrent and decoupled into separate units (as it remains in today's x86 processors): The \"bus interface unit\" feeds the instruction stream to the \"execution unit\" through a 6-byte prefetch queue (a form of loosely coupled pipelining), speeding up operations on registers and immediates, while memory operations became slower (four years later, this performance problem was fixed with the 80186 and 80286). However, the full (instead of partial) 16-bit architecture with a full width ALU meant that 16-bit arithmetic instructions could now be performed with a single ALU cycle (instead of two, via internal carry, as in the 8080 and 8085), speeding up such instructions considerably. Combined with orthogonalizations of operations versus operand types and addressing modes, as well as other enhancements, this made the performance gain over the 8080 or 8085 fairly significant, despite cases where the older chips may be faster (see below).\n\n+ Execution times for typical instructions (in clock cycles)\n!align=left  instruction\n!align=left  register-register\n!align=left  register immediate\n!align=left  register-memory\n!align=left  memory-register\n!align=left  memory-immediate\nmov  2  4 8+EA  9+EA  10+EA\nALU  3 4 9+EA,  16+EA, 17+EA\njump  colspan=\"5\"  \"register\" ≥ 11 ; \"label\" ≥ 15 ; \"condition,label\" ≥ 16\ninteger multiply  colspan=\"5\"  70~160 (depending on operand \"data\" as well as size) \"including\" any EA\ninteger divide  colspan=\"5\"  80~190 (depending on operand \"data\" as well as size) \"including\" any EA\nBULLET::::- EA = time to compute effective address, ranging from 5 to 12 cycles.\nBULLET::::- Timings are best case, depending on prefetch status, instruction alignment, and other factors.\n\nAs can be seen from these tables, operations on registers and immediates were fast (between 2 and 4 cycles), while memory-operand instructions and jumps were quite slow; jumps took more cycles than on the simple 8080 and 8085, and the 8088 (used in the IBM PC) was additionally hampered by its narrower bus. The reasons why most memory related instructions were slow were threefold:\nBULLET::::- Loosely coupled fetch and execution units are efficient for instruction prefetch, but not for jumps and random data access (without special measures).\nBULLET::::- No dedicated address calculation adder was afforded; the microcode routines had to use the main ALU for this (although there was a dedicated \"segment\" + \"offset\" adder).\nBULLET::::- The address and data buses were multiplexed, forcing a slightly longer (33~50%) bus cycle than in typical contemporary 8-bit processors.\n\nHowever, memory access performance was drastically enhanced with Intel's next generation of 8086 family CPUs. The 80186 and 80286 both had dedicated address calculation hardware, saving many cycles, and the 80286 also had separate (non-multiplexed) address and data buses.\n\nThe 8086/8088 could be connected to a mathematical coprocessor to add hardware/microcode-based floating-point performance. The Intel 8087 was the standard math coprocessor for the 8086 and 8088, operating on 80-bit numbers. Manufacturers like Cyrix (8087-compatible) and Weitek (\"not\" 8087-compatible) eventually came up with high-performance floating-point coprocessors that competed with the 8087.\n\nThe clock frequency was originally limited to 5 MHz, but the last versions in HMOS were specified for 10 MHz. HMOS-III and CMOS versions were manufactured for a long time (at least a while into the 1990s) for embedded systems, although its successor, the 80186/80188 (which includes some on-chip peripherals), has been more popular for embedded use.\n\nThe 80C86, the CMOS version of the 8086, was used in the GRiDPad, Toshiba T1200, HP 110, and finally the 1998–1999 Lunar Prospector.\n\nFor the packaging, the Intel 8086 was available both in ceramic and plastic DIP packages.\n\n! Model number\n! Frequency\n! Technology\n! Temperature range\n! Date of release\n! Price (USD)\n\nCompatible—and, in many cases, enhanced—versions were manufactured by Fujitsu, Harris/Intersil, OKI, Siemens AG, Texas Instruments, NEC, Mitsubishi, and AMD. For example, the NEC V20 and NEC V30 pair were hardware-compatible with the 8088 and 8086 even though NEC made original Intel clones μPD8088D and μPD8086D respectively, but incorporated the instruction set of the 80186 along with some (but not all) of the 80186 speed enhancements, providing a drop-in capability to upgrade both instruction set and processing speed without manufacturers having to modify their designs. Such relatively simple and low-power 8086-compatible processors in CMOS are still used in embedded systems.\n\nThe electronics industry of the Soviet Union was able to replicate the 8086 through . The resulting chip, K1810VM86, was binary and pin-compatible with the 8086.\n\ni8086 and i8088 were respectively the cores of the Soviet-made PC-compatible EC1831 and EC1832 desktops. (EC1831 is the EC identification of IZOT 1036C and EC1832 is the EC identification of IZOT 1037C, developed and manufactured in Bulgaria. EC stands for Единая Система.) However, the EC1831 computer (IZOT 1036C) had significant hardware differences from the IBM PC prototype. The EC1831 was the first PC-compatible computer with dynamic bus sizing (US Pat. No 4,831,514). Later some of the EC1831 principles were adopted in PS/2 (US Pat. No 5,548,786) and some other machines (UK Patent Application, Publication No. GB-A-2211325, Published June 28, 1989).\n\nBULLET::::- Intel 8237: direct memory access (DMA) controller\nBULLET::::- Intel 8251: universal synchronous/asynchronous receiver/transmitter at 19.2 kbit/s\nBULLET::::- Intel 8253: programmable interval timer, 3x 16-bit max 10 MHz\nBULLET::::- Intel 8255: programmable peripheral interface, 3x 8-bit I/O pins used for printer connection etc.\nBULLET::::- Intel 8259: programmable interrupt controller\nBULLET::::- Intel 8279: keyboard/display controller, scans a keyboard matrix and display matrix like 7-seg\nBULLET::::- Intel 8282/8283: 8-bit latch\nBULLET::::- Intel 8284: clock generator\nBULLET::::- Intel 8286/8287: bidirectional 8-bit driver. In 1980 both Intel I8286/I8287 (industrial grade) version were available for US$16.25 in quantities of 100.\nBULLET::::- Intel 8288: bus controller\nBULLET::::- Intel 8289: bus arbiter\nBULLET::::- NEC µPD765 or Intel 8272A: floppy controller\n\nBULLET::::- The Intel Multibus-compatible single-board computer ISBC 86/12 was announced in 1978.\nBULLET::::- The Xerox NoteTaker was one of the earliest portable computer designs in 1978 and used three 8086 chips (as CPU, graphics processor, and I/O processor), but never entered commercial production.\nBULLET::::- Seattle Computer Products shipped S-100 bus based 8086 systems (SCP200B) as early as November 1979.\nBULLET::::- The Norwegian Mycron 2000, introduced in 1980.\nBULLET::::- One of the most influential microcomputers of all, the IBM PC, used the Intel 8088, a version of the 8086 with an 8-bit data bus (as mentioned above).\nBULLET::::- The first Compaq Deskpro used an 8086 running at 7.16 MHz, but was compatible with add-in cards designed for the 4.77 MHz IBM PC XT and could switch the CPU down to the lower speed (which also switched in a memory bus buffer to simulate the 8088's slower access) to avoid software timing issues.\nBULLET::::- An 8 MHz 8086-2 was used in the AT&T 6300 PC (built by Olivetti, and known globally under several brands and model numbers), an IBM PC-compatible desktop microcomputer. The M24 / PC 6300 has IBM PC/XT compatible 8-bit expansion slots, but some of them have a proprietary extension providing the full 16-bit data bus of the 8086 CPU (similar in concept to the 16-bit slots of the IBM PC AT, but different in the design details, and physically incompatible), and all system peripherals including the onboard video system also enjoy 16-bit data transfers. The later Olivetti M24SP featured an 8086-2 running at the full maximum 10 MHz.\nBULLET::::- The IBM PS/2 models 25 and 30 were built with an 8 MHz 8086.\nBULLET::::- The Amstrad/Schneider PC1512, PC1640, PC2086, PC3086 and PC5086 all used 8086 CPUs at 8 MHz.\nBULLET::::- The NEC PC-9801.\nBULLET::::- The Tandy 1000 SL-series and RL machines used 9.47 MHz 8086 CPUs.\nBULLET::::- The IBM Displaywriter word processing machine and the Wang Professional Computer, manufactured by Wang Laboratories, also used the 8086.\nBULLET::::- NASA used original 8086 CPUs on equipment for ground-based maintenance of the Space Shuttle Discovery until the end of the space shuttle program in 2011. This decision was made to prevent software regression that might result from upgrading or from switching to imperfect clones.\nBULLET::::- KAMAN Process and Area Radiation Monitors\n\nBULLET::::- Transistor count\nBULLET::::- iAPX, for the iAPX name\n\nBULLET::::- Intel datasheets\nBULLET::::- List of 8086 CPUs and their clones at CPUworld.com\nBULLET::::- 8086 Pinouts\nBULLET::::- Maximum Mode Interface\nBULLET::::- The 8086 User's manual October 1979 INTEL Corporation (PDF document)\nBULLET::::- 8086 program codes using emu8086 (Version 4.08) Emulator\nBULLET::::- Intel 8086/80186 emulator written in C, this file is part of a larger PC emulator\n"}
{"id": "15064", "url": "https://en.wikipedia.org/wiki?curid=15064", "title": "Intel 8088", "text": "Intel 8088\n\nThe Intel 8088 (\"\"eighty-eighty-eight\"\", also called iAPX 88) microprocessor is a variant of the Intel 8086. Introduced on June 1, 1979, the 8088 had an eight-bit external data bus instead of the 16-bit bus of the 8086. The 16-bit registers and the one megabyte address range were unchanged, however. In fact, according to the Intel documentation, the 8086 and 8088 have the same execution unit (EU)—only the bus interface unit (BIU) is different. The original IBM PC was based on the 8088, as were its clones.\n\nThe 8088 was designed at Intel's laboratory in Haifa, Israel, as were a large number of Intel's processors. The 8088 was targeted at economical systems by allowing the use of an eight-bit data path and eight-bit support and peripheral chips; complex circuit boards were still fairly cumbersome and expensive when it was released. The prefetch queue of the 8088 was shortened to four bytes, from the 8086's six bytes, and the prefetch algorithm was slightly modified to adapt to the narrower bus. These modifications of the basic 8086 design were one of the first jobs assigned to Intel's then-new design office and laboratory in Haifa.\n\nVariants of the 8088 with more than 5 MHz maximal clock frequency include the 8088-2, which was fabricated using Intel's new enhanced nMOS process called HMOS and specified for a maximal frequency of 8 MHz. Later followed the 80C88, a fully static CHMOS design, which could operate with clock speeds from 0 to 8 MHz. There were also several other, more or less similar, variants from other manufacturers. For instance, the NEC V20 was a pin-compatible and slightly faster (at the same clock frequency) variant of the 8088, designed and manufactured by NEC. Successive NEC 8088 compatible processors would run at up to 16 MHz. In 1984, Commodore International signed a deal to manufacture the 8088 for use in a licensed Dynalogic Hyperion clone, in a move that was regarded as signaling a major new direction for the company.\n\nWhen announced, the list price of the 8088 was US$124.80.\n\nThe 8088 is architecturally very similar to the 8086. The main difference is that there are only eight data lines instead of the 8086's 16 lines. All of the other pins of the device perform the same function as they do with the 8086 with two exceptions. First, pin 34 is no longer (this is the high-order byte select on the 8086—the 8088 does not have a high-order byte on its eight-bit data bus). Instead it outputs a maximum mode status, . Combined with the IO/ and DT/ signals, the bus cycles can be decoded (it generally indicates when a write operation or an interrupt is in progress). The second change is the pin that signals whether a memory access or input/output access is being made has had it sense reversed. The pin on the 8088 is IO/. On the 8086 part it is /M. The reason for the reversal is that it makes the 8088 compatible with the 8085.\n\nDepending on the clock frequency, the number of memory wait states, as well as on the characteristics of the particular application program, the \"average\" performance for the Intel 8088 ranged approximately from 0.33 to 1 million instructions per second. Meanwhile, the codice_1 and codice_2 instructions, taking two and three cycles respectively, yielded an \"absolute peak\" performance of between and  MIPS per MHz, that is, somewhere in the range 3–5 MIPS at 10 MHz.\n\nThe speed of the execution unit (EU) and the bus of the 8086 CPU was well balanced; with a typical instruction mix, an 8086 could execute instructions out of the prefetch queue a good bit of the time. Cutting down the bus to eight bits made it a serious bottleneck in the 8088. With the speed of instruction fetch reduced by 50% in the 8088 as compared to the 8086, a sequence of fast instructions can quickly drain the four-byte prefetch queue. When the queue is empty, instructions take as long to complete as they take to fetch. Both the 8086 and 8088 take four clock cycles to complete a bus cycle; whereas for the 8086 this means four clocks to transfer two bytes, on the 8088 it is four clocks per byte. Therefore, for example, a two-byte shift or rotate instruction, which takes the EU only two clock cycles to execute, actually takes eight clock cycles to complete if it is not in the prefetch queue. A sequence of such fast instructions prevents the queue from being filled as fast as it is drained, and \nin general, because so many basic instructions execute in fewer than four clocks per instruction byte—including almost all the ALU and data-movement instructions on register operands and some of these on memory operands—it is practically impossible to avoid idling the EU in the 8088 at least ¼ of the time while executing useful real-world programs, and it is not hard to idle it half the time. In short, an 8088 typically runs about half as fast as 8086 clocked at the same rate, because of the bus bottleneck (the only major difference).\n\nA side effect of the 8088 design, with the slow bus and the small prefetch queue, is that the speed of code execution can be very dependent on instruction order. When programming the 8088, for CPU efficiency, it is vital to interleave long-running instructions with short ones whenever possible. For example, a repeated string operation or a shift by three or more will take long enough to allow time for the 4-byte prefetch queue to completely fill. If short instructions (i.e. ones totaling few bytes) are placed between slower instructions like these, the short ones can execute at full speed out of the queue. If, on the other hand, the slow instructions are executed sequentially, back to back, then after the first of them the bus unit will be forced to idle because the queue will already be full, with the consequence that later more of the faster instructions will suffer fetch delays that might have been avoidable. As some instructions, such as single-bit-position shifts and rotates, take literally 4 times as long to fetch as to execute, the overall effect can be a slowdown by a factor of two or more. If those code segments are the bodies of loops, the difference in execution time may be very noticeable on the human timescale.\n\nThe 8088 is also (like the 8086) slow at accessing memory. The same ALU that is used to execute arithmetic and logic instructions is also used to calculate effective addresses. There is a separate adder for adding a shifted segment register to the offset address, but the offset EA itself is always calculated entirely in the main ALU. Furthermore, the loose coupling of the EU and BIU (bus unit) inserts communication overhead between the units, and the four-clock period bus transfer cycle is not particularly streamlined. Contrast this with the two-clock period bus cycle of the 6502 CPU and the 80286's three-clock period bus cycle with pipelining down to two cycles for most transfers. Most 8088 instructions that can operate on either registers or memory, including common ALU and data-movement operations, are at least four times slower for memory operands than for only register operands. Therefore, efficient 8088 (and 8086) programs avoid repeated access of memory operands when possible, loading operands from memory into registers to work with them there and storing back only the finished results. The relatively large general register set of the 8088 compared to its contemporaries assists this strategy. When there are not enough registers for all variables that are needed at once, saving registers by pushing them onto the stack and popping them back to restore them is the fastest way to use memory to augment the registers, as the stack PUSH and POP instructions are the fastest memory operations. The same is probably not true on the 80286 and later; they have dedicated address ALUs and perform memory accesses much faster than the 8088 and 8086.\n\nFinally, because calls, jumps, and interrupts reset the prefetch queue, and because loading the IP register requires communication between the EU and the BIU (since the IP register is in the BIU, not in the EU, where the general registers are), these operations are costly. All jumps and calls take at least 15 clock cycles. Any conditional jump requires four clock cycles if not taken, but if taken, it requires 16 cycles in addition to resetting the prefetch queue; therefore, conditional jumps should be arranged to be not taken most of the time, especially inside loops. In some cases, a sequence of logic and movement operations is faster than a conditional jump that skips over one or two instructions to achieve the same result.\n\nIntel datasheets for the 8086 and 8088 advertised the dedicated multiply and divide instructions (MUL, IMUL, DIV, and IDIV), but they are very slow, on the order of 100–200 clock cycles each. Many simple multiplications by small constants (besides powers of 2, for which shifts can be used) can be done much faster using dedicated short subroutines. The 80286 and 80386 each greatly increased the execution speed of these multiply and divide instructions. \n\nThe original IBM PC was the most influential microcomputer to use the 8088. It used a clock frequency of 4.77 MHz (4/3 the NTSC colorburst frequency). Some of IBM's engineers and other employees wanted to use the IBM 801 processor, some would have preferred the new Motorola 68000, while others argued for a small and simple microprocessor, such as the MOS Technology 6502 or Zilog Z80, which had been used in earlier personal computers. However, IBM already had a history of using Intel chips in its products and had also acquired the rights to manufacture the 8086 family.\n\nIBM chose the 8088 over the 8086 because Intel offered a better price for the former and could supply more units. Another factor was that the 8088 allowed the computer to be based on a modified 8085 design, as it could easily interface with most nMOS chips with 8-bit databuses, i.e. existing and mature, and therefore economical, components. This included ICs originally intended for support and peripheral functions around the 8085 and similar processors (not exclusively Intel's), which were already well known by many engineers, further reducing cost.\n\nThe descendants of the 8088 include the 80188, 80186, 80286, 80386, 80486, and later software-compatible processors, which are in use today.\n\nBULLET::::- Intel 8282/8283: 8-bit latch\nBULLET::::- Intel 8284: clock generator\nBULLET::::- Intel 8286/8287: bidirectional 8-bit driver. Both Intel I8286/I8287 (industrial grade) version were available for US$16.25 in quantities of 100.\nBULLET::::- Intel 8288: bus controller\nBULLET::::- Intel 8289: bus arbiter\nBULLET::::- Intel 8087: Math Co-Processor\n\nBULLET::::- x86 architecture\nBULLET::::- IBM Personal Computer\nBULLET::::- Motorola 68008\nBULLET::::- Maximum mode\nBULLET::::- Minimum mode\nBULLET::::- iAPX for the iAPX designation\nBULLET::::- Professional Graphics Controller\nBULLET::::- Transistor count\n\nBULLET::::- chipdb.org - Intel datasheet for 8088\nBULLET::::- PCJS: Original IBM PC simulation that runs in your web browser\n"}
{"id": "15066", "url": "https://en.wikipedia.org/wiki?curid=15066", "title": "Insulator (electricity)", "text": "Insulator (electricity)\n\nAn electrical insulator is a material whose internal electric charges do not flow freely; very little electric current will flow through it under the influence of an electric field. This contrasts with other materials, semiconductors and conductors, which conduct electric current more easily. The property that distinguishes an insulator is its resistivity; insulators have higher resistivity than semiconductors or conductors. The most common examples are non metals.\n\nA perfect insulator does not exist because even insulators contain small numbers of mobile charges (charge carriers) which can carry current. In addition, all insulators become electrically conductive when a sufficiently large voltage is applied that the electric field tears electrons away from the atoms. This is known as the breakdown voltage of an insulator. Some materials such as glass, paper and Teflon, which have high resistivity, are very good electrical insulators. A much larger class of materials, even though they may have lower bulk resistivity, are still good enough to prevent significant current from flowing at normally used voltages, and thus are employed as insulation for electrical wiring and cables. Examples include rubber-like polymers and most plastics which can be thermoset or thermoplastic in nature.\n\nInsulators are used in electrical equipment to support and separate electrical conductors without allowing current through themselves. An insulating material used in bulk to wrap electrical cables or other equipment is called \"insulation\". The term \"insulator\" is also used more specifically to refer to insulating supports used to attach electric power distribution or transmission lines to utility poles and transmission towers. They support the weight of the suspended wires without allowing the current to flow through the tower to ground.\n\nElectrical insulation is the absence of electrical conduction. Electronic band theory (a branch of physics) said that a charge flows if states are available into which electrons can be excited. This allows electrons to gain energy and thereby move through a conductor such as a metal. If no such states are available, the material is an insulator.\n\nMost (though not all, see Mott insulator) insulators have a large band gap. This occurs because the \"valence\" band containing the highest energy electrons is full, and a large energy gap separates this band from the next band above it. There is always some voltage (called the breakdown voltage) that gives electrons enough energy to be excited into this band. Once this voltage is exceeded the material ceases being an insulator, and charge begins to pass through it. However, it is usually accompanied by physical or chemical changes that permanently degrade the material's insulating properties.\n\nMaterials that lack electron conduction are insulators if they lack other mobile charges as well. For example, if a liquid or gas contains ions, then the ions can be made to flow as an electric current, and the material is a conductor. Electrolytes and plasmas contain ions and act as conductors whether or not electron flow is involved.\n\nWhen subjected to a high enough voltage, insulators suffer from the phenomenon of electrical breakdown. When the electric field applied across an insulating substance exceeds in any location the threshold breakdown field for that substance, the insulator suddenly becomes a conductor, causing a large increase in current, an electric arc through the substance. Electrical breakdown occurs when the electric field in the material is strong enough to accelerate free charge carriers (electrons and ions, which are always present at low concentrations) to a high enough velocity to knock electrons from atoms when they strike them, ionizing the atoms. These freed electrons and ions are in turn accelerated and strike other atoms, creating more charge carriers, in a chain reaction. Rapidly the insulator becomes filled with mobile charge carriers, and its resistance drops to a low level. In a solid, the breakdown voltage is proportional to the band gap energy. When corona discharge occurs, the air in a region around a high-voltage conductor can break down and ionise without a catastrophic increase in current. However, if the region of air breakdown extends to another conductor at a different voltage it creates a conductive path between them, and a large current flows through the air, creating an \"electric arc\". Even a vacuum can suffer a sort of breakdown, but in this case the breakdown or vacuum arc involves charges ejected from the surface of metal electrodes rather than produced by the vacuum itself.\n\nIn addition, all insulators become conductors at very high temperatures as the thermal energy of the valence electrons is sufficient to put them in the conduction band.\n\nIn certain capacitors, shorts between electrodes formed due to dielectric breakdown can disappear when the applied electric field is reduced.\n\nA very flexible coating of an insulator is often applied to electric wire and cable, this is called \"insulated wire\". Wires sometimes don't use an insulating coating, just air, since a solid (e.g. plastic) coating may be impractical. However, wires that touch each other produce cross connections, short circuits, and fire hazards. In coaxial cable the center conductor must be supported exactly in the middle of the hollow shield to prevent EM wave reflections. Finally, wires that expose voltages higher than 60 V can cause human shock and electrocution hazards. Insulating coatings help to prevent all of these problems.\n\nSome wires have a mechanical covering with no voltage rating—e.g.: service-drop, welding, doorbell, thermostat wire. An insulated wire or cable has a voltage rating and a maximum conductor temperature rating. It may not have an ampacity (current-carrying capacity) rating, since this is dependent upon the surrounding environment (e.g. ambient temperature).\n\nIn electronic systems, printed circuit boards are made from epoxy plastic and fibreglass. The nonconductive boards support layers of copper foil conductors. In electronic devices, the tiny and delicate active components are embedded within nonconductive epoxy or phenolic plastics, or within baked glass or ceramic coatings.\n\nIn microelectronic components such as transistors and ICs, the silicon material is normally a conductor because of doping, but it can easily be selectively transformed into a good insulator by the application of heat and oxygen. Oxidised silicon is quartz, i.e. silicon dioxide, the primary component of glass.\n\nIn high voltage systems containing transformers and capacitors, liquid insulator oil is the typical method used for preventing arcs. The oil replaces air in spaces that must support significant voltage without electrical breakdown. Other high voltage system insulation materials include ceramic or glass wire holders, gas, vacuum, and simply placing wires far enough apart to use air as insulation.\n\nOverhead conductors for high-voltage electric power transmission are bare, and are insulated by the surrounding air. Conductors for lower voltages in distribution may have some insulation but are often bare as well. Insulating supports called \"insulators\" are required at the points where they are supported by utility poles or transmission towers. Insulators are also required where the wire enters buildings or electrical devices, such as transformers or circuit breakers, to insulate the wire from the case. These hollow insulators with a conductor inside them are called bushings.\n\nInsulators used for high-voltage power transmission are made from glass, porcelain or composite polymer materials. Porcelain insulators are made from clay, quartz or alumina and feldspar, and are covered with a smooth glaze to shed water. Insulators made from porcelain rich in alumina are used where high mechanical strength is a criterion. Porcelain has a dielectric strength of about 4–10 kV/mm. Glass has a higher dielectric strength, but it attracts condensation and the thick irregular shapes needed for insulators are difficult to cast without internal strains. Some insulator manufacturers stopped making glass insulators in the late 1960s, switching to ceramic materials.\n\nRecently, some electric utilities have begun converting to polymer composite materials for some types of insulators. These are typically composed of a central rod made of fibre reinforced plastic and an outer weathershed made of silicone rubber or ethylene propylene diene monomer rubber (EPDM). Composite insulators are less costly, lighter in weight, and have excellent hydrophobic capability. This combination makes them ideal for service in polluted areas. However, these materials do not yet have the long-term proven service life of glass and porcelain.\n\nThe electrical breakdown of an insulator due to excessive voltage can occur in one of two ways:\nBULLET::::- A \"puncture arc\" is a breakdown and conduction of the material of the insulator, causing an electric arc through the interior of the insulator. The heat resulting from the arc usually damages the insulator irreparably. \"Puncture voltage\" is the voltage across the insulator (when installed in its normal manner) that causes a puncture arc.\nBULLET::::- A \"flashover arc\" is a breakdown and conduction of the air around or along the surface of the insulator, causing an arc along the outside of the insulator. Insulators are usually designed to withstand flashover without damage. \"Flashover voltage\" is the voltage that causes a flash-over arc.\nMost high voltage insulators are designed with a lower flashover voltage than puncture voltage, so they flash over before they puncture, to avoid damage.\n\nDirt, pollution, salt, and particularly water on the surface of a high voltage insulator can create a conductive path across it, causing leakage currents and flashovers. The flashover voltage can be reduced by more than 50% when the insulator is wet. High voltage insulators for outdoor use are shaped to maximise the length of the leakage path along the surface from one end to the other, called the creepage length, to minimise these leakage currents. To accomplish this the surface is moulded into a series of corrugations or concentric disc shapes. These usually include one or more \"sheds\"; downward facing cup-shaped surfaces that act as umbrellas to ensure that the part of the surface leakage path under the 'cup' stays dry in wet weather. Minimum creepage distances are 20–25 mm/kV, but must be increased in high pollution or airborne sea-salt areas.\n\nThese are the common classes of insulator:\nBULLET::::- \"Pin type insulator\" - As the name suggests, the pin type insulator is mounted on a pin on the cross-arm on the pole. There is a groove on the upper end of the insulator. The conductor passes through this groove and is tied to the insulator with annealed wire of the same material as the conductor. Pin type insulators are used for transmission and distribution of communications, and electric power at voltages up to 33 kV. Insulators made for operating voltages between 33 kV and 69 kV tend to be very bulky and have become uneconomical in recent years.\nBULLET::::- \"Post insulator\" - A type of insulator in the 1930s that is more compact than traditional pin-type insulators and which has rapidly replaced many pin-type insulators on lines up to 69 kV and in some configurations, can be made for operation at up to 115 kV.\nBULLET::::- \"Suspension insulator\" - For voltages greater than 33 kV, it is a usual practice to use suspension type insulators, consisting of a number of glass or porcelain discs connected in series by metal links in the form of a string. The conductor is suspended at the bottom end of this string while the top end is secured to the cross-arm of the tower. The number of disc units used depends on the voltage.\nBULLET::::- \"Strain insulator\" - A \"dead end\" or \"anchor\" pole or tower is used where a straight section of line ends, or angles off in another direction. These poles must withstand the lateral (horizontal) tension of the long straight section of wire. To support this lateral load, strain insulators are used. For low voltage lines (less than 11 kV), shackle insulators are used as strain insulators. However, for high voltage transmission lines, strings of cap-and-pin (suspension) insulators are used, attached to the crossarm in a horizontal direction. When the tension load in lines is exceedingly high, such as at long river spans, two or more strings are used in parallel.\nBULLET::::- \"Shackle insulator\" - In early days, the shackle insulators were used as strain insulators. But nowaday, they are frequently used for low voltage distribution lines. Such insulators can be used either in a horizontal position or in a vertical position. They can be directly fixed to the pole with a bolt or to the cross arm.\nBULLET::::- \"Bushing\" - enables one or several conductors to pass through a partition such as a wall or a tank, and insulates the conductors from it.\nBULLET::::- \"Line post insulator\"\nBULLET::::- \"Station post insulator\"\nBULLET::::- \"Cut-out\"\n\nPin-type insulators are unsuitable for voltages greater than about 69 kV line-to-line. Higher transmission voltages use suspension insulator strings, which can be made for any practical transmission voltage by adding insulator elements to the string.\n\nHigher voltage transmission lines usually use modular suspension insulator designs. The wires are suspended from a 'string' of identical disc-shaped insulators that attach to each other with metal clevis pin or ball and socket links. The advantage of this design is that insulator strings with different breakdown voltages, for use with different line voltages, can be constructed by using different numbers of the basic units. Also, if one of the insulator units in the string breaks, it can be replaced without discarding the entire string.\n\nEach unit is constructed of a ceramic or glass disc with a metal cap and pin cemented to opposite sides. To make defective units obvious, glass units are designed so that an overvoltage causes a puncture arc through the glass instead of a flashover. The glass is heat-treated so it shatters, making the damaged unit visible. However the mechanical strength of the unit is unchanged, so the insulator string stays together.\n\nStandard suspension disc insulator units are in diameter and long, can support a load of 80-120 kN (18-27 klbf), have a dry flashover voltage of about 72 kV, and are rated at an operating voltage of 10-12 kV. However, the flashover voltage of a string is less than the sum of its component discs, because the electric field is not distributed evenly across the string but is strongest at the disc nearest to the conductor, which flashes over first. Metal \"grading rings\" are sometimes added around the disc at the high voltage end, to reduce the electric field across that disc and improve flashover voltage.\n\nIn very high voltage lines the insulator may be surrounded by corona rings. These typically consist of toruses of aluminium (most commonly) or copper tubing attached to the line. They are designed to reduce the electric field at the point where the insulator is attached to the line, to prevent corona discharge, which results in power losses.\n\n+Typical number of disc insulator units for standard line voltages\n! Line voltage(kV) !! Discs\n\nThe first electrical systems to make use of insulators were telegraph lines; direct attachment of wires to wooden poles was found to give very poor results, especially during damp weather.\n\nThe first glass insulators used in large quantities had an unthreaded pinhole. These pieces of glass were positioned on a tapered wooden pin, vertically extending upwards from the pole's crossarm (commonly only two insulators to a pole and maybe one on top of the pole itself). Natural contraction and expansion of the wires tied to these \"threadless insulators\" resulted in insulators unseating from their pins, requiring manual reseating.\n\nAmongst the first to produce ceramic insulators were companies in the United Kingdom, with Stiff and Doulton using stoneware from the mid-1840s, Joseph Bourne (later renamed Denby) producing them from around 1860 and Bullers from 1868. Utility patent number 48,906 was granted to Louis A. Cauvet on 25 July 1865 for a process to produce insulators with a threaded pinhole: pin-type insulators still have threaded pinholes.\n\nThe invention of suspension-type insulators made high-voltage power transmission possible. As transmission line voltages reached and passed 60,000 volts, the insulators required become very large and heavy, with insulators made for a safety margin of 88,000 volts being about the practical limit for manufacturing and installation. Suspension insulators, on the other hand, can be connected into strings as long as required for the line's voltage.\n\nA large variety of telephone, telegraph and power insulators have been made; some people collect them, both for their historic interest and for the aesthetic quality of many insulator designs and finishes. One collectors organisation is the US National Insulator Association, which has over 9,000 members.\n\nOften a broadcasting radio antenna is built as a mast radiator, which means that the entire mast structure is energised with high voltage and must be insulated from the ground. Steatite mountings are used. They have to withstand not only the voltage of the mast radiator to ground, which can reach values up to 400 kV at some antennas, but also the weight of the mast construction and dynamic forces. Arcing horns and lightning arresters are necessary because lightning strikes to the mast are common.\n\nGuy wires supporting antenna masts usually have strain insulators inserted in the cable run, to keep the high voltages on the antenna from short circuiting to ground or creating a shock hazard. Often guy cables have several insulators, placed to break up the cable into lengths that prevent unwanted electrical resonances in the guy. These insulators are usually ceramic and cylindrical or egg-shaped (see picture). This construction has the advantage that the ceramic is under compression rather than tension, so it can withstand greater load, and that if the insulator breaks, the cable ends are still linked.\n\nThese insulators also have to be equipped with overvoltage protection equipment. For the dimensions of the guy insulation, static charges on guys have to be considered. For high masts, these can be much higher than the voltage caused by the transmitter, requiring guys divided by insulators in multiple sections on the highest masts. In this case, guys which are grounded at the anchor basements via a coil - or if possible, directly - are the better choice.\n\nFeedlines attaching antennas to radio equipment, particularly twin lead type, often must be kept at a distance from metal structures. The insulated supports used for this purpose are called \"standoff insulators\".\n\nThe most important insulation material is air. A variety of solid, liquid, and gaseous insulators are also used in electrical apparatus. In smaller transformers, generators, and electric motors, insulation on the wire coils consists of up to four thin layers of polymer varnish film. Film insulated magnet wire permits a manufacturer to obtain the maximum number of turns within the available space. Windings that use thicker conductors are often wrapped with supplemental fiberglass insulating tape. Windings may also be impregnated with insulating varnishes to prevent electrical corona and reduce magnetically induced wire vibration. Large power transformer windings are still mostly insulated with paper, wood, varnish, and mineral oil; although these materials have been used for more than 100 years, they still provide a good balance of economy and adequate performance. Busbars and circuit breakers in switchgear may be insulated with glass-reinforced plastic insulation, treated to have low flame spread and to prevent tracking of current across the material.\n\nIn older apparatus made up to the early 1970s, boards made of compressed asbestos may be found; while this is an adequate insulator at power frequencies, handling or repairs to asbestos material can release dangerous fibers into the air and must be carried cautiously. Wire insulated with felted asbestos was used in high-temperature and rugged applications from the 1920s. Wire of this type was sold by General Electric under the trade name \"Deltabeston.\"\n\nLive-front switchboards up to the early part of the 20th century were made of slate or marble. Some high voltage equipment is designed to operate within a high pressure insulating gas such as sulfur hexafluoride. Insulation materials that perform well at power and low frequencies may be unsatisfactory at radio frequency, due to heating from excessive dielectric dissipation.\n\nElectrical wires may be insulated with polyethylene, crosslinked polyethylene (either through electron beam processing or chemical crosslinking), PVC, Kapton, rubber-like polymers, oil impregnated paper, Teflon, silicone, or modified ethylene tetrafluoroethylene (ETFE). Larger power cables may use compressed inorganic powder, depending on the application.\n\nFlexible insulating materials such as PVC (polyvinyl chloride) are used to insulate the circuit and prevent human contact with a 'live' wire – one having voltage of 600 volts or less. Alternative materials are likely to become increasingly used due to EU safety and environmental legislation making PVC less economic.\n\nAll portable or hand-held electrical devices are insulated to protect their user from harmful shock.\n\nClass I insulation requires that the metal body and other exposed metal parts of the device be connected to earth via a \"grounding wire\" that is earthed at the main service panel—but only needs basic insulation on the conductors. This equipment needs an extra pin on the power plug for the grounding connection.\n\nClass II insulation means that the device is \"double insulated\". This is used on some appliances such as electric shavers, hair dryers and portable power tools. Double insulation requires that the devices have both basic and supplementary insulation, each of which is sufficient to prevent electric shock. All internal electrically energized components are totally enclosed within an insulated body that prevents any contact with \"live\" parts. In the EU, double insulated appliances all are marked with a symbol of two squares, one inside the other.\n\nBULLET::::- Dielectric material\nBULLET::::- Electrical conductivity\nBULLET::::- Electrical substation\nBULLET::::- Michael Faraday\nBULLET::::- Henry Clay Fry\nBULLET::::- Grounding kit\nBULLET::::- Kondo insulator\nBULLET::::- Function of Grading rings to Composite Insulator\n"}
