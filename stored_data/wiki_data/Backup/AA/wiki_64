{"id": "6734", "url": "https://en.wikipedia.org/wiki?curid=6734", "title": "Garbage collection (computer science)", "text": "Garbage collection (computer science)\n\nIn computer science, garbage collection (GC) is a form of automatic memory management. The \"garbage collector\", or just \"collector\", attempts to reclaim \"garbage\", or memory occupied by objects that are no longer in use by the program. Garbage collection was invented by John McCarthy around 1959 to simplify manual memory management in Lisp.\n\nGarbage collection is essentially the opposite of manual memory management, which requires the programmer to specify which objects to deallocate and return to the memory system. However, many systems use a combination of approaches, including other techniques such as stack allocation and region inference. Like other memory management techniques, garbage collection may take a significant proportion of total processing time in a program and, as a result, can have significant influence on performance. With good implementations and with enough memory, depending on application, garbage collection can be faster than manual memory management, while the opposite can also be true and has often been the case in the past with sub-optimal GC algorithms.\n\nResources other than memory, such as network sockets, database handles, user interaction windows, file and device descriptors, are not typically handled by garbage collection. Methods used to manage such resources, particularly destructors, may suffice to manage memory as well, leaving no need for GC. Some GC systems allow such other resources to be associated with a region of memory that, when collected, causes rforms the work of reclaiming.\n\nThe basic principles of garbage collection are to find data objects in a program that cannot be accessed in the future, and to reclaim the resources used by those objects.\n\nMany programming languages require garbage collection, either as part of the language specification (for example, Java, C#, D, Go and most scripting languages) or effectively for practical implementation (for example, formal languages like lambda calculus); these are said to be \"garbage collected languages\". Other languages were designed for use with manual memory management, but have garbage-collected implementations available (for example, C and C++). Some languages, like Ada, Modula-3, and C++/CLI, allow both garbage collection and manual memory management to co-exist in the same application by using separate heaps for collected and manually managed objects; others, like D, are garbage-collected but allow the user to manually delete objects and also entirely disable garbage collection when speed is required.\n\nWhile integrating garbage collection into the language's compiler and runtime system enables a much wider choice of methods, \"post-hoc\" GC systems exist, such as Automatic Reference Counting (ARC), including some that do not require recompilation. (\"Post-hoc\" GC is sometimes distinguished as \"litter collection\".) The garbage collector will almost always be closely integrated with the memory allocator.\n\nGarbage collection frees the programmer from manually dealing with memory deallocation. As a result, certain categories of bugs are eliminated or substantially reduced:\n\nBULLET::::- \"Dangling pointer bugs\", which occur when a piece of memory is freed while there are still pointers to it, and one of those pointers is dereferenced. By then the memory may have been reassigned to another use, with unpredictable results.\nBULLET::::- \"Double free bugs\", which occur when the program tries to free a region of memory that has already been freed, and perhaps already been allocated again.\nBULLET::::- Certain kinds of \"memory leaks\", in which a program fails to free memory occupied by objects that have become unreachable, which can lead to memory exhaustion. (Garbage collection typically does not deal with the unbounded accumulation of data that is reachable, but that will actually not be used by the program.)\nBULLET::::- Efficient implementations of persistent data structures\n\nSome of the bugs addressed by garbage collection have security implications.\n\nTypically, garbage collection has certain disadvantages, including consuming additional resources, performance impacts, possible stalls in program execution, and incompatibility with manual resource management.\n\nGarbage collection consumes computing resources in deciding which memory to free, even though the programmer may have already known this information. The penalty for the convenience of not annotating object lifetime manually in the source code is overhead, which can lead to decreased or uneven performance. A peer-reviewed paper from 2005 came to the conclusion that GC needs five times the memory to compensate for this overhead and to perform as fast as explicit memory management. Interaction with memory hierarchy effects can make this overhead intolerable in circumstances that are hard to predict or to detect in routine testing. The impact on performance was also given by Apple as a reason for not adopting garbage collection in iOS despite being the most desired feature.\n\nThe moment when the garbage is actually collected can be unpredictable, resulting in stalls (pauses to shift/free memory) scattered throughout a session. Unpredictable stalls can be unacceptable in real-time environments, in transaction processing, or in interactive programs. Incremental, concurrent, and real-time garbage collectors address these problems, with varying trade-offs.\n\nThe modern GC implementations try to minimize blocking \"stop-the-world\" stalls by doing as much work as possible on the background (i.e. on a separate thread), for example marking unreachable garbage instances while the application process continues to run. In spite of these advancements, for example in the .NET CLR paradigm it is still very difficult to maintain large heaps (millions of objects) with resident objects that get promoted to older generations without incurring noticeable delays (sometimes a few seconds).\n\nThe need for explicit manual resource management (release/close) for non-GCed resources in an object oriented language becomes transitive to composition. That is: in a non-deterministic GC system, if a resource or a resource-like object requires manual resource management (release/close), and this object is used as \"part of\" another object, then the composed object will also become a resource-like object that itself requires manual resource management (release/close).\n\nTracing garbage collection is the most common type of garbage collection, so much so that \"garbage collection\" often refers to tracing garbage collection, rather than other methods such as reference counting. The overall strategy consists of determining which objects should be garbage collected by tracing which objects are \"reachable\" by a chain of references from certain root objects, and considering the rest as garbage and collecting them. However, there are a large number of algorithms used in implementation, with widely varying complexity and performance characteristics.\n\nReference counting garbage collection is where each object has a count of the number of references to it. Garbage is identified by having a reference count of zero. An object's reference count is incremented when a reference to it is created, and decremented when a reference is destroyed. When the count reaches zero, the object's memory is reclaimed.\n\nAs with manual memory management, and unlike tracing garbage collection, reference counting guarantees that objects are destroyed as soon as their last reference is destroyed, and usually only accesses memory which is either in CPU caches, in objects to be freed, or directly pointed by those, and thus tends to not have significant negative side effects on CPU cache and virtual memory operation.\n\nThere are a number of disadvantages to reference counting; this can generally be solved or mitigated by more sophisticated algorithms:\n\nBULLET::::- Cycles: If two or more objects refer to each other, they can create a cycle whereby neither will be collected as their mutual references never let their reference counts become zero. Some garbage collection systems using reference counting (like the one in CPython) use specific cycle-detecting algorithms to deal with this issue. Another strategy is to use weak references for the \"backpointers\" which create cycles. Under reference counting, a weak reference is similar to a weak reference under a tracing garbage collector. It is a special reference object whose existence does not increment the reference count of the referent object. Furthermore, a weak reference is safe in that when the referent object becomes garbage, any weak reference to it \"lapses\", rather than being permitted to remain dangling, meaning that it turns into a predictable value, such as a null reference.\nBULLET::::- Space overhead (reference count): Reference counting requires space to be allocated for each object to store its reference count. The count may be stored adjacent to the object's memory or in a side table somewhere else, but in either case, every single reference-counted object requires additional storage for its reference count. Memory space with the size of an unsigned pointer is commonly used for this task, meaning that 32 or 64 bits of reference count storage must be allocated for each object. On some systems, it may be possible to mitigate this overhead by using a tagged pointer to store the reference count in unused areas of the object's memory. Often, an architecture does not actually allow programs to access the full range of memory addresses that could be stored in its native pointer size; certain number of high bits in the address is either ignored or required to be zero. If an object reliably has a pointer at a certain location, the reference count can be stored in the unused bits of the pointer. For example, each object in Objective-C has a pointer to its class at the beginning of its memory; on the ARM64 architecture using iOS 7, 19 unused bits of this class pointer are used to store the object's reference count.\nBULLET::::- Speed overhead (increment/decrement):In naive implementations, each assignment of a reference and each reference falling out of scope often require modifications of one or more reference counters. However, in the common case, when a reference is copied from an outer scope variable into an inner scope variable, such that the lifetime of the inner variable is bounded by the lifetime of the outer one, the reference incrementing can be eliminated. The outer variable \"owns\" the reference. In the programming language C++, this technique is readily implemented and demonstrated with the use of codice_1 references. Reference counting in C++ is usually implemented using \"smart pointers\" whose constructors, destructors and assignment operators manage the references. A smart pointer can be passed by reference to a function, which avoids the need to copy-construct a new smart pointer (which would increase the reference count on entry into the function and decrease it on exit). Instead the function receives a reference to the smart pointer which is produced inexpensively. The Deutsch-Bobrow method of reference counting capitalizes on the fact that most reference count updates are in fact generated by references stored in local variables. It ignores these references, only counting references in the heap, but before an object with reference count zero can be deleted, the system must verify with a scan of the stack and registers that no other reference to it still exists. A further substantial decrease in the overhead on counter updates can be obtained by update coalescing introduced by Levanoni and Petrank. Consider a pointer that in a given interval of the execution is updated several times. It first points to an object codice_2, then to an object codice_3, and so forth until at the end of the interval it points to some object codice_4. A reference counting algorithm would typically execute codice_5, codice_6, codice_7, codice_8, codice_9, ..., codice_10. But most of these updates are redundant. In order to have the reference count properly evaluated at the end of the interval it is enough to perform codice_5 and codice_10. Levanoni and Petrank measured an elimination of more than 99% of the counter updates in typical Java benchmarks.\n\nBULLET::::- Requires atomicity:When used in a multithreaded environment, these modifications (increment and decrement) may need to be atomic operations such as compare-and-swap, at least for any objects which are shared, or potentially shared among multiple threads. Atomic operations are expensive on a multiprocessor, and even more expensive if they have to be emulated with software algorithms. It is possible to avoid this issue by adding per-thread or per-CPU reference counts and only accessing the global reference count when the local reference counts become or are no longer zero (or, alternatively, using a binary tree of reference counts, or even giving up deterministic destruction in exchange for not having a global reference count at all), but this adds significant memory overhead and thus tends to be only useful in special cases (it is used, for example, in the reference counting of Linux kernel modules). Update coalescing by Levanoni and Petrank can be used to eliminate all atomic operations from the write-barrier. Counters are never updated by the program threads in the course of program execution execution. They are only modified by the collector which executes as a single additional thread with no synchronization. This method can be used as a stop-the-world mechanism for parallel programs, and also with a concurrent reference counting collector.\n\nBULLET::::- Not real-time: Naive implementations of reference counting do not in general provide real-time behavior, because any pointer assignment can potentially cause a number of objects bounded only by total allocated memory size to be recursively freed while the thread is unable to perform other work. It is possible to avoid this issue by delegating the freeing of objects whose reference count dropped to zero to other threads, at the cost of extra overhead.\n\nEscape analysis is a compile-time technique that can convert heap allocations to stack allocations, thereby reducing the amount of garbage collection to be done. This analysis determines whether an object allocated inside a function is accessible outside of it. If a function-local allocation is found to be accessible to another function or thread, the allocation is said to “escape” and cannot be done on the stack. Otherwise, the object may be allocated directly on the stack and released when the function returns, bypassing the heap and associated memory management costs.\n\nGenerally speaking, higher-level programming languages are more likely to have garbage collection as a standard feature. In some languages that do not have built in garbage collection, it can be added through a library, as with the Boehm garbage collector for C and C++.\n\nMost functional programming languages, such as ML, Haskell, and APL, have garbage collection built in. Lisp is especially notable as both the first functional programming language and the first language to introduce garbage collection.\n\nOther dynamic languages, such as Ruby and Julia (but not Perl 5 or PHP before version 5.3, which both use reference counting), JavaScript and ECMAScript also tend to use GC. Object-oriented programming languages such as Smalltalk and Java usually provide integrated garbage collection. Notable exceptions are C++ and Delphi which have destructors.\n\nHistorically, languages intended for beginners, such as BASIC and Logo, have often used garbage collection for heap-allocated variable-length data types, such as strings and lists, so as not to burden programmers with manual memory management. On early microcomputers, with their limited memory and slow processors, BASIC garbage collection could often cause apparently random, inexplicable pauses in the midst of program operation.\n\nSome BASIC interpreters, such as Applesoft BASIC on the Apple II family, repeatedly scanned the string descriptors for the string having the highest address in order to compact it toward high memory, resulting in O(n) performance, which could introduce minutes-long pauses in the execution of string-intensive programs. A replacement garbage collector for Applesoft BASIC published in Call-A.P.P.L.E. (January 1981, pages 40–45, Randy Wigginton) identified a group of strings in every pass over the heap, which cut collection time dramatically. BASIC.System, released with ProDOS in 1983, provided a windowing garbage collector for BASIC that reduced most collections to a fraction of a second.\n\nWhile the Objective-C traditionally had no garbage collection, with the release of OS X 10.5 in 2007 Apple introduced garbage collection for Objective-C 2.0, using an in-house developed runtime collector.\nHowever, with the 2012 release of OS X 10.8, garbage collection was deprecated in favor of LLVM's automatic reference counter (ARC) that was introduced with OS X 10.7. Furthermore, since May 2015 Apple even forbids the usage of garbage collection for new OS X applications in the App Store. For iOS, garbage collection has never been introduced due to problems in application responsivity and performance; instead, iOS uses ARC.\n\nGarbage collection is rarely used on embedded or real-time systems because of the usual need for very tight control over the use of limited resources. However, garbage collectors compatible with many limited environments have been developed. The Microsoft .NET Micro Framework, .NET nanoFramework and Java Platform, Micro Edition are embedded software platforms that, like their larger cousins, include garbage collection.\n\nAmong the most popular Garbage Collectors for JDK could be named:\n\nBULLET::::- G1\n\nBULLET::::- Parallel\n\nBULLET::::- Concurrent mark sweep collector (CMS)\n\nBULLET::::- Serial\n\nBULLET::::- Shenandoah\nBULLET::::- ZGC\n\nComparing Garbage Collectors is a complex task, and different applications can have very different needs. Among the factors of importance are performance overhead (how is the application slowed by the garbage collector), stop-the-world pauses times, frequency and distribution (when the garbage collector freezes the application, how long does it take to complete the process, and how often does this phenomenon happens), scalability, memory allocation performance, ...\n\nCompile-time garbage collection is a form of static analysis allowing memory to be reused and reclaimed based on invariants known during compilation. \n\nThis form of garbage collection has been studied in the Mercury programming language, and it saw greater usage with the introduction of LLVM's automatic reference counter (ARC) into Apple's ecosystem (iOS and OS X) in 2011.\n\nIncremental, concurrent, and real-time garbage collectors have been developed, such as Baker's algorithm or Lieberman's algorithm.\n\nIn Baker's algorithm, the allocation is done in either half of a single region of memory. When it becomes half full, a garbage collection is performed which moves the live objects into the other half and the remaining objects are implicitly deallocated. The running program (the 'mutator') has to check that any object it references is in the correct half, and if not move it across, while a background task is finding all of the objects.\n\nGenerational garbage collection schemes are based on the empirical observation that most objects die young. In generational garbage collection two or more allocation regions (generations) are kept, which are kept separate based on object's age. New objects are created in the \"young\" generation that is regularly collected, and when a generation is full, the objects that are still referenced from older regions are copied into the next oldest generation. Occasionally a full scan is performed.\n\nSome high-level language computer architectures include hardware support for real-time garbage collection.\n\nMost implementations of real-time garbage collectors use tracing. Such real-time garbage collectors meet hard real-time constraints when used with a real-time operating system.\n\nBULLET::::- Destructor (computer programming)\nBULLET::::- International Symposium on Memory Management\nBULLET::::- Memory management\nBULLET::::- Dead-code elimination\nBULLET::::- Smart pointer\nBULLET::::- Virtual memory compression\n\nBULLET::::- (511 pages)\nBULLET::::- (404 pages)\n\nBULLET::::- The Memory Management Reference\nBULLET::::- The Very Basics of Garbage Collection\nBULLET::::- Java SE 6 HotSpot™ Virtual Machine Garbage Collection Tuning\nBULLET::::- TinyGC - an independent implementation of the BoehmGC API\nBULLET::::- Conservative Garbage Collection Implementation for C Language\nBULLET::::- MeixnerGC - an incremental mark and sweep garbage collector for C++ using smart pointers\n"}
{"id": "6736", "url": "https://en.wikipedia.org/wiki?curid=6736", "title": "Canidae", "text": "Canidae\n\nThe biological family Canidae \n\nThe cat-like feliformia and dog-like caniforms emerged within the Carnivoramorpha Around 45–42 Mya (million years ago). The caniforms included the fox-like genus \"Leptocyon\" whose various species existed from 34 Mya before branching 11.9 Mya into Vulpini (foxes) and Canini (canines).\n\nCanids are found on all continents except Antarctica, having arrived independently or accompanied human beings over extended periods of time. Canids vary in size from the gray wolf to the fennec fox. The body forms of canids are similar, typically having long muzzles, upright ears, teeth adapted for cracking bones and slicing flesh, long legs, and bushy tails. They are mostly social animals, living together in family units or small groups and behaving co-operatively. Typically, only the dominant pair in a group breeds, and a litter of young is reared annually in an underground den. Canids communicate by scent signals and vocalizations. One canid, the domestic dog, long ago entered into a partnership with humans and today remains one of the most widely kept domestic animals.\n\nIn the history of the carnivores, the family Canidae is represented by the two extinct subfamilies designated as Hesperocyoninae and Borophaginae, and the extant subfamily Caninae. This subfamily includes all living canids and their most recent fossil relatives. All living canids as a group form a dental monophyletic relationship with the extinct borophagines, with both groups having a bicuspid (two points) on the lower carnassial talonid, which gives this tooth an additional ability in mastication. This, together with the development of a distinct entoconid cusp and the broadening of the talonid of the first lower molar, and the corresponding enlargement of the talon of the upper first molar and reduction of its parastyle distinguish these late Cenozoic canids and are the essential differences that identify their clade.\n\nWithin the Canidae, the results of allozyme and chromosome analyses have previously suggested several phylogenetic divisions:\nBULLET::::1. The wolf-like canids (genus \"Canis\", \"Cuon\", and \"Lycaon\") include the domestic dog \"(Canis lupus familiaris)\", gray wolf (\"Canis lupus\"), red wolf (\"Canis rufus\"), eastern wolf (\"Canis lycaon\"), coyote (\"Canis latrans\"), Eurasian golden jackal (\"Canis aureus\"), African golden wolf (\"Canis anthus\"), Ethiopian wolf (\"Canis simensis\"), black-backed jackal (\"Canis mesomelas\"), side-striped jackal (\"Canis adustus\"), dhole (\"Cuon alpinus\"), and African wild dog (\"Lycaon pictus\").\nBULLET::::2. The fox-like canids include the kit fox (\"Vulpes velox\"), red fox (\"Vulpes vulpes\"), Cape fox (\"Vulpes chama\"), Arctic fox (\"Vulpes lagopus\"), and fennec fox (\"Vulpes zerda\").\nBULLET::::3. The South American canids include the bush dog (\"Speothos venaticus\"), hoary fox (\"Lycalopex uetulus\"), crab-eating fox (\"Cerdocyon thous\"), and maned wolf (\"Chrysocyon brachyurus\").\nBULLET::::4. Various monotypic taxa include the bat-eared fox (\"Otocyon megalotis\"), gray fox (\"Urocyon cinereoargenteus\"), and raccoon dog (\"Nyctereutes procyonoides\").\n\nDNA analysis shows that the first three form monophyletic clades. The wolf-like canids and the South American canids together form the tribe Canini. Molecular data imply a North American origin of living Canidae some 10 Mya and an African origin of wolf-like canines (\"Canis\", \"Cuon\", and \"Lycaon\"), with the jackals being the most basal of this group. The South American clade is rooted by the maned wolf and bush dog, and the fox-like canids by the fennec fox and Blanford's fox. The gray fox and island fox are basal to the other clades; however, this topological difference is not strongly supported.\n\nThe cladogram below is based on the phylogeny of Lindblad-Toh et al. (2005), modified to incorporate recent findings on \"Canis\", \"Vulpes\", \"Lycalopex\", and \"Dusicyon\" species.\n\nThe Canidae today include a diverse group of some 34 species ranging in size from the maned wolf with its long limbs to the short-legged bush dog. Modern canids inhabit forests, tundra, savannahs, and deserts throughout tropical and temperate parts of the world. The evolutionary relationships between the species have been studied in the past using morphological approaches, but more recently, molecular studies have enabled the investigation of phylogenetics relationships. In some species, genetic divergence has been suppressed by the high level of gene flow between different populations and where the species have hybridized, large hybrid zones exist.\n\nCarnivorans evolved from miacoids about 55 Mya during the late Paleocene. Some 5 million years later, the carnivorans split into two main divisions: caniforms (dog-like) and feliforms (cat-like). By 40 Mya, the first member of the dog family proper had arisen. Called \"Prohesperocyon wilsoni\", its fossilized remains have been found in what is now the southwestern part of Texas. The chief features which identify it as a canid include the loss of the upper third molar (part of a trend toward a more shearing bite), and the structure of the middle ear which has an enlarged bulla (the hollow bony structure protecting the delicate parts of the ear). \"Prohesperocyon\" probably had slightly longer limbs than its predecessors, and also had parallel and closely touching toes which differ markedly from the splayed arrangements of the digits in bears.\n\nThe canid family soon subdivided into three subfamilies, each of which diverged during the Eocene: Hesperocyoninae (about 39.74–15 Mya), Borophaginae (about 34–2 Mya), and Caninae (about 34–0 Mya). The Caninae are the only surviving subfamily and all present-day canids, including wolves, foxes, coyotes, jackals, and domestic dogs, belong to it. Members of each subfamily showed an increase in body mass with time and some exhibited specialized hypercarnivorous diets that made them prone to extinction.\n\nBy the Oligocene, all three subfamilies of canids (Hesperocyoninae, Borophaginae, and Caninae) had appeared in the fossil records of North America. The earliest and most primitive branch of the Canidae was the Hesperocyoninae lineage, which included the coyote-sized \"Mesocyon\" of the Oligocene (38–24 Mya). These early canids probably evolved for the fast pursuit of prey in a grassland habitat; they resembled modern civets in appearance. Hesperocyonines eventually became extinct in the middle Miocene. One of the early members of the Hesperocyonines, the genus \"Hesperocyon\", gave rise to \"Archaeocyon\" and \"Leptocyon\". These branches led to the borophagine and canine radiations.\n\nAround 9–10 Mya during the Late Miocene, the \"Canis\", \"Urocyon\", and \"Vulpes\" genera expanded from southwestern North America, where the canine radiation began. The success of these canines was related to the development of lower carnassials that were capable of both mastication and shearing. Around 8 Mya, the Beringian land bridge allowed members of the genus \"Eucyon\" a means to enter Asia and they continued on to colonize Europe.\n\nDuring the Pliocene, around 4–5 Mya, \"Canis lepophagus\" appeared in North America. This was small and sometimes coyote-like. Others were wolf-like in characteristics. \"C. latrans\" (the coyote) is theorized to have descended from \"C. lepophagus\".\n\nThe formation of the Isthmus of Panama, about 3 Mya, joined South America to North America, allowing canids to invade South America, where they diversified. However, the most recent common ancestor of the South American canids lived in North America some 4 Mya and more than one incursion across the new land bridge is likely. One of the resulting lineages consisted of the gray fox (\"Urocyon cinereoargentus\") and the now-extinct dire wolf (\"Canis dirus\"). The other lineage consisted of the so-called South American endemic species; the maned wolf (\"Chrysocyon brachyurus\"), the short-eared dog (\"Atelocynus microtis\"), the bush dog (\"Speothos venaticus\"), the crab-eating fox (\"Cerdocyon thous\"), and the South American foxes (\"Lycalopex\" spp.). The monophyly of this group has been established by molecular means.\n\nDuring the Pleistocene, the North American wolf line appeared, with \"Canis edwardii\", clearly identifiable as a wolf, and \"Canis rufus\" appeared, possibly a direct descendant of \"C. edwardii\". Around 0.8 Mya, \"Canis ambrusteri\" emerged in North America. A large wolf, it was found all over North and Central America and was eventually supplanted by its descendant, the dire wolf, which then spread into South America during the late Pleistocene.\n\nBy 0.3 Mya, a number of subspecies of the gray wolf (\"C. lupus\") had developed and had spread throughout Europe and northern Asia. The gray wolf colonized North America during the late Rancholabrean era across the Bering land bridge, with at least three separate invasions, with each one consisting of one or more different Eurasian gray wolf clades. MtDNA studies have shown that there are at least four extant \"C. lupus\" lineages. The dire wolf shared its habitat with the gray wolf, but became extinct in a large-scale extinction event that occurred around 11,500 years ago. It may have been more of a scavenger than a hunter; its molars appear to be adapted for crushing bones and it may have gone extinct as a result of the extinction of the large herbivorous animals on whose carcasses it relied.\n\nIn 2015, a study of mitochondrial genome sequences and whole genome nuclear sequences of African and Eurasian canids indicated that extant wolf-like canids have colonized Africa from Eurasia at least five times throughout the Pliocene and Pleistocene, which is consistent with fossil evidence suggesting that much of African canid fauna diversity resulted from the immigration of Eurasian ancestors, likely coincident with Plio-Pleistocene climatic oscillations between arid and humid conditions. When comparing the African and Eurasian golden jackals, the study concluded that the African specimens represented a distinct monophyletic lineage that should be recognized as a separate species, \"Canis anthus\" (African golden wolf). According to a phylogeny derived from nuclear sequences, the Eurasian golden jackal (\"Canis aureus\") diverged from the wolf/coyote lineage 1.9 Mya, but the African golden wolf separated 1.3 Mya. Mitochondrial genome sequences indicated the Ethiopian wolf diverged from the wolf/coyote lineage slightly prior to that.\n\nWild canids are found on every continent except Antarctica, and inhabit a wide range of different habitats, including deserts, mountains, forests, and grasslands. They vary in size from the fennec fox, which may be as little as in length and weigh , to the gray wolf, which may be up to long, and can weigh up to . Only a few species are arboreal—the gray fox, the closely related island fox and the raccoon dog habitually climb trees.\n\nAll canids have a similar basic form, as exemplified by the gray wolf, although the relative length of muzzle, limbs, ears and tail vary considerably between species. With the exceptions of the bush dog, the raccoon dog and some domestic breeds of \"Canis lupus\", canids have relatively long legs and lithe bodies, adapted for chasing prey. The tails are bushy and the length and quality of the pelage varies with the season. The muzzle portion of the skull is much more elongated than that of the cat family. The zygomatic arches are wide, there is a transverse lambdoidal ridge at the rear of the cranium and in some species, a sagittal crest running from front to back. The bony orbits around the eye never form a complete ring and the auditory bullae are smooth and rounded. Females have three to seven pairs of mammae.\n\nAll canids are digitigrade, meaning they walk on their toes. The tip of the nose is always naked, as are the cushioned pads on the soles of the feet. These latter consist of a single pad behind the tip of each toe and a more-or-less three-lobed central pad under the roots of the digits. Hairs grow between the pads and in the Arctic fox, the sole of the foot is densely covered with hair at some times of year. With the exception of the four-toed African wild dog (\"Lycaon pictus\"), five toes are on the forefeet, but the pollex (thumb) is reduced and does not reach the ground. On the hind feet are four toes, but in some domestic dogs, a fifth vestigial toe, known as a dewclaw, is sometimes present, but has no anatomical connection to the rest of the foot. The slightly curved nails are not retractile and more-or-less blunt.\n\nThe penis in male canids is supported by a bone called the baculum. It also contains a structure at the base called the bulbus glandis, which helps to create a copulatory tie during mating, locking the animals together for up to an hour. Young canids are born blind, with their eyes opening a few weeks after birth. All living canids (Caninae) have a ligament analogous to the nuchal ligament of ungulates used to maintain the posture of the head and neckwith little active muscle exertion; this ligament allows them to conserve energy while running long distances following scent trails with their nose to the ground. However, based on skeletal details of the neck, at least some of the Borophaginae (such as \"Aelurodon\") are believed to have lacked this ligament.\n\nDentition relates to the arrangement of teeth in the mouth, with the dental notation for the upper-jaw teeth using the upper-case letters I to denote incisors, C for canines, P for premolars, and M for molars, and the lower-case letters i, c, p and m to denote the mandible teeth. Teeth are numbered using one side of the mouth and from the front of the mouth to the back. In carnivores, the upper premolar P4 and the lower molar m1 form the carnassials that are used together in a scissor-like action to shear the muscle and tendon of prey.\n\nCanids use their premolars for cutting and crushing except for the upper fourth premolar P4 (the upper carnassial) that is only used for cutting. They use their molars for grinding except for the lower first molar m1 (the lower carnassial) that has evolved for both cutting and grinding depending on the candid's dietary adaptation. On the lower carnassial, the trigonid is used for slicing and the talonid is used for grinding. The ratio between the trigonid and the talonid indicates a carnivore's dietary habits, with a larger trigonid indicating a hypercarnivore and a larger talonid indicating a more omnivorous diet. Because of its low variability, the length of the lower carnassial is used to provide an estimate of a carnivore's body size.\n\nA study of the estimated bite force at the canine teeth of a large sample of living and fossil mammalian predators, when adjusted for their body mass, found that for placental mammals the bite force at the canines was greatest in the extinct dire wolf (163), followed among the modern canids by the four hypercarnivores that often prey on animals larger than themselves: the African wild dog (142), the gray wolf (136), the dhole (112), and the dingo (108). The bite force at the carnassials showed a similar trend to the canines. A predator's largest prey size is strongly influenced by its biomechanical limits.\n\nMost canids have 42 teeth, with a dental formula of: . The bush dog has only one upper molar with two below, the dhole has two above and two below. and the bat-eared fox has three or four upper molars and four lower ones. The molar teeth are strong in most species, allowing the animals to crack open bone to reach the marrow. The deciduous, or baby teeth, formula in canids is , molars being completely absent.\n\nAlmost all canids are social animals and live together in groups. In general, they are territorial or have a home range and sleep in the open, using their dens only for breeding and sometimes in bad weather. In most foxes, and in many of the true dogs, a male and female pair work together to hunt and to raise their young. Gray wolves and some of the other larger canids live in larger groups called packs. African wild dogs have packs which may consist of 20 to 40 animals and packs of fewer than about seven individuals may be incapable of successful reproduction. Hunting in packs has the advantage that larger prey items can be tackled. Some species form packs or live in small family groups depending on the circumstances, including the type of available food. In most species, some individuals live on their own. Within a canid pack, there is a system of dominance so that the strongest, most experienced animals lead the pack. In most cases, the dominant male and female are the only pack members to breed.\n\nCanids communicate with each other by scent signals, by visual clues and gestures, and by vocalizations such as growls, barks and howls. In most cases, groups have a home territory from which they drive out other conspecifics. The territory is marked by leaving urine scent marks, which warn trespassing individuals. Social behaviour is also mediated by secretions from glands on the upper surface of the tail near its root and from the anal glands.\n\nCanids as a group exhibit several reproductive traits that are uncommon among mammals as a whole. They are typically monogamous, provide paternal care to their offspring, have reproductive cycles with lengthy proestral and dioestral phases and have a copulatory tie during mating. They also retain adult offspring in the social group, suppressing the ability of these to breed while making use of the alloparental care they can provide to help raise the next generation of offspring.\n\nDuring the proestral period, increased levels of oestradiol make the female attractive to the male. There is a rise in progesterone during the oestral phase and the female is now receptive. Following this, the level of oestradiol fluctuates and there is a lengthy dioestrous phase during which the female is pregnant. Pseudo-pregnancy frequently occurs in canids that have ovulated but failed to conceive. A period of anoestrus follows pregnancy or pseudo-pregnancy, there being only one oestral period during each breeding season. Small and medium-sized canids mostly have a gestation period of 50 to 60 days, while larger species average 60 to 65 days. The time of year in which the breeding season occurs is related to the length of day, as has been demonstrated in the case of several species that have been translocated across the equator to the other hemisphere and experiences a six-month shift of phase. Domestic dogs and certain small canids in captivity may come into oestrus more frequently, perhaps because the photoperiod stimulus breaks down under conditions of artificial lighting.\n\nThe size of a litter varies, with from one to 16 or more pups being born. The young are born small, blind and helpless and require a long period of parental care. They are kept in a den, most often dug into the ground, for warmth and protection. When the young begin eating solid food, both parents, and often other pack members, bring food back for them from the hunt. This is most often vomited up from the adult's stomach. Where such pack involvement in the feeding of the litter occurs, the breeding success rate is higher than is the case where females split from the group and rear their pups in isolation. Young canids may take a year to mature and learn the skills they need to survive. In some species, such as the African wild dog, male offspring usually remain in the natal pack, while females disperse as a group and join another small group of the opposite sex to form a new pack.\n\nOne canid, the domestic dog, entered into a partnership with humans a long time ago. The dog was the first domesticated species. The archaeological record shows the first undisputed dog remains buried beside humans 14,700 years ago, with disputed remains occurring 36,000 years ago. These dates imply that the earliest dogs arose in the time of human hunter-gatherers and not agriculturists.\n\nThe fact that wolves are pack animals with cooperative social structures may have been the reason that the relationship developed. Humans benefited from the canid's loyalty, cooperation, teamwork, alertness and tracking abilities, while the wolf may have benefited from the use of weapons to tackle larger prey and the sharing of food. Humans and dogs may have evolved together.\n\nAmong canids, only the gray wolf has widely been known to prey on humans. Nonetheless, at least two records of coyotes killing humans have been published, and at least two other reports of golden jackals killing children. Human beings have trapped and hunted some canid species for their fur and some, especially the gray wolf, the coyote and the red fox, for sport. Canids such as the dhole are now endangered in the wild because of persecution, habitat loss, a depletion of ungulate prey species and transmission of diseases from domestic dogs.\n\nBULLET::::- Canid hybrid\nBULLET::::- Free-ranging dog\n\n"}
{"id": "6739", "url": "https://en.wikipedia.org/wiki?curid=6739", "title": "Subspecies of Canis lupus", "text": "Subspecies of Canis lupus\n\nThere are 38 subspecies of \"Canis lupus\" listed in the taxonomic authority \"Mammal Species of the World\" (2005, 3rd edition). These subspecies were named over the past 250 years, and since their naming a number of them have gone extinct. The nominate subspecies is the Eurasian wolf \"Canis lupus lupus\".\n\nIn 1758, the Swedish botanist and zoologist Carl Linnaeus published in his \"Systema Naturae\" the binomial nomenclature – or the two-word naming – of species. \"Canis\" is the Latin word meaning \"dog\", and under this genus he listed the dog-like carnivores including domestic dogs, wolves, and jackals. He classified the domestic dog as \"Canis familiaris\", and on the next page he classified the wolf as \"Canis lupus\". Linnaeus considered the dog to be a separate species from the wolf because of its \"cauda recurvata\" - its upturning tail which is not found in any other canid.\n\nIn 1999, a study of mitochondrial DNA indicated that the domestic dog may have originated from multiple wolf populations, with the dingo and New Guinea singing dog \"breeds\" having developed at a time when human populations were more isolated from each other. In the third edition of \"Mammal Species of the World\" published in 2005, the mammalogist W. Christopher Wozencraft listed under the wolf \"Canis lupus\" some 36 wild subspecies, and proposed two additional subspecies: \"familiaris\" Linneaus, 1758 and \"dingo\" Meyer, 1793. Wozencraft included \"hallstromi\" – the New Guinea singing dog – as a taxonomic synonym for the dingo. Wozencraft referred to the mDNA study as one of the guides in forming his decision, and listed the 38 subspecies under the biological common name of \"wolf\", with the nominate subspecies being the Eurasian wolf (\"Canis lupus lupus\") based on the type specimen that Linnaeus studied in Sweden. However, the classification of several of these canines as either species or subspecies has recently been challenged.\n\nLiving subspecies recognized by \"MSW3\" and divided into Old World and New World:\n\nSokolov and Rossolimo (1985) recognised nine Old World subspecies of wolf. These were the \"C. l. lupus\", \"C. l. albus\", \"C. l. pallipes\", \"C. l. cubanensis\", \"C. l. campestris\", \"C. l. chanco\", \"C. l. desortorum\", , \"C. l. hattai\", and \"C. l. hodophilax\". In his 1995 statistical analysis of skull morphometrics, mammalogist Robert Nowak recognized the first four of those subspecies, synonymized \"campestris\", \"chanco\" and \"desortorum\" with \"C. l. lupus\", but didn't examine the two Japanese subspecies. In addition, he recognised \"C. l. communis\" as a subspecies distinct from \"C. l. lupus\". In 2003, Nowak also recognized the distinctiveness of \"C. l.\", \"C. l. hattai\", \"C. l. italicus\", and \"C. l. hodophilax\". In 2005, \"MSW3\" included \"C. l. filchneri\". In 2003, two forms were distinguished in southern China and Inner Mongolia as being separate from \"C. l. chanco\" and \"C. l. filchneri\" and have yet to be named.\n+ Eurasian and Australian subspecies of \"Canis lupus\"\n! scope=\"col\" width=12% Subspecies\n! scope=\"col\" width=12% Image\n! scope=\"col\" width=12% Authority\n! scope=\"col\" width=20% Description\n! scope=\"col\" width=18% Range\n! scope=\"col\" width=26% Taxonomic synonyms\n! scope=\"row\"  \"C. l. albus\"Tundra wolf\n\n! scope=\"row\"  \"C. l. arabs\"Arabian wolf\n\n! scope=\"row\"  \"C. l. campestris\"Steppe wolf\n\n! scope=\"row\"  \"C. l. chanco\"Mongolian wolf\n\n! scope=\"row\"  \"C. l. dingo\"Dingo and New Guinea singing dog\n\n! scope=\"row\"  \"C. l. familiaris\"Domestic dog\nThe domestic dog is a divergent subspecies of the gray wolf and was derived from a now-extinct population of Late Pleistocene wolves. Through selective pressure and selective breeding, the domestic dog has developed into hundreds of varied breeds and shows more behavioral and morphological variation than any other land mammal.\n\n! scope=\"row\"  \"C. l. filchneri\"Tibetan wolf\n\n! scope=\"row\"  \"C. l. lupus\"Eurasian wolf(nominate subspecies)\n\n! scope=\"row\"  \"C. l. pallipes\"Indian wolf\nIndia, Pakistan, Iran, Turkey, Saudi Arabia and southern Israel\n\nFor North America, in 1944 the zoologist Edward Goldman recognized as many as 23 subspecies based on morphology. In 1959, E. Raymond Hall proposed that there had been 24 subspecies of \"lupus\" in North America. In 1970, L. David Mech proposed that there was \"probably far too many subspecific designations...in use\" as most did not exhibit enough points of differentiation to be classified as a separate subspecies. The 24 subspecies were accepted by many authorities in 1981 and these were based on morphological or geographical differences, or a unique history. In 1995, the American mammologist Robert M. Nowak analyzed data on the skull morphology of wolf specimens from around the world. For North America, he proposed that there were only five subspecies of the wolf. These include a large-toothed Arctic wolf named \"C. l. arctos\", a large wolf from Alaska and western Canada named \"C. l. occidentalis\", a small wolf from southeastern Canada named \"C. l. lycaon\", a small wolf from the southwestern U.S. named \"C. l. baileyi\" and a moderate-sized wolf that was originally found from Texas to Hudson Bay and from Oregon to Newfoundland named \"C. l. nubilus\".\n\nThe taxonomic classification of \"Canis lupus\" in \"Mammal Species of the World\" (3rd edition, 2005) listed 27 subspecies of North American wolf, corresponding to the 24 \"Canis lupus\" subspecies and the three \"Canis rufus\" subspecies of Hall (1981). The table below shows the extant subspecies, with the extinct ones listed in the following section.\n+ North American subspecies of \"Canis lupus\"\n! scope=\"col\" width=12% Subspecies\n! scope=\"col\" width=12% Image\n! scope=\"col\" width=12% Authority\n! scope=\"col\" width=20% Description\n! scope=\"col\" width=18% Range\n! scope=\"col\" width=26% Taxanomic synonyms\n! scope=\"row\"  \"C. l. arctos\"Arctic wolf\n\n! scope=\"row\"  \"C. l. baileyi\"Mexican wolf\n\n! scope=\"row\"  \"C. l. columbianus\"British Columbian wolf\n\n! scope=\"row\"  \"C. l. crassodon\"Vancouver Island wolf\n\n! scope=\"row\"  \"C. l. hudsonicus\"Hudson Bay wolf\n\n! scope=\"row\"  \"C. l. irremotus\"Northern Rocky Mountain wolf\n\n! scope=\"row\"  \"C. l. labradorius\"Labrador wolf\n\n! scope=\"row\"  \"C. l. ligoni\"Alexander Archipelago wolf\n\n! scope=\"row\"  \"C. l. lycaon\"Eastern wolf\n\n! scope=\"row\"  \"C. l. mackenzii\"Mackenzie River wolf\n\n! scope=\"row\"  \"C. l. manningi\"Baffin Island wolf\n\n! scope=\"row\"  \"C. l. occidentalis\"Northwestern wolf\n\n! scope=\"row\"  \"C. l. orion\"Greenland wolf\n\n! scope=\"row\"  \"C. l. pambasileus\"Alaskan Interior wolf\n\n! scope=\"row\"  \"C. l. rufus\"Red wolf \n\n! scope=\"row\"  \"C. l. tundrarum\"Alaskan tundra wolf\n\nSubspecies recognized by \"MSW3\" which have gone extinct over the past 150 years: \n\n+ Extinct subspecies of \"Canis lupus\"\n! scope=\"col\" width=12% Subspecies\n! scope=\"col\" width=12% Image\n! scope=\"col\" width=12% Authority\n! scope=\"col\" width=20% Description\n! scope=\"col\" width=18% Range\n! scope=\"col\" width=26% Taxanomic synonyms\n! scope=\"row\"  † \"C. l. alces\"Kenai Peninsula wolf\nThe Kenai Peninsula, Alaska\n\n! scope=\"row\"  † \"C. l. beothucus\"Newfoundland wolf\n\n! scope=\"row\"  † \"C. l. bernardi\"Banks Island wolf\nAnderson, 1943\nLimited to Banks and Victoria Islands in the Canadian Arctic \n\n! scope=\"row\"  † \"C. l. floridanus\"Florida black wolf\n\n! scope=\"row\"  † \"C. l. fuscus\"Cascade Mountains wolf\n\n! scope=\"row\"  † \"C. l. gregoryi\"Mississippi Valley wolf\nIn and around the lower Mississippi River basin\n\n! scope=\"row\"  † \"C. l. griseoalbus\"Manitoba wolf\n\n! scope=\"row\"  † \"C. l. hattai\"Hokkaidō wolf\n\n! scope=\"row\"  † \"C. l. hodophilax\"Japanese wolf\n\n! scope=\"row\"  † \"C. l. mogollonensis\"Mogollon Mountains wolf\n\n! scope=\"row\"  † \"C. l. monstrabilis\"Texas wolf\n\n! scope=\"row\"  † \"C. l. nubilus\"Great Plains wolf\n\n! scope=\"row\"  † \"C. l. youngi\"Southern Rocky Mountain wolf\n\nSubspecies discovered since the publishing of \"MSW3\" in 2005 which have gone extinct over the past 150 years: \n\n+ Extinct subspecies of \"Canis lupus\"\n! scope=\"col\" width=12% Subspecies\n! scope=\"col\" width=12% Image\n! scope=\"col\" width=12% Authority\n! scope=\"col\" width=20% Description\n! scope=\"col\" width=18% Range\n! scope=\"col\" width=26% Taxanomic synonyms\n! scope=\"row\"  † \"Canis lupus cristaldii\"Sicilian wolf\n\nThe classification as subspecies of the wolf for the domestic dog, dingo, and New Guinea singing dog is debated by mammalogists.\n\nThe Italian wolf (or Apennine wolf) was first recognised as a distinct subspecies \"Canis lupus italicus\" in 1921 by zoologist Giuseppe Altobello. Altobello's classification was later rejected by several authors, including Reginald Innes Pocock, who synonymised \"C. l. italicus\" with \"C. l. lupus\". In 2002, the noted paleontologist R.M. Nowak reaffirmed the morphological distinctiveness of the Italian wolf and recommended the recognition of \"Canis lupus italicus\". A number of DNA studies have found the Italian wolf to be genetically distinct. In 2004, the genetic distinction of the Italian wolf subspecies was supported by analysis which consistently assigned all the wolf genotypes of a sample in Italy to a single group. This population also showed a unique mitochondrial DNA control-region haplotype, the absence of private alleles and lower heterozygosity at microsatellite loci, as compared to other wolf populations. In 2010, a genetic analysis indicated that a single wolf haplotype (w22) unique to the Apennine Peninsula and one of the two haplotypes (w24, w25), unique to the Iberian Peninsula, belonged to the same haplogroup as the prehistoric wolves of Europe. Another haplotype (w10) was found to be common to the Iberian peninsula and the Balkans. These three populations with geographic isolation exhibited a near lack of gene flow and spatially correspond to three glacial refugia.\n\nThe taxonomic reference \"Mammal Species of the World\" (3rd edition, 2005) does not recognize \"Canis lupus italicus\"; however, NCBI/Genbank publishes research papers under that name.\n\nThe Iberian wolf was first recognised as a distinct subspecies (\"Canis lupus signatus\") in 1907 by zoologist Ángel Cabrera. The wolves of the Iberian peninsula have morphologically distinct features from other Eurasian wolves and each are considered by their researchers to represent their own subspecies.\n\nThe taxonomic reference \"Mammal Species of the World\" (3rd edition, 2005) does not recognize \"Canis lupus signatus\"; however, NCBI/Genbank does list it.\n\nThe Himalayan wolf is a proposed clade within the Tibetan wolf (\"Canis lupus filchneri\") that is distinguished by its mitochondrial DNA, which is basal to all other wolves. The taxonomic status of this wolf is disputed, with the species \"Canis himalayensis\" being proposed based on two limited DNA studies. In 2017, a study of mitochondrial DNA, X-chromosome (maternal lineage) markers and Y-chromosome (male lineage) markers found that the Himalayan wolf was genetically basal to the holarctic grey wolf and has an association with the African golden wolf.\n\nThe taxonomic reference \"Mammal Species of the World\" (3rd edition, 2005) does not recognize \"Canis himalayensis\", however NCBI/Genbank lists it as a new subspecies \"Canis lupus himalayensis\".\n\nThe Indian plains wolf is a proposed clade within the Indian wolf (\"Canis lupus pallipes\") that is distinguished by its mitochondrial DNA, which is basal to all other wolves except for the Himalayan wolf. The taxonomic status of this wolf clade is disputed, with the separate species \"Canis indica\" being proposed based on two limited DNA studies. The proposal has not been endorsed because they relied on a limited number of museum and zoo samples that may not have been representative of the wild population and a call for further fieldwork has been made.\n\nThe taxonomic reference \"Mammal Species of the World\" (3rd edition, 2005) does not recognize \"Canis indica\", however NCBI/Genbank lists it as a new subspecies \"Canis lupus indica\".\n\nA study of the three coastal wolves indicates a close phylogenetic relationship across regions that are geographically and ecologically contiguous, and the study proposed that \"Canis lupus ligoni\" (the Alexander Archipelago wolf), \"Canis lupus columbianus\" (the British Columbian wolf), and \"Canis lupus crassodon\" (the Vancouver Island wolf) should be recognized as a single subspecies of \"Canis lupus\". They share the same habitat and prey species, and form one study's six identified North American ecotypes - a genetically and ecologically distinct population separated from other populations by their different type of habitat.\n\nThe eastern wolf has two proposals over its origin. One is that the eastern wolf is a distinct species (\"C. lycaon\") that evolved in North America, as opposed to the gray wolf that evolved in the Old World, and is related to the red wolf. The other is that it is derived from admixture between gray wolves which inhabited the Great Lakes area and coyotes, forming a hybrid that was classified as a distinct species by mistake.\n\nThe taxonomic reference \"Mammal Species of the World\" (3rd edition, 2005) does not recognize \"Canis lycaon\", however NCBI/Genbank does list it.\n\nThe red wolf is an enigmatic taxon, of which there are two proposals over its origin. One is that the red wolf was a distinct species (\"C. rufus\") that has undergone human-influenced admixture with coyotes. The other is that it was never a distinct species but was derived from admixture between coyotes and gray wolves, due to the gray wolf population being eliminated by humans.\n\nThe taxonomic reference \"Mammal Species of the World\" (3rd edition, 2005) does not recognize \"Canis rufus\", however NCBI/Genbank does list it.\n\nBULLET::::- List of gray wolf populations by country\n\nBULLET::::- Canis lupus on the ITIS (Integrated Taxonomic Information System)\nBULLET::::- Citations for Mammal Species of the World, as a PDF\n"}
{"id": "6742", "url": "https://en.wikipedia.org/wiki?curid=6742", "title": "Central Asia", "text": "Central Asia\n\nCentral Asia is a region which stretches from the Caspian Sea in the west to China in the east, and from Afghanistan and Iran in the south to Russia in the north. The region consists of the former Soviet republics of Kazakhstan, Kyrgyzstan, Tajikistan, Turkmenistan, and Uzbekistan. It is also colloquially referred to as \"the stans\" as the countries generally considered to be within the region all have names ending with the Persian suffix \"-stan\", meaning \"land of\".\n\nIn pre-Islamic and early Islamic times, Central Asia was predominantly Iranian, populated by Eastern Iranian-speaking Bactrians, Sogdians, Chorasmians and the semi-nomadic Scythians and Dahae. After expansion by Turkic peoples, Central Asia also became the homeland for the Kazakhs, Uzbeks, Tatars, Turkmen, Kyrgyz, and Uyghurs; Turkic languages largely replaced the Iranian languages spoken in the area.\n\nCentral Asia has historically been closely tied to its nomadic peoples and the Silk Road. It has acted as a crossroads for the movement of people, goods, and ideas between Europe, Western Asia, South Asia, and East Asia. The Silk Road connected Muslim lands with the people of Europe, India, and China. This crossroads position has intensified the conflict between tribalism and traditionalism and modernization.\n\nFrom the mid-19th century until almost the end of the 20th century, most of Central Asia was part of the Russian Empire and later the Soviet Union, both Slavic-majority countries, and the five former Soviet \"-stans\" are still home to about 7 million ethnic Russians and 500,000 Ukrainians.\n\nCentral Asia (2019) has a population of about 72 million, consisting of five republics: Kazakhstan (pop. /1e6 round 0 million), Kyrgyzstan (/1e6 round 0 million), Tajikistan (/1e6 round 0 million), Turkmenistan (/1e6 round 0 million), and Uzbekistan (33 million).\n\nThe idea of Central Asia as a distinct region of the world was introduced in 1843 by the geographer Alexander von Humboldt. The borders of Central Asia are subject to multiple definitions. Historically built political geography and geoculture are two significant parameters widely used in the scholarly literature about the definitions of the Central Asia. Humboldt's definition included these countries: Afghanistan, Uzbekistan, Turkmenistan, Tajikistan, Kyrgyzstan, Kazakhstan and East Turkestan (Xinjiang).\n\nHowever, the Russian culture has two distinct terms: \"Средняя Азия\" (\"Srednyaya Aziya\" or \"Middle Asia\", the narrower definition, which includes only those traditionally non-Slavic, Central Asian lands that were incorporated within those borders of historical Russia) and \"Центральная Азия\" (\"Tsentralnaya Aziya\" or \"Central Asia\", the wider definition, which includes Central Asian lands that have never been part of historical Russia).\n\nThe most limited definition was the official one of the Soviet Union, which defined Middle Asia as consisting solely of Uzbekistan, Turkmenistan, Tajikistan, and Kyrgyzstan, hence omitting Kazakhstan. This definition was also often used outside the USSR during this period.\nSoon after the dissolution of the Soviet Union in 1991, the leaders of the four former Soviet Central Asian Republics met in Tashkent and declared that the definition of Central Asia should include Kazakhstan as well as the original four included by the Soviets. Since then, this has become the most common definition of Central Asia.\n\nThe UNESCO \"History of the Civilizations of Central Asia\", published in 1992, defines the region as \"Afghanistan, northeastern Iran, northern and central Pakistan, northern India, western China, Mongolia and the former Soviet Central Asian republics.\"\n\nAn alternative method is to define the region based on ethnicity, and in particular, areas populated by Eastern Turkic, Eastern Iranian, or Mongolian peoples. These areas include Xinjiang Uyghur Autonomous Region, the Turkic regions of southern Siberia, the five republics, and Afghan Turkestan. Afghanistan as a whole, the northern and western areas of Pakistan and the Kashmir Valley of India may also be included. The Tibetans and Ladakhi are also included. Insofar, most of the mentioned peoples are considered the \"indigenous\" peoples of the vast region. Central Asia is sometimes referred to as Turkestan.\n\nThere are several places that claim to be the geographic center of Asia, for example Kyzyl, the capital of Tuva in the Russian Federation, and a village north of Ürümqi, the capital of the Xinjiang region of China.\n\nCentral Asia is an extremely large region of varied geography, including high passes and mountains (Tian Shan), vast deserts (Kyzyl Kum, Taklamakan), and especially treeless, grassy steppes. The vast steppe areas of Central Asia are considered together with the steppes of Eastern Europe as a homogeneous geographical zone known as the Eurasian Steppe.\n\nMuch of the land of Central Asia is too dry or too rugged for farming. The Gobi desert extends from the foot of the Pamirs, 77° E, to the Great Khingan (Da Hinggan) Mountains, 116°–118° E.\n\nCentral Asia has the following geographic extremes:\nBULLET::::- The world's northernmost desert (sand dunes), at Buurug Deliin Els, Mongolia, 50°18' N.\nBULLET::::- The Northern Hemisphere's southernmost permafrost, at Erdenetsogt sum, Mongolia, 46°17' N.\nBULLET::::- The world's shortest distance between non-frozen desert and permafrost: .\nBULLET::::- The Eurasian pole of inaccessibility.\n\nA majority of the people earn a living by herding livestock. Industrial activity centers in the region's cities.\n\nMajor rivers of the region include the Amu Darya, the Syr Darya, Irtysh, the Hari River and the Murghab River. Major bodies of water include the Aral Sea and Lake Balkhash, both of which are part of the huge west-central Asian endorheic basin that also includes the Caspian Sea.\n\nBoth of these bodies of water have shrunk significantly in recent decades due to diversion of water from rivers that feed them for irrigation and industrial purposes. Water is an extremely valuable resource in arid Central Asia and can lead to rather significant international disputes.\n\nCentral Asia is bounded on the north by the forests of Siberia. The northern half of Central Asia (Kazakhstan) is the middle part of the Eurasian steppe. Westward the Kazakh steppe merges into the Russian-Ukrainian steppe and eastward into the steppes and deserts of Dzungaria and Mongolia. Southward the land becomes increasingly dry and the nomadic population increasingly thin. The south supports areas of dense population and cities wherever irrigation is possible. The main irrigated areas are along the eastern mountains, along the Oxus and Jaxartes Rivers and along the north flank of the Kopet Dagh near the Persian border. East of the Kopet Dagh is the important oasis of Merv and then a few places in Afghanistan like Herat and Balkh. Two projections of the Tian Shan create three \"bays\" along the eastern mountains. The largest, in the north, is eastern Kazakhstan, traditionally called Jetysu or Semirechye which contains Lake Balkhash. In the center is the small but densely-populated Ferghana valley. In the south is Bactria, later called Tocharistan, which is bounded on the south by the Hindu Kush mountains of Afghanistan. The Syr Darya (Jaxartes) rises in the Ferghana valley and the Amu Darya (Oxus) rises in Bactria. Both flow northwest into the Aral Sea. Where the Oxus meets the Aral Sea it forms a large delta called Khwarazm and later the Khanate of Khiva. North of the Oxus is the less-famous but equally important Zarafshan River which waters the great trading cities of Bokhara and Samarkand. The other great commercial city was Tashkent northwest of the mouth of the Ferghana valley. The land immediately north of the Oxus was called Transoxiana and also Sogdia, especially when referring to the Sogdian merchants who dominated the silk road trade.\n\nTo the east, Dzungaria and the Tarim Basin were united into the Chinese province of Xinjiang about 1759. Caravans from China usually went along the north or south side of the Tarim basin and joined at Kashgar before crossing the mountains northwest to Ferghana or southwest to Bactria. A minor branch of the silk road went north of the Tian Shan through Dzungaria and Zhetysu before turning southwest near Tashkent. Nomadic migrations usually moved from Mongolia through Dzungaria before turning southwest to conquer the settled lands or continuing west toward Europe.\n\nThe Kyzyl Kum Desert or semi-desert is between the Oxus and Jaxartes, and the Karakum Desert is between the Oxus and Kopet Dagh in Turkmenistan. Khorasan meant approximately northeast Persia and northern Afghanistan. Margiana was the region around Merv. The Ustyurt Plateau is between the Aral and Caspian Seas.\n\nTo the southwest, across the Kopet Dagh, lies Persia. From here Persian and Islamic civilization penetrated Central Asia and dominated its high culture until the Russian conquest. In the southeast is the route to India. In early times Buddhism spread north and throughout much of history warrior kings and tribes would move southeast to establish their rule in northern India. Most nomadic conquerors entered from the northeast. After 1800 western civilization in its Russian and Soviet form penetrated from the northwest.\n\nBULLET::::- Ariana\nBULLET::::- Bactria\nBULLET::::- Dihistan (Dahistan)\nBULLET::::- Khorasan\nBULLET::::- Khwarazm\nBULLET::::- Margiana\nBULLET::::- Parthia\nBULLET::::- Sogdia\nBULLET::::- Transoxiana\nBULLET::::- Turan\nBULLET::::- Turkestan\n\nBecause Central Asia is not buffered by a large body of water, temperature fluctuations are often severe, excluding the hot, sunny summer months. In most areas the climate is dry and continental, with hot summers and cool to cold winters, with occasional snowfall. Outside high-elevation areas, the climate is mostly semi-arid to arid. In lower elevations, summers are hot with blazing sunshine. Winters feature occasional rain and/or snow from low-pressure systems that cross the area from the Mediterranean Sea. Average monthly precipitation is extremely low from July to September, rises in autumn (October and November) and is highest in March or April, followed by swift drying in May and June. Winds can be strong, producing dust storms sometimes, especially toward the end of the dry season in September and October. Specific cities that exemplify Central Asian climate patterns include Tashkent and Samarkand, Uzbekistan, Ashgabat, Turkmenistan, and Dushanbe, Tajikistan, the last of these representing one of the wettest climates in Central Asia, with an average annual precipitation of over 22 inches.\n\nAccording to the WWF Ecozones system, Central Asia is part of the Palearctic ecozone. The largest biomes in Central Asia are the temperate grasslands, savannas, and shrublands biome. Central Asia also contains the montane grasslands and shrublands, deserts and xeric shrublands as well as temperate coniferous forests biomes.\n\nAlthough, during the golden age of Orientalism the place of Central Asia in the world history was marginalized, contemporary historiography has rediscovered the \"centrality\" of the Central Asia. The history of Central Asia is defined by the area's climate and geography. The aridness of the region made agriculture difficult, and its distance from the sea cut it off from much trade. Thus, few major cities developed in the region; instead, the area was for millennia dominated by the nomadic horse peoples of the steppe.\n\nRelations between the steppe nomads and the settled people in and around Central Asia were long marked by conflict. The nomadic lifestyle was well suited to warfare, and the steppe horse riders became some of the most militarily potent people in the world, limited only by their lack of internal unity. Any internal unity that was achieved was most probably due to the influence of the Silk Road, which traveled along Central Asia. Periodically, great leaders or changing conditions would organize several tribes into one force and create an almost unstoppable power. These included the Hun invasion of Europe, the Wu Hu attacks on China and most notably the Mongol conquest of much of Eurasia.\n\nDuring pre-Islamic and early Islamic times, southern Central Asia was inhabited predominantly by speakers of Iranian languages. Among the ancient sedentary Iranian peoples, the Sogdians and Chorasmians played an important role, while Iranian peoples such as Scythians and the later on Alans lived a nomadic or semi-nomadic lifestyle. The well-preserved Tarim mummies with Caucasoid features have been found in the Tarim Basin.\n\nThe main migration of Turkic peoples occurred between the 5th and 10th centuries, when they spread across most of Central Asia. The Tang Chinese were defeated by the Arabs at the battle of Talas in 751, marking the end of the Tang Dynasty's western expansion. The Tibetan Empire would take the chance to rule portion of Central Asia along with South Asia. During the 13th and 14th centuries, the Mongols conquered and ruled the largest contiguous empire in recorded history. Most of Central Asia fell under the control of the Chagatai Khanate. \nThe dominance of the nomads ended in the 16th century, as firearms allowed settled peoples to gain control of the region. Russia, China, and other powers expanded into the region and had captured the bulk of Central Asia by the end of the 19th century. After the Russian Revolution, the western Central Asian regions were incorporated into the Soviet Union. The eastern part of Central Asia, known as East Turkistan or Xinjiang, was incorporated into the People's Republic of China. Mongolia remained independent but became a Soviet satellite state. Afghanistan remained relatively independent of major influence by the USSR until the Saur Revolution of 1978.\n\nThe Soviet areas of Central Asia saw much industrialization and construction of infrastructure, but also the suppression of local cultures, hundreds of thousands of deaths from failed collectivization programs, and a lasting legacy of ethnic tensions and environmental problems. Soviet authorities deported millions of people, including entire nationalities, from western areas of the USSR to Central Asia and Siberia. According to Touraj Atabaki and Sanjyot Mehendale, \"From 1959 to 1970, about two million people from various parts of the Soviet Union migrated to Central Asia, of which about one million moved to Kazakhstan.\"\n\nWith the collapse of the Soviet Union, five countries gained independence. In nearly all the new states, former Communist Party officials retained power as local strongmen. None of the new republics could be considered functional democracies in the early days of independence, although in recent years Kyrgyzstan, Kazakhstan and Mongolia have made further progress towards more open societies, unlike Uzbekistan, Tajikistan, and Turkmenistan, which have maintained many Soviet-style repressive tactics.\n\nAt the crossroads of Asia, shamanistic practices live alongside Buddhism. Thus, Yama, Lord of Death, was revered in Tibet as a spiritual guardian and judge. Mongolian Buddhism, in particular, was influenced by Tibetan Buddhism. The Qianlong Emperor of Qing China in the 18th century was Tibetan Buddhist and would sometimes travel from Beijing to other cities for personal religious worship.\n\nCentral Asia also has an indigenous form of improvisational oral poetry that is over 1000 years old. It is principally practiced in Kyrgyzstan and Kazakhstan by \"akyns\", lyrical improvisationists. They engage in lyrical battles, the \"aitysh\" or the \"alym sabak\". The tradition arose out of early bardic oral historians. They are usually accompanied by a stringed instrument—in Kyrgyzstan, a three-stringed komuz, and in Kazakhstan, a similar two-stringed instrument, the dombra.\n\nPhotography in Central Asia began to develop after 1882, when a Russian Mennonite photographer named Wilhelm Penner moved to the Khanate of Khiva during the Mennonite migration to Central Asia led by Claas Epp, Jr. Upon his arrival to Khanate of Khiva, Penner shared his photography skills with a local student Khudaybergen Divanov, who later became the founder of Uzbek photography.\n\nSome also learn to sing the \"Manas\", Kyrgyzstan's epic poem (those who learn the \"Manas\" exclusively but do not improvise are called \"manaschis\"). During Soviet rule, \"akyn\" performance was co-opted by the authorities and subsequently declined in popularity. With the fall of the Soviet Union, it has enjoyed a resurgence, although \"akyns\" still do use their art to campaign for political candidates. A 2005 \"The Washington Post\" article proposed a similarity between the improvisational art of \"akyns\" and modern freestyle rap performed in the West.\n\nAs a consequence of Russian colonization, European fine arts – painting, sculpture and graphics – have developed in Central Asia. The first years of the Soviet regime saw the appearance of modernism, which took inspiration from the Russian avant-garde movement. Until the 1980s, Central Asian arts had developed along with general tendencies of Soviet arts. In the 90's, arts of the region underwent some significant changes. Institutionally speaking, some fields of arts were regulated by the birth of the art market, some stayed as representatives of official views, while many were sponsored by international organizations. The years of 1990–2000 were times for the establishment of contemporary arts. In the region, many important international exhibitions are taking place, Central Asian art is represented in European and American museums, and the Central Asian Pavilion at the Venice Biennale has been organized since 2005.\n\nEquestrian sports are traditional in Central Asia, with disciplines like endurance riding, buzkashi, dzhigit and kyz kuu.\n\nAssociation football is popular across Central Asia. Most countries are members of the Central Asian Football Association, a region of the Asian Football Confederation. However, Kazakhstan is a member of the UEFA.\n\nWrestling is popular across Central Asia, with Kazakhstan having claimed 14 Olympic medals, Uzbekistan seven, and Kyrgyzstan three. As former Soviet states, Central Asian countries have been successful in gymnastics.\n\nMixed Martial Arts is one of more common sports in Central Asia, Kyrgyz athlete Valentina Shevchenko holding the UFC Flyweight Champion title.\n\nCricket is the most popular sport in Afghanistan. The Afghanistan national cricket team, first formed in 2001, has claimed wins over Bangladesh, West Indies and Zimbabwe.\n\nNotable Kazakh competitors include cyclists Alexander Vinokourov and Andrey Kashechkin, boxer Vassiliy Jirov and Gennady Golovkin, runner Olga Shishigina, decathlete Dmitriy Karpov, gymnast Aliya Yussupova, judoka Askhat Zhitkeyev and Maxim Rakov, skier Vladimir Smirnov, weightlifter Ilya Ilyin, and figure skaters Denis Ten and Elizabet Tursynbaeva.\n\nNotable Uzbekistani competitors include cyclist Djamolidine Abdoujaparov, boxer Ruslan Chagaev, canoer Michael Kolganov, gymnast Oksana Chusovitina, tennis player Denis Istomin, chess player Rustam Kasimdzhanov, and figure skater Misha Ge.\n\nSince gaining independence in the early 1990s, the Central Asian republics have gradually been moving from a state-controlled economy to a market economy. The ultimate aim is to emulate the Asian Tigers by becoming the local equivalent, Central Asian snow leopards. However, reform has been deliberately gradual and selective, as governments strive to limit the social cost and ameliorate living standards. All five countries are implementing structural reforms to improve competitiveness. In particular, they have been modernizing the industrial sector and fostering the development of service industries through business-friendly fiscal policies and other measures, to reduce the share of agriculture in GDP. Between 2005 and 2013, the share of agriculture dropped in all but Tajikistan, where it progressed to the detriment of industry. The fastest growth in industry was observed in Turkmenistan, whereas the services sector progressed most in the other four countries.\n\nPublic policies pursued by Central Asian governments focus on buffering the political and economic spheres from external shocks. This includes maintaining a trade balance, minimizing public debt and accumulating national reserves. They cannot totally insulate themselves from negative exterior forces, however, such as the persistently weak recovery of global industrial production and international trade since 2008. Notwithstanding this, they have emerged relatively unscathed from the global financial crisis of 2008–2009. Growth faltered only briefly in Kazakhstan, Tajikistan and Turkmenistan and not at all in Uzbekistan, where the economy grew by more than 7% per year on average between 2008 and 2013. Turkmenistan flirted with growth of 15% (14.7%) in 2011. Kyrgyzstan's performance has been more erratic but this phenomenon was visible well before 2008.\n\nThe republics which have fared best surfed on the wave of the commodities boom during the first decade of the new century. Kazakhstan and Turkmenistan have abundant oil and natural gas reserves and Uzbekistan's own reserves make it more or less self-sufficient. Kyrgyzstan, Tajikistan and Uzbekistan all have gold reserves and Kazakhstan has the world's largest uranium reserves. Fluctuating global demand for cotton, aluminium and other metals (except gold) in recent years has hit Tajikistan hardest, since aluminium and raw cotton are its chief exports − the Tajik Aluminium Company is the country's primary industrial asset. In January 2014, the Minister of Agriculture announced the government's intention to reduce the acreage of land cultivated by cotton to make way for other crops. Uzbekistan and Turkmenistan are major cotton exporters themselves, ranking fifth and ninth respectively worldwide for volume in 2014.\n\nAlthough both exports and imports have grown impressively over the past decade, Central Asian republics countries remain vulnerable to economic shocks, owing to their reliance on exports of raw materials, a restricted circle of trading partners and a negligible manufacturing capacity. Kyrgyzstan has the added disadvantage of being considered resource poor, although it does have ample water. Most of its electricity is generated by hydropower.\n\nThe Kyrgyz economy was shaken by a series of shocks between 2010 and 2012. In April 2010, President Kurmanbek Bakiyev was deposed by a popular uprising, with former minister of foreign affairs Roza Otunbayeva assuring the interim presidency until the election of Almazbek Atambayev in November 2011. Food prices rose two years in a row and, in 2012, production at the major Kumtor gold mine fell by 60% after the site was perturbed by geological movements. According to the World Bank, 33.7% of the population was living in absolute poverty in 2010 and 36.8% a year later.\n\nDespite high rates of economic growth in recent years, GDP per capita in Central Asia was higher than the average for developing countries only in Kazakhstan in 2013 (PPP$23 206) and Turkmenistan (PPP$14 201). It dropped to PPP$5 167 for Uzbekistan, home to 45% of the region's population, and was even lower for Kyrgyzstan and Tajikistan.\n\nKazakhstan leads the Central Asian region in terms of foreign direct investments. The Kazakh economy accounts for more than 70% of all the investment attracted in Central Asia.\n\nIn terms of the economic influence of big powers, China is viewed as one of the key economic players in Central Asia, especially after Beijing launched its grand development strategy known as the Belt and Road Initiative (BRI) in 2013.\n\nBolstered by strong economic growth in all but Kyrgyzstan, national development strategies are fostering new high-tech industries, pooling resources and orienting the economy towards export markets. Many national research institutions established during the Soviet era have since become obsolete with the development of new technologies and changing national priorities. This has led countries to reduce the number of national research institutions since 2009 by grouping existing institutions to create research hubs. Several of the Turkmen Academy of Science's institutes were merged in 2014: the Institute of Botany was merged with the Institute of Medicinal Plants to become the Institute of Biology and Medicinal Plants; the Sun Institute was merged with the Institute of Physics and Mathematics to become the Institute of Solar Energy; and the Institute of Seismology merged with the State Service for Seismology to become the Institute of Seismology and Atmospheric Physics. In Uzbekistan, more than 10 institutions of the Academy of Sciences have been reorganized, following the issuance of a decree by the Cabinet of Ministers in February 2012. The aim is to orient academic research towards problem-solving and ensure continuity between basic and applied research. For example, the Mathematics and Information Technology Research Institute has been subsumed under the National University of Uzbekistan and the Institute for Comprehensive Research on Regional Problems of Samarkand has been transformed into a problem-solving laboratory on environmental issues within Samarkand State University. Other research institutions have remained attached to the Uzbek Academy of Sciences, such as the Centre of Genomics and Bioinformatics.\n\nKazakhstan and Turkmenistan are also building technology parks as part of their drive to modernize infrastructure. In 2011, construction began of a technopark in the village of Bikrova near Ashgabat, the Turkmen capital. It will combine research, education, industrial facilities, business incubators and exhibition centres. The technopark will house research on alternative energy sources (sun, wind) and the assimilation of nanotechnologies. Between 2010 and 2012, technological parks were set up in the east, south and north Kazakhstan oblasts (administrative units) and in the capital, Nur-Sultan. A Centre for Metallurgy was also established in the east Kazakhstan oblast, as well as a Centre for Oil and Gas Technologies which will be part of the planned Caspian Energy Hub. In addition, the Centre for Technology Commercialization has been set up in Kazakhstan as part of the Parasat National Scientific and Technological Holding, a joint stock company established in 2008 that is 100% state-owned. The centre supports research projects in technology marketing, intellectual property protection, technology licensing contracts and start-ups. The centre plans to conduct a technology audit in Kazakhstan and to review the legal framework regulating the commercialization of research results and technology.Countries are seeking to augment the efficiency of traditional extractive sectors but also to make greater use of information and communication technologies and other modern technologies, such as solar energy, to develop the business sector, education and research. In March 2013, two research institutes were created by presidential decree to foster the development of alternative energy sources in Uzbekistan, with funding from the Asian Development Bank and other institutions: the SPU Physical−Technical Institute (Physics Sun Institute) and the International Solar Energy Institute. Three universities have been set up since 2011 to foster competence in strategic economic areas: Nazarbayev University in Kazakhstan (first intake in 2011), an international research university, Inha University in Uzbekistan (first intake in 2014), specializing in information and communication technologies, and the International Oil and Gas University in Turkmenistan (founded in 2013). Kazakhstan and Uzbekistan are both generalizing the teaching of foreign languages at school, in order to facilitate international ties. Kazakhstan and Uzbekistan have both adopted the three-tier bachelor's, master's and PhD degree system, in 2007 and 2012 respectively, which is gradually replacing the Soviet system of Candidates and Doctors of Science. In 2010, Kazakhstan became the only Central Asian member of the Bologna Process, which seeks to harmonize higher education systems in order to create a European Higher Education Area.\n\nThe Central Asian republics' ambition of developing the business sector, education and research is being hampered by chronic low investment in research and development. Over the decade to 2013, the region's investment in research and development hovered around 0.2–0.3% of GDP. Uzbekistan broke with this trend in 2013 by raising its own research intensity to 0.41% of GDP.\n\nKazakhstan is the only country where the business enterprise and private non-profit sectors make any significant contribution to research and development – but research intensity overall is low in Kazakhstan: just 0.18% of GDP in 2013. Moreover, few industrial enterprises conduct research in Kazakhstan. Only one in eight (12.5%) of the country's manufacturing firms were active in innovation in 2012, according to a survey by the UNESCO Institute for Statistics. Enterprises prefer to purchase technological solutions that are already embodied in imported machinery and equipment. Just 4% of firms purchase the license and patents that come with this technology. Nevertheless, there appears to be a growing demand for the products of research, since enterprises spent 4.5 times more on scientific and technological services in 2008 than in 1997.\nKazakhstan and Uzbekistan count the highest researcher density in Central Asia. The number of researchers per million population is close to the world average (1,083 in 2013) in Kazakhstan (1,046) and higher than the world average in Uzbekistan (1,097).\n\nKazakhstan is the only Central Asian country where the business enterprise and private non-profit sectors make any significant contribution to research and development. Uzbekistan is in a particularly vulnerable position, with its heavy reliance on higher education: three-quarters of researchers were employed by the university sector in 2013 and just 6% in the business enterprise sector. With most Uzbek university researchers nearing retirement, this imbalance imperils Uzbekistan's research future. Almost all holders of a Candidate of Science, Doctor of Science or PhD are more than 40 years old and half are aged over 60; more than one in three researchers (38.4%) holds a PhD degree, or its equivalent, the remainder holding a bachelor's or master's degree.\n\nKazakhstan, Kyrgyzstan and Uzbekistan have all maintained a share of women researchers above 40% since the fall of the Soviet Union. Kazakhstan has even achieved gender parity, with Kazakh women dominating medical and health research and representing some 45–55% of engineering and technology researchers in 2013. In Tajikistan, however, only one in three scientists (34%) was a woman in 2013, down from 40% in 2002. Although policies are in place to give Tajik women equal rights and opportunities, these are underfunded and poorly understood. Turkmenistan has offered a state guarantee of equality for women since a law adopted in 2007 but the lack of available data makes it impossible to draw any conclusions as to the law's impact on research. As for Turkmenistan, it does not make data available on higher education, research expenditure or researchers.\n\nTable: PhDs obtained in science and engineering in Central Asia, 2013 or closest year\nWomen (%\n247\n51\n60\n38\n2.3\n499\n63\n16.6\n10.4\n63\n14\nUzbekistan\n}Source: \"UNESCO Science Report: towards 2030\" (2015), Table 14.1\n\n\"Note: PhD graduates in science cover life sciences, physical sciences, mathematics and statistics, and computing; PhDs in engineering also cover manufacturing and construction. For Central Asia, the generic term of PhD also encompasses Candidate of Science and Doctor of Science degrees. Data are unavailable for Turkmenistan.\"\n\nTable: Central Asian researchers by field of science and gender, 2013 or closest year\nTotal\nWomen (%)\nWomen (%)\n2013\n5 091\n1 776\n57.5\nKyrgyzstan\n2011\n44.0\n42.9\n2013\n728\n30.3\n18.0\n472\n256\nUzbekistan\n2011\n1 097\n4 982\n3 659\n1 872\n24.8\n52.0\n}Source: \"UNESCO Science Report: towards 2030\" (2015), Table 14.1\n\nThe number of scientific papers published in Central Asia grew by almost 50% between 2005 and 2014, driven by Kazakhstan, which overtook Uzbekistan over this period to become the region's most prolific scientific publisher, according to Thomson Reuters' Web of Science (Science Citation Index Expanded). Between 2005 and 2014, Kazakhstan's share of scientific papers from the region grew from 35% to 56%. Although two-thirds of papers from the region have a foreign co-author, the main partners tend to come from beyond Central Asia, namely the Russian Federation, USA, German, United Kingdom and Japan.\n\nFive Kazakh patents were registered at the US Patent and Trademark Office between 2008 and 2013, compared to three for Uzbek inventors and none at all for the other three Central Asian republics, Kyrgyzstan, Tajikistan and Turkmenistan.\nKazakhstan is Central Asia's main trader in high-tech products. Kazakh imports nearly doubled between 2008 and 2013, from US$2.7 billion to US$5.1 billion. There has been a surge in imports of computers, electronics and telecommunications; these products represented an investment of US$744 million in 2008 and US$2.6 billion five years later. The growth in exports was more gradual – from US$2.3 billion to US$3.1 billion – and dominated by chemical products (other than pharmaceuticals), which represented two-thirds of exports in 2008 (US$1.5 billion) and 83% (US$2.6 billion) in 2013.\n\nThe five Central Asian republics belong to several international bodies, including the Organization for Security and Co-operation in Europe, the Economic Cooperation Organization and the Shanghai Cooperation Organisation. They are also members of the Central Asia Regional Economic Cooperation (CAREC) Programme, which also includes Afghanistan, Azerbaijan, China, Mongolia and Pakistan. In November 2011, the 10 member countries adopted the \"CAREC 2020 Strategy\", a blueprint for furthering regional co-operation. Over the decade to 2020, US$50 billion is being invested in priority projects in transport, trade and energy to improve members' competitiveness. The landlocked Central Asian republics are conscious of the need to co-operate in order to maintain and develop their transport networks and energy, communication and irrigation systems. Only Kazakhstan and Turkmenistan border the Caspian Sea and none of the republics has direct access to an ocean, complicating the transportation of hydrocarbons, in particular, to world markets.\n\nKazakhstan is also one of the three founding members of the Eurasian Economic Union in 2014, along with Belarus and the Russian Federation. Armenia and Kyrgyzstan have since joined this body. As co-operation among the member states in science and technology is already considerable and well-codified in legal texts, the Eurasian Economic Union is expected to have a limited additional impact on co-operation among public laboratories or academia but it should encourage business ties and scientific mobility, since it includes provision for the free circulation of labour and unified patent regulations.\n\nKazakhstan and Tajikistan participated in the Innovative Biotechnologies Programme (2011–2015) launched by the Eurasian Economic Community, the predecessor of the Eurasian Economic Union, The programme also involved Belarus and the Russian Federation. Within this programme, prizes were awarded at an annual bio-industry exhibition and conference. In 2012, 86 Russian organizations participated, plus three from Belarus, one from Kazakhstan and three from Tajikistan, as well as two scientific research groups from Germany. At the time, Vladimir Debabov, Scientific Director of the Genetika State Research Institute for Genetics and the Selection of Industrial Micro-organisms in the Russian Federation, stressed the paramount importance of developing bio-industry. 'In the world today, there is a strong tendency to switch from petrochemicals to renewable biological sources,' he said. 'Biotechnology is developing two to three times faster than chemicals.'\n\nKazakhstan also participated in a second project of the Eurasian Economic Community, the establishment of the Centre for Innovative Technologies on 4 April 2013, with the signing of an agreement between the Russian Venture Company (a government fund of funds), the Kazakh JSC National Agency and the Belarusian Innovative Foundation. Each of the selected projects is entitled to funding of US$3–90 million and is implemented within a public–private partnership. The first few approved projects focused on supercomputers, space technologies, medicine, petroleum recycling, nanotechnologies and the ecological use of natural resources. Once these initial projects have spawned viable commercial products, the venture company plans to reinvest the profits in new projects. This venture company is not a purely economic structure; it has also been designed to promote a common economic space among the three participating countries.\n\nFour of the five Central Asian republics have also been involved in a project launched by the European Union in September 2013, IncoNet CA. The aim of this project is to encourage Central Asian countries to participate in research projects within Horizon 2020, the European Union's eighth research and innovation funding programme. The focus of this research projects is on three societal challenges considered as being of mutual interest to both the European Union and Central Asia, namely: climate change, energy and health. IncoNet CA builds on the experience of earlier projects which involved other regions, such as Eastern Europe, the South Caucasus and the Western Balkans. IncoNet CA focuses on twinning research facilities in Central Asia and Europe. It involves a consortium of partner institutions from Austria, the Czech Republic, Estonia, Germany, Hungary, Kazakhstan, Kyrgyzstan, Poland, Portugal, Tajikistan, Turkey and Uzbekistan. In May 2014, the European Union launched a 24-month call for project applications from twinned institutions – universities, companies and research institutes – for funding of up to €10, 000 to enable them to visit one another's facilities to discuss project ideas or prepare joint events like workshops.\n\nThe International Science and Technology Center (ISTC) was established in 1992 by the European Union, Japan, the Russian Federation and the US to engage weapons scientists in civilian research projects and to foster technology transfer. ISTC branches have been set up in the following countries party to the agreement: Armenia, Belarus, Georgia, Kazakhstan, Kyrgyzstan and Tajikistan. The headquarters of ISTC were moved to Nazarbayev University in Kazakhstan in June 2014, three years after the Russian Federation announced its withdrawal from the centre.\n\nKyrgyzstan, Tajikistan and Kazakhstan have been members of the World Trade Organization since 1998, 2013 and 2015 respectively.\n\n style=\"background:#ececec;\"\n! scope=\"col\" style=\"width:120px;\"Country\n! scope=\"col\" style=\"width:80px;\"Areakm²\n! scope=\"col\" style=\"width:100px;\"Population()\n! scope=\"col\" style=\"width:50px;\"Population densityper km\n! scope=\"col\" style=\"width:100px;\"Nominal GDP (2017)\n! GDP per capita (2017)\n! HDI (2017)\n! Capital\n! Official languages\n\nBy a broad definition including Mongolia and Afghanistan, more than 90 million people live in Central Asia, about 2% of Asia's total population. Of the regions of Asia, only North Asia has fewer people. It has a population density of 9 people per km, vastly less than the 80.5 people per km of the continent as a whole.\n\nRussian, as well as being spoken by around six million ethnic Russians and Ukrainians of Central Asia, is the de facto lingua franca throughout the former Soviet Central Asian Republics. Mandarin Chinese has an equally dominant presence in Inner Mongolia, Qinghai and Xinjiang.\n\nThe languages of the majority of the inhabitants of the former Soviet Central Asian Republics belong to the Turkic language group. Turkmen, is mainly spoken in Turkmenistan, and as a minority language in Afghanistan, Russia, Iran and Turkey. Kazakh and Kyrgyz are related languages of the Kypchak group of Turkic languages and are spoken throughout Kazakhstan, Kyrgyzstan, and as a minority language in Tajikistan, Afghanistan and Xinjiang. Uzbek and Uyghur are spoken in Uzbekistan, Tajikistan, Kyrgyzstan, Afghanistan and Xinjiang.\n\nThe Turkic languages may belong to a larger, but controversial, Altaic language family, which includes Mongolian. Mongolian is spoken throughout Mongolia and into Buryatia, Kalmyk, Tuva, Inner Mongolia, and Xinjiang.\n\nMiddle Iranian languages were once spoken throughout Central Asia, such as the once prominent Sogdian, Khwarezmian, Bactrian and Scythian, which are now extinct and belonged to the Eastern Iranian family. The Eastern Iranian Pashto language is still spoken in Afghanistan and northwestern Pakistan. Other minor Eastern Iranian languages such as Shughni, Munji, Ishkashimi, Sarikoli, Wakhi, Yaghnobi and Ossetic are also spoken at various places in Central Asia. Varieties of Persian are also spoken as a major language in the region, locally known as Dari (in Afghanistan), Tajik (in Tajikistan and Uzbekistan), and Bukhori (by the Bukharan Jews of Central Asia).\n\nTocharian, another Indo-European language group, which was once predominant in oases on the northern edge of the Tarim Basin of Xinjiang, is now extinct.\n\nOther language groups include the Tibetic languages, spoken by around six million people across the Tibetan Plateau and into Qinghai, Sichuan, Ladakh and Baltistan, and the Nuristani languages of northeastern Afghanistan. Dardic languages, such as Shina, Kashmiri, Pashayi and Khowar, are also spoken in eastern Afghanistan, the Gilgit-Baltistan and Khyber Pakhtunkhwa of Pakistan and the disputed territory of Kashmir. Korean is spoken by the Koryo-saram minority, mainly in Kazakhstan and Uzbekistan.\n\nIslam is the religion most common in the Central Asian Republics, Afghanistan, Xinjiang and the peripheral western regions, such as Bashkortostan. Most Central Asian Muslims are Sunni, although there are sizable Shia minorities in Afghanistan and Tajikistan.\n\nBuddhism and Zoroastrianism were the major faiths in Central Asia prior to the arrival of Islam. Zoroastrian influence is still felt today in such celebrations as Nowruz, held in all five of the Central Asian states.\n\nBuddhism was a prominent religion in Central Asia prior to the arrival of Islam, and the transmission of Buddhism along the Silk Road eventually brought the religion to China. Amongst the Turkic peoples, Tengrianism was the popular religion before arrival of Islam. Tibetan Buddhism is most common in Tibet, Mongolia, Ladakh and the southern Russian regions of Siberia.\n\nThe form of Christianity most practiced in the region in previous centuries was Nestorianism, but now the largest denomination is the Russian Orthodox Church, with many members in Kazakhstan where about 25% of the population of 19 million identify as Christian, 17% in Uzbekistan and 5% in Kyrgyzstan.\n\nThe Bukharan Jews were once a sizable community in Uzbekistan and Tajikistan, but nearly all have emigrated since the dissolution of the Soviet Union.\n\nIn Siberia, Shamanism is practiced, including forms of divination, such as Kumalak.\n\nContact and migration with Han people from China has brought Confucianism, Daoism, Mahayana Buddhism, and other Chinese folk beliefs into the region.\n\nCentral Asia has long been a strategic location merely because of its proximity to several great powers on the Eurasian landmass. The region itself never held a dominant stationary population nor was able to make use of natural resources. Thus, it has rarely throughout history become the seat of power for an empire or influential state. Central Asia has been divided, redivided, conquered out of existence, and fragmented time and time again. Central Asia has served more as the battleground for outside powers than as a power in its own right.\n\nCentral Asia had both the advantage and disadvantage of a central location between four historical seats of power. From its central location, it has access to trade routes to and from all the regional powers. On the other hand, it has been continuously vulnerable to attack from all sides throughout its history, resulting in political fragmentation or outright power vacuum, as it is successively dominated.\nBULLET::::- To the North, the steppe allowed for rapid mobility, first for nomadic horseback warriors like the Huns and Mongols, and later for Russian traders, eventually supported by railroads. As the Russian Empire expanded to the East, it would also push down into Central Asia towards the sea, in a search for warm water ports. The Soviet bloc would reinforce dominance from the North and attempt to project power as far south as Afghanistan.\nBULLET::::- To the East, the demographic and cultural weight of Chinese empires continually pushed outward into Central Asia since the Silk Road period of Han Dynasty. However, with the Sino-Soviet split and collapse of Soviet Union, China would project its soft power into Central Asia, most notably in the case of Afghanistan, to counter Russian dominance of the region.\nBULLET::::- To the Southeast, the demographic and cultural influence of India was felt in Central Asia, notably in Tibet, the Hindu Kush, and slightly beyond. From its base in India, the British Empire competed with the Russian Empire for influence in the region in the 19th and 20th centuries.\nBULLET::::- To the Southwest, Western Asian powers have expanded into the southern areas of Central Asia (usually Uzbekistan, Afghanistan, and Turkmenistan). Several Persian empires would conquer and reconquer parts of Central Asia; Alexander the Great's Hellenic empire would extend into Central Asia; two Islamic empires would exert substantial influence throughout the region; and the modern state of Iran has projected influence throughout the region as well. Turkey, through a common Turkic nation identity, has gradually increased its ties and influence as well in the region. Furthermore, since Uzbekistan announced their intention to join in April 2018, Turkey and all of the Central Asian Turkic-speaking states except Turkmenistan are together part of the Turkic Council.\n\nIn the post–Cold War era, Central Asia is an ethnic cauldron, prone to instability and conflicts, without a sense of national identity, but rather a mess of historical cultural influences, tribal and clan loyalties, and religious fervor. Projecting influence into the area is no longer just Russia, but also Turkey, Iran, China, Pakistan, India and the United States:\nBULLET::::- Russia continues to dominate political decision-making throughout the former SSRs; although, as other countries move into the area, Russia's influence has begun to wane though Russia still maintains military bases in Kyrgyzstan and Tajikistan.\nBULLET::::- The United States, with its military involvement in the region and oil diplomacy, is also significantly involved in the region's politics. The United States and other NATO members are the main contributors to the International Security Assistance Force in Afghanistan and also exert considerable influence in other Central Asian nations.\nBULLET::::- China has security ties with Central Asian states through the Shanghai Cooperation Organisation, and conducts energy trade bilaterally.\nBULLET::::- India has geographic proximity to the Central Asian region and, in addition, enjoys considerable influence on Afghanistan. India maintains a military base at Farkhor, Tajikistan, and also has extensive military relations with Kazakhstan and Uzbekistan.\nBULLET::::- Turkey also exerts considerable influence in the region on account of its ethnic and linguistic ties with the Turkic peoples of Central Asia and its involvement in the Baku-Tbilisi-Ceyhan oil pipeline. Political and economic relations are growing rapidly (e.g., Turkey recently eliminated visa requirements for citizens of the Central Asian Turkic republics).\nBULLET::::- Iran, the seat of historical empires that controlled parts of Central Asia, has historical and cultural links to the region and is vying to construct an oil pipeline from the Caspian Sea to the Persian Gulf.\nBULLET::::- Pakistan, a nuclear-armed Islamic state, has a history of political relations with neighboring Afghanistan and is termed capable of exercising influence. For some Central Asian nations, the shortest route to the ocean lies through Pakistan. Pakistan seeks natural gas from Central Asia and supports the development of pipelines from its countries. According to an independent study, Turkmenistan is supposed to be the fifth largest natural gas field in the world. The mountain ranges and areas in northern Pakistan lie on the fringes of greater Central Asia; the Gilgit–Baltistan region of Pakistan lies adjacent to Tajikistan, separated only by the narrow Afghan Wakhan Corridor. Being located on the northwest of South Asia, the area forming modern-day Pakistan maintained extensive historical and cultural links with the central Asian region.\nBULLET::::- Japan has an important and growing influence in Central Asia, with the master plan of the capital city of Nur-Sultan in Kazakhstan being designed by Japanese architect Kisho Kurokawa, and the Central Asia plus Japan initiative designed to strengthen ties between them and promote development and stability of the region.\n\nRussian historian Lev Gumilev wrote that Xiongnu, Mongols (Mongol Empire, Zunghar Khanate) and Turkic peoples (Turkic Khaganate, Uyghur Khaganate) played a role to stop Chinese aggression to the north. The Turkic Khaganate had special policy against Chinese assimilation policy. Another interesting theoretical analysis on the historical-geopolitics of the Central Asia was made through the reinterpretation of Orkhun Inscripts.\n\nThe region, along with Russia, is also part of \"the great pivot\" as per the Heartland Theory of Halford Mackinder, which says that the power which controls Central Asia—richly endowed with natural resources—shall ultimately be the \"empire of the world\".\n\nIn the context of the United States' War on Terror, Central Asia has once again become the center of geostrategic calculations. Pakistan's status has been upgraded by the U.S. government to Major non-NATO ally because of its central role in serving as a staging point for the invasion of Afghanistan, providing intelligence on Al-Qaeda operations in the region, and leading the hunt on Osama bin Laden.\n\nAfghanistan, which had served as a haven and source of support for Al-Qaeda under the protection of Mullah Omar and the Taliban, was the target of a U.S. invasion in 2001 and ongoing reconstruction and drug-eradication efforts. U.S. military bases have also been established in Uzbekistan and Kyrgyzstan, causing both Russia and the People's Republic of China to voice their concern over a permanent U.S. military presence in the region.\n\nWestern governments have accused Russia, China and the former Soviet republics of justifying the suppression of separatist movements, and the associated ethnics and religion with the War on Terror.\n\n! style=\"width:100px;\"City\n! style=\"width:150px;\"Country\n! style=\"width:100px;\"Population\n! style=\"width:200px;\"Image\n! style=\"width:350px;\"Information\nTashkent\n\nBULLET::::- Chinese Central Asia: Western Regions\nBULLET::::- Central Asian studies\nBULLET::::- Central Asian Union\nBULLET::::- Central Asian Football Federation\nBULLET::::- Continental pole of inaccessibility\nBULLET::::- Economic Cooperation Organization\nBULLET::::- Inner Asia\nBULLET::::- Hindutash\nBULLET::::- Central Asians in Ancient Indian literature\nBULLET::::- Soviet Central Asia\n\nBULLET::::- Chow, Edward. \"Central Asia's Pipelines: Field of Dreams and Reality\", in \"Pipeline Politics in Asia: The Intersection of Demand, Energy Markets, and Supply Routes\". National Bureau of Asian Research, 2010.\nBULLET::::- Farah, Paolo Davide, Energy Security, Water Resources and Economic Development in Central Asia, World Scientific Reference on Globalisation in Eurasia and the Pacific Rim, Imperial College Press (London, UK) & World Scientific Publishing, Nov. 2015.. Available at SSRN: http://ssrn.com/abstract=2701215\nBULLET::::- Dani, A.H. and V.M. Masson, eds. \"History of Civilizations of Central Asia\". Paris: UNESCO, 1992.* Gorshunova. Olga V. \"Svjashennye derevja Khodzhi Barora...\", (\" Sacred Trees of Khodzhi Baror: Phytolatry and the Cult of Female Deity in Central Asia\") in Etnoragraficheskoe Obozrenie, 2008, n° 1, pp. 71–82. . .\nBULLET::::- Mandelbaum, Michael, ed. \"Central Asia and the World: Kazakhstan, Uzbekistan, Tajikistan, Kyrgyzstan, and Turkmenistan\". New York: Council on Foreign Relations Press, 1994.\nBULLET::::- Marcinkowski, M. Ismail. \"Persian Historiography and Geography: Bertold Spuler on Major Works Produced in Iran, the Caucasus, Central Asia, Pakistan and Early Ottoman Turkey\". Singapore: Pustaka Nasional, 2003.\nBULLET::::- Olcott, Martha Brill. \"Central Asia's New States: Independence, Foreign policy, and Regional security\". Washington, D.C.: United States Institute of Peace Press, 1996.\nBULLET::::- Hasan Bulent Paksoy. \"ALPAMYSH: Central Asian Identity under Russian Rule\". Hartford: AACAR, 1989. http://vlib.iue.it/carrie/texts/carrie_books/paksoy-1/\nBULLET::::- Soucek, Svatopluk. \"A History of Inner Asia\". Cambridge: Cambridge University Press, 2000.\nBULLET::::- Rall, Ted. \"Silk Road to Ruin: Is Central Asia the New Middle East?\" New York: NBM Publishing, 2006.\nBULLET::::- Stone, L.A. \"The International Politics of Central Eurasia\" (272 pp). Central Eurasian Studies On Line: Accessible via the Web Page of the International Eurasian Institute for Economic and Political Research: https://web.archive.org/web/20071103154944/http://www.iicas.org/forumen.htm\nBULLET::::- Vakulchuk, Roman (2014) Kazakhstan's Emerging Economy: Between State and Market, Peter Lang: Frankfurt/Main. Available at: www.researchgate.net/publication/299731455\nBULLET::::- Weston, David. \"Teaching about Inner Asia\", Bloomington, Indiana: ERIC Clearinghouse for Social Studies, 1989.\n\nBULLET::::- The Library: Central on politics, universities, culture, languages, etc.\nBULLET::::- General Map of Central Asia: I is a historic map from 1874\n"}
{"id": "6744", "url": "https://en.wikipedia.org/wiki?curid=6744", "title": "Constantine II", "text": "Constantine II\n\nConstantine II may refer to:\n\nBULLET::::- Constantine II (emperor) (317–340), Roman Emperor 337–340\nBULLET::::- Constantine III (usurper) (died 411), known as Constantine II of Britain in British legend\nBULLET::::- Constantine II of Byzantine (630–668)\nBULLET::::- Antipope Constantine II (died 768), antipope from 767–768\nBULLET::::- Constantine II of Scotland (c.878 – 952), King of Scotland 900–942 or 943\nBULLET::::- Constantine II, Prince of Armenia (died 1129)\nBULLET::::- Constantine II of Cagliari (c. 1100 – 1163)\nBULLET::::- Constantine II of Torres (died 1198), called de Martis, was the giudice of Logudoro\nBULLET::::- Constantine II the Woolmaker (died 1322), Catholicos of the Armenian Apostolic Church\nBULLET::::- Constantine II, King of Armenia (died 1344), first Latin King of Armenian Cilicia of the Lusignan dynasty\nBULLET::::- Constantine II of Bulgaria (early 1370s–1422), last emperor of Bulgaria 1396–1422.\nBULLET::::- Eskender (1471–1494), Emperor of Ethiopia sometimes known as Constantine II\nBULLET::::- Constantine II of Georgia (c. 1447 – 1505)\nBULLET::::- Constantine II of Kakheti (died 1732), King of Kakheti 1722–1732\nBULLET::::- Constantine II of Greece (born 1940), Olympic champion (1960) and formerly King of the Hellenes March 6, 1964 – December 8, 1974\n\nBULLET::::- Constantius II (317 – 361), Roman Emperor from 337 to 361\n"}
{"id": "6745", "url": "https://en.wikipedia.org/wiki?curid=6745", "title": "Couscous", "text": "Couscous\n\nCouscous (Berber : ⵙⴽⵙⵓ \"seksu,\" \"\") originated as a Maghrebi dish of small (about ) steamed balls of crushed durum wheat semolina that is traditionally served with a stew spooned on top. Pearl millet and sorghum, especially in the Sahel, and other cereals can be cooked in a similar way and the resulting dishes are also sometimes called couscous.\n\nCouscous is a staple food throughout the North African cuisines of Morocco, Algeria, Tunisia, Mauritania, and Libya, as well as in Israel, due to the large population of Jews of North African origin. In Western supermarkets, it is sometimes sold in instant form with a flavor packet, and may be served as a side or on its own as a main dish.\n\nThe original name may be derived from the Arabic word \"kaskasa\", meaning \"to pound small\" or the Berber \"seksu\", meaning \"well rolled\", \"well formed\", or \"rounded\".\n\nNumerous different names and pronunciations for couscous exist around the world. Couscous is or in the United Kingdom and only the latter in the United States. It is sometimes pronounced \"kuskusi\" () in Arabic, while it is also known in Morocco as \"seksu\"; \"kesksu\" or () ; in Algeria as \"kosksi\" or as \"ṭa`ām\" (, literally meaning \"food\"); in Tunisia and Libya \"kosksi\" or \"kuseksi\", in Egypt \"kuskusi\" (), in Israel it is known as (), or \"kuskus\", in Sicily \"cuscusu\" and \"keskes\" in Tuareg.\n\nThe origin of couscous appears to be in the region from eastern to northern Africa where Berbers used it as early as the 7th century. Recognized as a traditional North African delicacy, it is a common cuisine component among Maghreb countries.\n\nIbn Battuta (b. Morocco, 1304–1368? AD) stated in his \"Rihlah\" (Travels), indicating what may be the earliest mention of couscous (kuskusu) in West Africa from the early 1350s:\n\nCouscous is traditionally made from the hard part of the durum, the part of the grain that resisted the grinding of the millstone. The semolina is sprinkled with water and rolled with the hands to form small pellets, sprinkled with dry flour to keep them separate, and then sieved. Any pellets that are too small to be finished granules of couscous fall through the sieve and are again rolled and sprinkled with dry semolina and rolled into pellets. This labor-intensive process continues until all the semolina has been formed into tiny granules of couscous. In the traditional method of preparing couscous, groups of women come together to make large batches over several days, which were then dried in the sun and used for several months. Handmade couscous may need to be re-hydrated as it is prepared; this is achieved by a process of moistening and steaming over stew until the couscous reaches the desired light and fluffy consistency.\n\nIn some regions couscous is made from farina or coarsely ground barley or pearl millet. In Brazil, the traditional couscous is made from cornmeal.\nIn modern times, couscous production is largely mechanized, and the product is sold in markets around the world. This couscous can be sauteed before it is cooked in water or another liquid. Properly cooked couscous is light and fluffy, not gummy or gritty. Traditionally, North Africans use a food steamer (called a\"taseksut\" in Berber, a \"kiskas\" in Arabic or a \"couscoussier\" in French). The base is a tall metal pot shaped rather like an oil jar in which the meat and vegetables are cooked as a stew. On top of the base, a steamer sits where the couscous is cooked, absorbing the flavours from the stew. The lid to the steamer has holes around its edge so steam can escape. It is also possible to use a pot with a steamer insert. If the holes are too big, the steamer can be lined with damp cheesecloth. There is little archaeological evidence of early diets including couscous, possibly because the original \"couscoussier\" was probably made from organic materials that could not survive extended exposure to the elements.\n\nThe couscous that is sold in most Western supermarkets has been pre-steamed and dried. It is typically prepared by adding 1.5 measures of boiling water or stock to each measure of couscous then leaving covered tightly for about five minutes. Pre-steamed couscous takes less time to prepare than regular couscous, most dried pasta, or dried grains (such as rice).\n\nIn Morocco, Algeria, Tunisia, and Libya, couscous is generally served with vegetables (carrots, potatoes, and turnips) cooked in a spicy or mild broth or stew, and some meat (generally, chicken, lamb or mutton). In Algeria and Morocco it may be served at the end of a meal or by itself as a delicacy called \"\"sfouff\"\". The couscous is usually steamed several times until it is fluffy and pale in color. It is then sprinkled with almonds, cinnamon and sugar. Traditionally, this dessert is served with milk perfumed with orange flower water, or it can be served plain with buttermilk in a bowl as a cold light soup for supper. Moroccan couscous uses saffron. Algerian couscous includes tomatoes and a variety of legumes and vegetables, Saharan couscous is served without legumes and without broth.\n\nIn Tunisia, it is made mostly spicy with harissa sauce and served commonly with any dish, including lamb, fish, seafood, beef and sometimes in southern regions, camel. Fish couscous is a Tunisian specialty and can also be made with octopus, squid or other seafood in hot, red, spicy sauce.\n\nIn Libya, it is mostly served with meat, specifically mostly lamb, but also camel, and rarely beef, in Tripoli and the western parts of Libya, but not during official ceremonies or weddings. Another way to eat couscous is as a dessert; it is prepared with dates, sesame, and pure honey, and locally referred to as \"maghrood\".\n\nIn Israel, and among members of Sephardic Jewish communities in the diaspora, couscous is a common food. Couscous is not indigenous to the Eastern Mediterranean, and arrived in the region with the migration of Jews from North Africa to Israel in the 20th century. Since then it has become a very popular dish in the country, and it is a staple of the Sephardic community and people of all backgrounds.\n\nIn addition, Israelis of all backgrounds commonly eat ptitim, also known as Israeli couscous, or pearl couscous, which is similar to regular couscous except it is larger like the Ashkenazi farfel or the Levantine maftoul (though ptitim does not contain bulgur unlike maftoul). Ptitim is a staple food and is very popular, especially with children, and is commonly served with butter or cooked with vegetables or chicken broth. However, it is prepared more similarly to pasta and is only boiled for a few minutes, and it is not steamed and fluffed like North African couscous. There are other shapes of ptitim, including a shape which resembled rice, which is also known as Ben Gurion’s rice, which was created in the 1950s after Israel's independence, as there was rationing and food shortages and rice was unavailable in the country.\n\nIn addition to ptitim, there are many varieties of couscous that can be found throughout Israel. Among Israelis of Sephardic origin (whose families moved to Israel from North African countries), couscous is a very common food and is served at almost every meal, especially on holidays, special occasions and celebrations, as well as on Shabbat (Jewish sabbath), for their Friday night dinners. Many people make their own couscous by hand, but it is a very labor intensive process. It is also common to buy instant couscous, and there are a large variety of brands and varieties for sale in Israel.\n\nDifferent communities have different styles and sizes of couscous, similar to the differences in size and style between the couscous of the different cuisines of the Maghreb. Moroccan Jewish couscous is larger, and is frequently prepared with aromatic spices and served with meat. Algerian Jewish couscous is smaller. The smallest is Tunisian Jewish couscous, which is not much larger than grains of coarse sand. Tunisian Jewish couscous is often served with harissa or shkug, or cooked with vegetables such as carrots, zucchini, or potatoes and served with chamin, a North African Jewish beef stew similar to cholent, that is often served for Shabbat. Couscous is also be prepared with cinnamon sticks, orange and other citrus peel, and aromatic spices for special occasions and celebrations. It is not common to find sweet couscous or dessert couscous in Israel, as in Egypt and other countries.\n\nIn Egypt, couscous is eaten more as a dessert. It is prepared with butter, sugar, cinnamon, raisins, and nuts and topped with cream.\n\nIn the Palestinian community, North African style couscous is not consumed. The Palestinians instead prepare a dish called maftoul, which is also consumed in Lebanon, Syria and Jordan and is called mograhbieh. Maftoul can be considered to be a special type of couscous but made from different ingredients and a different shape. It is significantly larger than North African couscous, and is similar in size to Israeli couscous, but has a different preparation. Maffoul is similarly steamed as North African couscous and often served on special occasions in a chicken broth with garbanzo beans and tender pieces of chicken taken off the bone. Maftoul is an Arabic word derived from the root \"fa-ta-la\", which means to roll or to twist, which is exactly describing the method used to make maftoul by hand rolling bulgur with wheat flour.< Couscous may be used to make a breakfast tabbouleh salad. Though usually cooked it water, it can also be cooking in another liquid, like apple juice, and served with dried fruit and honey.\n\nIn the Levant (excluding Israel and the Palestinian territories) they consume a large type of couscous with bulgur at the center, similar to the Palestinian maftoul called mograhbieh, which is commonly served in Lebanon, Syria and Jordan as part of a stew or cooked in chicken broth with cinnamon, caraway and chickpeas.\n\nCouscous is also consumed in France, where it is considered a traditional dish, and has also become common in Spain, Portugal, Italy, and Greece. Indeed, many polls have indicated that it is often a favorite dish. In France, Spain and Italy, the word \"couscous\" (\"cuscús\" in Catalan, Spanish and Italian; \"cuscuz\" in Portuguese) usually refers to couscous together with the stew. Packaged sets containing a box of quick-preparation couscous and a can of vegetables and, generally, meat are sold in French, Spanish and Italian grocery stores and supermarkets. In France, it is generally served with harissa sauce, a style inherited from the Tunisian cuisine. Indeed, couscous was voted as the third-favourite dish of French people in 2011 in a study by TNS Sofres for magazine \"Vie Pratique Gourmand\", and the first in the east of France.\n\nIn the Sahelian countries of West Africa, such as Mali and Senegal, the equivalent of couscous is called \"thiep\", and it is made out of pearl millet pounded or milled to the size and consistency of couscous.\n\nIn a one cup reference amount, wheat couscous provides 6 grams of protein, 36 grams of carbohydrates, and negligible fat.\n\nCouscous is distinct from pasta, even pasta such as orzo and risoni of similar size, in that it is made from crushed durum wheat semolina, while pasta is made from ground wheat. Couscous and pasta have similar nutritional value, although pasta is usually more refined. Pasta is cooked by boiling and couscous is steamed. Burghul or bulgur is a kind of parboiled dried cracked wheat of similar size to couscous, cooked by adding boiling water and leaving for a few minutes to soften.\nBULLET::::- Attiéké is a variety of couscous that is a staple food in Côte d'Ivoire and is also known to surrounding regions of West Africa, made from grated cassava.\nBULLET::::- Berkoukes are pasta bullets made by the same process but are larger than the grains of couscous.\nBULLET::::- In Brazilian cuisine, \"cuscuz marroquino\" is a version, usually eaten cold, of couscous. Brazilian cuscuz is usually made out of cornmeal rather than semolina wheat. Another festive moulded couscous dish containing chicken, vegetables, spices, steamed in a mould and decorated with orange slices is called \"cuscuz de galinha\".\nBULLET::::- Israeli couscous is a modification of Palestinian or Jordanian maftoul, the granules of which are toasted without coating.\nBULLET::::- Kouskousaki (Κουσκουσάκι in Greek or \"kuskus\" in Turkish), a pasta from Greece and Turkey, that is boiled and served with cheese and walnuts.\nBULLET::::- In Lebanese cuisine, Jordanian cuisine and Palestinian cuisine, a similar but larger product is known as \"maftoul\" or \"moghrabieh\".\nBULLET::::- Upma, eaten in South India, Western India, and Sri Lanka is a thick porridge made with dry roasted semolina. It also uses vegetables such as peas, carrots, etc.\nBULLET::::- \"Cuscuz\" (), a popular recipe usually associated with Northeastern Brazil and its diaspora, a steamed cake of corn meal served alone or with sugar and milk, varied meats, cheese and eggs or other ingredients.\n\nBULLET::::- List of African dishes\nBULLET::::- List of steamed foods\nBULLET::::- North African cuisine\nBULLET::::- Algerian cuisine\nBULLET::::- Berber cuisine\nBULLET::::- Libyan cuisine\nBULLET::::- Moroccan cuisine\nBULLET::::- Tunisian cuisine\n"}
{"id": "6746", "url": "https://en.wikipedia.org/wiki?curid=6746", "title": "Constantius II", "text": "Constantius II\n\nConstantius II (; ; 7 August 317 – 3 November 361) was Roman Emperor from 337 to 361. His reign saw constant warfare on the borders against the Sasanian Empire and Germanic peoples, while internally the Roman Empire went through repeated civil wars and usurpations. His religious policies inflamed domestic conflicts that would continue after his death.\n\nThe second son of Constantine I and Fausta, Constantius was made \"Caesar\" by his father in 324. He led the Roman army in war against the Sasanian Empire in 336. A year later, Constantine I died, and Constantius became \"Augustus\" with his brothers Constantine II and Constans. He promptly oversaw the massacre of eight of his relatives, consolidating his hold on power. The brothers divided the empire among themselves, with Constantius receiving the eastern provinces. In 340, his brothers Constantine and Constans clashed over the western provinces of the empire. The resulting conflict left Constantine dead and Constans as ruler of the west. The war against the Sasanians continued, with Constantius losing a major battle at Singara in 344. Constans was overthrown and assassinated in 350 by the usurper Magnentius. \n\nUnwilling to accept Magnentius as co-ruler, Constantius waged a civil war against the usurper, defeating him at the battles of Mursa Major in 351 and Mons Seleucus in 353. Magnentius committed suicide after the latter battle, leaving Constantius as sole ruler of the empire. In 351, Constantius elevated his cousin Constantius Gallus to the subordinate rank of \"Caesar\" to rule in the east, but had him executed three years later after receiving scathing reports of his violent and corrupt nature. Shortly thereafter, in 355, Constantius promoted his last surviving cousin, Gallus' younger half-brother Julian, to the rank of \"Caesar\".\n\nAs emperor, Constantius promoted Arian Christianity, persecuted pagans by banning sacrifices and closing pagan temples and issued laws discriminating against Jews. His military campaigns against Germanic tribes were successful: he defeated the Alamanni in 354 and campaigned across the Danube against the Quadi and Sarmatians in 357. The war against the Sasanians, which had been in a lull since 350, erupted with renewed intensity in 359 and Constantius traveled to the east in 360 to restore stability after the loss of several border fortresses to the Sasanians. However, Julian claimed the rank of Augustus in 360, leading to war between the two after Constantius' attempts to convince Julian to back down failed. No battle was fought, as Constantius became ill and died of fever on 3 November 361 in Mopsuestia, naming Julian as his rightful successor before his death.\n\nConstantius was born in 317 at Sirmium, Pannonia. He was the third son of Constantine the Great, and second by his second wife Fausta, the daughter of Maximian. Constantius was made Caesar by his father on 13 November 324. \nIn 336, religious unrest in Armenia and tense relations between Constantine and king Shapur II caused war to break out between Rome and Sassanid Persia. Though he made initial preparations for the war, Constantine fell ill and sent Constantius east to take command of the eastern frontier. Before Constantius arrived, the Persian general Narses, who was possibly the king's brother, overran Mesopotamia and captured Amida. Constantius promptly attacked Narses, and after suffering minor setbacks defeated and killed Narses at the Battle of Narasara. Constantius captured Amida and initiated a major refortification of the city, enhancing the city's circuit walls and constructing large towers. He also built a new stronghold in the hinterland nearby, naming it \"Antinopolis\".\n\nIn early 337, Constantius hurried to Constantinople after receiving news that his father was near death. After Constantine died, Constantius buried him with lavish ceremony in the Church of the Holy Apostles. Soon after his father's death Constantius supposedly ordered a massacre of his relatives descended from the second marriage of his paternal grandfather Constantius Chlorus, though the details are unclear. Eutropius, writing between 350 and 370, states that Constantius merely sanctioned “\"the act, rather than commanding it\"”. The massacre killed two of Constantius' uncles and six of his cousins, including Hannibalianus and Dalmatius, rulers of Pontus and Moesia respectively. The massacre left Constantius, his older brother Constantine II, his younger brother Constans, and three cousins Gallus, Julian and Nepotianus as the only surviving male relatives of Constantine the Great.\n\nSoon after, Constantius met his brothers in Pannonia at Sirmium to formalize the partition of the empire. Constantius received the eastern provinces, including Constantinople, Thrace, Asia Minor, Syria, Egypt, and Cyrenaica; Constantine received Britannia, Gaul, Hispania, and Mauretania; and Constans, initially under the supervision of Constantine II, received Italy, Africa, Illyricum, Pannonia, Macedonia, and Achaea.\n\nConstantius then hurried east to Antioch to resume the war with Persia. While Constantius was away from the eastern frontier in early 337, King Shapur II assembled a large army, which included war elephants, and launched an attack on Roman territory, laying waste to Mesopotamia and putting the city of Nisibis under siege. Despite initial success, Shapur lifted his siege after his army missed an opportunity to exploit a collapsed wall. When Constantius learned of Shapur's withdrawal from Roman territory, he prepared his army for a counter-attack.\n\nConstantius repeatedly defended the eastern border against invasions by the aggressive Sassanid Empire under Shapur. These conflicts were mainly limited to Sassanid sieges of the major fortresses of Roman Mesopotamia, including Nisibis (Nusaybin), Singara, and Amida (Diyarbakir). Although Shapur seems to have been victorious in most of these confrontations, the Sassanids were able to achieve little. However, the Romans won a decisive victory at the Battle of Narasara, killing Shapur's brother, Narses. Ultimately, Constantius was able to push back the invasion, and Shapur failed to make any significant gains.\n\nMeanwhile, Constantine II desired to retain control of Constans' realm, leading the brothers into open conflict. Constantine was killed in 340 near Aquileia during an ambush. As a result, Constans took control of his deceased brother's realms and became sole ruler of the Western two-thirds of the empire. This division lasted until 350, when Constans was assassinated by forces loyal to the usurper Magnentius.\n\nAs the only surviving son of Constantine the Great, Constantius felt that the position of emperor was his alone, and he determined to march west to fight the usurper, Magnentius. However, feeling that the east still required some sort of imperial presence, he elevated his cousin Constantius Gallus to Caesar of the eastern provinces. As an extra measure to ensure the loyalty of his cousin, he married the elder of his two sisters, Constantina, to him.\n\nBefore facing Magnentius, Constantius first came to terms with Vetranio, a loyal general in Illyricum who had recently been acclaimed emperor by his soldiers. Vetranio immediately sent letters to Constantius pledging his loyalty, which Constantius may have accepted simply in order to stop Magnentius from gaining more support. These events may have been spurred by the action of Constantina, who had since traveled east to marry Gallus. Constantius subsequently sent Vetranio the imperial diadem and acknowledged the general‘s new position as \"Augustus\". However, when Constantius arrived, Vetranio willingly resigned his position and accepted Constantius’ offer of a comfortable retirement in Bithynia.\n\nIn 351, Constantius clashed with Magnentius in Pannonia with a large army. The ensuing Battle of Mursa Major was one of the largest and bloodiest battles ever between two Roman armies. The result was a victory for Constantius, but a costly one. Magnentius survived the battle and, determined to fight on, withdrew into northern Italy. Rather than pursuing his opponent, however, Constantius turned his attention to securing the Danubian border, where he spent the early months of 352 campaigning against the Sarmatians along the middle Danube. After achieving his aims, Constantius advanced on Magnentius in Italy. This action led the cities of Italy to switch their allegiance to him and eject the usurper's garrisons. Again, Magnentius withdrew, this time to southern Gaul.\n\nIn 353, Constantius and Magnentius met for the final time at the Battle of Mons Seleucus in southern Gaul, and again Constantius emerged the victor. Magnentius, realizing the futility of continuing his position, committed suicide on 10 August 353.\n\nConstantius spent much of the rest of 353 and early 354 on campaign against the Alamanni on the Danube frontier. The campaign was successful and raiding by the Alamanni ceased temporarily. In the meantime, Constantius had been receiving disturbing reports regarding the actions of his cousin Gallus. Possibly as a result of these reports, Constantius concluded a peace with the Alamanni and traveled to Mediolanum (Milan).\n\nIn Mediolanum, Constantius first summoned Ursicinus, Gallus’ \"magister equitum\", for reasons that remain unclear. Constantius then summoned Gallus and Constantina. Although Gallus and Constantina complied with the order at first, when Constantina died in Bithynia, Gallus began to hesitate. However, after some convincing by one of Constantius’ agents, Gallus continued his journey west, passing through Constantinople and Thrace to Poetovio (Ptuj) in Pannonia.\n\nIn Poetovio, Gallus was arrested by the soldiers of Constantius under the command of Barbatio. Gallus was then moved to Pola and interrogated. Gallus claimed that it was Constantina who was to blame for all the trouble while he was in charge of the eastern provinces. This angered Constantius so greatly that he immediately ordered Gallus' execution. He soon changed his mind, however, and recanted the order. Unfortunately for Gallus, this second order was delayed by Eusebius, one of Constantius' eunuchs, and Gallus was executed.\n\nIn spite of some of the edicts issued by Constantius, he never made any attempt to disband the various Roman priestly colleges or the Vestal Virgins, he never acted against the various pagan schools, and, at times, he actually made some effort to protect paganism. In fact, he even ordered the election of a priest for Africa. Also, he remained pontifex maximus and was deified by the Roman Senate after his death. His relative moderation toward paganism is reflected by the fact that it was over twenty years after his death, during the reign of Gratian, that any pagan senator protested his treatment of their religion.\n\nAlthough often considered an Arian, Constantius ultimately preferred a third, compromise version that lay somewhere in between Arianism and the Nicene Creed, retrospectively called Semi-Arianism. During his reign he attempted to mold the Christian church to follow this compromise position, convening several Christian councils. The most notable of these were the Council of Rimini and its twin at Seleucia, which met in 359 and 360 respectively. \"Unfortunately for his memory the theologians whose advice he took were ultimately discredited and the malcontents whom he pressed to conform emerged victorious,\" writes the historian A.H.M. Jones. \"The great councils of 359–60 are therefore not reckoned ecumenical in the tradition of the church, and Constantius II is not remembered as a restorer of unity, but as a heretic who arbitrarily imposed his will on the church.\"\n\nChristian-related edicts issued by Constantius (by himself or with others) included:\nBULLET::::- Exemption from compulsory public service for the clergy\nBULLET::::- Exemption from compulsory public service for the sons of clergy\nBULLET::::- Tax exemptions for clergy and their servants, and later for their family\nBULLET::::- Clergy and the issue of private property\nBULLET::::- Bishops exempted from being tried in secular courts\nBULLET::::- Christian prostitutes only able to be bought by Christians\n\nJudaism faced some severe restrictions under Constantius, who seems to have followed an anti-Jewish policy in line with that of his father. Early in his reign, Constantius issued a double edict in concert with his brothers limiting the ownership of slaves by Jewish people and banning marriages between Jews and Christian women. A later edict issued by Constantius after becoming sole emperor decreed that a person who was proven to have converted from Christianity to Judaism would have all of his property confiscated by the state. However, Constantius' actions in this regard may not have been so much to do with Jewish religion as with Jewish business—apparently, privately owned Jewish businesses were often in competition with state-owned businesses. As a result, Constantius may have sought to provide an advantage to state-owned businesses by limiting the skilled workers and slaves available to Jewish businesses.\n\nJew-related edicts issued by Constantius (by himself or with others) included:\nBULLET::::- Weaving women who moved from working for the government to working for Jews must be restored to the government\nBULLET::::- Jews may not marry Christian women\nBULLET::::- Jews may not attempt to convert Christian women\nBULLET::::- Any non-Jewish slave bought by a Jew will be confiscated by the state\nBULLET::::- If a Jew attempts to circumcise a non-Jewish slave, the slave will be freed and the Jew shall face capital punishment\nBULLET::::- Any Christian slaves owned by a Jew will be taken away and freed\nBULLET::::- A person who is proven to have converted from Christianity to Judaism shall have their property confiscated by the state\n\nOn 11 August 355, the magister militum Claudius Silvanus revolted in Gaul. Silvanus had surrendered to Constantius after the Battle of Mursa Major. Constantius had made him magister militum in 353 with the purpose of blocking the German threats, a feat that Silvanus achieved by bribing the German tribes with the money he had collected. A plot organized by members of Constantius' court led the emperor to recall Silvanus. After Silvanus revolted, he received a letter from Constantius recalling him to Milan, but which made no reference to the revolt. Ursicinus, who was meant to replace Silvanus, bribed some troops, and Silvanus was killed.\n\nConstantius realised that too many threats still faced the Empire, however, and he could not possibly handle all of them by himself. So on 6 November 355, he elevated his last remaining male relative, Julian, to the rank of Caesar. A few days later, Julian was married to Helena, the last surviving sister of Constantius. Constantius soon sent Julian off to Gaul.\n\nConstantius spent the next few years overseeing affairs in the western part of the empire primarily from his base at Mediolanum. In 357 he visited Rome for the only time in his life. The same year, he forced Sarmatian and Quadi invaders out of Pannonia and Moesia Inferior, then led a successful counter-attack across the Danube.\n\nIn the winter of 357–58, Constantius received ambassadors from Shapur II who demanded that Rome restore the lands surrendered by Narseh. Despite rejecting these terms, Constantius tried to avert war with the Sassanid Empire by sending two embassies to Shapur II. Shapur II nevertheless launched another invasion of Roman Mesopotamia. In 360, when news reached Constantius that Shapur II had destroyed Singara, and taken Kiphas (Hasankeyf), Amida, and Ad Tigris (Cizre), he decided to travel east to face the re-emergent threat.\n\nIn the meantime, Julian had won some victories against the Alamanni, who had once again invaded Roman Gaul. However, when Constantius requested reinforcements from Julian's army for the eastern campaign, the Gallic legions revolted and proclaimed Julian \"Augustus\".\n\nOn account of the immediate Sassanid threat, Constantius was unable to directly respond to his cousin's usurpation, other than by sending missives in which he tried to convince Julian to resign the title of \"Augustus\" and be satisfied with that of \"Caesar\". By 361, Constantius saw no alternative but to face the usurper with force, and yet the threat of the Sassanids remained. Constantius had already spent part of early 361 unsuccessfully attempting to re-take the fortress of Ad Tigris. After a time he had withdrawn to Antioch to regroup and prepare for a confrontation with Shapur II. The campaigns of the previous year had inflicted heavy losses on the Sassanids, however, and they did not attempt another round of campaigns that year. This temporary respite in hostilities allowed Constantius to turn his full attention to facing Julian.\n\nConstantius immediately gathered his forces and set off west. However, by the time he reached Mopsuestia in Cilicia, it was clear that he was fatally ill and would not survive to face Julian. Apparently, realising his death was near, Constantius had himself baptised by Euzoius, the Semi-Arian bishop of Antioch, and then declared that Julian was his rightful successor. Constantius II died of fever on 3 November 361.\n\nConstantius II was married three times:\n\nFirst to a daughter of his half-uncle Julius Constantius, whose name is unknown. She was a full-sister of Gallus and a half-sister of Julian. She died c. 352/3.\n\nSecond, to Eusebia, a woman of Macedonian origin, originally from the city of Thessaloniki, whom Constantius married before his defeat of Magnentius in 353. She died in 360.\n\nThird and lastly, in 360, to Faustina, who gave birth to Constantius' only child, a posthumous daughter named Flavia Maxima Constantia, who later married Emperor Gratian.\n\nConstantius II is a particularly difficult figure to judge properly due to the hostility of most sources toward him. A. H. M. Jones writes that Constantius \"appears in the pages of Ammianus as a conscientious emperor but a vain and stupid man, an easy prey to flatterers. He was timid and suspicious, and interested persons could easily play on his fears for their own advantage.\" However, Kent and M. and A. Hirmer suggest that Constantius \"has suffered at the hands of unsympathetic authors, ecclesiastical and civil alike. To orthodox churchmen he was a bigoted supporter of the Arian heresy, to Julian the Apostate and the many who have subsequently taken his part he was a murderer, a tyrant and inept as a ruler\". They go on to add, \"Most contemporaries seem in fact to have held him in high esteem, and he certainly inspired loyalty in a way his brother could not\".\n\nBULLET::::- Persian wars of Constantius II\nBULLET::::- Flavia (gens)\nBULLET::::- Itineraries of the Roman emperors, 337–361\nBULLET::::- List of Byzantine emperors\n\nBULLET::::- Ammianus Marcellinus. \"Res Gestae\".\nBULLET::::- Yonge, Charles Duke, trans. \"Roman History\". London: Bohn, 1862. Online at Tertullian. Accessed 15 August 2009.\nBULLET::::- Rolfe, J.C., trans. \"History\". 3 vols. Loeb ed. London: Heinemann, 1939–52. Online at LacusCurtius. Accessed 15 August 2009.\nBULLET::::- Hamilton, Walter, trans. \"The Later Roman Empire (A.D. 354–378)\". Harmondsworth: Penguin, 1986. [Abridged edition]\nBULLET::::- Athanasius of Alexandria.\nBULLET::::- \"Festal Index\".\nBULLET::::- \"Epistula encyclica\" (\"Encyclical letter\"). Summer 339.\nBULLET::::- \"Apologia Contra Arianos\" (\"Defense against the Arians\"). 349.\nBULLET::::- \"Apologia ad Constantium\" (\"Defense before Constantius\"). 353.\nBULLET::::- \"Historia Arianorum\" (\"History of the Arians\"). 357.\nBULLET::::- \"De Synodis\" (\"On the Councils of Arminium and Seleucia\"). Autumn 359.\nBULLET::::- \"Historia acephala\". 368 – c. 420.\nBULLET::::- \"Chronica minora\" 1, 2.\nBULLET::::- Mommsen, T., ed. \"Chronica Minora saec. IV, V, VI, VII\" 1, 2 (in Latin). \"Monumenta Germaniae Historia\", Auctores Antiquissimi 9, 11. Berlin, 1892, 1894. Online at . Accessed 25 August 2009.\nBULLET::::- \"Codex Theodosianus\".\nBULLET::::- Mommsen, T. and Paul M. Meyer, eds. \"Theodosiani libri XVI cum Constitutionibus Sirmondianis et Leges novellae ad Theodosianum pertinentes\" (in Latin). Berlin: Weidmann, [1905] 1954. Complied by Nicholas Palmer, revised by Tony Honoré for Oxford Text Archive, 1984. Prepared for online use by R.W.B. Salway, 1999. Preface, books 1–8. Online at University College London and the University of Grenoble. Accessed 25 August 2009.\nBULLET::::- Unknown edition (in Latin). Online at AncientRome.ru. Accessed 15 August 2009.\nBULLET::::- \"Codex Justinianus\".\nBULLET::::- Scott, Samuel P., trans. \"The Code of Justinian\", in \"The Civil Law\". 17 vols. 1932. Online at the Constitution Society. Accessed 14 August 2009.\nBULLET::::- Ephraem the Syrian. \"Carmina Nisibena\" (\"Songs of Nisibis\").\nBULLET::::- Stopford, J.T. Sarsfield, trans. \"The Nisibene Hymns\". From \"Nicene and Post-Nicene Fathers\", Second Series, Vol. 13. Edited by Philip Schaff and Henry Wace. Buffalo, NY: Christian Literature Publishing Co., 1890. Revised and edited for New Advent by Kevin Knight. Online at New Advent. Accessed 16 August 2009.\nBULLET::::- Bickell, Gustav, trans. \"S. Ephraemi Syri Carmina Nisibena: additis prolegomenis et supplemento lexicorum Syriacorum\" (in Latin). Lipetsk: Brockhaus, 1866. Online at Google Books. Accessed 15 August 2009.\nBULLET::::- \"Epitome de Caesaribus\".\nBULLET::::- Banchich, Thomas M., trans. \"A Booklet About the Style of Life and the Manners of the Imperatores\". \"Canisius College Translated Texts\" 1. Buffalo, NY: Canisius College, 2009. Online at De Imperatoribus Romanis. Accessed 15 August 2009.\nBULLET::::- Eunapius. \"Lives of the Sophists\".\nBULLET::::- Eusebius of Caesarea.\nBULLET::::- \"Oratio de Laudibus Constantini\" (\"Oration in Praise of Constantine\", sometimes the \"Tricennial Oration\").\nBULLET::::- \"Vita Constantini\" (\"Life of Constantine\").\nBULLET::::- Eutropius. \"Historiae Romanae Breviarium\".\nBULLET::::- Watson, John Selby, trans. \"Abridgment of Roman History\". London: George Bell & Sons, 1886. Revised and edited for Tertullian by Roger Pearse, 2003. Online at Tertullian. Accessed 11 June 2010.\nBULLET::::- Festus. \"Breviarium\".\nBULLET::::- Banchich, Thomas M., and Jennifer A. Meka, trans. \"Breviarium of the Accomplishments of the Roman People\". \"Canisius College Translated Texts\" 2. Buffalo, NY: Canisius College, 2001. Online at De Imperatoribus Romanis. Accessed 15 August 2009.\nBULLET::::- Firmicus Maternus. \"De errore profanarum religionum\" (\"On the error of profane religions\").\nBULLET::::- Baluzii and Rigaltii, eds. \"Divi Cæcilii Cypriani, Carthaginensis Episcopi, Opera Omnia; accessit J. Firmici Materni, Viri Clarissimi, De Errore Profanarum Religionum\" (in Latin). Paris: Gauthier Brothers and the Society of Booksellers, 1836. Online at Google Books. Accessed 15 August 2009.\nBULLET::::- Hilary of Poitiers. \"Ad Constantium\" (\"To Constantius\").\nBULLET::::- Feder, Alfred Leonhard, ed. \"S. Hilarii episcopi Pictaviensis Tractatus mysteriorum. Collectanea Antiariana Parisina (fragmenta historica) cum appendice (liber I Ad Constantium). Liber ad Constantium imperatorem (Liber II ad Constantium). Hymni. Fragmenta minora. Spuria\" (in Latin). In the \"Corpus Scriptorum Ecclesiasticorum Latinorum\", Vol. 65. Vienna: Tempsky, 1916.\nBULLET::::- \"Itinerarium Alexandri\" (\"Itinerary of Alexander\").\nBULLET::::- Mai, Angelo, ed. \"Itinerarium Alexandri ad Constantium Augustum, Constantini M. Filium\" (in Latin). Regiis Typis, 1818. Online at Google Books. Accessed 15 August 2009.\nBULLET::::- Davies, Iolo, trans. \"Itinerary of Alexander\". 2009. Online at DocStoc. Accessed 15 August 2009.\nBULLET::::- Jerome.\nBULLET::::- \"Chronicon\" (\"Chronicle\").\nBULLET::::- \"de Viris Illustribus\" (\"On Illustrious Men\").\nBULLET::::- Julian.\nBULLET::::- Wright, Wilmer Cave, trans. \"Works of the Emperor Julian\". 3 vols. Loeb ed. London: Heinemann, 1913. Online at the Internet Archive: Vol. 1, 2, 3.\nBULLET::::- Libanius. \"Oratio\" 59 (\"Oration\" 59).\nBULLET::::- M.H. Dodgeon, trans. \"The Sons of Constantine: Libanius Or. LIX\". In \"From Constantine to Julian: Pagan and Byzantine Views, A Source History\", edited by S.N.C. Lieu and Dominic Montserrat, 164–205. London: Routledge, 1996.\nBULLET::::- \"Origo Constantini Imperatoris\".\nBULLET::::- Rolfe, J.C., trans. \"Excerpta Valesiana\", in vol. 3 of Rolfe's translation of Ammianus Marcellinus' \"History\". Loeb ed. London: Heinemann, 1952. Online at LacusCurtius. Accessed 16 August 2009.\nBULLET::::- \"Papyri Abinnaeus\".\nBULLET::::- \"The Abinnaeus Archive: Papers of a Roman Officer in the Reign of Constantius II\" (in Greek). Duke Data Bank of Documentary Papyri. Online at Perseus and the Duke Data Bank. Accessed 15 August 2009.\nBULLET::::- \"Papyri Laurentius\".\nBULLET::::- \"Dai Papiri della Biblioteca Medicea Laurenziana\" (in Greek). Duke Data Bank of Documentary Papyri. Online at Perseus and the Duke Data Bank. Accessed 15 August 2009.\nBULLET::::- Philostorgius. \"Historia Ecclesiastica\".\nBULLET::::- Walford, Edward, trans. \"Epitome of the Ecclesiastical History of Philostorgius, Compiled by Photius, Patriarch of Constantinople\". London: Henry G. Bohn, 1855. Online at Tertullian. Accessed 15 August 2009.\nBULLET::::- Socrates. \"Historia Ecclesiastica\" (\"History of the Church\").\nBULLET::::- Zenos, A.C., trans. \"Ecclesiastical History\". From \"Nicene and Post-Nicene Fathers\", Second Series, Vol. 2. Edited by Philip Schaff and Henry Wace. Buffalo, NY: Christian Literature Publishing Co., 1890. Revised and edited for New Advent by Kevin Knight. Online at New Advent. Accessed 14 August 2009.\nBULLET::::- Sozomen. \"Historia Ecclesiastica\" (\"History of the Church\").\nBULLET::::- Hartranft, Chester D. \"Ecclesiastical History\". From \"Nicene and Post-Nicene Fathers\", Second Series, Vol. 2. Edited by Philip Schaff and Henry Wace. Buffalo, NY: Christian Literature Publishing Co., 1890. Revised and edited for New Advent by Kevin Knight. Online at New Advent. Accessed 15 August 2009.\nBULLET::::- Sulpicius Severus. \"Sacred History\".\nBULLET::::- Roberts, Alexander, trans. \"Sacred History\". From \"Nicene and Post-Nicene Fathers\", Second Series, Vol. 11. Edited by Philip Schaff and Henry Wace. Buffalo, NY: Christian Literature Publishing Co., 1894. Revised and edited for New Advent by Kevin Knight. Online at New Advent. Accessed 14 August 2009.\nBULLET::::- Theodoret. \"Historia Ecclesiastica\" (\"History of the Church\").\nBULLET::::- Jackson, Blomfield, trans. \"Ecclesiastical History\". From \"Nicene and Post-Nicene Fathers\", Second Series, Vol. 3. Edited by Philip Schaff and Henry Wace. Buffalo, NY: Christian Literature Publishing Co., 1892. Revised and edited for New Advent by Kevin Knight. Online at New Advent. Accessed 15 August 2009.\nBULLET::::- Themistius. \"Orationes\" (\"Orations\").\nBULLET::::- Theophanes. \"Chronicle\".\nBULLET::::- Zonaras. \"Extracts of History\".\nBULLET::::- Zosimus. \"Historia Nova\" (\"New History\").\nBULLET::::- Unknown trans. \"The History of Count Zosimus\". London: Green and Champlin, 1814. Online at Tertullian. Accessed 15 August 2009. [An unsatisfactory edition.]\nBULLET::::- Unknown trans. \"Histoire Nouvelle\" and \"ΖΩΣΙΜΟΥ ΚΟΜΙΤΟΣ ΚΑΙ ΑΠΟΦΙΣΚΟΣΥΝΗΓΟΡΟΥ\" (in French and Greek). Online at the Catholic University of Louvain. Accessed 16 November 2009.\n\nBULLET::::- Banchich, T.M., 'DIR-Gallus' from \"De Imperatoribus Romanis\"\nBULLET::::- Dignas, B. & Winter, E., \"Rome and Persia in Late Antiquity\" (Cambridge University Press, 2007)\nBULLET::::- DiMaio, M., and Frakes, R., 'DIR-Constantius II' from \"De Imperatoribus Romanis\" \"Constantius II,\".\nBULLET::::- Gaddis, M., \"There is No Crime for Those who Have Christ\" (University of California Press, 2005).„\nBULLET::::- Hunt, \"Constantius II in the Ecclesiastical Historians\"orians, Ph.D. diss. (Fordham University, 2010), AAT 3431914.\nBULLET::::- Jones, A.H.M, \"The Later Roman Empire, 284–602: a Social, Economic and Administrative Survey\" (Baltimore: Johns Hopkins University, 1986)\nBULLET::::- Kent, J.P.C., Hirmer, M. & Hirmer, A. \"Roman Coins\" (Thames and Hudson, 1978)\nBULLET::::- Moser, Muriel. 2018. Emperor and Senators in the Reign of Constantius II. Cambridge University Press.\nBULLET::::- Odahl, C.M., \"Constantine and the Christian Empire\" (Routledge, 2004)\nBULLET::::- Pelikan, J.J., \"The Christian Tradition\" (University of Chicago, 1989)\nBULLET::::- Potter, D.S., \"The Roman Empire at Bay: AD 180–395\" (Routledge, 2004)\nBULLET::::- Salzman, M.R., \"The Making of a Christian Aristocracy: Social and Religious Change in the Western Roman Empire\" (Harvard University Press, 2002)\nBULLET::::- Schäfer, P., \"The History of the Jews in the Greco-Roman World\" (Routledge, 2003)\nBULLET::::- Vagi, D.L. & Coquand, T., \"Coinage and History of the Roman Empire\" (Taylor & Francis, 2001)\nBULLET::::- Vasiliev, A.A., \"History of the Byzantine Empire 324–1453\" (University of Wisconsin Press, 1958)\nBULLET::::- This list of Roman laws of the fourth century shows laws passed by Constantius II relating to Christianity.\n"}
{"id": "6747", "url": "https://en.wikipedia.org/wiki?curid=6747", "title": "Constans", "text": "Constans\n\nConstans (; ; c. 323 – 350) or Constans I was Roman Emperor from 337 to 350. He defeated his brother Constantine II in 340, but anger in the army over his personal life (homosexuality) and favouritism towards his barbarian bodyguards led the general Magnentius to rebel, resulting in the assassination of Constans in 350.\n\nConstans was the third and youngest son of Constantine the Great and Fausta, his father's second wife. He was educated at the court of his father at Constantinople under the tutelage of the poet Aemilius Magnus Arborius.\n\nOn 25 December 333, Constantine I elevated Constans to the rank of \"Caesar\" at Constantinople. Constans became engaged to Olympias, the daughter of the Praetorian Prefect Ablabius, but the marriage never came to pass. With Constantine’s death in 337, Constans and his two brothers, Constantine II and Constantius II, divided the Roman world among themselves and disposed of virtually all relatives who could possibly have a claim to the throne. The army proclaimed them \"Augusti\" on September 9, 337. Almost immediately, Constans was required to deal with a Sarmatian invasion in late 337, in which he won a resounding victory.\nConstans was initially under the guardianship of Constantine II. The original settlement assigned Constans the praetorian prefecture of Italy, which included Northern Africa. Constans was unhappy with this division, so the brothers met at Viminacium in 338 to revise the boundaries. Constans managed to extract the prefecture of Illyricum and the diocese of Thrace, provinces that were originally to be ruled by his cousin Dalmatius, as per Constantine I’s proposed division after his death. Constantine II soon complained that he had not received the amount of territory that was his due as the eldest son.\n\nAnnoyed that Constans had received Thrace and Macedonia after the death of Dalmatius, Constantine demanded that Constans hand over the African provinces, which he agreed to do in order to maintain a fragile peace. Soon, however, they began quarreling over which parts of the African provinces belonged to Carthage, and thus Constantine, and which belonged to Italy, and therefore Constans. This led to growing tensions between the two brothers, which were only heightened by Constans finally coming of age and Constantine refusing to give up his guardianship. In 340 Constantine II invaded Italy. Constans, at that time in Dacia, detached and sent a select and disciplined body of his Illyrian troops, stating that he would follow them in person with the remainder of his forces. Constantine was eventually trapped at Aquileia, where he died, leaving Constans to inherit all of his brother’s former territories – Hispania, Britannia and Gaul.\n\nConstans began his reign in an energetic fashion. In 341-42, he led a successful campaign against the Franks, and in the early months of 343 he visited Britain. The source for this visit, Julius Firmicus Maternus, does not provide a reason, but the quick movement and the danger involved in crossing the channel in the winter months suggests it was in response to a military emergency, possibly to repel the Picts and Scots.\n\nRegarding religion, Constans was tolerant of Judaism and promulgated an edict banning pagan sacrifices in 341. He suppressed Donatism in Africa and supported Nicene orthodoxy against Arianism, which was championed by his brother Constantius. Although Constans called the Council of Serdica in 343 to settle the conflict, it was a complete failure, and by 346 the two emperors were on the point of open warfare over the dispute. The conflict was only resolved by an interim agreement which allowed each emperor to support their preferred clergy within their own spheres of influence.\n\nThe Roman historian Eutropius says Constans \"indulged in great vices,\" in reference to his homosexuality, and Aurelius Victor stated that Constans had a reputation for scandalous behaviour with \"handsome barbarian hostages.\" Nevertheless, Constans did sponsor a decree alongside Constantius II that ruled that marriage based on \"unnatural\" sex should be punished meticulously. Boswell argues that the decree outlawed homosexual marriages only, rather than homosexual activity more generally. However, it was likely the case that Constans promulgated the legislation under pressure from the growing band of Christian leaders, in an attempt to placate public outrage at his own perceived indecencies.\n\nIn the final years of his reign, Constans developed a reputation for cruelty and misrule. Dominated by favourites and openly preferring his select bodyguard, he lost the support of the legions. In 350, the general Magnentius declared himself emperor at Augustodunum with the support of the troops on the Rhine frontier and, later, the western provinces of the Empire. Constans was enjoying himself nearby when he was notified of the elevation of Magnentius. Lacking any support beyond his immediate household, he was forced to flee for his life. As he was trying to reach Hispania, supporters of Magnentius cornered him in a fortification in \"Helena\" (now Elne) in the eastern Pyrenees of southwestern Gaul, where he was killed after seeking sanctuary in a temple. An alleged prophecy at his birth had said Constans would die \"in the arms of his grandmother\". His place of death happens to have been named after Helena, mother of Constantine and his own grandmother, thus realizing the prophecy.\n\nBULLET::::- Itineraries of the Roman emperors, 337–361\nBULLET::::- List of Byzantine emperors\n\nBULLET::::- Zosimus, \"Historia Nova\", Book 2 Historia Nova\nBULLET::::- Aurelius Victor, Epitome de Caesaribus\nBULLET::::- Eutropius, Breviarium ab urbe condita\n\nBULLET::::- DiMaio, Michael; Frakes, Robert, \"Constans I (337–350 A.D.)\", in \"De Imperatoribus Romanis\" (D.I.R.), An Online Encyclopedia of Roman Emperors\nBULLET::::- Jones, A.H.M., Martindale, J.R. \"The Prosopography of the Later Roman Empire, Vol. I: AD260-395\", Cambridge University Press, 1971\nBULLET::::- Gibbon. Edward \"Decline & Fall of the Roman Empire\" (1888)\n"}
{"id": "6749", "url": "https://en.wikipedia.org/wiki?curid=6749", "title": "Cheerleading", "text": "Cheerleading\n\nCheerleading is an activity in which the participants (called \"cheerleaders\") cheer for their team as a form of encouragement. It can range from chanting slogans to intense physical activity. It can be performed to motivate sports teams, to entertain the audience, or for competition. Competitive routines typically range anywhere from one to three minutes, and contain components of tumbling, dance, jumps, cheers, and stunting.\n\nCheerleading originated in the United States, and remains predominantly in America, with an estimated 1.5 million participants in all-star cheerleading. The global presentation of cheerleading was led by the 1997 broadcast of ESPN's International cheerleading competition, and the worldwide release of the 2000 film \"Bring It On\". Due in part to this recent exposure, there are now an estimated 100,000 participants scattered around the globe in Australia, Canada, China, Colombia, Finland, France, Germany, Japan, the Netherlands, New Zealand, and the United Kingdom.\n\nCheerleading began during the late 18th century with the rebellion of male students. After the American Revolutionary War, students experienced harsh treatment from teachers. In response to faculty's abuse, college students violently acted out. The undergraduates began to riot, burn down buildings located on their college campuses, and assault faculty members. As a more subtle way to gain independence, however, students invented and organized their own extracurricular activities outside their professors' control. This brought about American sports, beginning first with collegiate teams.\n\nIn the 1860s, students from Great Britain began to cheer and chant in unison for their favorite athletes at sporting events. Soon, that gesture of support crossed overseas to America.\n\nOn November 6, 1869, the United States witnessed its first intercollegiate football game. It took place between Princeton and Rutgers University, and marked the day the original \"Sis Boom Rah!\" cheer was shouted out by student fans.\n\nOrganized cheerleading started as an all-male activity. As early as 1877, Princeton University had a \"Princeton Cheer\", documented in the February 22, 1877, March 12, 1880, and November 4, 1881, issues of \"The Daily Princetonian\". This cheer was yelled from the stands by students attending games, as well as by the athletes themselves. The cheer, \"Hurrah! Hurrah! Hurrah! Tiger! S-s-s-t! Boom! A-h-h-h!\" remains in use with slight modifications today, where it is now referred to as the \"Locomotive\".\n\nPrinceton class of 1882 graduate Thomas Peebles moved to Minnesota in 1884. He transplanted the idea of organized crowds cheering at football games to the University of Minnesota. The term \"Cheer Leader\" had been used as early as 1897, with Princeton's football officials having named three students as \"Cheer Leaders:\" Thomas, Easton, and Guerin from Princeton's classes of 1897, 1898, and 1899, respectively, on October 26, 1897. These students would cheer for the team also at football practices, and special cheering sections were designated in the stands for the games themselves for both the home and visiting teams.\n\nIt was not until 1898 that University of Minnesota student Johnny Campbell directed a crowd in cheering \"Rah, Rah, Rah! Ski-u-mah, Hoo-Rah! Hoo-Rah! Varsity! Varsity! Varsity, Minn-e-So-Tah!\", making Campbell the very first cheerleader.\n\nNovember 2, 1898 is the official birth date of organized cheerleading. Soon after, the University of Minnesota organized a \"yell leader\" squad of six male students, who still use Campbell's original cheer today. In 1903, the first cheerleading fraternity, Gamma Sigma, was founded.\n\nIn 1923, at the University of Minnesota, women were permitted to participate in cheerleading. However, it took time for other schools to follow. In the late 1920s, many school manuals and newspapers that were published still referred to cheerleaders as \"chap,\" \"fellow,\" and \"man\". Women cheerleaders were overlooked until the 1940s. In the 1940s, collegiate men were drafted for World War II, creating the opportunity for more women to make their way onto sporting event sidelines. As noted by Kieran Scott in \"Ultimate Cheerleading\": \"Girls really took over for the first time.\" An overview written on behalf of cheerleading in 1955 explained that in larger schools, \"occasionally boys as well as girls are included,\", and in smaller schools, \"boys can usually find their place in the athletic program, and cheerleading is likely to remain solely a feminine occupation.\" During the 1950s, cheerleading in America also increased in popularity. By the 1960s, some began to consider cheerleading a feminine extracurricular for boys, and by the 1970s, girls primarily cheered at public school games. However, this did not stop its growth. Cheerleading could be found at almost every school level across the country, even pee wee and youth leagues began to appear.\n\nIn 1975, it was estimated by a man named Randy Neil that over 500,000 students actively participated in American cheerleading from elementary school to the collegiate level. He also approximated that ninety-five percent of cheerleaders within America were female. Since 1973, cheerleaders have started to attend female basketball and other all-female sports as well.\n\nAs of 2005, overall statistics show around 97% of all modern cheerleading participants are female, although at the collegiate level, cheerleading is co-ed with about 50% of participants being male.\n\nIn 1948, Lawrence \"Herkie\" Herkimer, of Dallas, Texas, a former cheerleader at Southern Methodist University, formed the National Cheerleaders Association (NCA) in order to hold clinics for cheerleading. In 1949, The NCA held its first clinic in Huntsville, Texas, with 52 girls in attendance. Herkimer contributed many firsts to cheerleading: the founding of the Cheerleader & Danz Team cheerleading uniform supply company, inventing the herkie jump (where one leg is bent towards the ground as if kneeling and the other is out to the side as high as it will stretch in toe-touch position), and creating the \"Spirit Stick\". By the 1960s, college cheerleaders began hosting workshops across the nation, teaching fundamental cheer skills to high-school-age girls. In 1965, Fred Gastoff invented the vinyl pom-pom, which was introduced into competitions by the International Cheerleading Foundation (ICF, now the World Cheerleading Association, or WCA). Organized cheerleading competitions began to pop up with the first ranking of the \"Top Ten College Cheerleading Squads\" and \"Cheerleader All America\" awards given out by the ICF in 1967. In 1978, America was introduced to competitive cheerleading by the first broadcast of Collegiate Cheerleading Championships on CBS.\n\nIn the 1950s, the formation of professional cheerleading started. The first recorded cheer squad in National Football League (NFL) history was for the Baltimore Colts. Professional cheerleaders put a new perspective on American cheerleading. Women were selected for two reasons: visual sex appeal, and the ability to dance. Women were exclusively chosen because men were the targeted marketing group. The Dallas Cowboys Cheerleaders soon gained the spotlight with their revealing outfits and sophisticated dance moves, debuting in the 1972–1973 season, but were first widely seen in Super Bowl X (1976). These pro squads of the 1970s established cheerleaders as \"American icons of wholesome sex appeal.\" By 1981, a total of seventeen Nation Football League teams had their own cheerleaders. The only teams without NFL cheerleaders at this time were New Orleans, New York, Detroit, Cleveland, Denver, Minnesota, Pittsburg, San Francisco, and San Diego. Professional cheerleading eventually spread to soccer and basketball teams as well.\n\nThe 1980s saw the beginning of modern cheerleading, adding difficult stunt sequences and gymnastics into routines. All-star teams popped up, and with them, the creation of the United States All-Star Federation (USASF). ESPN first broadcast the National High School Cheerleading Competition nationwide in 1983. Cheerleading organizations such as the American Association of Cheerleading Coaches and Advisors (AACCA), founded in 1987, started applying universal safety standards to decrease the number of injuries and prevent dangerous stunts, pyramids, and tumbling passes from being included in the cheerleading routines. In 2003, the National Council for Spirit Safety and Education (NCSSE) was formed to offer safety training for youth, school, all-star, and college coaches. The NCAA requires college cheer coaches to successfully complete a nationally recognized safety-training program. The NCSSE or AACCA certification programs are both recognized by the NCAA.\n\nEven with its athletic and competitive development, cheerleading at the school level has retained its ties to its spirit leading traditions. Cheerleaders are quite often seen as ambassadors for their schools, and leaders among the student body. At the college level, cheerleaders are often invited to help at university fundraisers and events.\n\nCheerleading is very closely associated with American football and basketball. Sports such as association football (soccer), ice hockey, volleyball, baseball, and wrestling will sometimes sponsor cheerleading squads. The ICC Twenty20 Cricket World Cup in South Africa in 2007 was the first international cricket event to have cheerleaders. The Florida Marlins were the first Major League Baseball team to have a cheerleading team. Debuting in 2003, the \"Marlin Mermaids\" gained national exposure, and have influenced other MLB teams to develop their own cheer/dance squads.\n\nCompetitive cheerleading is scored subjectively based on components including, but not limited to, the cheer itself, dance/choreography, pyramids, stunting, and tumbling. In order to prevent injuries, there are certain rules that cheerleading teams have to follow according to their level (high school, all-star, or college). According to the Encyclopedia of Sports Medicine, there are two purposes of cheerleading - to cheer on the sidelines for other athletes, and to be a \"highly skilled competing athlete.\"\n\nAlong with this evolution to the sport's structure, there have been significant advancements made to the typical cheerleading uniform. What began as the classic sweater and mid-calf pleated skirt uniform has now come to incorporate materials that allow for stretch and flexibility. Uniform changes are a result of the changing culture since the 1930s.\n\nCheerleading may seem like a light-hearted activity to some, but injuries that can come from practice or a competition can be severe if the athlete is not properly trained. There have been many catastrophic injuries from cheer, especially from tumbling and stunting. Because of the lack of studies on injuries in competitive cheerleading, many injuries that happen could be avoided. Most studies in sports medicine pertaining to cheerleading are focused on whether it is a sport or not.\n\nMost American middle schools, high schools, and colleges have organized cheerleading squads. Many colleges offer cheerleading scholarships for students. A cheerleading team may compete locally, regionally, or nationally, as well as cheer for sporting events and encourage audience participation. Cheerleading is quickly becoming a year-round activity, starting with tryouts during the spring semester of the preceding school year. Teams may attend organized summer cheerleading camps and practices to improve skills and create routines for competition.\n\nStudent cheerleaders compete with recreational-style routine at competitions year-round. Teams practice intensely for competition and perform a routine no longer than 2 minutes and 30 seconds. Like other school-level athletes, teams compete to win league titles, and move on to bigger competitions with the hopes of reaching a national competition. The advantages to a school squad versus an all-star squad is cheering at various sporting events.\n\nThe tryout process can sometimes take place over a multiple day period. The cheerleading coach will arrange for a cheerleading clinic, during which basic materials are taught or reviewed before the final day of tryouts. The clinic gives returning cheerleaders and new cheerleaders an equal chance of becoming familiar with the material. Skills that are necessary to be a cheerleader include jumps, tumbling, motions, and dance ability. Tryouts often take place during the spring, so that the coach has the squad chosen in time to attend summer camp as a team.\n\nMiddle school cheerleading evolved shortly after high school squads were created. In middle school, cheerleading squads serve the same purpose, and follow the same rules as high school squads. Squads cheer for basketball teams, football teams, and other sports teams in their school. Squads also perform at pep rallies and compete against other local schools from the area. Cheerleading in middle school sometimes can be a two-season activity: fall and winter. However, many middle school cheer squads will go year-round like high school squads. Middle school cheerleaders use the same cheerleading movements as their older counterparts, yet they perform less extreme stunts. These stunts range from preps, thigh stands, and extensions, to harder one-legged stunts.\n\nIn high school, there are usually two squads per school: varsity and a junior varsity. High school cheerleading contains aspects of school spirit as well as competition. These squads have become part of a year-round cycle. Starting with tryouts in the spring, year-round practice, cheering on teams in the fall and winter, and participating in cheerleading competitions. Most squads practice at least three days a week for about two hours each practice during the summer. Many teams also attend separate tumbling sessions outside of practice. During the school year, cheerleading is usually practiced five- to six-days-a-week. During competition season, it often becomes seven days with practice twice a day sometimes. The school spirit aspect of cheerleading involves cheering, supporting, and \"pumping up\" the crowd at football games, basketball games, and even at wrestling meets. Along with this, they perform at pep rallies, and bring school spirit to other students. In May 2009, the National Federation of State High School Associations released the results of their first true high school participation study. They estimated that the number of high school cheerleaders from public high schools is around 394,700.\n\nThere are different cheerleading organizations that put on competitions; some of the major ones include state and regional competitions. Many high schools will often host cheerleading competitions, bringing in IHSA judges. The regional competitions are qualifiers for national competitions, such as the UCA (Universal Cheerleaders Association) in Orlando, Florida every year. The competition aspect of cheerleading can be very enduring; styles and rules change every year, making it important and difficult to find the newest and hottest routines. Most teams have a professional choreographer that choreographs their routine in order to ensure they are not breaking rules or regulations. For a list of rules, visit AACCA (American Association of Cheerleading Coaches and Administrators). All high school coaches are required to attend an IHSA rules meeting at the beginning of the season. This ensures their knowledge of changed rules and their compliance with these rules.\n\nMost American universities have a cheerleading squad to cheer for football, basketball, volleyball, and soccer. Most college squads tend to be large coed although in recent years; all-girl and small coed college squads have increased rapidly.\n\nCollege squads perform more difficult stunts which include pyramids, as well as flipping and twisting basket tosses.\n\nNot only do college cheerleaders cheer on the other sports at their university, many teams at universities compete with other schools at either UCA College Nationals or NCA College Nationals. This requires the teams to make a 2-minute 30 seconds that is full of tumbling, stunting, basket tosses, and pyramids. Winning these competitions are very prestigious accomplishments and is seen as another national title for most schools.\n\nOrganizations that sponsor youth cheer teams usually sponsor either youth league football or basketball teams as well. This allows for the two, under the same sponsor, to be intermingled. Both teams have the same mascot name and the cheerleaders will perform at their football or basketball games. Examples of such sponsors include Pop Warner and Pasco Police Athletic League (PPAL). The YMCA (Young Men's Christian Association) is also a well-known sponsor for youth cheerleading leagues.\n\nDuring the early 1980s, cheerleading squads not associated with a school or sports leagues, whose main objective was competition, began to emerge. The first organization to call themselves all-stars and go to competitions were the Q94 Rockers from Richmond, Virginia, founded in 1982. All-star teams competing prior to 1987 were placed into the same divisions as teams that represented schools and sports leagues. In 1986, the National Cheerleaders Association (NCA) addressed this situation by creating a separate division for teams lacking a sponsoring school or athletic association, calling it the All-Star Division and debuting it at their 1987 competitions. As the popularity of this type of team grew, more and more of them were formed, attending competitions sponsored by many different organizations and companies, each using its own set of rules, regulations, and divisions. This situation became a concern to gym owners because the inconsistencies caused coaches to keep their routines in a constant state of flux, detracting from time that could be better utilized for developing skills and providing personal attention to their athletes. More importantly, because the various companies were constantly vying for a competitive edge, safety standards had become more and more lax. In some cases, unqualified coaches and inexperienced squads were attempting dangerous stunts as a result of these expanded sets of rules.\n\nThe USASF was formed in 2003 by the competition companies to act as the national governing body for all star cheerleading and to create a standard set of rules and judging standards to be followed by all competitions sanctioned by the Federation, ultimately leading to the Cheerleading Worlds. The USASF hosted the first Cheerleading Worlds on April 24, 2004. In 2009, the first All-Level Worlds was held. It included teams from all levels, with each winner continuing to the online championships, where teams from across the nation competed to win the Worlds Title. At the same time, cheerleading coaches from all over the country organized themselves for the same rule making purpose, calling themselves the National All Star Cheerleading Coaches Congress (NACCC). In 2005, the NACCC was absorbed by the USASF to become their rule making body. In late 2006, the USASF facilitated the creation of the International All-Star Federation (IASF).\n\n, all-star cheerleading as sanctioned by the USASF involves a squad of 6–36 females and/or males. All-star differs from sideline cheerleading because all-star focuses on performing, while sideline cheers for others sport such as football or basketball. All-star is competitive teams that perform a routine for the purpose of entertainment against other teams, typically in the same divisions, to try to win. The squad prepares year-round for many different competition appearances, but they actually perform only for up to 2½ minutes during their team's routines. The numbers of competitions a team participates in varies from team to team, but generally, most teams tend to participate in eight to twelve competitions a year. These competitions include locals, which are normally taken place in school gymnasiums or local venues, nationals, hosted in big venues all around the U.S. with national champions, and the Cheerleading Worlds, taken place at Disney World in Orlando, Florida. During a competition routine, a squad performs carefully choreographed stunting, tumbling, jumping, and dancing to their own custom music. Teams create their routines to an eight-count system and apply that to the music so that the team members execute the elements with precise timing and synchronization.\n\nThere are many different organizations that host their own state and national competitions. Some major companies include: Universal Spirit, AmeriCheer, Cheersport, Planet Spirit, Eastern Cheer and Dance Association, and The JAM Brands. This means that many gyms within the same area could be state and national champions for the same year and never have competed against each other. Currently, there is no system in place that awards only one state or national title.\n\nJudges at the competition watch closely for illegal moves from the group or any individual member. Here, an illegal move is something that is not allowed in that division due to difficulty and/or safety restrictions. They look out for deductions, or things that go wrong, such as a dropped stunt. They also look for touch downs in tumbling for deductions. More generally, judges look at the difficulty and execution of jumps, stunts and tumbling, synchronization, creativity, the sharpness of the motions, showmanship, and overall routine execution.\n\nAll-star cheerleaders are placed into divisions, which are grouped based upon age, size of the team, gender of participants, and ability level. The age levels vary from under 4 year of age to 18 years and over. The divisions used by the USASF/IASF are currently Tiny, Mini, Youth, Junior, Junior International, Junior Coed, Senior, Senior Coed, Special Needs, and Open International. It originally began with \"all girl\" teams and later co-ed teams began to gain popularity. That being said, the all-girl squad remains the most prevalent.\n\nIf a team places high enough at selected USASF/IASF sanctioned national competitions, they could be included in the Cheerleading Worlds and compete against teams from all over the world, as well as receive money for placing. Each team receives a bid from another cheerleading company and goes in the name of that company. One must get a bid from a company in order to compete at the Cheerleading Worlds. For example, a team could get a bid from Cheersport, and they compete as a team representing that company. Cheerleading companies give out three types of bids to go to Cheerleading Worlds, Full Paid Bid, Partial Bid, or an Un-paid bid. The Cheerleading Worlds are only for teams that are level 5 and up.\n\nProfessional cheerleaders and dancers cheer for sports such as football, basketball, baseball, wrestling, or hockey. There are only a small handful of professional cheerleading leagues around the world; some professional leagues include the NBA Cheerleading League, the NFL Cheerleading League, the CFL Cheerleading League, the MLS Cheerleading League, the MLB Cheerleading League, and the NHL Ice Dancers. Although professional cheerleading leagues exist in multiple countries, there are no Olympic teams.\n\nIn addition to cheering at games and competing, professional cheerleaders also, as teams, can often do a lot of philanthropy and charity work, modeling, motivational speaking, television performances, and advertising.\n\nAmericheer: Americheer was founded in 1987 by Elizabeth Rossetti. It is the parent company to Ameridance and Eastern Cheer and Dance Association. In 2005, Americheer became one of the founding members of the NLCC. This means that Americheer events offer bids to The U.S. Finals: The Final Destination. AmeriCheer InterNational Championship competition is held every March at the Walt Disney World Resort in Orlando, Florida.\n\nInternational Cheer Union (ICU): Established on April 26, 2004, the ICU is recognized by the SportAccord as the world governing body of cheerleading and the authority on all matters with relation to it. Including participation from its 105-member national federations reaching 3.5 million athletes globally, the ICU continues to serve as the unified voice for those dedicated to cheerleading's positive development around the world.\n\nFollowing a positive vote by the SportAccord General Assembly on May 31, 2013, in Saint Petersburg, the International Cheer Union (ICU) became SportAccord's 109th member, and SportAccord's 93rd international sports federation to join the international sports family. In accordance with the SportAccord statutes, the ICU is recognized as the world governing body of cheerleading and the authority on all matters related to it.\n\nAs of the 2016–17 season, the ICU has introduced a Junior aged team (12-16) to compete at the Cheerleading Worlds, because cheerleading is now in provisional status to become a sport in the Olympics. For cheerleading to one day be in the Olympics, there must be a junior and senior team that competes at the world championships. The first junior cheerleading team that was selected to become the junior national team was Eastside Middle School, located in Mount Washington Kentucky and will represent the United States in the inaugural junior division at the world championships.\n\nThe ICU holds training seminars for judges and coaches, global events and the World Cheerleading Championships. The ICU is also fully applied to the International Olympic Committee (IOC) and is compliant under the code set by the World Anti-Doping Agency (WADA).\n\nInternational Federation of Cheerleading (IFC): Established on July 5, 1998, the International Federation of Cheerleading (IFC) is a non-profit federation based in Tokyo, Japan, and is the world governing body of cheerleading. The IFC objectives are to promote cheerleading worldwide, to spread knowledge of cheerleading, and to develop friendly relations among the member associations and federations.\n\nNational Cheerleaders Association: The NCA was founded in 1948 by Lawrence Herkimer. Every year, the NCA hosts the NCA High School Cheerleading Nationals and the NCA All-Star Cheerleading Nationals in Dallas, Texas. They also host the NCA/NDA Collegiate Cheer & Dance Championship in Daytona Beach, Florida.\n\nUnited Spirit Association: In 1950, Robert Olmstead directed his first summer training camp, and USA later sprouted from this. USA's focus is on the game day experience as a way to enhance audience entertainment. This focus led to the first American football half-time shows to reach adolescences from around the world and expose them to American style cheerleading. USA has choreographed material for professional and competitive cheerleaders alike. USA provides competitions for cheerleading squads without prior qualifications needed in order to participate. The organization also allows the opportunity for cheerleaders to become an All-American, participate in the Macy's Thanksgiving Day Parade, and partake in London's New Year's Day Parade and other special events much like UCA and NCA allow participants to do.\n\nUniversal Cheerleaders Association: Universal Cheerleaders Association was founded in 1974 by Jeff Webb. Since 1980, UCA has hosted the National High School Cheerleading Championship in Walt Disney World Resort. They also host the National All-Star Cheerleading Championship, and the College Cheerleading National Championship at Walt Disney World Resort. To qualify for these events, all teams must submit a video. All of these events air on ESPN.\n\nAsian Thailand Cheerleading Invitational (ATCI): Organised by the Cheerleading Association of Thailand (CAT) in accordance with the rules and regulations of the International Federation of Cheerleading (IFC). The ATCI is held every year since 2009. At the ATCI, many teams from all over Thailand compete, joining them are many invited neighbouring nations who also send cheer squads.\n\nCheerleading Asia International Open Championships (CAIOC): Hosted by the Foundation of Japan Cheerleading Association (FJCA) in accordance with the rules and regulations of the IFC. The CAIOC has been a yearly event since 2007. Every year, many teams from all over Asia converge in Tokyo to compete.\n\nCheerleading World Championships (CWC): Organised by the IFC. The IFC is a non-profit organisation founded in 1998 and based in Tokyo, Japan.\nThe CWC has been held every two years since 2001, and to date, the competition has been held in Japan, the United Kingdom, Finland, Germany, and Hong Kong. The 6th CWC was held at the Hong Kong Coliseum on November 26–27, 2011.\n\nICU World Championships: The International Cheer Union currently encompasses 105 National Federations from countries across the globe. Every year, the ICU host the World Cheerleading Championship. Unlike the USASF Worlds, this competition uses Level 6/ Collegiate style rules. Countries assemble and send only one team to represent them.\n\nNational Cheerleading Championships (NCC): The NCC is the annual IFC-sanctioned national cheerleading competition in Indonesia organised by the Indonesian Cheerleading Community (ICC). Since NCC 2010, the event is now open to international competition, representing a significant step forward for the ICC. Teams from many countries such as Japan, Thailand, the Philippines, and Singapore participated in the ground breaking event.\n\nNLCC Final Destination: Nation's Leading Cheer Companies is a multi brand company, partnered with other companies such as: Americheer/Ameridance, American Cheer & Dance Academy, Eastern Cheer & Dance Association, and Spirit Unlimited. Every year, starting in 2006, the NLCC hosts The US Finals: The Final Destination of Cheerleading and Dance. Every team that attends must qualify and receive a bid at a partner company's competition. In May 2008, the NLCC and The JAM Brands announced a partnership to produce The U.S. Finals - Final Destination. There are nine Final Destination locations across the country. After the regional events, videos of all the teams that competed are sent to a new panel of judges and rescored to rank teams against those against whom they may never have had a chance to compete.\n\nPan-American Cheerleading Championships (PCC): The PCC was held for the first time in 2009 in the city of Latacunga, Ecuador and is the continental championship organised by the Pan-American Federation of Cheerleading (PFC). The PFC, operating under the umbrella of the IFC, is the non-profit continental body of cheerleading whose aim it is to promote and develop cheerleading in the Americas. The PCC is a biennial event, and was held for the second time in Lima, Peru, in November 2010.\n\nThe JAM Brands: The JAM Brands, headquartered in Louisville, Kentucky, provides products and services for the cheerleading and dance industry. It is made up of approximately 12 different brands that produce everything from competitions to camps to uniforms to merchandise and apparel. JAMfest, the original brand of The JAM Brands, has been around since 1996 and was founded by Aaron Flaker and Emmitt Tyler. Dan Kessler has since become a co-owner of The JAM Brands along with Flaker and Tyler.\n\nUSASF/IASF Worlds: Many United States cheerleading organizations form and register the not-for-profit entity the United States All Star Federation (USASF) and also the International All Star Federation (IASF) to support international club cheerleading and the World Cheerleading Club Championships. The first World Cheerleading Championships, or Cheerleading Worlds, were hosted by the USASF/IASF at the Walt Disney World Resort and taped for an ESPN global broadcast in 2004. This competition is only for All-Star/Club cheer. Only levels Junior 5, Senior 5, Senior Open 5, International 5, International Open 5, International 6, and International Open 6 may attend. Teams must receive a bid from a partner company to attend.\n\nVarsity: Partnered with the UCA, Varsity created the National High School Cheerleading Championship in 1980. Varsity All-Star owns or partners with many of the largest cheerleading events in the country.\n\nThere is a large debate on whether or not cheerleading should be considered a sport for Title IX (a portion of the United States Education Amendments of 1972 forbidding discrimination under any education program on the basis of sex) purposes. Supporters consider cheerleading, as a whole, a sport, citing the heavy use of athletic talents while critics see it as a physical activity because a \"sport\" implies a competition among all squads and not all squads compete, along with subjectivity of competitions where—as with gymnastics, diving, and figure skating—scores are assessed based on human judgment and not an objective goal or measurement of time.\n\nOn January 27, 2009, in a lawsuit involving an accidental injury sustained during a cheerleading practice, the Wisconsin Supreme Court ruled that cheerleading is a full-contact sport in that state, not allowing any participants to be sued for accidental injury. In contrast, on July 21, 2010, in a lawsuit involving whether college cheerleading qualified as a sport for purposes of Title IX, a federal court, citing a current lack of program development and organization, ruled that it is not a sport at all.\n\nCheerleading carries the highest rate of catastrophic injuries to girl athletes in sports. The risks of cheerleading were highlighted when Kristi Yamaoka, a cheerleader for Southern Illinois University, suffered a fractured vertebra when she hit her head after falling from a human pyramid. She also suffered from a concussion, and a bruised lung. The fall occurred when Yamaoka lost her balance during a basketball game between Southern Illinois University and Bradley University at the Savvis Center in St. Louis on March 5, 2006. The fall gained \"national attention\", because Yamaoka continued to perform from a stretcher as she was moved away from the game. Yamaoka has since made a full recovery.\n\nThe accident caused the Missouri Valley Conference to ban its member schools from allowing cheerleaders to be \"launched or tossed and from taking part in formations higher than two levels\" for one week during a women's basketball conference tournament, and also resulted in a recommendation by the NCAA that conferences and tournaments do not allow pyramids two and one half levels high or higher, and a stunt known as basket tosses, during the rest of the men's and women's basketball season. On July 11, 2006, the bans were made permanent by the AACCA rules committee:\n\nThe committee unanimously voted for sweeping revisions to cheerleading safety rules, the most major of which restricts specific upper-level skills during basketball games. Basket tosses, 2½ high pyramids, one-arm stunts, stunts that involve twisting or flipping, and twisting tumbling skills may be performed only during halftime and post-game on a matted surface and are prohibited during game play or time-outs.\nAnother major cheerleading accident was the death of Lauren Chang. Chang died on April 14, 2008 after competing in a competition where her teammate had kicked her so hard in the chest that her lungs collapsed.\n\nOf the United States' 2.9 million female high school athletes, only 3% are cheerleaders, yet cheerleading accounts for nearly 65% of all catastrophic injuries in girls' high school athletics. The NCAA does not recognize cheerleading as a collegiate sport; there are no solid numbers on college cheerleading, yet when it comes to injuries, 67% of female athlete injuries at the college level are due to cheerleading mishaps. Another study found that between 1982 and 2007, there were 103 fatal, disabling, or serious injuries recorded among female high school athletes, with the vast majority (67) occurring in cheerleading.\n\nIn the early 2000s, cheerleading was considered one of the most dangerous school activities. The main source of injuries comes from stunting, also known as pyramids. These stunts are performed at games and pep rallies, as well as competitions. Sometimes competition routines are focused solely around the use of difficult and risky stunts. These stunts usually include a flyer (the person on top), along with one or two bases (the people on the bottom), and one or two spotters in the front and back on the bottom. The most common cheerleading related injury is a concussion. 96% of those concussions are stunt related. Others injuries are: sprained ankles, sprained wrists, back injuries, head injuries (sometimes concussions), broken arms, elbow injuries, knee injuries, broken noses, and broken collarbones. Sometimes, however, injuries can be as serious as whiplash, broken necks, broken vertebrae, and death.\n\nThe journal \"Pediatrics\" has reportedly said that the number of cheerleaders suffering from broken bones, concussions, and sprains has increased by over 100 percent between the years of 1990 and 2002, and that in 2001, there were 25,000 hospital visits reported for cheerleading injuries dealing with the shoulder, ankle, head, and neck. Meanwhile, in the US, cheerleading accounted for 65.1% of all major physical injuries to high school females, and to 66.7% of major injuries to college students due to physical activity from 1982 to 2007, with 22,900 minors being admitted to hospital with cheerleading-related injuries in 2002.\n\nIn October 2009, the American Association of Cheerleading Coaches and Advisors (AACCA), a subsidiary of Varsity Brands, released a study that analyzed the data from emergency room visits of all high school athletes. The study asserted that contrary to many perceptions, cheerleading injuries are in line with female sports.\n\nCheerleading (for both girls and boys) was one of the sports studied in the Pediatric Injury Prevention, Education and Research Program of the Colorado School of Public Health in 2009/10–2012/13. Data on cheerleading injuries is included in the report for 2012–13.\n\nBULLET::::- Movies and television\nThe revamped and provocative Dallas Cowboys Cheerleaders of the 1970s—and the many imitators that followed—firmly established the cheerleader as an American icon of wholesome sex appeal. In response, a new subgenre of exploitation films suddenly sprang up with titles such as \"The Cheerleaders\" (1972), \"The Swinging Cheerleaders\" (1974), \"Revenge of the Cheerleaders\" (1975), \"The Pom Pom Girls\" (1976), \"Satan's Cheerleaders\" (1977), \"Cheerleaders Beach Party\" (1978), \"Cheerleaders's Wild Weekend\" (1979), and \"Gimme an 'F'\" (1984). In addition to R-rated sex comedies and horror films, cheerleaders became a staple of the adult film industry, starting with \"Debbie Does Dallas\" (1978) and its four sequels.\n\nBULLET::::- Video games\nNintendo has released a pair of video games in Japan for the Nintendo DS, \"Osu! Tatakae! Ouendan\" and its sequel \"Moero! Nekketsu Rhythm Damashii\" that star teams of male cheer squads, or Ouendan that practice a form of cheerleading. Each of the games' most difficult modes replaces the male characters with female cheer squads that dress in western cheerleading uniforms. The games task the cheer squads with assisting people in desperate need of help by cheering them on and giving them the motivation to succeed. There are also an \"All Star Cheerleader\" and \"We Cheer\" for the Wii in which one does routines at competitions with the Wiimote & Nunchuck. \"All Star Cheerleader\" is also available for Nintendo DS.\n\nCheerleading in Canada is rising in popularity among the youth in co-curricular programs. Cheerleading has grown from the sidelines to a competitive activity throughout the world and in particular Canada. Cheerleading has a few streams in Canadian sports culture. It is available at the middle-school, high-school, collegiate, and best known for all-star. There are multiple regional, provincial, and national championship opportunities for all athletes participating in cheerleading. Canada does not have provincial teams, just a national program referred to as CCU or Team Canada. Their first year as a national team was in 2009 when they represented Canada at the International Cheer Union World Cheerleading Championships International Cheer Union (ICU).\n\nThere is no official governing body for Canadian cheerleading. The rules and guidelines for cheerleading used in Canada are the ones set out by the USASF. However, there are many organizations in Canada that put on competitions and have separate and individual rules and scoresheets for each competition. Cheer Evolution is the largest cheerleading and dance organization for Canada. They hold many competitions as well as provide a competition for bids to Worlds. There are other organizations such as the Ontario Cheerleading Federation (Ontario), Power Cheerleading Association (Ontario), Kicks Athletics (Quebec), and the International Cheer Alliance (Vancouver). There are over forty recognized competitive gym clubs with numerous teams that compete at competitions across Canada.\n\nThere are two world championship competitions that Canada participates in. There is the ICU World Championships where the national teams compete against each other and then there are the club team world championships. These club teams are referred to as \"all-star\" teams who compete at the USASF World Championships of Cheerleading. This is where teams must have earned a bid from their own country to attend. National team members who compete at the ICU Worlds can also compete with their \"all-star club\" teams. Although athletes can compete in both International Cheer Union (ICU) and USASF, crossovers between teams at each individual competition are not permitted. Teams compete against the other teams from their countries on the first day of competition and the top three teams from each country in each division continue to finals. At the end of finals, the top team scoring the highest for their country earns the \"Nations Cup\". Canada has multiple teams across their country that compete in the USASF Cheerleading Worlds Championship.\n\nThe International Cheer Union (ICU) is built of 103 countries that compete against each other in four divisions; Coed Premier, All-girl Premier, Coed Elite, and All-girl Elite. Canada has a national team ran by the Canadian Cheer Union (CCU). Their Coed Elite Level 5 Team and their All-girl Elite Level 5 team are 4-time world champions. They are found from all over the country. In 2013, they added two more teams to their roster. A new division that will compete head-to-head with the United States: in both the All-girl and Coed Premier Level 6 divisions. Members tryout and are selected on the basis of their skills and potential to succeed. Athletes are selected from all over. Canada's national program has grown to be one of the most successful programs.\n\nBULLET::::- \"Cheerleader Nation\"\nBULLET::::- Cheerleading Philippines\nBULLET::::- Color guard\nBULLET::::- Dance squad\nBULLET::::- Dance\nBULLET::::- Gymnastics\nBULLET::::- List of cheerleading jumps\nBULLET::::- List of cheerleading stunts\nBULLET::::- Majorette (dancer)\nBULLET::::- National Basketball Association Cheerleading\nBULLET::::- National Football League Cheerleading\nBULLET::::- Ōendan\nBULLET::::- Pep squad\nBULLET::::- Pom-pom\nBULLET::::- UAAP Cheerdance Competition\nBULLET::::- Varsity Brands\n"}
{"id": "6751", "url": "https://en.wikipedia.org/wiki?curid=6751", "title": "Cottingley Fairies", "text": "Cottingley Fairies\n\nThe Cottingley Fairies appear in a series of five photographs taken by Elsie Wright (1901–1988) and Frances Griffiths (1907–1986), two young cousins who lived in Cottingley, near Bradford in England. In 1917, when the first two photographs were taken, Elsie was 16 years old and Frances was 9. The pictures came to the attention of writer Sir Arthur Conan Doyle, who used them to illustrate an article on fairies he had been commissioned to write for the Christmas 1920 edition of \"The Strand Magazine\". Doyle, as a spiritualist, was enthusiastic about the photographs, and interpreted them as clear and visible evidence of psychic phenomena. Public reaction was mixed; some accepted the images as genuine, others believed that they had been faked.\n\nInterest in the Cottingley Fairies gradually declined after 1921. Both girls married and lived abroad for a time after they grew up, yet the photographs continued to hold the public imagination. In 1966 a reporter from the \"Daily Express\" newspaper traced Elsie, who had by then returned to the UK. Elsie left open the possibility that she believed she had photographed her thoughts, and the media once again became interested in the story.\n\nIn the early 1980s Elsie and Frances admitted that the photographs were faked, using cardboard cutouts of fairies copied from a popular children's book of the time, but Frances maintained that the fifth and final photograph was genuine. Currently the photographs and two of the cameras used are on display in the National Science and Media Museum in Bradford, England. In December 2019 the third camera used to take the images was acquired and is scheduled to complete the exhibition.\n\nIn mid-1917 nine-year-old Frances Griffiths and her mother—both newly arrived in the UK from South Africa—were staying with Frances' aunt, Elsie Wright's mother, in the village of Cottingley in West Yorkshire; Elsie was then 16 years old. The two girls often played together beside the beck (stream) at the bottom of the garden, much to their mothers' annoyance, because they frequently came back with wet feet and clothes. Frances and Elsie said they only went to the beck to see the fairies, and to prove it, Elsie borrowed her father's camera, a Midg quarter-plate. The girls returned about 30 minutes later, \"triumphant\".\n\nElsie's father, Arthur, was a keen amateur photographer, and had set up his own darkroom. The picture on the photographic plate he developed showed Frances behind a bush in the foreground, on which four fairies appeared to be dancing. Knowing his daughter's artistic ability, and that she had spent some time working in a photographer's studio, he dismissed the figures as cardboard cutouts. Two months later the girls borrowed his camera again, and this time returned with a photograph of Elsie sitting on the lawn holding out her hand to a gnome. Exasperated by what he believed to be \"nothing but a prank\", and convinced that the girls must have tampered with his camera in some way, Arthur Wright refused to lend it to them again. His wife Polly, however, believed the photographs to be authentic.\n\nTowards the end of 1918, Frances sent a letter to Johanna Parvin, a friend in Cape Town, South Africa, where Frances had lived for most of her life, enclosing the photograph of herself with the fairies. On the back she wrote \"It is funny, I never used to see them in Africa. It must be too hot for them there.\"\n\nThe photographs became public in mid-1919, after Elsie's mother attended a meeting of the Theosophical Society in Bradford. The lecture that evening was on \"fairy life\", and at the end of the meeting Polly Wright showed the two fairy photographs taken by her daughter and niece to the speaker. As a result, the photographs were displayed at the society's annual conference in Harrogate, held a few months later. There they came to the attention of a leading member of the society, Edward Gardner. One of the central beliefs of theosophy is that humanity is undergoing a cycle of evolution, towards increasing \"perfection\", and Gardner recognised the potential significance of the photographs for the movement:\nGardner sent the prints along with the original glass-plate negatives to Harold Snelling, a photography expert. Snelling's opinion was that \"the two negatives are entirely genuine, unfaked photographs ... [with] no trace whatsoever of studio work involving card or paper models\". He did not go so far as to say that the photographs showed fairies, stating only that \"these are straight forward photographs of whatever was in front of the camera at the time\". Gardner had the prints \"clarified\" by Snelling, and new negatives produced, \"more conducive to printing\", for use in the illustrated lectures he gave around the UK. Snelling supplied the photographic prints which were available for sale at Gardner's lectures.\nAuthor and prominent spiritualist Sir Arthur Conan Doyle learned of the photographs from the editor of the spiritualist publication \"Light\". Doyle had been commissioned by \"The Strand Magazine\" to write an article on fairies for their Christmas issue, and the fairy photographs \"must have seemed like a godsend\" according to broadcaster and historian Magnus Magnusson. Doyle contacted Gardner in June 1920 to determine the background to the photographs, and wrote to Elsie and her father to request permission from the latter to use the prints in his article. Arthur Wright was \"obviously impressed\" that Doyle was involved, and gave his permission for publication, but he refused payment on the grounds that, if genuine, the images should not be \"soiled\" by money.\n\nGardner and Doyle sought a second expert opinion from the photographic company Kodak. Several of the company's technicians examined the enhanced prints, and although they agreed with Snelling that the pictures \"showed no signs of being faked\", they concluded that \"this could not be taken as conclusive evidence ... that they were authentic photographs of fairies\". Kodak declined to issue a certificate of authenticity. Gardner believed that the Kodak technicians might not have examined the photographs entirely objectively, observing that one had commented \"after all, as fairies couldn't be true, the photographs must have been faked somehow\". The prints were also examined by another photographic company, Ilford, who reported unequivocally that there was \"some evidence of faking\". Gardner and Doyle, perhaps rather optimistically, interpreted the results of the three expert evaluations as two in favour of the photographs' authenticity and one against.\n\nDoyle also showed the photographs to the physicist and pioneering psychical researcher Sir Oliver Lodge, who believed the photographs to be fake. He suggested that a troupe of dancers had masqueraded as fairies, and expressed doubt as to their \"distinctly 'Parisienne hairstyles.\n\nOn October 4, 2018 the first two of the photographs, \"Alice and the Fairies\" and \"Iris and the Gnome,\" were to be sold by Dominic Winter Auctioneers, in Gloucestershire. The prints, suspected to have been made in 1920 to sell at theosophical lectures, were expected to bring £700-£1000 each. As it turned out, 'Iris with the Gnome' sold for a hammer price of £5,400 (plus 24% buyer's premium incl. VAT), while 'Alice and the Fairies' sold for a hammer price of £15,000 (plus 24% buyer's premium incl. VAT).\n\nDoyle was preoccupied with organising an imminent lecture tour of Australia, and in July 1920, sent Gardner to meet the Wright family. Frances was by then living with her parents in Scarborough, but Elsie's father told Gardner that he had been so certain the photographs were fakes that while the girls were away he searched their bedroom and the area around the beck (stream), looking for scraps of pictures or cutouts, but found nothing \"incriminating\".\nGardner believed the Wright family to be honest and respectable. To place the matter of the photographs' authenticity beyond doubt, he returned to Cottingley at the end of July with two W. Butcher & Sons Cameo folding plate cameras and 24 secretly marked photographic plates. Frances was invited to stay with the Wright family during the school summer holiday so that she and Elsie could take more pictures of the fairies. Gardner described his briefing in his 1945 \"Fairies: A Book of Real Fairies\":\n\nUntil 19 August the weather was unsuitable for photography. Because Frances and Elsie insisted that the fairies would not show themselves if others were watching, Elsie's mother was persuaded to visit her sister's for tea, leaving the girls alone. In her absence the girls took several photographs, two of which appeared to show fairies. In the first, \"Frances and the Leaping Fairy\", Frances is shown in profile with a winged fairy close by her nose. The second, \"Fairy offering Posy of Harebells to Elsie\", shows a fairy either hovering or tiptoeing on a branch, and offering Elsie a flower. Two days later the girls took the last picture, \"Fairies and Their Sun-Bath\".\n\nThe plates were packed in cotton wool and returned to Gardner in London, who sent an \"ecstatic\" telegram to Doyle, by then in Melbourne. Doyle wrote back:\n\nDoyle's article in the December 1920 issue of \"The Strand\" contained two higher-resolution prints of the 1917 photographs, and sold out within days of publication. To protect the girls' anonymity, Frances and Elsie were called Alice and Iris respectively, and the Wright family was referred to as the \"Carpenters\". An enthusiastic and committed spiritualist, Doyle hoped that if the photographs convinced the public of the existence of fairies then they might more readily accept other psychic phenomena. He ended his article with the words:\n\nEarly press coverage was \"mixed\", generally a combination of \"embarrassment and puzzlement\". The historical novelist and poet Maurice Hewlett published a series of articles in the literary journal \"John O' London's Weekly\", in which he concluded: \"And knowing children, and knowing that Sir Arthur Conan Doyle has legs, I decide that the Miss Carpenters have pulled one of them.\" The Sydney newspaper \"Truth\" on 5 January 1921 expressed a similar view; \"For the true explanation of these fairy photographs what is wanted is not a knowledge of occult phenomena but a knowledge of children.\" Some public figures were more sympathetic. Margaret McMillan, the educational and social reformer, wrote: \"How wonderful that to these dear children such a wonderful gift has been vouchsafed.\" The novelist Henry De Vere Stacpoole decided to take the fairy photographs and the girls at face value. In a letter to Gardner he wrote: \"Look at Alice's [Frances'] face. Look at Iris's [Elsie's] face. There is an extraordinary thing called Truth which has 10 million faces and forms – it is God's currency and the cleverest coiner or forger can't imitate it.\"\n\nMajor John Hall-Edwards, a keen photographer and pioneer of medical X-ray treatments in Britain, was a particularly vigorous critic:\n\nDoyle used the later photographs in 1921 to illustrate a second article in \"The Strand\", in which he described other accounts of fairy sightings. The article formed the foundation for his 1922 book \"The Coming of the Fairies\". As before, the photographs were received with mixed credulity. Sceptics noted that the fairies \"looked suspiciously like the traditional fairies of nursery tales\" and that they had \"very fashionable hairstyles\".\n\nGardner made a final visit to Cottingley in August 1921. He again brought cameras and photographic plates for Frances and Elsie, but was accompanied by the clairvoyant Geoffrey Hodson. Although neither of the girls claimed to see any fairies, and there were no more photographs, \"on the contrary, he [Hodson] saw them [fairies] everywhere\" and wrote voluminous notes on his observations.\n\nBy now Elsie and Frances were tired of the whole fairy business. Years later Elsie looked at a photograph of herself and Frances taken with Hodson and said: \"Look at that, fed up with fairies.\" Both Elsie and Frances later admitted that they \"played along\" with Hodson \"out of mischief\", and that they considered him \"a fake\".\n\nPublic interest in the Cottingley Fairies gradually subsided after 1921. Elsie and Frances eventually married and lived abroad for many years. In 1966, a reporter from the \"Daily Express\" newspaper traced Elsie, who was by then back in England. She admitted in an interview given that year that the fairies might have been \"figments of my imagination\", but left open the possibility she believed that she had somehow managed to photograph her thoughts. The media subsequently became interested in Frances and Elsie's photographs once again. BBC television's \"Nationwide\" programme investigated the case in 1971, but Elsie stuck to her story: \"I've told you that they're photographs of figments of our imagination, and that's what I'm sticking to\".\n\nElsie and Frances were interviewed by journalist Austin Mitchell in September 1976, for a programme broadcast on Yorkshire Television. When pressed, both women agreed that \"a rational person doesn't see fairies\", but they denied having fabricated the photographs. In 1978 the magician and scientific sceptic James Randi and a team from the Committee for the Scientific Investigation of Claims of the Paranormal examined the photographs, using a \"computer enhancement process\". They concluded that the photographs were fakes, and that strings could be seen supporting the fairies. Geoffrey Crawley, editor of the \"British Journal of Photography\", undertook a \"major scientific investigation of the photographs and the events surrounding them\", published between 1982 and 1983, \"the first major postwar analysis of the affair\". He also concluded that the pictures were fakes.\n\nIn 1983, the cousins admitted in an article published in the magazine \"The Unexplained\" that the photographs had been faked, although both maintained that they really had seen fairies. Elsie had copied illustrations of dancing girls from a popular children's book of the time, \"Princess Mary's Gift Book\", published in 1914, and drew wings on them. They said they had then cut out the cardboard figures and supported them with hatpins, disposing of their props in the beck once the photograph had been taken. But the cousins disagreed about the fifth and final photograph, which Doyle in his \"The Coming of the Fairies\" described in this way:\n\nElsie maintained it was a fake, just like all the others, but Frances insisted that it was genuine. In an interview given in the early 1980s Frances said:\n\nBoth Frances and Elsie claimed to have taken the fifth photograph. In a letter published in \"The Times\" newspaper on 9 April 1983, Geoffrey Crawley explained the discrepancy by suggesting that the photograph was \"an unintended double exposure of fairy cutouts in the grass\", and thus \"both ladies can be quite sincere in believing that they each took it\".\n\nIn a 1985 interview on Yorkshire Television's \"Arthur C. Clarke's World of Strange Powers\", Elsie said that she and Frances were too embarrassed to admit the truth after fooling Doyle, the author of Sherlock Holmes: \"Two village kids and a brilliant man like Conan Doyle – well, we could only keep quiet.\" In the same interview Frances said: \"I never even thought of it as being a fraud – it was just Elsie and I having a bit of fun and I can't understand to this day why they were taken in – they wanted to be taken in.\"\n\nFrances died in 1986, and Elsie in 1988. Prints of their photographs of the fairies, along with a few other items including a first edition of Doyle's book \"The Coming of the Fairies\", were sold at auction in London for £21,620 in 1998. That same year, Geoffrey Crawley sold his Cottingley Fairy material to the National Museum of Film, Photography and Television in Bradford (now the National Science and Media Museum), where it is on display. The collection included prints of the photographs, two of the cameras used by the girls, watercolours of fairies painted by Elsie, and a nine-page letter from Elsie admitting to the hoax.\nThe glass photographic plates were bought for £6,000 by an unnamed buyer at a London auction held in 2001.\n\nFrances' daughter, Christine Lynch, appeared in an episode of the television programme \"Antiques Roadshow\" in Belfast, broadcast on BBC One in January 2009, with the photographs and one of the cameras given to the girls by Doyle. Christine told the expert, Paul Atterbury, that she believed, as her mother had done, that the fairies in the fifth photograph were genuine. Atterbury estimated the value of the items at between £25,000 and £30,000. The first edition of Frances' memoirs was published a few months later, under the title \"Reflections on the Cottingley Fairies\". The book contains correspondence, sometimes \"bitter\", between Elsie and Frances. In one letter, dated 1983, Frances wrote:\nThe 1997 films \"\" and \"Photographing Fairies\" were inspired by the events surrounding the Cottingley Fairies. The photographs were parodied in a 1994 book written by Terry Jones and Brian Froud, \"Lady Cottington's Pressed Fairy Book\".\n\nIn 2017 a further two fairy photographs were presented as evidence that the girls' parents were part of the conspiracy. Dating from 1917 and 1918, both photographs are poorly executed copies of two of the original fairy photographs. One was published in 1918 in \"The Sphere\" newspaper, which was before the originals had been seen by anyone outside the girls' immediate family.\n\nIn 2019, a print of the first of the five photographs, sold for £1,050. A print of the second photograph was also put up sale but failed to sell as it did not meet its £500 reserve price. The photographs previously belonged to the Reverend George Vale Owen.\n\nBULLET::::- Bihet, Francesca (2013). \"Sprites, spiritualists and sleuths: the intersecting ownership of transcendent proofs in the Cottingley Fairy Fraud\". In: Afterlife: 18th Postgraduate Religion and Theology Conference, 8–9 March 2013, University of Bristol. (Unpublished)\n\nBULLET::::- \"The Coming of the Fairies\" – scans of the original version of Sir Arthur Conan Doyle's book (1922)\nBULLET::::- The Case of the Cottingley Fairies at The James Randi Educational Foundation\nBULLET::::- Cottingley Fairies at Cottingley.Net – The Cottingley Network\nBULLET::::- Cottingley Fairies at Cottingley Connect\nBULLET::::- Archival Material at\n"}
{"id": "6752", "url": "https://en.wikipedia.org/wiki?curid=6752", "title": "Cheka", "text": "Cheka\n\nThe All-Russian Extraordinary Commission (), abbreviated as VChK (, \"Ve-Che-Ka\") and commonly known as Cheka (from the initialism ChK - ), was the first of a succession of Soviet secret-police organizations. Established on December 5 (Old Style) 1917 by the Sovnarkom, it came under the leadership of Felix Dzerzhinsky, a Polish aristocrat-turned-communist.\nBy late 1918 hundreds of Cheka committees had sprung up in the RSFSR at the oblast, guberniya, raion, uyezd, and volost levels.\n\nThe official designation was All-Russian Extraordinary (or Emergency) Commission for Combating Counter-Revolution and Sabotage under the Council of People's Commissars of the RSFSR (, \"Vserossiyskaya chrezvychaynaya komissiya po borbe s kontrrevolyutsiyey i sabotazhem pri Sovete narodnykh komisarov RSFSR\").\n\nIn 1918 its name was changed, becoming All-Russian Extraordinary Commission for Combating Counter-Revolution, Profiteering and Corruption.\n\nA member of Cheka was called a \"chekist\". Also, the term \"chekist\" often referred to Soviet secret police throughout the Soviet period, despite official name changes over time. In \"The Gulag Archipelago\", Alexander Solzhenitsyn recalls that \"zeks\" in the labor camps used \"old chekist\" as a mark of special esteem for particularly experienced camp administrators. The term is still found in use in Russia today (for example, President Vladimir Putin has been referred to in the Russian media as a \"chekist\" due to his career in the KGB and as head of the KGB's successor, FSB).\n\nThe chekists commonly dressed in black leather, including long flowing coats, reportedly after being issued such distinctive coats early in their existence. Western communists adopted this clothing fashion. The Chekists also often carried with them Greek-style worry beads made of amber, which had become \"fashionable among high officials during the time of the 'cleansing'\".\n\nIn 1921, the \"Troops for the Internal Defense of the Republic\" (a branch of the Cheka) numbered at least 200,000. These troops policed labor camps, ran the Gulag system, conducted requisitions of food, and subjected political opponents to secret arrest, detention, torture and summary execution. They also put down rebellions and riots by workers or peasants, and mutinies in the desertion-plagued Red Army.\n\nAfter 1922 Cheka groups underwent the first of a series of reorganizations; however the theme of a government dominated by \"the organs\" persisted indefinitely afterward, and Soviet citizens continued to refer to members of the various organs as Chekists.\n\nIn the first month and half after the October Revolution (1917), the duty of \"extinguishing the resistance of exploiters\" was assigned to the Petrograd Military Revolutionary Committee (or PVRK). It represented a temporary body working under directives of the Council of People's Commissars (Sovnarkom) and Central Committee of RDSRP(b). The VRK created new bodies of government, organized food delivery to cities and the Army, requisitioned products from \"bourgeoisie\", and sent its emissaries and agitators into provinces. One of its most important functions was the security of \"revolutionary order\", and the fight against \"counterrevolutionary\" activity (see: Anti-Soviet agitation).\n\nOn December 1, 1917, the All-Russian Central Executive Committee (VTsIK or TsIK) reviewed a proposed reorganization of the VRK, and possible replacement of it. On December 5, the Petrograd VRK published an announcement of dissolution and transferred its functions to the department of TsIK for the fight against \"counterrevolutionaries\". On December 6, the Council of People's Commissars (Sovnarkom) strategized how to persuade government workers to strike across Russia. They decided that a special commission was needed to implement the \"most energetically revolutionary\" measures. Felix Dzerzhinsky (the Iron Felix) was appointed as Director and invited the participation of the following individuals: V. K. Averin, Vasili Vasilyevich Yakovlev, D. G. Yevseyev, N. A. Zhydelev, I. K. Ksenofontov, G. K. Ordjonikidze, Ya. Kh. Peters, K. A. Peterson, V. A. Trifonov.\n\nOn December 7, 1917, all invited except Zhydelev and Vasilevsky gathered in the Smolny Institute to discuss the competence and structure of the commission to combat counterrevolution and sabotage. The obligations of the commission were: \"to liquidate to the root all of the counterrevolutionary and sabotage activities and all attempts to them in all of Russia, to hand over counter-revolutionaries and saboteurs to the revolutionary tribunals, develop measures to combat them and relentlessly apply them in real world applications. The commission should only conduct a preliminary investigation\". The commission should also observe the press and counterrevolutionary parties, sabotaging officials and other criminals. \n\nThree sections were created: informational, organizational, and a unit to combat counter-revolution and sabotage. Upon the end of the meeting, Dzerzhinsky reported to the Sovnarkom with the requested information. The commission was allowed to apply such measures of repression as 'confiscation, deprivation of ration cards, publication of lists of enemies of the people etc.'\". That day, Sovnarkom officially confirmed the creation of VCheKa. The commission was created not under the VTsIK as was previously anticipated, but rather under the Council of the People's Commissars.\n\nOn December 8, 1917, some of the original members of the VCheka were replaced. Averin, Ordzhonikidze, and Trifonov were replaced by V. V. Fomin, S. E. Shchukin, Ilyin, and Chernov. On the meeting of December 8, the presidium of VChK was elected of five members, and chaired by Dzerzhinsky. The issue of \"speculation\" was raised at the same meeting, which was assigned to Peters to address and report with results to one of the next meetings of the commission. A circular, published on , gave the address of VCheka's first headquarters as \"Petrograd, Gorokhovaya 2, 4th floor\". On December 11, Fomin was ordered to organize a section to suppress \"speculation.\" And in the same day, VCheKa offered Shchukin to conduct arrests of counterfeiters.\n\nIn January 1918, a subsection of the anti-counterrevolutionary effort was created to police bank officials. The structure of VCheKa was changing repeatedly. By March 1918, when the organization came to Moscow, it contained the following sections: against counterrevolution, speculation, non-residents, and information gathering. By the end of 1918–1919, some new units were created: secretly operative, investigatory, of transportation, military (special), operative, and instructional. By 1921, it changed once again, forming the following sections: directory of affairs, administrative-organizational, secretly operative, economical, and foreign affairs.\n\nIn the first months of its existence, VCheKa consisted of only 40 officials. It commanded a team of soldiers, the Sveaborgesky regiment, as well as a group of Red Guardsmen. On January 14, 1918, Sovnarkom ordered Dzerzhinsky to organize teams of \"energetic and ideological\" sailors to combat speculation. By the spring of 1918, the commission had several teams: in addition to the Sveaborge team, it had an intelligence team, a team of sailors, and a strike team. Through the winter of 1917–1918, all activities of VCheKa were centralized mainly in the city of Petrograd. It was one of several other commissions in the country which fought against counterrevolution, speculation, banditry, and other activities perceived as crimes. Other organizations included: the Bureau of Military Commissars, and an Army-Navy investigatory commission to attack the counterrevolutionary element in the Red Army, plus the Central Requisite and Unloading Commission to fight speculation. The investigation of counterrevolutionary or major criminal offenses was conducted by the Investigatory Commission of Revtribunal. The functions of VCheKa were closely intertwined with the Commission of V. D. Bonch-Bruyevich, which beside the fight against wine pogroms was engaged in the investigation of most major political offenses (see: Bonch-Bruyevich Commission).\n\nAll results of its activities, VCheKa had either to transfer to the Investigatory Commission of Revtribunal, or to dismiss. The control of the commission's activity was provided by the People's Commissariat for Justice (Narkomjust, at that time headed by Isidor Steinberg) and Internal Affairs (NKVD, at that time headed by Grigory Petrovsky). Although the VCheKa was officially an independent organization from the NKVD, its chief members such as Dzerzhinsky, Latsis, Unszlicht, and Uritsky (all main chekists), since November 1917 composed the collegiate of NKVD headed by Petrovsky. In November 1918, Petrovsky was appointed as head of the All-Ukrainian Central Military Revolutionary Committee during VCheKa's expansion to provinces and front-lines. At the time of political competition between Bolsheviks and SRs (January 1918), Left SRs attempted to curb the rights of VCheKa and establish through the Narkomiust their control over its work. Having failed in attempts to subordinate the VCheKa to Narkomiust, the Left SRs tried to gain control of the Extraordinary Commission in a different way: they requested that the Central Committee of the party was granted the right to directly enter their representatives into the VCheKa. Sovnarkom recognized the desirability of including five representatives of the Left Socialist-Revolutionary faction of VTsIK. Left SRs were granted the post of a companion (deputy) chairman of VCheKa. However, Sovnarkom, in which the majority belonged to the representatives of RSDLP(b) retained the right to approve members of the collegium of the VCheKa.\n\nOriginally, members of the Cheka were exclusively Bolshevik; however, in January 1918, Left SRs also joined the organization The Left SRs were expelled or arrested later in 1918, following the attempted assassination of Lenin by an SR, [[Fanni Kaplan\n\nBy the end of January 1918, the Investigatory Commission of [[Petrograd Soviet]] (probably same as of Revtribunal) petitioned [[Sovnarkom]] to delineate the role of detection and judicial-investigatory organs. It offered to leave, for the VCheKa and the Commission of Bonch-Bruyevich, only the functions of detection and suppression, while investigative functions entirely transferred to it. The Investigatory Commission prevailed. On January 31, 1918, Sovnarkom ordered to relieve VCheKa of the investigative functions, leaving for the commission only the functions of detection, suppression, and prevention of anti revolutionary crimes. At the meeting of the Council of People's Commissars on January 31, 1918, a merger of VCheKa and the Commission of Bonch-Bruyevich was proposed. The existence of both commissions, VCheKa of Sovnarkom and the Commission of Bonch-Bruyevich of VTsIK, with almost the same functions and equal rights, became impractical. A decision followed two weeks later.\n\nOn February 23, 1918, VCheKa sent a radio telegram to all Soviets with a petition to immediately organize emergency commissions to combat counter-revolution, sabotage and speculation, if such commissions had not been yet organized. February 1918 saw the creation of local Extraordinary Commissions. One of the first founded was the [[Moscow]] Cheka. Sections and commissariats to combat counterrevolution were established in other cities. The Extraordinary Commissions arose, usually in the areas during the moments of the greatest aggravation of political situation. On February 25, 1918, as the counterrevolutionary organization \"Union of Front-liners\" was making advances, the executive committee of the [[Saratov]] Soviet formed a counter-revolutionary section. On March 7, 1918, because of the move from [[Petrograd]] to Moscow, the Petrograd Cheka was created. On March 9, a section for combating counterrevolution was created under the [[Omsk]] Soviet. Extraordinary commissions were also created in [[Penza]], [[Perm]], [[Novgorod]], [[Cherepovets]], [[Rostov]], [[Taganrog]]. On March 18, VCheKa adopted a resolution, \"The Work of VCheKa on the All-Russian Scale\", foreseeing the formation everywhere of Extraordinary Commissions after the same model, and sent a letter that called for the widespread establishment of the Cheka in combating counterrevolution, speculation, and sabotage. Establishment of provincial Extraordinary Commissions was largely completed by August 1918. In the Soviet Republic, there were 38 [[guberniyagubernatorial]] Chekas (Gubcheks) by this time.\n\nOn June 12, 1918, the All-Russian Conference of Cheka adopted the \"Basic Provisions on the Organization of Extraordinary Commissions\". They set out to form Extraordinary Commissions not only at [[Oblast]] and [[Guberniya]] levels, but also at the large [[Uyezd]] Soviets. In August 1918, in the Soviet Republic had accounted for some 75 Uyezd-level Extraordinary Commissions. By the end of the year, 365 Uyezd-level Chekas were established. In 1918, the All-Russia Extraordinary Commission and the Soviets managed to establish a local Cheka apparatus. It included Oblast, Guberniya, [[Raion]], [[Uyezd]], and [[Volost]] Chekas, with Raion and Volost Extraordinary Commissioners. In addition, border security Chekas were included in the system of local Cheka bodies.\n\nIn the autumn of 1918, as consolidation of the political situation of the republic continued, a move toward elimination of Uyezd-, Raion-, and Volost-level Chekas, as well as the institution of Extraordinary Commissions was considered. On January 20, 1919, VTsIK adopted a resolution prepared by VCheKa, \"On the abolition of Uyezd Extraordinary Commissions\". On January 16 the presidium of VCheKa approved the draft on the establishment of the Politburo at Uyezd [[militsiya]]. This decision was approved by the Conference of the Extraordinary Commission IV, held in early February 1920.\n\n[[File:1988 CPA 6011.jpgthumbuprightPortrait of [[Martin Latsis]] on a Soviet [[postage stamp]].]]\n\nOn August 3, a VCheKa section for combating counterrevolution, speculation and sabotage on railways was created. On August 7, 1918, [[Sovnarkom]] adopted a decree on the organization of the railway section at VCheKa. Combating counterrevolution, speculation, and [[malfeasance]] on railroads was passed under the jurisdiction of the railway section of VCheKa and local Cheka. In August 1918, railway sections were formed under the Gubcheks. Formally, they were part of the non-resident sections, but in fact constituted a separate division, largely autonomous in their activities. The gubernatorial and oblast-type Chekas retained in relationship to the transportation sections only control and investigative functions.\n\nThe beginning of a systematic work of organs of VCheKa in [[RKKA]] refers to July 1918, the period of extreme tension of the [[Russian Civil Warcivil war]] and class struggle in the country. On July 16, 1918, the Council of People's Commissars formed the Extraordinary Commission for combating counterrevolution at the Czechoslovak (Eastern) Front, led by [[Martin LatsisM. I. Latsis]]. In the fall of 1918, Extraordinary Commissions to combat counterrevolution on the Southern (Ukraine) Front were formed. In late November, the Second All-Russian Conference of the Extraordinary Commissions accepted a decision after a report from [[I. N. Polukarov]] to establish at all frontlines, and army sections of the Cheka and granted them the right to appoint their commissioners in military units. On December 9, 1918, the collegiate (or presidium) of VCheKa had decided to form a military section, headed by [[Mikhail Sergeevich KedrovM. S. Kedrov]], to combat counterrevolution in the Army. In early 1919, the military control and the military section of VCheKa were merged into one body, the [[Special Section of the Republic]], with Kedrov as head. On January 1, he issued an order to establish the Special Section. The order instructed agencies everywhere to unite the Military control and the military sections of Chekas and to form special sections of frontlines, armies, military districts, and [[guberniya]]s.\n\nIn November 1920 the [[Soviet of Labor and Defense]] created a Special Section of VCheKa for the security of the state border. On February 6, 1922, after the Ninth All-Russian Soviet Congress, the Cheka was dissolved by VTsIK, \"with expressions of gratitude for heroic work.\" It was replaced by the [[State Political Administration]] or OGPU, a section of the [[NKVD]] of the [[Russian Soviet Federative Socialist Republic]] (RSFSR). Dzerzhinsky remained as chief of the new organization.\nInitially formed to fight against counter-revolutionaries and saboteurs, as well as [[Speculationfinancial speculators]], the Cheka had its own classifications. Those counter-revolutionaries fell under these categories:\n\nBULLET::::1. any civil or military servicemen suspected of working for Imperial Russia;\nBULLET::::2. families of officers-volunteers (including children);\nBULLET::::3. all clergy;\nBULLET::::4. workers and peasants who were under suspicion of not supporting the Soviet government;\nBULLET::::5. any other person whose private property was valued at over 10,000 rubles.\n\nAs its name implied, the Extraordinary Commission had virtually unlimited powers and could interpret them in any way it wished. No standard procedures were ever set up, except that the Commission was supposed to send the arrested to the Military-Revolutionary tribunals if outside of a war zone. This left an opportunity for a wide range of interpretations, as the whole country was in total chaos. At the direction of Lenin, the Cheka performed mass arrests, imprisonments, and executions of \"[[enemies of the people]]\". In this, the Cheka said that they targeted \"class enemies\" such as the [[bourgeoisie]], and members of the [[clergy]]; the first organized mass repression began against the [[Libertarianismlibertarians]] and socialists of Petrograd in April 1918. Over the next few months, 800 were arrested and shot without trial.\n\nWithin a month, the Cheka had extended its repression to all political opponents of the communist government, including [[Anarchismanarchists]] and others on the left. On April 11/12, 1918, some 26 anarchist political centres in Moscow were attacked. Forty anarchists were killed by Cheka forces, and about 500 were arrested and jailed after a pitched battle took place between the two groups. In response to the anarchists' resistance, the Cheka orchestrated a massive retaliatory campaign of repression, executions, and arrests against all opponents of the Bolshevik government, in what came to be known as \"[[Red Terror]]\". The \"Red Terror\", implemented by Dzerzhinsky on September 5, 1918, was vividly described by the [[Red Army]] journal \"Krasnaya Gazeta\":\n\nWithout mercy, without sparing, we will kill our enemies in scores of hundreds. Let them be thousands, let them drown themselves in their own blood. For the blood of Lenin and [[Moisei UritskyUritsky]] … let there be floods of blood of the [[bourgeoisie]] – more blood, as much as possible...\"\n\nAn early Bolshevik, [[Victor Serge]] described in his book \"Memoirs of a Revolutionary\":\n\nThe Cheka was also used against the armed anarchist [[Revolutionary Insurrectionary Army of UkraineBlack Army]] of [[Nestor Makhno]] in the Ukraine. After the Black Army had served its purpose in aiding the [[Red Army]] to stop the [[White ArmyWhites]] under [[Anton DenikinDenikin]], the Soviet communist government decided to eliminate the anarchist forces. In May 1919, two Cheka agents sent to assassinate Makhno were caught and executed.\n\nMany victims of Cheka repression were \"bourgeois hostages\" rounded up and held in readiness for [[summary execution]] in reprisal for any alleged counter-revolutionary act. Wholesale, indiscriminate arrests became an integral part of the system. The Cheka used trucks disguised as delivery trucks, called \"Black Marias\", for the secret arrest and transport of prisoners.\n\nIt was during the [[Red Terror]] that the Cheka, hoping to avoid the bloody aftermath of having half-dead victims writhing on the floor, developed a technique for execution known later by the German words \"\"Nackenschuss\" or \"\"Genickschuss\", a shot to the [[nape]] of the neck, which caused minimal blood loss and instant death. The victim's head was bent forward, and the executioner fired slightly downward at point blank range. This had become the standard method used later by the [[NKVD]] to liquidate [[Joseph Stalin]]'s [[Great Purgepurge]] victims and others.\n\nIt is believed that there were more than three million [[deserter]]s from the Red Army in 1919 and 1920. Approximately 500,000 deserters were arrested in 1919 and close to 800,000 in 1920, by troops of the 'Special Punitive Department' of the Cheka, created to punish desertions. These troops were used to forcibly [[Repatriationrepatriate]] deserters, taking and shooting hostages to force compliance or to set an example. Throughout the course of the civil war, several thousand deserters were shot – a number comparable to that of belligerents during [[World War I]].\n\nIn September 1918, according to \"[[The Black Book of Communism]]\", in only twelve provinces of Russia, 48,735 deserters and 7,325 \"bandits\" were arrested, 1,826 were killed and 2,230 were executed. The exact identity of these individuals is confused by the fact that the Soviet Bolshevik government used the term 'bandit' to cover ordinary criminals as well as armed and unarmed political opponents, such as the anarchists.\n\nEstimates on Cheka executions vary widely. The lowest figures (\"disputed below\") are provided by Dzerzhinsky's lieutenant [[Martin LatsisMartyn Latsis]], limited to RSFSR over the period 1918–1920:\n\nBULLET::::- For the period 1918 – July 1919, covering only twenty provinces of central Russia:\n\nBULLET::::- For the whole period 1918–19:\n\nBULLET::::- For the whole period 1918–20:\n\nExperts generally agree these semi-official figures are vastly understated. Pioneering historian of the [[Red Terror]] [[Sergei Melgunov]] claims that this was done deliberately in an attempt to demonstrate the government's humanity. For example, he refutes the claim made by Latsis that only 22 executions were carried out in the first six months of the Cheka's existence by providing evidence that the true number was 884 executions. W. H. Chamberlin claims, \"It is simply impossible to believe that the Cheka only put to death 12,733 people in all of Russia up to the end of the civil war.\" [[Donald Rayfield]] concurs, noting that, \"Plausible evidence reveals that the actual numbers . . . vastly exceeded the official figures.\" Chamberlin provides the \"reasonable and probably moderate\" estimate of 50,000, while others provide estimates ranging up to 500,000. Several scholars put the number of executions at about 250,000. Some believe it is possible more people were murdered by the Cheka than died in battle. Historian James Ryan gives a modest estimate of 28,000 executions per year from December 1917 to February 1922.\n\nLenin himself seemed unfazed by the killings. On 12 January 1920, while addressing trade union leaders, he said: \"We did not hesitate to shoot thousands of people, and we shall not hesitate, and we shall save the . On 14 May 1921, the [[Politburo]], chaired by Lenin, passed a motion \"broadening the rights of the [Cheka] in relation to the use of the [death penalty].\"\n\nThe Cheka is reported to have practiced [[torture]]. Depending on Cheka committees in various cities, the methods included: being skinned alive, scalped, \"crowned\" with barbed wire, impaled, crucified, hanged, stoned to death, tied to planks and pushed slowly into furnaces or tanks of boiling water, or rolled around naked in internally nail-studded barrels. Chekists reportedly poured water on naked prisoners in the winter-bound streets until they became living ice statues. Others reportedly beheaded their victims by twisting their necks until their heads could be torn off. The [[Chinese in Russian RevolutionCheka detachments]] stationed in [[Kiev]] reportedly would attach an iron tube to the torso of a bound victim and insert a rat in the tube closed off with wire netting, while the tube was held over a flame until the rat began gnawing through the victim's guts in an effort to escape. [[Anton Denikin]]'s investigation discovered corpses whose lungs, throats, and mouths had been packed with earth.\n\nWomen and children were also victims of Cheka terror. Women would sometimes be tortured and raped before being shot. Children between the ages of 8 and 13 were imprisoned and occasionally executed.\n\nAll of these atrocities were published on numerous occasions in \"[[Pravda]]\" and \"[[Izvestiya]]\": January 26, 1919 \"Izvestiya\" #18 article \"Is it really a medieval imprisonment?\" («Неужели средневековый застенок?»); February 22, 1919 \"Pravda\" #12 publishes details of the [[Vladimir, RussiaVladimir]] Cheka's tortures, September 21, 1922 \"Socialist Herald\" publishes details of series of tortures conducted by the [[Stavropol]] Cheka (hot basement, cold basement, skull measuring etc.).\n\nThe Chekists were also supplemented by the militarized Units of Special Purpose (the Party's Spetsnaz or ).\n\nCheka was actively and openly utilizing kidnapping methods. With kidnapping methods Cheka was able to extinguish numerous cases of discontent especially among the rural population. Among the notorious ones was the [[Tambov rebellion]].\n\nVillages were bombarded to complete annihilation like in the case of Tretyaki, Novokhopersk uyezd, [[Voronezh Governorate]]. \n\nAs a result of this relentless violence more than a few Chekists ended up with psychopathic disorders, which [[Nikolai Bukharin]] said were \"an occupational hazard of the Chekist profession.\" Many hardened themselves to the executions by heavy drinking and drug use. Some developed a gangster-like slang for the verb to kill in an attempt to distance themselves from the killings, such as 'shooting partridges', of 'sealing' a victim, or giving him a \"natsokal\" (onomatopoeia of the trigger action).\n\nOn November 30, 1992, by the initiative of the [[President of the Russian Federation]] the Constitutional Court of the Russian Federation recognized the Red Terror as unlawful, which in turn led to suspension of the Communist Party of the RSFSR.\n\nCheka departments were organized not only in big cities and [[guberniya]] seats, but also in each [[uyezd]], at any front-lines and military formations. Nothing is known on what resources they were created. Many who were hired to head those departments were so-called \"nestlings of [[Alexander Keren]]\".\nBULLET::::- Moscow Cheka (1918–1919)\nBULLET::::- Chairman – [[Felix Dzerzhynsky]], Deputy – [[Yakov Peters]] (initially heading the Petrograd Department), other members – Shklovsky, Kneyfis, Tseystin, Razmirovich, Kronberg, Khaikina, Karlson, Shauman, Lentovich, Rivkin, Antonov, Delafabr, Tsytkin, G.Sverdlov, Bizensky, [[Yakov Blumkin]], Aleksandrovich, Fines, Zaks, [[Yakov Goldin]], Galpershtein, Kniggisen, [[Martin Latsis]] (later transfer (chief of jail), Fogel, Zakis, Shillenkus, Yanson).\nBULLET::::- Petrograd Cheka (1918–1919)\nBULLET::::- Chairman – Meinkman, [[Moisei Uritsky]] (reiller, Kozlovsky, Model, Rozmirovich, I.Diesporov, Iselevich, Krassikov, Bukhan, Merbis, Paykis, Anvelt.\nBULLET::::- Kharkov Cheka\nBULLET::::- Deych, Vikhman, Timofey, Vera (Dora) Grebenshchikova, Aleksandra (ag\nBULLET::::- Ashykin.\n\nBULLET::::- The Cheka were popular staples in Soviet film and literature. This was partly due to a romanticization of the organisation in the post-Stalin period, and also because they provided a useful action/detection template. Films featuring the Cheka include [[Ostern]]'s \"[[Miles of Fire]]\", [[Nikita Mikhalkov]]'s \"[[At Home among Strangers]]\", the miniseries \"[[The Adjutant of His Excellency]]\", and also \"[[Dead Season]]\" (starring [[Donatas Banionis]]), and the [[1992 in film1992]] [[cinema of RussiaRussian]] [[Drama (film and television)drama film]] \"[[The Chekist]]\".\nBULLET::::- In [[Spain]], during the [[Spanish Civil War]], the detention and torture centers operated by the Communists were named \"\"checas\"\" after the Soviet organization. [[Alfonso Laurencic]] was their promoter, ideologist and builder.\nBULLET::::- Dzerzhinsky, who rarely drank, is said to have told Lenin – on an occasion in which he did so excessively – that secret police work could be done by \"only saints or scoundrels ... but now the saints are running away from me and I am left with the scoundrels\".\n\n[[Konstantin Preobrazhenskiy]] criticised the continuing celebration of the professional holiday of the [[NKVDold]] and the modern Russian security services on the anniversary of the creation of the Cheka, with the assent of the Presidents of [[Russia]]. (Vladimir Putin, former KGB officer, chose not to change the date to another): \"The [[Federal Security Servicesuccessors]] of the [[KGB]] still haven't renounced anything; they even celebrate their professional [[holiday]] the same day, as during [[Political repression in the Soviet Unionrepression]], on the 20th of December. It is as if the present intelligence and counterespionage services of [[Germany]] celebrated [[Gestapo]] Day. I can imagine how indignant our press would be!\"\n\nBULLET::::- [[Chekism]]\nBULLET::::- [[Commanders of the border troops USSR and RF]]\nBULLET::::- [[Central Case Examination Group]]\nBULLET::::- [[Chronology of Soviet secret police agencies]]\nBULLET::::- [[Great Purge]]\nBULLET::::- [[Ministry for State Security (Soviet Union)]]\nBULLET::::- [[Okhrana]]\nBULLET::::- [[People's Commissariat for State Security (Soviet Union)]]\nBULLET::::- [[Russian Revolution of 1917]]\nBULLET::::- [[Christopher Andrew (historian)Andrew, Christopher M.]] and [[Vasili Mitrokhin]] (1999) \"The Sword and the Shield : The [[Mitrokhin Archive]] and the Secret History of the KGB.\" New York: [[Basic Books]]. .\nBULLET::::- [[E. H. CarrCarr, E. H.]] (1958) \"The Origin and Status of the Cheka.\" [[Soviet Studies]], vol. 10, no. 1, pp. 1–11.\nBULLET::::- Chamberlin, W. H. (1935) \"The Russian Revolution 1917–1921, \" 2 vols. London and New York. The Macmillan Company.\nBULLET::::- Dziak, John. (1988) \"Chekisty: A History of the KGB.\" Lexington, Mass. Lexington Books.\nBULLET::::- [[Orlando FigesFiges, Orlando]] (1997) \"A People's Tragedy: The Russian Revolution 1891–1924.\" [[Penguin Books]]. .\nBULLET::::- Leggett, George (1986) \"The Cheka: Lenin's Political Police.\" [[Oxford University Press]], New York.\nBULLET::::- Lincoln, Bruce W. (1999) \"Red Victory: A History of the Russian Civil War.\" Da Capo Press.\nBULLET::::- [[Sergei MelgunovMelgounov, Sergey Petrovich]] (1925) \"The Red Terror in Russia.\" London & Toronto: J. M. Dent & Sons Ltd.\nBULLET::::- [[Richard OveryOvery, Richard]] (2004) \"The Dictators: Hitler's Germany, Stalin's Russia.\" W. W. Norton & Company; 1st American edition.\nBULLET::::- [[R. J. RummelRummel, Rudolph Joseph]] (1990) \"Lethal Politics: Soviet Genocide and Mass Murder Since 1917.\" Transaction Publishers.\nBULLET::::- [[Leonard SchapiroSchapiro, Leonard B.]] (1984) \"The Russian Revolutions of 1917 : The Origins of Modern Communism.\" New York: Basic Books.\nBULLET::::- [[Dmitri VolkogonovVolkogonov, Dmitri]] (1994) \"Lenin: A New Biography.\" [[Free Press (publisher)Free Press]].\nBULLET::::- Volkogonov, Dmitri (1998) \"Autopsy of an Empire: The Seven Leaders Who Built the Soviet Regime\" [[Free Press (publisher)Free Press]].\n\nBULLET::::- The Cheka – Spartacus Schoolnet collection of primary source extracts relating to the Cheka\nBULLET::::- Development of the Soviet system of punitive organs\n\n[[Category:Cheka ]]\n[[Category:Defunct law enforcement agencies of Russia]]\n[[Category:Russian intelligence agencies]]\n[[Category:Defunct intelligence agencies]]\n[[Category:Organizations of the Russian Revolution]]\n[[Category:Law enforcement in communist states]]\n[[Category:Paramilitary organizations based in Russia]]\n[[Category:Political repression in Russia]]\n[[Category:Secret police]]\n[[Category:Soviet intelligence agencies]]\n[[Category:State-sponsored terrorism]]\n[[Category:Bolshevik finance]]\n[[Category:Dirty wars]]\n[[Category:Extrajudicial killings]]\n[[Category:Forced disappearance]]\n[[Category:Terrorism tactics]]\n[[Category:War crimes]]\n[[Category:1917 establishments in Russia]]\n[[Category:1922 disestablishments in Russia]]\n[[Category:Government agencies established in 1917]]\n[[Category:Government agencies disestablished in 1922]]"}
{"id": "6753", "url": "https://en.wikipedia.org/wiki?curid=6753", "title": "Clitic", "text": "Clitic\n\nA clitic (, backformed from Greek \"leaning\" or \"enclitic\") is a morpheme in morphology and syntax that has syntactic characteristics of a word, but depends phonologically on another word or phrase. In this sense, it is syntactically independent but phonologically dependent—always attached to a host. The term derives from the Greek for \"leaning\". A clitic is pronounced like an affix, but plays a syntactic role at the phrase level. In other words, clitics have the \"form\" of affixes, but the distribution of function words. For example, the contracted forms of the auxiliary verbs in \"I'm\" and \"we've\" are clitics.\n\nClitics can belong to any grammatical category, although they are commonly pronouns, determiners, or adpositions. Note that orthography is not always a good guide for distinguishing clitics from affixes: clitics may be written as separate words, but sometimes they are joined to the word they depend on (like the Latin clitic \"-que\", meaning \"and\"), or separated by special characters such as hyphens or apostrophes (like the English clitic \"’s\" in \"it's\" for \"it has\" or \"it is\").\n\nClitics fall into various categories depending on their position in relation to the word they connect to.\n\nA proclitic appears before its host. It is common in Romance languages. For example, in French, there is \"il s'est réveillé\" (\"he woke up\"), or \"je t'aime\" (\"I love you\").\n\nAn enclitic appears after its host.\nBULLET::::- Latin: Senatus \"Populus\"que Romanus\nBULLET::::- Ancient Greek: \"ánthrōpoí\" (te) \"theoí\" te\nBULLET::::- Sanskrit: \"naro gajaś\" \"'ca\" 'नरो गजश्च' i.e. \"naraḥ gajaḥ ca\" \"नरः गजः च\" with sandhi,:::\"\"the man the elephant and\"\" = \"the man and the elephant\"\nBULLET::::- Sanskrit: Namaste < \"namaḥ\" + \"te\", (Devanagari: नमः + ते = नमस्ते), with sandhi change \"namaḥ\" > \"namas\".\nBULLET::::- Czech: \"Nevím, chtělo-li by se mi si to tam však také vyzkoušet\".\nBULLET::::- Tamil: \"idhu en poo\" = இது என் பூ (This is my flower). With enclitic vē, which indicates certainty, this sentence becomes\nBULLET::::- Telugu: \"idi nā puvvu\" = ఇది నా పువ్వు (This is my flower). With enclitic ē, which indicates certainty, this sentence becomes\n\nA mesoclitic appears between the stem of the host and other affixes. For example, in Portuguese, \"conquistar-se-á\" (\"it will be conquered\"), \"dá-lo-ei\" (\"I will give it\"), \"matá-la-ia\" (\"he/she/it would kill her\"). These are found much more often in writing than in speech. It is even possible to use two pronouns inside the verb, as in \"dar-no-lo-á\" (\"he/she/it will give it to us\"), or \"dar-ta-ei\" (\"ta\" = \"te\" + \"a\", \"I will give it/her to you\"). As in other Romance languages, the Portuguese synthetic future tense comes from the merging of the infinitive and the corresponding finite forms of the verb \"haver\" (from Latin \"habēre\"), which explains the possibility of separating it from the infinitive.\n\nThe endoclitic splits apart the root and is inserted between the two pieces. Endoclitics defy the Lexical Integrity Hypothesis (or Lexicalist hypothesis) and so were long thought impossible. However, evidence from the Udi language suggests that they exist. Endoclitics are also found in Pashto and are reported to exist in Degema.\n\nOne important distinction divides the broad term 'clitics' into two categories, simple clitics and special clitics. This distinction is, however, disputed.\n\nSimple clitics are free morphemes, meaning they can stand alone in a phrase or sentence. They are unaccented and thus phonologically dependent upon a nearby word. They only derive meaning from this \"host\".\n\nSpecial clitics are morphemes that are bound to the word they are dependent upon, meaning they exist as a part of their host. This form, which is unaccented, represents a variant of a free form that does carry stress. While the two variants carry similar meaning and phonological makeup, the special clitic is bound to a host word and unaccented.\n\nSome clitics can be understood as elements undergoing a historical process of grammaticalization:\n\nlexical item → clitic → affix\n\nAccording to this model from Judith Klavans, an autonomous lexical item in a particular context loses the properties of a fully independent word over time and acquires the properties of a morphological affix (prefix, suffix, infix, etc.). At any intermediate stage of this evolutionary process, the element in question can be described as a \"clitic\". As a result, this term ends up being applied to a highly heterogeneous class of elements, presenting different combinations of word-like and affix-like properties.\n\nOne characteristic shared by many clitics is a lack of prosodic independence. A clitic attaches to an adjacent word, known as its \"host\". Orthographic conventions treat clitics in different ways: Some are written as separate words, some are written as one word with their hosts, and some are attached to their hosts, but set off by punctuation (a hyphen or an apostrophe, for example).\n\nAlthough the term \"clitic\" can be used descriptively to refer to any element whose grammatical status is somewhere in between a typical word and a typical affix, linguists have proposed various definitions of \"clitic\" as a technical term. One common approach is to treat clitics as words that are prosodically deficient: they cannot appear without a host, and they can only form an accentual unit in combination with their host. The term \"postlexical clitic\" is used for this narrower sense of the term.\n\nGiven this basic definition, further criteria are needed to establish a dividing line between postlexical clitics and morphological affixes, since both are characterized by a lack of prosodic autonomy. There is no natural, clear-cut boundary between the two categories (since from a historical point of view, a given form can move gradually from one to the other by morphologization). However, by identifying clusters of observable properties that are associated with core examples of clitics on the one hand, and core examples of affixes on the other, one can pick out a battery of tests that provide an empirical foundation for a clitic/affix distinction.\n\nAn affix syntactically and phonologically attaches to a base morpheme of a limited part of speech, such as a verb, to form a new word. A clitic syntactically functions above the word level, on the phrase or clause level, and attaches only phonetically to the first, last, or only word in the phrase or clause, whichever part of speech the word belongs to.\nThe results of applying these criteria sometimes reveal that elements that have traditionally been called \"clitics\" actually have the status of affixes (e.g., the Romance pronominal clitics discussed below).\n\nZwicky and Pullum postulated five characteristics that distinguish clitics from affixes:\nBULLET::::1. Clitics do not select their hosts. That is, they are \"promiscuous\", attaching to whichever word happens to be in the right place. Affixes do select their host: They only attach to the word they are connected to semantically, and generally attach to a particular part of speech.\nBULLET::::2. Clitics do not exhibit arbitrary gaps. Affixes, on the other hand, are often lexicalized and may simply not occur with certain words. (English plural -s, for example, does not occur with \"child\".)\nBULLET::::3. Clitics do not exhibit morphophonological idiosyncrasies. That is, they follow the morphophonological rules of the rest of the language. Affixes may be irregular in this regard.\nBULLET::::4. Clitics do not exhibit semantic idiosyncrasies. That is, the meaning of the phrase-plus-clitic is predictable from the meanings of the phrase and the clitic. Affixes may have irregular meanings.\nBULLET::::5. Clitics can attach to material already containing clitics (and affixes). Affixes can attach to other affixes, but not to material containing clitics.\nAn example of differing analyses by different linguists is the discussion of the possessive ('s) in English, some linguists treating it as an affix, while others treat it as a special clitic. \n\nSimilar to the discussion above, clitics must be distinguishable from words. Linguists have proposed a number of tests to differentiate between the two categories. Some tests, specifically, are based upon the understanding that when comparing the two, clitics resemble affixes, while words resemble syntactic phrases. Clitics and words resemble different categories, in the sense that they share certain properties. Six such tests are described below. These, of course, are not the only ways to differentiate between words and clitics.\n\nBULLET::::1. If a morpheme is bound to a word and can never occur in complete isolation, then it is likely a clitic. In contrast, a word is not bound and can appear on its own.\nBULLET::::2. If the addition of a morpheme to a word prevents further affixation, then it is likely a clitic.\nBULLET::::3. If a morpheme combines with single words to convey a further degree of meaning, then it is likely a clitic. A word combines with a group of words or phrases to denote further meaning.\nBULLET::::4. If a morpheme must be in a certain order with respect to other morphemes within the construction, then it is likely a clitic. Independent words enjoy free ordering with respect to other words, within the confines of the word order of the language.\nBULLET::::5. If a morpheme’s allowable behavior is determined by one principle, it is likely a clitic. For example, \"a\" precedes indefinite nouns in English. Words can rarely be described with one such description.\nBULLET::::6. In general, words are more morphologically complex than clitics. Clitics are rarely composed of more than one morpheme.\n\nClitics do not always appear next to the word or phrase that they are associated with grammatically. They may be subject to global word order constraints that act on the entire sentence. Many Indo-European languages, for example, obey Wackernagel's law (named after Jacob Wackernagel), which requires sentential clitics to appear in \"second position\", after the first syntactic phrase or the first stressed word in a clause:\nBULLET::::- Latin had three enclitics that appeared in second or third position of a clause: \"enim\" 'indeed, for', \"autem\" 'but, moreover', \"vero\" 'however'. For example, \"quis enim potest negare?\" (from Martial's epigram LXIV, literally \"who indeed can deny [her riches]?\"). Spevak (2010) reports that in her corpus of Caesar, Cicero and Sallust, these three words appear in such position in 100% of the cases.\n\nEnglish enclitics include the contracted versions of auxiliary verbs, as in \"I'm\" and \"we've\". Some also regard the possessive marker, as in \"The Queen of England's crown\" as an enclitic, rather than a (phrasal) genitival inflection.\n\nSome consider the English articles \"a, an, the\" and the infinitive marker \"to\" proclitics.\n\nThe negative marker \"n’t\" as in \"couldn’t\" etc. is typically considered a clitic that developed from the lexical item \"not\". Linguists Arnold Zwicky and Geoffrey Pullum argue, however, that the form has the properties of an affix rather than a syntactically independent clitic.\n\nBULLET::::- Old Norse: The definite article was the enclitic \"-inn\", \"-in\", \"-itt\" (masculine, feminine and neuter nominative singular), as in \"álfrinn\" \"the elf\", \"gjǫfin\" \"the gift\", and \"tréit\" \"the tree\", an abbreviated form of the independent pronoun \"hinn\", cognate of the German pronoun \"jener\". It was fully declined for gender, case and number. Since both the noun and enclitic were declined, this led to \"double declension\". The situation remains similar in modern Faroese and Icelandic, but in Danish, Norwegian and Swedish, the enclitics have become endings. Old Norse had also some enclitics of personal pronouns that were attached to verbs. These were \"-sk\" (from \"sik\"), \"-mk\" (from \"mik\"), \"-k\" (from \"ek\"), and \"-ðu\" / \"-du\" / \"-tu\" (from \"þú\"). These could even be stacked up, e.g. \"fásktu\" (from Hávamál, stanza 116).\nBULLET::::- Dutch: \"'t\" definite article of neuter nouns and third person singular neuter pronoun, \"'k\" first person pronoun, \"je\" second person singular pronoun, \"ie\" third person masculine singular pronoun, \"ze\" third person plural pronoun\nBULLET::::- Plautdietsch: \"\"Deit'a't vondoag?\"\": \"Will he do it today?\"\nBULLET::::- Gothic: Sentence clitics appear in second position in accordance with Wackernagel's Law, including \"-u\" (yes-no question), \"-uh\" \"and\", \"þan\" \"then\", \"ƕa\" \"anything\", for example \"ab-u þus silbin\" \"of thyself?\". Multiple clitics can be stacked up, and split a preverb from the rest of the verb if the preverb comes at the beginning of the clause, e.g. \"diz-uh-þan-sat ijōs\" \"and then he seized them (fem.)\", \"ga-u-ƕa-sēƕi\" \"whether he saw anything\".\n\nIn Romance languages, some feel the object personal pronoun forms are clitics. Others consider them affixes, as they only attach to the verb they are the object of. There is no general agreement on the issue. For the Spanish object pronouns, for example:\n\nBULLET::::- \"lo atamos\" (\"it tied-\" = \"we tied it\" or \"we tied him\"; can only occur with the verb it is the object of)\nBULLET::::- \"dámelo\" (\"give me it\")\n\nColloquial European Portuguese allows object suffixes before the conditional and future suffixes of the verbs:\n\nBULLET::::- \"Ela levá-lo-ia\" (\"\"She take-it-would\"\" – \"She would take it\").\nBULLET::::- \"Eles dar-no-lo-ão\" (\"\"They give-us-it-will\"\" – \"They will give it to us\").\n\nColloquial Portuguese of Brazil and Portugal and Spanish of the former Gran Colombia allow ser to be conjugated as a verbal clitic adverbial adjunct to emphasize the importance of the phrase compared to its context, or with the meaning of \"really\" or \"in truth\":\n\nBULLET::::- \"Ele estava era gordo\" (\"\"He was was fat\"\" – \"He was very fat\").\nBULLET::::- \"Ele ligou é para Paula\" (\"\"He phoned is Paula\"\" – \"He phoned Paula (\"with emphasis\")\").\n\nNote that this clitic form is only for the verb ser and is restricted to only third-person singular conjugations. It is not used as a verb in the grammar of the sentence but introduces prepositional phrases and adds emphasis. It does not need to concord with the tense of the main verb, as in the second example, and can be usually removed from the sentence without affecting the simple meaning.\n\nIn the Indo-European languages, some clitics can be traced back to Proto-Indo-European: for example, *\"\" is the original form of Sanskrit \"च\" (\"-ca\"), Greek \"τε\" (\"-te\"), and Latin \"-que\".\nBULLET::::- Latin: \"-que\" \"and\", \"-ve\" \"or\", \"-ne\" (yes-no question)\nBULLET::::- Greek: \"τε\" \"and\", \"δέ\" \"but\", \"γάρ\" \"for\" (in a logical argument), \"οὖν\" \"therefore\"\n\nBULLET::::- Russian: ли (yes-no question), же (emphasis), то (emphasis), не \"not\" (proclitic), бы (subjunctive)\nBULLET::::- Czech: special clitics: weak personal and reflexive pronouns (\"mu\", \"him\"), certain auxiliary verbs (\"by\", \"would\"), and various short particles and adverbs (\"tu\", \"here\"; \"ale\", \"though\"). \"\"Nepodařilo by se mi mu to dát\"\" \"I would not succeed in giving it to him\". In addition there are various simple clitics including short prepositions.\nBULLET::::- Polish: \"-by\" (conditional mood particle), \"się\" (reflexive, also modifies meaning of certain verbs), \"no\" and \"-że\" (emphasis), \"-m, -ś, -śmy, -ście\" (personal auxiliary), \"mi, ci, cię, go, mu\" &c. (unstressed personal pronouns in oblique cases)\nBULLET::::- Croatian: the reflexive pronoun forms \"si\" and \"se\", \"li\" (yes-no question), unstressed present and aorist tense forms of \"biti\" (\"to be\"; \"sam, si, je, smo, ste, su\"; and \"bih, bi, bi, bismo, biste, bi\", for the respective tense), unstressed personal pronouns in genitive (\"me, te, ga, je, nas, vas, ih\"), dative (\"mi, ti, mu, joj, nam, vam, im\") and accusative (\"me, te, ga (nj), je (ju), nas, vas, ih\"), and unstressed present tense of \"htjeti\" (\"want/will\"; \"ću, ćeš, će, ćemo, ćete, će\")\n\nIn Croatian these clitics follow the first stressed word in the sentence or clause in most cases, which may have been inherited from Proto-Indo-European (see Wackernagel's Law), even though many of the modern clitics became cliticised much more recently in the language (e.g. auxiliary verbs or the accusative forms of pronouns). In subordinate clauses and questions, they follow the connector and/or the question word respectively. \n\nExamples (clitics – \"sam\" \"I am\", \"biste\" \"you would (pl.)\", \"mi\" \"to me\", \"vam\" \"to you (pl.)\", \"ih\" \"them\"): \n\nBULLET::::- \"Pokažite mi ih.\" \"Show (pl.) them to me.\"\nBULLET::::- \"Pokazao sam vam ih jučer.\" \"I showed them to you (pl.) yesterday.\"\nBULLET::::- \"Sve sam vam ih (jučer) pokazao. / Sve sam vam ih pokazao (jučer).\" \"I showed all of them to you (yesterday).\" (focus on \"all\")\nBULLET::::- \"Jučer sam vam ih (sve) pokazao.\" \"I showed (all of) them to you yesterday.\" (focus on \"yesterday\")\nBULLET::::- \"Znam da sam vam ih već pokazao.\" \"I know that I have already shown them to you.\"\nBULLET::::- \"Zašto sam vam ih jučer pokazao?\" \"Why did I show them to you yesterday?\"\nBULLET::::- \"Zar sam vam ih jučer pokazao?\" \"Did I (really) show them to you yesterday?\"\nBULLET::::- \"Kad biste mi ih sada dali...\" \"If you (pl.) gave them to me now...\" (lit. If you-would to-me them now give-PARTICIPLE...)\nBULLET::::- \"Što sam god vidio...\" \"Whatever I saw...\" (lit. What I-am ever see-PARTICIPLE...)\n\nIn certain rural dialects this rule is (or was until recently) very strict, whereas elsewhere various exceptions occur. These include phrases containing conjunctions (e. g. \"Ivan i Ana\" \"Ivan and Ana\"), nouns with a genitival attribute (e. g. \"vrh brda\" \"the top of the hill\"), proper names and titles and the like (e. g. \"(gospođa) Ivana Marić\" \"(Mrs) Ivana Marić\", \"grad Zagreb\" \"the city (of) Zagreb\"), and in many local varieties clitics are hardly ever inserted into any phrases (e. g. \"moj najbolji prijatelj\" \"my best friend\", \"sutra ujutro\" \"tomorrow morning\"). In cases like these, clitics normally follow the initial phrase, although some Standard grammar handbooks recommend that they should be placed immediately after the verb (many native speakers find this unnatural). \n\nExamples: \nBULLET::::- \"Ja smo i on otišli u grad.\" \"He and I went to town.\" (lit. I are and him gone to town) – this is dialectal.\nBULLET::::- \"Ja i on smo otišli u grad.\" – commonly heard\nBULLET::::- \"Ja i on otišli smo u grad.\" – prescribed by some Standard grammars\n\nBULLET::::- \"Moja mu je starija sestra to rekla.\" \"My elder sister told him that.\" (lit. my to-him is elder sister that say-PARTICIPLE) – standard and usual in many dialects\nBULLET::::- \"Moja starija sestra mu je to rekla.\" – common in many dialects\n\nClitics are however never inserted after the negative particle \"ne\", which always precedes the verb in Croatian, or after prefixes (earlier preverbs), and the interrogative particle \"li\" always immediately follows the verb. Colloquial interrogative particles such as \"da li\", \"dal\", \"jel\" appear in sentence-initial position and are followed by clitics (if there are any). \n\nExamples: \nBULLET::::- \"Ne vidim te.\" \"I don't (or can't) see you.\"\nBULLET::::- \"Dovedite ih.\" \"Bring them (over here)!\" (a prefixed verb: \"do+vedite\")\nBULLET::::- \"Vidiš li me?\" \"Do/can you see me?\"\nBULLET::::- \"Vidiš li sestru?\" \"Do you see the sister?\" (It is impossible to say, e. g. **Sestru li vidiš?, although \"Sestru vidiš.\" \"It's the sister that you see.\" is natural)\nBULLET::::- \"Jel (me) vidiš?\" \"Do/Can you see (me)?\" (colloquial)\n\nBULLET::::- Arabic: Suffixes standing for direct object pronouns and/or indirect object pronouns (as found in Indo-European languages) are suffixed to verbs, possessive determiners are suffixed to nouns, and pronouns are suffixed to particles.\nBULLET::::- Finnish: Finnish has seven clitics, which change according to the vowel harmony: \"-kO\" (\"-ko ~ -kö\"), \"-kA\" (\"-ka ~ -kä\"), \"-kin\", \"-kAAn\" (\"-kaan ~ -kään\"), \"-pA\" (\"-pa\" ~ \"-pä\"), \"-hAn\" (\"-han ~ -hän\") and \"-s\". One word can have multiple clitics attached to it: \"onkohan?\" \"I wonder if it is?\"\nBULLET::::- \"-kO\" attached to a verb makes it a question: \"Katsot televisiota\" \"You are watching television\" → \"Katsotko televisiota?\" \"Are you watching television?\"\nBULLET::::- \"-kA\" gives the host word a colloquial tone: \"miten\" ~ \"mitenkä\" (\"how\"). When attached to a negative verb it corresponds with \"and\": \"En pidä mansikoista enkä mustikoista\" \"I don't like strawberries nor blueberries\". It can also make a negative verb stronger: \"Enkä tule!\" \"I definitely won't come!\"\nBULLET::::- \"-kin\" is a focus particle, often used instead of \"myös\" (\"also\" / \"as well\"): \"Minäkin olin siellä\" \"I was there too\". Depending on the context when attached to a verb it can also express that something happened according to the plan or as a surprise and not according to the plan. It can also make exclamations stronger. It can be attached to several words in the same sentence, changing the focus of the host word, but can only appear once per sentence: \"Minäkin olin siellä\" (\"I was there too\"), \"Minä olinkin siellä\" (\"Surprisingly I was there\"), \"Minä olin sielläkin\" (\"I was there too\")\nBULLET::::- \"-kAAn\" is also a focus particle and it corresponds with \"-kin\" in negative sentences: \"Minäkään en ollut siellä\" \"I wasn't there either\". Like \"-kin\" it can be attached to several host words in the same sentence. The only word it cannot be attached to is a negative verb. In questions it acts as a confirmation, like the word \"again\" in English: \"Missä sanoitkaan asuvasi?\" \"Where did you say you lived again?\"\nBULLET::::- \"-pA\" is a tone particle which can either add an arguing or patronising tone, or strengthen the host word: \"Minäpä tiedän paremmin!\" \"Well I know better!\", \"Onpa kaunis kissa!\" \"Wow what a beautiful cat!\"\nBULLET::::- \"-hAn\" is also a tone particle. In interrogative sentences it can make the question more polite and not as pressing: \"Onkohan isäsi kotona?\" \"(I wonder) Is your dad home?\" In command phrases it makes the command softer: \"Tulehan tänne\" \"Come here you\". It can also make a sentence more explanatory, make a claim more self-evident, express that something happened according to one's expectations, or that something came as a surprise etc. \"Pekka tuntee minut, onhan hän minun opettajani\" \"Pekka knows me, he is my teacher after all\", \"Kaikkihan niin tekevät\" \"Everyone do so after all\", \"Maijahan se siinä!\" \"Oh but it is Maija!\" \"Luulin, ettette osaisi, mutta tehän puhutte suomea hyvin\" \"I thought you wouldn't be able to, but you speak Finnish well\"\nBULLET::::- \"-s\" is a tone particle as well. It can also be used as a mitigating or softening phrase like \"-hAn\": \"Annikos se on?\" \"Oh but isn't it Anni?\", \"Tules tänne\" \"Come here you\", \"Miksikäs ei?\" \"Well why not?\", \"Paljonkos kello on?\" \"I wonder what time it is?\"\nBULLET::::- Ganda: \"-nga\" attached to a verb to form the progressive; \"-wo\" 'in' (also attached to a verb)\nBULLET::::- Georgian: \"-o\" (2nd and 3rd person speakers) and \"-metki\" (1st person speakers) is added to the end of a sentence to show reported speech. Examples: \"K'atsma miutxra, xval gnaxe-o\" = The man told me that he would see you tomorrow (Literally, \"The man told me, tomorrow I see you [reported]\") vs. \"K'atss vutxari, xval gnaxe-metki =\" I told the man that I would see you tomorrow (Literally, \"To man I told, tomorrow I see you [first person reported]).\nBULLET::::- Hungarian: the marker of indirect questions is \"-e:\" \"Nem tudja még, jön-e.\" \"He doesn't know yet if he'll come.\" This clitic can also mark direct questions with a falling intonation. \"Is\" (\"as well\") and \"se\" (\"not... either\") also function as clitics: although written separately, they are pronounced together with the preceding word, without stress: \"Ő is jön.\" \"He'll come too.\" \"Ő sem jön.\" \"He won't come, either.\"\nBULLET::::- Japanese: all particles, such as the genitive postposition の (\"no\") and the topic marker は (\"wa\").\nBULLET::::- Korean: The copula 이다 (\"ida\") and the adjectival 하다 (\"hada\"), as well as some nominal and verbal particles (e.g. 는, \"neun\"). However, alternative analysis suggests that the nominal particles do not function as clitics, but as phrasal affixes.\nBULLET::::- Somali: pronominal clitics, either subject or object clitics, are required in Somali. These exist as simple clitics postponed to the noun they apply to. Lexical arguments can be omitted from sentences, but pronominal clitics cannot be.\n\nBULLET::::- Clitic climbing\nBULLET::::- Clitic doubling\nBULLET::::- Functional item\nBULLET::::- Genitive case\nBULLET::::- Grammatical particle\nBULLET::::- Possessive case\nBULLET::::- Separable affix\nBULLET::::- Tmesis\nBULLET::::- V2 word order\nBULLET::::- Weak and strong forms in English\nBULLET::::- Weak pronoun\n"}
{"id": "6759", "url": "https://en.wikipedia.org/wiki?curid=6759", "title": "Context-free grammar", "text": "Context-free grammar\n\nIn formal language theory, a context-free grammar (CFG) is a certain type of formal grammar: a set of production rules that describe all possible strings in a given formal language. Production rules are simple replacements. For example, the rule\n\nformula_1\n\nreplaces formula_2 with formula_3. There can be multiple replacement rules for any given value. For example,\n\nformula_1\n\nformula_5\n\nmeans that formula_2 can be replaced with either formula_3 or formula_8.\n\nIn context-free grammars, all rules are one-to-one, one-to-many, or one-to-none. These rules can be applied regardless of context. The left-hand side of the production rule is always a nonterminal symbol. This means that the symbol does not appear in the resulting formal language. So in our case, our language contains the letters formula_3 and formula_8 but not formula_11\n\nRules can also be applied in reverse to check whether a string is grammatically correct according to the grammar.\n\nHere is an example context-free grammar that describes all two-letter strings containing the letters formula_3 or formula_8.\n\nformula_14\n\nformula_1\n\nformula_5\n\nIf we start with the nonterminal symbol formula_17 then we can use the rule\nformula_14 to turn formula_17 into formula_20. We can then apply one of the two later rules. For example, if we apply formula_5 to the first formula_2 we get formula_23. If we then apply formula_1 to the second formula_2 we get formula_26. Since both formula_3 and formula_8 are terminal symbols, and in context-free grammars terminal symbols never appear on the left hand side of a production rule, there are no more rules that can be applied. This same process can be used, applying the last two rules in different orders in order to get all possible strings within our simple context-free grammar.\n\nLanguages generated by context-free grammars are known as context-free languages (CFL). Different context-free grammars can generate the same context-free language. It is important to distinguish the properties of the language (intrinsic properties) from the properties of a particular grammar (extrinsic properties). The language equality question (do two given context-free grammars generate the same language?) is undecidable.\n\nContext-free grammars arise in linguistics where they are used to describe the structure of sentences and words in a natural language, and they were in fact invented by the linguist Noam Chomsky for this purpose. By contrast, in computer science, as the use of recursively-defined concepts increased, they were used more and more. In an early application, grammars are used to describe the structure of programming languages. In a newer application, they are used in an essential part of the Extensible Markup Language (XML) called the \"Document Type Definition\".\n\nIn linguistics, some authors use the term phrase structure grammar to refer to context-free grammars, whereby phrase-structure grammars are distinct from dependency grammars. In computer science, a popular notation for context-free grammars is Backus–Naur form, or \"BNF\".\n\nSince the time of Pāṇini, at least, linguists have described the grammars of languages in terms of their block structure, and described how sentences are recursively built up from smaller phrases, and eventually individual words or word elements. An essential property of these block structures is that logical units never overlap. For example, the sentence:\ncan be logically parenthesized (with the logical metasymbols [ ]) as follows:\nA context-free grammar provides a simple and mathematically precise mechanism for describing the methods by which phrases in some natural language are built from smaller blocks, capturing the \"block structure\" of sentences in a natural way. Its simplicity makes the formalism amenable to rigorous mathematical study. Important features of natural language syntax such as agreement and reference are not part of the context-free grammar, but the basic recursive structure of sentences, the way in which clauses nest inside other clauses, and the way in which lists of adjectives and adverbs are swallowed by nouns and verbs, is described exactly.\n\nContext-free grammars are a special form of Semi-Thue systems that in their general form date back to the work of Axel Thue.\n\nThe formalism of context-free grammars was developed in the mid-1950s by Noam Chomsky, and also their classification as a special type of formal grammar (which he called phrase-structure grammars). What Chomsky called a phrase structure grammar is also known now as a constituency grammar, whereby constituency grammars stand in contrast to dependency grammars. In Chomsky's generative grammar framework, the syntax of natural language was described by context-free rules combined with transformation rules.\n\nBlock structure was introduced into computer programming languages by the Algol project (1957–1960), which, as a consequence, also featured a context-free grammar to describe the resulting Algol syntax. This became a standard feature of computer languages, and the notation for grammars used in concrete descriptions of computer languages came to be known as Backus–Naur form, after two members of the Algol language design committee. The \"block structure\" aspect that context-free grammars capture is so fundamental to grammar that the terms syntax and grammar are often identified with context-free grammar rules, especially in computer science. Formal constraints not captured by the grammar are then considered to be part of the \"semantics\" of the language.\n\nContext-free grammars are simple enough to allow the construction of efficient parsing algorithms that, for a given string, determine whether and how it can be generated from the grammar. An Earley parser is an example of such an algorithm, while the widely used LR and LL parsers are simpler algorithms that deal only with more restrictive subsets of context-free grammars.\n\nA context-free grammar is defined by the 4-tuple:\n\nformula_29\nwhere\nBULLET::::1. is a finite set; each element formula_30 is called \"a nonterminal character\" or a \"variable\". Each variable represents a different type of phrase or clause in the sentence. Variables are also sometimes called syntactic categories. Each variable defines a sub-language of the language defined by .\nBULLET::::2. is a finite set of \"terminal\"s, disjoint from , which make up the actual content of the sentence. The set of terminals is the alphabet of the language defined by the grammar .\nBULLET::::3. is a finite relation from to formula_31, where the asterisk represents the Kleene star operation. The members of are called the \"(rewrite) rule\"s or \"production\"s of the grammar. (also commonly symbolized by a )\nBULLET::::4. is the start variable (or start symbol), used to represent the whole sentence (or program). It must be an element of .\n\nA production rule in is formalized mathematically as a pair formula_32, where formula_33 is a nonterminal and formula_34 is a string of variables and/or terminals; rather than using ordered pair notation, production rules are usually written using an arrow operator with as its left hand side and as its right hand side:\nformula_35.\n\nIt is allowed for to be the empty string, and in this case it is customary to denote it by ε. The form formula_36 is called an -production.\n\nIt is common to list all right-hand sides for the same left-hand side on the same line, using  (the pipe symbol) to separate them. Rules formula_37 and formula_38 can hence be written as formula_39. In this case, formula_40 and formula_41 is called the first and second alternative, respectively.\n\nFor any strings formula_42, we say directly yields , written as formula_43, if formula_44 with formula_33 and formula_46 such that formula_47 and formula_48. Thus, is a result of applying the rule formula_49 to .\n\nFor any strings formula_50 we say yields , written as formula_51 (or formula_52 in some textbooks), if formula_53 such that formula_54. In this case, if formula_55 (i.e., formula_56), the relation formula_57 holds. In other words, formula_58 and formula_59 are the reflexive transitive closure (allowing a word to yield itself) and the transitive closure (requiring at least one step) of formula_60, respectively.\n\nThe language of a grammar formula_29 is the set\n\nA language is said to be a context-free language (CFL), if there exists a CFG , such that formula_63.\n\nNon-deterministic pushdown automata recognize exactly the context-free languages.\n\nA context-free grammar is said to be \"proper\", if it has\nBULLET::::- no unreachable symbols: formula_64\nBULLET::::- no unproductive symbols: formula_65\nBULLET::::- no ε-productions: formula_66\nBULLET::::- no cycles: formula_67\n\nEvery context-free grammar can be effectively transformed into a weakly equivalent one without unreachable symbols, a weakly equivalent one without unproductive symbols, and a weakly equivalent one without cycles.\nEvery context-free grammar not producing ε can be effectively transformed into a weakly equivalent one without ε-productions; altogether, every such grammar can be effectively transformed into a weakly equivalent proper CFG.\n\nThe grammar formula_68, with productions\n\nis context-free. It is not proper since it includes an ε-production. A typical derivation in this grammar is\nThis makes it clear that \nformula_69. \nThe language is context-free, however, it can be proved that it is not regular.\n\nIf the productions\n\nare added, a context-free grammar for the set of all palindromes over the alphabet is obtained.\n\nThe canonical example of a context-free grammar is parenthesis matching, which is representative of the general case. There are two terminal symbols \"(\" and \")\" and one nonterminal symbol S. The production rules are\n\nThe first rule allows the S symbol to multiply; the second rule allows the S symbol to become enclosed by matching parentheses; and the third rule terminates the recursion.\n\nA second canonical example is two different kinds of matching nested parentheses, described by the productions:\n\nwith terminal symbols [ ] ( ) and nonterminal S.\n\nThe following sequence can be derived in that grammar:\n\nIn a context-free grammar, we can pair up characters the way we do with brackets. The simplest example:\n\nThis grammar generates the language formula_70, which is not regular (according to the pumping lemma for regular languages).\n\nThe special character ε stands for the empty string. By changing the above grammar to\nwe obtain a grammar generating the language formula_71 instead. This differs only in that it contains the empty string while the original grammar did not.\n\nA context-free grammar for the language consisting of all strings over {a,b} containing an unequal number of a's and b's:\nHere, the nonterminal T can generate all strings with the same number of a's as b's, the nonterminal U generates all strings with more a's than b's and the nonterminal V generates all strings with fewer a's than b's. Omitting the third alternative in the rule for U and V doesn't restrict the grammar's language.\n\nAnother example of a non-regular language is formula_72. It is context-free as it can be generated by the following context-free grammar:\n\nThe formation rules for the terms and formulas of formal logic fit the definition of context-free grammar, except that the set of symbols may be infinite and there may be more than one start symbol.\n\nIn contrast to well-formed nested parentheses and square brackets in the previous section, there is no context-free grammar for generating all sequences of two different types of parentheses, each separately balanced \"disregarding the other\", where the two types need not nest inside one another, for example:\n\nor\n\nThe fact that this language is not context free can be proven using Pumping lemma for context-free languages and a proof by contradiction, observing that all words of the form \nformula_73\nshould belong to the language. This language belongs instead to a more general class and can be described by a conjunctive grammar, which in turn also includes other non-context-free languages, such as the language of all words of the form\nformula_74.\n\nEvery regular grammar is context-free, but not all context-free grammars are regular. The following context-free grammar, however, is also regular.\n\nThe terminals here are \"a\" and \"b\", while the only nonterminal is S.\nThe language described is all nonempty strings of formula_75s and formula_76s that end in formula_75.\n\nThis grammar is regular: no rule has more than one nonterminal in its right-hand side, and each of these nonterminals is at the same end of the right-hand side.\n\nEvery regular grammar corresponds directly to a nondeterministic finite automaton, so we know that this is a regular language.\n\nUsing pipe symbols, the grammar above can be described more tersely as follows:\n\nA \"derivation\" of a string for a grammar is a sequence of grammar rule applications that transform the start symbol into the string.\nA derivation proves that the string belongs to the grammar's language.\n\nA derivation is fully determined by giving, for each step:\nBULLET::::- the rule applied in that step\nBULLET::::- the occurrence of its left-hand side to which it is applied\nFor clarity, the intermediate string is usually given as well.\n\nFor instance, with the grammar:\n\nthe string\n\ncan be derived with the derivation:\n\nOften, a strategy is followed that deterministically determines the next nonterminal to rewrite:\nBULLET::::- in a \"leftmost derivation\", it is always the leftmost nonterminal;\nBULLET::::- in a \"rightmost derivation\", it is always the rightmost nonterminal.\nGiven such a strategy, a derivation is completely determined by the sequence of rules applied. For instance, the leftmost derivation\n\ncan be summarized as\n\nThe distinction between leftmost derivation and rightmost derivation is important because in most parsers the transformation of the input is defined by giving a piece of code for every grammar rule that is executed whenever the rule is applied. Therefore, it is important to know whether the parser determines a leftmost or a rightmost derivation because this determines the order in which the pieces of code will be executed. See for an example LL parsers and LR parsers.\n\nA derivation also imposes in some sense a hierarchical structure on the string that is derived. For example, if the string \"1 + 1 + a\" is derived according to the leftmost derivation:\n\nthe structure of the string would be:\n\nwhere { ... } indicates a substring recognized as belonging to S. This hierarchy can also be seen as a tree:\n\nThis tree is called a \"parse tree\" or \"concrete syntax tree\" of the string, by contrast with the abstract syntax tree. In this case the presented leftmost and the rightmost derivations define the same parse tree; however, there is another (rightmost) derivation of the same string\n\nand this defines the following parse tree:\n\nNote however that both parse trees can be obtained by both leftmost and rightmost derivations. For example, the last tree can be obtained with the leftmost derivation as follows:\n\nIf a string in the language of the grammar has more than one parsing tree, then the grammar is said to be an \"ambiguous grammar\". Such grammars are usually hard to parse because the parser cannot always decide which grammar rule it has to apply. Usually, ambiguity is a feature of the grammar, not the language, and an unambiguous grammar can be found that generates the same context-free language. However, there are certain languages that can only be generated by ambiguous grammars; such languages are called \"inherently ambiguous languages\".\n\nHere is a context-free grammar for syntactically correct infix algebraic expressions in the variables x, y and z:\nBULLET::::1. S → x\nBULLET::::2. S → y\nBULLET::::3. S → z\nBULLET::::4. S → S + S\nBULLET::::5. S → S - S\nBULLET::::6. S → S * S\nBULLET::::7. S → S / S\nBULLET::::8. S → ( S )\n\nThis grammar can, for example, generate the string\n\nas follows:\n\nNote that many choices were made underway as to which rewrite was going to be performed next.\nThese choices look quite arbitrary. As a matter of fact, they are, in the sense that the string finally generated is always the same. For example, the second and third rewrites\n\ncould be done in the opposite order:\n\nAlso, many choices were made on which rule to apply to each selected codice_1.\nChanging the choices made and not only the order they were made in usually affects which terminal string comes out at the end.\n\nLet's look at this in more detail. Consider the parse tree of this derivation:\n\nStarting at the top, step by step, an S in the tree is expanded, until no more unexpanded codice_1es (nonterminals) remain.\nPicking a different order of expansion will produce a different derivation, but the same parse tree.\nThe parse tree will only change if we pick a different rule to apply at some position in the tree.\n\nBut can a different parse tree still produce the same terminal string,\nwhich is codice_3 in this case?\nYes, for this particular grammar, this is possible.\nGrammars with this property are called ambiguous.\n\nFor example, codice_4 can be produced with these two different parse trees:\n\nHowever, the \"language\" described by this grammar is not inherently ambiguous:\nan alternative, unambiguous grammar can be given for the language, for example:\n\n(once again picking codice_1 as the start symbol). This alternative grammar will produce codice_4 with a parse tree similar to the left one above, i.e. implicitly assuming the association codice_7, which is not according to standard operator precedence. More elaborate, unambiguous and context-free grammars can be constructed that produce parse trees that obey all desired operator precedence and associativity rules.\n\nEvery context-free grammar that does not generate the empty string can be transformed into one in which there is no ε-production (that is, a rule that has the empty string as a product). If a grammar does generate the empty string, it will be necessary to include the rule formula_78, but there need be no other ε-rule. Every context-free grammar with no ε-production has an equivalent grammar in Chomsky normal form, and a grammar in Greibach normal form. \"Equivalent\" here means that the two grammars generate the same language.\n\nThe especially simple form of production rules in Chomsky normal form grammars has both theoretical and practical implications. For instance, given a context-free grammar, one can use the Chomsky normal form to construct a polynomial-time algorithm that decides whether a given string is in the language represented by that grammar or not (the CYK algorithm).\n\nContext-free languages are closed under the various operations, that is, if the languages \"K\" and \"L\" are \ncontext-free, so is the result of the following operations:\n\nBULLET::::- union \"K\" ∪ \"L\"; concatenation \"K\" ∘ \"L\"; Kleene star \"L\"\nBULLET::::- substitution (in particular homomorphism)\nBULLET::::- inverse homomorphism\nBULLET::::- intersection with a regular language\n\nThey are not closed under general intersection (hence neither under complementation) and set difference.\n\nThe following are some decidable problems about context-free grammars.\n\nThe parsing problem, checking whether a given word belongs to the language given by a context-free grammar, is decidable, using one of the general-purpose parsing algorithms:\nBULLET::::- CYK algorithm (for grammars in Chomsky normal form)\nBULLET::::- Earley parser\nBULLET::::- GLR parser\nBULLET::::- LL parser (only for the proper subclass of for LL(\"k\") grammars)\nContext-free parsing for Chomsky normal form grammars was shown by Leslie G. Valiant to be reducible to boolean matrix multiplication, thus inheriting its complexity upper bound of \"O\"(\"n\"). Conversely, Lillian Lee has shown \"O\"(\"n\") boolean matrix multiplication to be reducible to \"O\"(\"n\") CFG parsing, thus establishing some kind of lower bound for the latter.\n\nIt is decidable whether a given non-terminal of a context-free grammar is reachable, whether it is productive, and whether it is nullable (that is, it can derive the empty string).\n\nIt is decidable whether a given \"grammar\" is a regular grammar, as well as whether it is an LL(\"k\") grammar for a given \"k\"≥0. If \"k\" is not given, the latter problem is undecidable.\n\nGiven a context-free \"language\", it is neither decidable whether it is regular, nor whether it is an LL(\"k\") language for a given \"k\".\n\nThere are algorithms to decide whether a language of a given context-free language is empty, as well as whether it is finite.\n\nSome questions that are undecidable for wider classes of grammars become decidable for context-free grammars; e.g. the emptiness problem (whether the grammar generates any terminal strings at all), is undecidable for context-sensitive grammars, but decidable for context-free grammars.\n\nHowever, many problems are undecidable even for context-free grammars. Examples are:\n\nGiven a CFG, does it generate the language of all strings over the alphabet of terminal symbols used in its rules?\n\nA reduction can be demonstrated to this problem from the well-known undecidable problem of determining whether a Turing machine accepts a particular input (the halting problem). The reduction uses the concept of a \"computation history\", a string describing an entire computation of a Turing machine. A CFG can be constructed that generates all strings that are not accepting computation histories for a particular Turing machine on a particular input, and thus it will accept all strings only if the machine doesn't accept that input.\n\nGiven two CFGs, do they generate the same language?\n\nThe undecidability of this problem is a direct consequence of the previous: it is impossible to even decide whether a CFG is equivalent to the trivial CFG defining the language of all strings.\n\nGiven two CFGs, can the first one generate all strings that the second one can generate?\n\nIf this problem was decidable, then language equality could be decided too: two CFGs G1 and G2 generate the same language if L(G1) is a subset of L(G2) and L(G2) is a subset of L(G1).\n\nUsing Greibach's theorem, it can be shown that the two following problems are undecidable:\n\nBULLET::::- Given a context-sensitive grammar, does it describe a context-free language?\nBULLET::::- Given a context-free grammar, does it describe a regular language?\n\nGiven a CFG, is it ambiguous?\n\nThe undecidability of this problem follows from the fact that if an algorithm to determine ambiguity existed, the Post correspondence problem could be decided, which is known to be undecidable.\n\nGiven two CFGs, is there any string derivable from both grammars?\n\nIf this problem was decidable, the undecidable Post correspondence problem could be decided, too: given strings formula_79 over some alphabet formula_80, let the grammar consist of the rule\nwhere formula_82 denotes the reversed string formula_83 and formula_76 doesn't occur among the formula_85; and let grammar consist of the rule\nThen the Post problem given by formula_79 has a solution if and only if and share a derivable string.\n\nAn obvious way to extend the context-free grammar formalism is to allow nonterminals to have arguments, the values of which are passed along within the rules. This allows natural language features such as agreement and reference, and programming language analogs such as the correct use and definition of identifiers, to be expressed in a natural way. E.g. we can now easily express that in English sentences, the subject and verb must agree in number. In computer science, examples of this approach include affix grammars, attribute grammars, indexed grammars, and Van Wijngaarden two-level grammars. Similar extensions exist in linguistics.\n\nAn extended context-free grammar (or regular right part grammar) is one in which the right-hand side of the production rules is allowed to be a regular expression over the grammar's terminals and nonterminals. Extended context-free grammars describe exactly the context-free languages.\n\nAnother extension is to allow additional terminal symbols to appear at the left-hand side of rules, constraining their application. This produces the formalism of context-sensitive grammars.\n\nThere are a number of important subclasses of the context-free grammars:\n\nBULLET::::- LR(\"k\") grammars (also known as deterministic context-free grammars) allow parsing (string recognition) with deterministic pushdown automata (PDA), but they can only describe deterministic context-free languages.\nBULLET::::- Simple LR, Look-Ahead LR grammars are subclasses that allow further simplification of parsing. SLR and LALR are recognized using the same PDA as LR, but with simpler tables, in most cases.\nBULLET::::- LL(\"k\") and LL(\"*\") grammars allow parsing by direct construction of a leftmost derivation as described above, and describe even fewer languages.\nBULLET::::- Simple grammars are a subclass of the LL(1) grammars mostly interesting for its theoretical property that language equality of simple grammars is decidable, while language inclusion is not.\nBULLET::::- Bracketed grammars have the property that the terminal symbols are divided into left and right bracket pairs that always match up in rules.\nBULLET::::- Linear grammars have no rules with more than one nonterminal on the right-hand side.\nBULLET::::- Regular grammars are a subclass of the linear grammars and describe the regular languages, i.e. they correspond to finite automata and regular expressions.\n\nLR parsing extends LL parsing to support a larger range of grammars; in turn, generalized LR parsing extends LR parsing to support arbitrary context-free grammars. On LL grammars and LR grammars, it essentially performs LL parsing and LR parsing, respectively, while on nondeterministic grammars, it is as efficient as can be expected. Although GLR parsing was developed in the 1980s, many new language definitions and parser generators continue to be based on LL, LALR or LR parsing up to the present day.\n\nChomsky initially hoped to overcome the limitations of context-free grammars by adding transformation rules.\n\nSuch rules are another standard device in traditional linguistics; e.g. passivization in English. Much of generative grammar has been devoted to finding ways of refining the descriptive mechanisms of phrase-structure grammar and transformation rules such that exactly the kinds of things can be expressed that natural language actually allows. Allowing arbitrary transformations does not meet that goal: they are much too powerful, being Turing complete unless significant restrictions are added (e.g. no transformations that introduce and then rewrite symbols in a context-free fashion).\n\nChomsky's general position regarding the non-context-freeness of natural language has held up since then, although his specific examples regarding the inadequacy of context-free grammars in terms of their weak generative capacity were later disproved.\nGerald Gazdar and Geoffrey Pullum have argued that despite a few non-context-free constructions in natural language (such as cross-serial dependencies in Swiss German and reduplication in Bambara), the vast majority of forms in natural language are indeed context-free.\n\nBULLET::::- Parsing expression grammar\nBULLET::::- Stochastic context-free grammar\nBULLET::::- Algorithms for context-free grammar generation\nBULLET::::- Pumping lemma for context-free languages\n\nBULLET::::- . Chapter 4: Context-Free Grammars, pp. 77–106; Chapter 6: Properties of Context-Free Languages, pp. 125–137.\nBULLET::::- . Chapter 2: Context-Free Grammars, pp. 91–122; Section 4.1.2: Decidable problems concerning context-free languages, pp. 156–159; Section 5.1.1: Reductions via computation histories: pp. 176–183.\n\nBULLET::::- Computer programmers may find the stack exchange answer to be useful.\nBULLET::::- Non-computer programmers will find more academic introductory materials to be enlightening.\n"}
{"id": "6760", "url": "https://en.wikipedia.org/wiki?curid=6760", "title": "Cryonics", "text": "Cryonics\n\nCryonics (from \"kryos\" meaning 'cold') is the low-temperature freezing (usually at ) and storage of a human corpse or severed head, with the speculative hope that resurrection may be possible in the future. Cryonics is regarded with skepticism within the mainstream scientific community. It is a pseudoscience, and its practice has been characterized as quackery.\n\nCryonics procedures can begin only after clinical death, and cryonics \"patients\" are legally dead. Cryonics procedures ideally begin within minutes of death, and use cryoprotectants to prevent ice formation during cryopreservation. It is however not possible for a corpse to be reanimated after undergoing vitrification, as this causes damage to the brain including its neural networks. The first corpse to be frozen was that of Dr. James Bedford in 1967. As of 2014, about 250 dead bodies had been cryopreserved in the United States, and 1,500 people had made arrangements for cryopreservation of their corpses.\n\nEconomic reality means it is highly improbable that any cryonics corporation could continue in business long enough to take advantage of the claimed long-term benefits offered. most of the early cryonics companies had gone out of business, and their stored corpses thawed and disposed of.\n\nCryonicists argue that as long as brain structure remains intact, there is no fundamental barrier, given our current understanding of physical law, to recovering its information content. Cryonics proponents go further than the mainstream consensus in saying that the brain does not have to be continuously active to survive or retain memory. Cryonics controversially states that a human survives even within an inactive brain that has been badly damaged, provided that original encoding of memory and personality can, in theory, be adequately inferred and reconstituted from what structure remains.\n\nThe cryonics argument that death does not occur as long as brain structure remains intact and the information is theoretically readable has received some mainstream medical discussion in the context of the ethical concept of brain death and organ donation.\n\nCryonics uses temperatures below −130 °C, called cryopreservation, in an attempt to preserve enough brain information to permit future revival of the cryopreserved person. Cryopreservation may be accomplished by freezing, freezing with cryoprotectant to reduce ice damage, or by vitrification to avoid ice damage. Even using the best methods, cryopreservation of whole bodies or brains is very damaging and irreversible with current technology.\n\nCryonics requires unknown future technology to repair or regenerate tissue that is diseased, damaged, or missing. Brain repairs in particular will require analysis at the molecular level. This far-future technology is usually assumed to be nanomedicine based on molecular nanotechnology. Biological repair methods or mind uploading have also been proposed.\n\nCryonics can be expensive. the cost of preparing and storing corpses using ranged from US$28,000 to $200,000. \n\nWhen used at high concentrations, cryoprotectants can stop ice formation completely. Cooling and solidification without crystal formation is called vitrification. The first cryoprotectant solutions able to vitrify at very slow cooling rates while still being compatible with whole organ survival were developed in the late 1990s by cryobiologists Gregory Fahy and Brian Wowk for the purpose of banking transplantable organs. This has allowed animal brains to be vitrified, warmed back up, and examined for ice damage using light and electron microscopy. No ice crystal damage was found; cellular damage was due to dehydration and toxicity of the cryoprotectant solutions.\n\nCosts can include payment for medical personnel to be on call for death, vitrification, transportation in dry ice to a preservation facility, and payment into a trust fund intended to cover indefinite storage in liquid nitrogen and future revival costs. As of 2011, U.S. cryopreservation costs can range from $28,000 to $200,000, and are often financed via life insurance. KrioRus, which stores bodies communally in large dewars, charges $12,000 to $36,000 for the procedure. Some patients opt to have only their brain cryopreserved (\"neuropreservation\"), rather than their whole body.\n\nAs of 2014, about 250 corpses have been cryogenically preserved in the U.S., and around 1,500 people have signed up to have their remains preserved. As of 2016, four facilities exist in the world to retain cryopreserved bodies: three in the U.S. and one in Russia.\n\nTaking into account the lifecycle of corporations, it is extremely unlikely that any cryonics company could continue to exist for sufficient time to take advantage even of the supposed benefits offered: historically, even the most robust corporations have only a one-in-a-thousand chance of surviving even one hundred years. Many cryonics companies have failed: all but one of the pre-1973 batch had gone out of business, and their stored corpses have been defrosted and disposed of.\n\nWithout cryoprotectants, cell shrinkage and high salt concentrations during freezing usually prevent frozen cells from functioning again after thawing. Ice crystals can also disrupt connections between cells that are necessary for organs to function. The difficulties of recovering large animals and their individual organs from a frozen state have been long known. Attempts to recover frozen mammals by simply rewarming them were abandoned by 1957. At humanity's present level of scientific knowledge, only cells, tissues, and some small organs can be reversibly cryopreserved.\n\nLarge vitrified organs tend to develop fractures during cooling, a problem worsened by the large tissue masses and very low temperatures of cryonics.\n\nActual cryonics organizations use vitrification without a chemical fixation step, sacrificing some structural preservation quality for less damage at the molecular level. Some scientists, like Joao Pedro Magalhaes, have questioned whether using a deadly chemical for fixation eliminates the possibility of biological revival, making chemical fixation unsuitable for cryonics.\n\nIn 2016, Robert L. McIntyre and Gregory Fahy at the cryobiology research company 21st Century Medicine, Inc. won the Small Animal Brain Preservation Prize of the Brain Preservation Foundation by demonstrating to the satisfaction of neuroscientist judges that a particular implementation of fixation and vitrification called \"aldehyde-stabilized cryopreservation\" could preserve a rabbit brain in \"near perfect\" condition at −135 °C, with the cell membranes, synapses, and intracellular structures intact in electron micrographs. Brain Preservation Foundation President, Ken Hayworth, said, \"This result directly answers a main skeptical and scientific criticism against cryonics—that it does not provably preserve the delicate synaptic circuitry of the brain.” However the price paid for perfect preservation as seen by microscopy was tying up all protein molecules with chemical crosslinks, completely eliminating biological viability.\n\nOutside the cryonics community, many scientists have strong skepticism toward cryonics methods. Cryobiologist Dayong Gao states that \"we simply don't know if (subjects have) been damaged to the point where they've 'died' during vitrification because the subjects are now inside liquid nitrogen canisters.\" Biochemist Ken Storey argues (based on experience with organ transplants), that \"even if you only wanted to preserve the brain, it has dozens of different areas, which would need to be cryopreserved using different protocols.\"\n\nThose who believe that revival may someday be possible generally look toward presently-nonexistent bioengineering, molecular nanotechnology, or nanomedicine as key technologies. Revival would require repairing damage from lack of oxygen, cryoprotectant toxicity, thermal stress (fracturing), freezing in tissues that do not successfully vitrify, finally followed by reversing the cause of death. In many cases extensive tissue regeneration would be necessary.\n\nAccording to Cryonics Institute president Ben Best, cryonics revival may be similar to a last in, first out process. People cryopreserved in the future, with better technology, may require less advanced technology to be revived because they will have been cryopreserved with better technology that caused less damage to tissue. In this view, preservation methods would get progressively better until eventually they are demonstrably reversible, after which medicine would begin to reach back and revive people cryopreserved by more primitive methods.\n\nHistorically, a person had little control regarding how their body was treated after death as religion held jurisdiction over the ultimate fate of their body. However, secular courts began to exercise jurisdiction over the body and use discretion in carrying out of the wishes of the deceased person. Most countries legally treat preserved individuals as deceased persons because of laws that forbid vitrifying someone who is medically alive. In France, cryonics is not considered a legal mode of body disposal; only burial, cremation, and formal donation to science are allowed. However, bodies may legally be shipped to other countries for cryonic freezing. As of 2015, the Canadian province of British Columbia prohibits the sale of arrangements for body preservation based on cryonics. In Russia, cryonics falls outside both the medical industry and the funeral services industry, making it easier in Russia than in the U.S. to get hospitals and morgues to release cryonics candidates.\n\nIn London in 2016, the English High Court ruled in favor of a mother's right to seek cryopreservation of her terminally ill 14-year-old daughter, as the girl wanted, contrary to the father's wishes. The decision was made on the basis that the case represented a conventional dispute over the disposal of the girl's body, although the judge urged ministers to seek \"proper regulation\" for the future of cryonic preservation following concerns raised by the hospital about the competence and professionalism of the team that conducted the preservation procedures. In Alcor Life Extension Foundation v. Richardson, the Iowa Court of Appeals ordered for the disinterment of Richardson, who was buried against his wishes for cryopreservation.\n\nA detailed legal examination by Jochen Taupitz concludes that cryonic storage is legal in Germany for an indefinite period of time.\n\nIn 2009, writing in \"Bioethics\", David Shaw examines the ethical status of cryonics. The arguments against it include changing the concept of death, the expense of preservation and revival, lack of scientific advancement to permit revival, temptation to use premature euthanasia, and failure due to catastrophe. Arguments in favor of cryonics include the potential benefit to society, the prospect of immortality, and the benefits associated with avoiding death. Shaw explores the expense and the potential payoff, and applies an adapted version of Pascal's Wager to the question.\n\nIn 2016, Charles Tandy wrote in favor of cryonics, arguing that honoring someone's last wishes is seen as a benevolent duty in American and many other cultures.\n\nCryopreservation was applied to human cells beginning in 1954 with frozen sperm, which was thawed and used to inseminate three women. The freezing of humans was first scientifically proposed by Michigan professor Robert Ettinger when he wrote \"The Prospect of Immortality\" (1962). In April 1966, the first human body was frozen—though it had been embalmed for two months—by being placed in liquid nitrogen and stored at just above freezing. The middle-aged woman from Los Angeles, whose name is unknown, was soon thawed out and buried by relatives.\n\nThe first body to be frozen with the hope of future revival was James Bedford's, a few hours after his cancer-caused death in 1967. His body was frozen by Robert Nelson, a former TV repairman with no scientific background, before the body was turned over to Bedford's relatives. Bedford's corpse is the only one frozen before 1974 still preserved today. In 1976, Ettinger founded the Cryonics Institute; his corpse was cryopreserved in 2011. Nelson was sued in 1981 for allowing nine bodies to thaw and decompose in the 1970s; in his defense, he claimed that the Cryonics Society of California had run out of money. This led to the lowered reputation of cryonics in the U.S.\n\nIn 2018, a Y-Combinator startup called Nectome was recognized for developing a method of preserving brains with chemicals rather than by freezing. The method is fatal, performed as euthanasia under general anethesia, but the hope is that future technology would allow the brain to be physically scanned into a computer simulation, neuron by neuron.\n\nAccording to \"The New York Times\", cryonicists are predominantly nonreligious white males, outnumbering women by about three to one. According to \"The Guardian\", as of 2008, while most cryonicists used to be young, male and \"geeky\" recent demographics have shifted slightly towards whole families.\n\nIn 2015 Du Hong, a 61-year-old female writer of children's literature, became the first known Chinese national to have their head cryopreserved.\n\nCryonics is generally regarded as a fringe pseudoscience. The Society for Cryobiology have rejected as members those who practiced cryonics, and have issued a public statement saying that cryonics is \"not science\", and that it is a \"personal choice\" how people want to have their dead bodies disposed of.\n\nRussian company KrioRus is the only non-US vendor of cryonics services. Yevgeny Alexandrov, chair of the Russian Academy of Sciences commission against pseudoscience, said there was \"no scientific basis\" for cryonics, and that the company's offering was based on \"unfounded speculation\".\n\nAlthough scientists have expressed skepticism about cryonics in media sources, philosopher has written that it only receives a \"miniscule\" amount of attention from academia.\n\nWhile some neuroscientists contend that all the subtleties of a human mind are contained in its anatomical structure, few neuroscientists will comment directly upon the topic of cryonics due to its speculative nature. Individuals who intend to be frozen are often \"looked at as a bunch of kooks\". Cryobiologist Kenneth B. Storey said in 2004 that cryonics is impossible and will never be possible, as cryonics proponents are proposing to \"over-turn the laws of physics, chemistry, and molecular science\".\n\nWilliam T. Jarvis has written that \"Cryonics might be a suitable subject for scientific research, but marketing an unproven method to the public is quackery\".\n\nAccording to cryonicist Aschwin de Wolf and others, cryonics can often produce intense hostility from spouses who are not cryonicists. James Hughes, the executive director of the pro-life-extension Institute for Ethics and Emerging Technologies, chooses not to personally sign up for cryonics, calling it a worthy experiment but stating laconically that \"I value my relationship with my wife.\"\n\nCryobiologist Dayong Gao states that \"People can always have hope that things will change in the future, but there is no scientific foundation supporting cryonics at this time.\" As well, while it is universally agreed that \"personal identity\" is uninterrupted when brain activity temporarily ceases during incidents of accidental drowning (where people have been restored to normal functioning after being completely submerged in cold water for up to 66 minutes), some people express concern that a centuries-long cryopreservation might interrupt their conception of personal identity, such that the revived person would \"not be you\".\n\nMany people say there would be no point in being revived in the far future if their friends and families are dead.\n\nSuspended animation is a popular subject in science fiction and fantasy settings. It is often the means by which a character is transported into the future.\n\nA survey in Germany found that about half of the respondents were familiar with cryonics, and about half of those familiar with cryonics had learned of the subject from films or television.\n\nCorpses subjected to the cryonics process include those of L. Stephen Coles (in 2014), Hal Finney (in 2014), and Ted Williams.\n\nDisgraced financier Jeffrey Epstein wanted to have his head and penis frozen after death so that he could \"seed the human race with his DNA\".\n\nThe urban legend suggesting Walt Disney's corpse was cryopreserved is false; it was cremated and interred at Forest Lawn Memorial Park Cemetery. Robert A. Heinlein, who wrote enthusiastically of the concept in \"The Door into Summer\" (serialized in 1956), was cremated and had his ashes distributed over the Pacific Ocean. Timothy Leary was a long-time cryonics advocate and signed up with a major cryonics provider, but he changed his mind shortly before his death, and was not cryopreserved.\n\n"}
{"id": "6761", "url": "https://en.wikipedia.org/wiki?curid=6761", "title": "Unitary patent", "text": "Unitary patent\n\n+ Unitary patent in the European Union\n\nThe European patent with unitary effect (EPUE), more commonly known as the unitary patent, is a new type of European patent in advanced stage of adoption which would be valid in participating member states of the European Union. Unitary effect can be registered for a European patent upon grant, replacing validation of the European patent in the individual countries concerned. The unitary effect means a single renewal fee, a single ownership, a single object of property, a single court (the Unified Patent Court) and uniform protection—which means that revocation as well as infringement proceedings are to be decided for the unitary patent as a whole rather than for each country individually. Licensing is however to remain possible for part of the unitary territory.\n\nOn 17 December 2012, agreement was reached between the European Council and European Parliament on the two EU regulations that made the unitary patent possible through enhanced cooperation at EU level. The legality of the two regulations was challenged by Spain and Italy, but all their claims were rejected by the European Court of Justice. Italy subsequently joined the unitary patent regulation in September 2015, so that all EU member states except Spain and Croatia now participate in the enhanced cooperation for a unitary patent. Unitary effect of newly granted European patents can be requested from the date when the related Unified Patent Court Agreement enters into force for the first group of ratifiers, and will extend to those participating member states for which the UPC Agreement had entered into force at the time of registration of the unitary patent. Previously granted unitary patents will not automatically get their unitary effect extended to the territory of participating states which ratify the UPC agreement at a later date. \n\nThe negotiations which resulted in the unitary patent can be traced back to various initiatives dating to the 1970s. At different times, the project, or very similar projects, have been referred to as the \"European Union patent\" (the name used in the EU treaties, which serve as the legal basis for EU competency), \"EU patent\", \"Community patent\", \"European Community Patent\", \"EC patent\" and \"COMPAT\".\n\nBy not requiring translations into a language of each contracting state, and by requiring the payment of only a single renewal fee for the group of contracting states, the unitary patent aims to be cheaper than European patents. Instead, unitary patents will be accepted in English, French, or German with no further translation required after grant. Machine translations will be provided, but will be, in the words of the regulation, \"for information purposes only and should not have any legal effect\". The maintenance fees, with a single fee for the whole area, are also expected to be lower compared to renewal fees for the whole area but the fees have yet to be announced.\n\nThe proposed unitary patent will be a particular type of European patent, granted under the European Patent Convention. A European patent, once granted, becomes a \"bundle of nationally enforceable patents\", in the states which are designated by the applicant, and the unitary effect would effectively create a single enforceable region in a subgroup of those 38 states, which may coexist with nationally enforceable patents (\"classical\" patents) in the remaining states. \"Classical\", non-unitary European patents hold exclusively for single countries and require the filing of a translation in some contracting states, in accordance with .\n\nJanuary 18, 2019, Kluwer Patent Blog wrote, \"a recurring theme for some years has been that ‘the UPC will start next year’\". Then Brexit and German constitutional court challenge by Dr Stjerna were considered as the main objects. The decision is expected early 2019. If Germany can then ratify the UPC Agreement, only one other country must do it, which is expected to happen soon, for the provisional application phase to start. It was also considered that Brexit will be postponed, thus allowing the UK participate in the unitary patent. The UK ratified the agreement in April 2018 and intends to remain in the UPC even after Brexit. Therefore, it seemed possible that the start would be late 2019 and even more likely in 2020.\n\nIn 2009, three draft documents were published regarding a community patent: a European patent in which the European Community was designated:\n\nBULLET::::1. Council regulation on the community patent,\nBULLET::::2. Agreement on the European and Community Patents Court (open to the European Community and all states of the European Patent Convention)\nBULLET::::3. Decision to open negotiations regarding this Agreement\n\nBased on those documents, the European Council requested on 6 July 2009 an opinion from the Court of Justice of the European Union, regarding the compatibility of the envisioned Agreement with EU law: \"‘Is the envisaged agreement creating a Unified Patent Litigation System (currently named European and Community Patents Court) compatible with the provisions of the Treaty establishing the European Community?’\"\n\nIn December 2010, the use of the enhanced co-operation procedure, under which of the Treaty on the Functioning of the European Union provides that a group of member states of the European Union can choose to co-operate on a specific topic, was proposed by twelve Member States to set up a unitary patent applicable in all participating European Union Member States. The use of this procedure had only been used once in the past, for harmonising rules regarding the applicable law in divorce across several EU Member States.\n\nIn early 2011, the procedure leading to the enhanced co-operation was reported to be progressing. Twenty-five Member States had written to the European Commission requesting to participate, with Spain and Italy remaining outside, primarily on the basis of ongoing concerns over translation issues. On 15 February, the European Parliament approved the use of the enhanced co-operation procedure for unitary patent protection by a vote of 471 to 160. and on 10 March 2011 the Council gave their authorisation. Two days earlier, on 8 March 2011, the Court of Justice of the European Union had issued its opinion, stating that the draft Agreement creating the European and Community Patent Court would be incompatible with EU law. The same day, the Hungarian Presidency of the Council insisted that this opinion would not affect the enhanced co-operation procedure.\n\nIn November 2011, negotiations on the enhanced co-operation system were reportedly advancing rapidly—too fast, in some views. It was announced that implementation required an enabling European Regulation, and a Court agreement between the states that elect to take part. The European Parliament approved the continuation of negotiations in September. A draft of the agreement was issued on 11 November 2011 and was open to all member states of the European Union, but not to other European Patent Convention states. However, serious criticisms of the proposal remained mostly unresolved. A meeting of the Competitiveness Council on 5 December failed to agree on the final text. In particular, there was no agreement on where the Central Division of a Unified Patent Court should be located, \"with London, Munich and Paris the candidate cities.\"\n\nThe Polish Presidency acknowledged on 16 December 2011 the failure to reach an agreement \"on the question of the location of the seat of the central division.\" The Danish Presidency therefore inherited the issue. According to the President of the European Commission in January 2012, the only question remaining to be settled was the location of the Central Division of the Court. However, evidence presented to the UK House of Commons European Scrutiny Committee in February suggested that the position was more complicated. At an EU summit at the end of January 2012, participants agreed to press on and finalise the system by June. On 26 April, Herman Van Rompuy, President of the European Council, wrote to members of the Council, saying \"This important file has been discussed for many years and we are now very close to a final deal... This deal is needed now, because this is an issue of crucial importance for innovation and growth. I very much hope that the last outstanding issue will be sorted out at the May Competitiveness Council. If not, I will take it up at the June European Council.\" The Competitiveness Council met on 30 May and failed to reach agreement.\n\nA compromise agreement on the seat(s) of the unified court was eventually reached at the June European Council (28–29 June 2012), splitting the central division according to technology between Paris (the main seat), London and Munich. However, on 2 July 2012, the European Parliament decided to postpone the vote following a move by the European Council to modify the arrangements previously approved by MEPs in negotiations with the European Council. The modification was considered controversial and included the deletion of three key articles (6–8) of the legislation, seeking to reduce the competence of the European Union Court of Justice in unitary patent litigation. On 9 July 2012, the Committee on Legal Affairs of the European Parliament debated the patent package following the decisions adopted by the General Council on 28–29 June 2012 in camera in the presence of MEP Bernhard Rapkay. A later press release by Rapkay quoted from a legal opinion submitted by the Legal Service of the European Parliament, which affirmed the concerns of MEPs to approve the decision of a recent EU summit to delete said articles as it \"nullifies central aspects of a substantive patent protection\". A Europe-wide uniform protection of intellectual property would thus not exist with the consequence that the requirements of the corresponding EU treaty would not be met and that the European Court of Justice could therefore invalidate the legislation. By the end of 2012 a new compromise was reached between the European Parliament and the European Council, including a limited role for the European Court of Justice. The Unified Court will apply the Unified Patent Court Agreement, which is considered national patent law from an EU law point of view, but still is equal for each participant. [However the draft statutory instrument aimed at implementation of the Unified Court and UPC in the UK provides for different infringement laws for: European patents (unitary or not) litigated through the Unified Court; European patents (UK) litigated before UK courts; and national patents]. The legislation for the enhanced co-operation mechanism was approved by the European Parliament on 11 December 2012 and the regulations were signed by the European Council and European Parliament officials on 17 December 2012.\n\nOn 30 May 2011, Italy and Spain challenged the Council's authorisation of the use of enhanced co-operation to introduce the trilingual (English, French, German) system for the unitary patent, which they viewed as discriminatory to their languages, with the CJEU on the grounds that it did not comply with the EU treaties. In January 2013, Advocate General Yves Bot delivered his recommendation that the court reject the complaint. Suggestions by the Advocate General are advisory only, but are generally followed by the court. The case was dismissed by the court in April 2013, however Spain launched two new challenges with the EUCJ in March 2013 against the regulations implementing the unitary patent package. The court hearing for both cases was scheduled for 1 July 2014. Advocate-General Yves Bot published his opinion on 18 November 2014, suggesting that both actions be dismissed ( and ). The court handed down its decisions on 5 May 2015 as and fully dismissing the Spanish claims. Following a request by the government of Italy, it became a participant of the unitary patent regulations in September 2015.\n\nEuropean patents are granted in accordance with the provisions of the European Patent Convention, via a unified procedure before the European Patent Office. A single patent application, in one language, may be filed at the European Patent Office or at a national patent office of certain Contracting States. Patentable inventions, according to the EPC, are \"any inventions, in all fields of technology, providing that they are new, involve an inventive step, and are susceptible of industrial application.\"\n\nIn contrast to the unified character of a European patent application, a granted European patent has, in effect, no unitary character, except for the centralized opposition procedure (which can be initiated within 9 months from grant, by somebody else than the patent proprietor), and the centralized limitation and revocation procedures (which can only be instituted by the patent proprietor). In other words, a European patent in one Contracting State, i.e. a \"national\" European patent, is effectively independent of the same European patent in each other Contracting State, except for the opposition, limitation and revocation procedures. The enforcement of a European patent is dealt with by national law. The abandonment, revocation or limitation of the European patent in one state does not affect the European patent in other states.\n\nThe EPC provides however the possibility for a group of member states to allow European patents to have a unitary character also after grant. Until now, only Liechtenstein and Switzerland have opted to create a unified protection area (see Unitary patent (Switzerland and Liechtenstein)).\n\nUpon filing of a European patent application, all 38 Contracting States are automatically designated, unless, when filing the application, the applicant withdraws one or more designations. This may be important to avoid conflicts with national (non European patent) applications. Designations may also be withdrawn after filing, at any time before grant. Upon grant, a European patent has immediate effect in all designated States, but to remain effective, yearly renewal fees have to be paid, in each State, and in certain countries translation requirements have to be met.\n\nThree instruments were proposed for the implementation of the unitary patent:\n\nBULLET::::- Regulation of the European Parliament and of the Council implementing enhanced co-operation in the area of the creation of unitary patent protection\nBULLET::::- Council Regulation implementing enhanced co-operation in the area of the creation of unitary patent protection with regard to the applicable translation arrangements\nBULLET::::- Agreement on a Unified Patent Court\n\nThe system is based on EU law as well as the European Patent Convention (EPC). provides the legal basis for establishing a common system of patents for Parties to the EPC. Previously, only Liechtenstein and Switzerland had used this possibility to create a unified protection area (see Unitary patent (Switzerland and Liechtenstein)).\n\nThe first two regulations were approved by the European Parliament on 11 December 2012, with future application set for the 25 member states then participating in the enhanced cooperation for a unitary patent (all current EU member states except Croatia, Italy and Spain). The instruments were adopted as regulations EU 1257/2012 and 1260/2012 on 17 December 2012, and entered into force in January 2013. Following a request by the government of Italy, it became a participant of the unitary patent regulations in September 2015.\n\nAs of March 2017, neither of the two remaining non-participants in the unitary patent (Spain and Croatia) had requested the European Commission to participate.\n\nAlthough formally the Regulations will apply to all 26 participating states from the moment the UPC Agreement enters into force for the first group of ratifiers, the unitary effect of newly granted unitary patents will only extend to those of the 26 states where the UPC Agreement has entered into force, while patent coverage for other participating states without UPC Agreement ratification will be covered by a coexisting normal European patent in each of those states. \n\nThe unitary effect of unitary patents means a single renewal fee, a single ownership, a single object of property, a single court (the Unified Patent Court) and uniform protection, which means that revocation as well as infringement proceedings are to be decided for the unitary patent as a whole rather than for each country individually. Licensing is however to remain possible for part of the unitary territory.\n\nSome administrative tasks relating to the European patents with unitary effect will be performed by the European Patent Office. These tasks include the collection of renewal fees and registration of unitary effect upon grant, exclusive licenses and statements that licenses are available to any person. Decisions of the European Patent Office regarding the unitary patent are open to appeal to the Unified Patent Court, rather than to the EPO Boards of Appeal.\n\nFor a unitary patent ultimately no translation will be required, which significantly reduces the cost for protection in the whole area. However, article 6 of EU regulation 1260/2012 provides that during a transition period of no more than twelve years one translation needs to be provided. A translation needs to be provided either into English if the application is in French or German, or into any EU official language if the application is in English. In addition, machine translations will be provided, which will be, in the words of the regulation, \"for information purposes only and should not have any legal effect\".\n\nIn several contracting states, for \"national\" European patents a translation has to be filed within a three-month time limit after the publication of grant in the European Patent Bulletin under , otherwise the patent is considered never to have existed (void ab initio) in that state. For the 21 parties to the London Agreement, this requirement has already been abolished or reduced (e.g. by dispensing with the requirement if the patent is available in English, and/or only requiring translation of the claims). Translation requirements for the participating states in the enhanced cooperation for a unitary patent are shown below:\n! Participating member State !! Translation requirements for a non-unitary European patent!! Translation requirements for a unitary patent\n\nArticle 7 of Regulation 1257/2012 provides that, as an object of property, a European patent with unitary effect will be treated \"in its entirety and in all participating Member States as a national patent of the participating Member State in which that patent has unitary effect and in which the applicant had her/his residence or principal place of business or, by default, had a place of business on the date of filing the application for the European patent.\" When the applicant had no domicile in a participating Member State, German law will apply. Ullrich has the criticized the system, which is similar to the Community Trademark and the Community Design, as being \"in conflict with both the purpose of the creation of unitary patent protection and with primary EU law.\"\n\nIn January 2013, after the two regulations about the unitary patent had entered into force, but before the regulations applied, the participating member states in the unitary patent established (as member states of the European Patent Convention) a Select Committee of the Administrative Council of the European Patent Organisation in order to prepare the work for implementation of the provisions. The committee held its inaugural meeting on 20 March 2013. The work of the Select Committee has to proceed in parallel to the work of the Preparatory Committee for the creation of the Unified Patent Court. Implementation of the Unitary Patent—including the legal, administrative and financial measures—shall be completed in due time before the entry into operation of the Unified Patent Court. In May 2015, the communicated target date for completion of the remaining preparatory work of the Select Committee was 30 June 2015. The Committee reached agreement on the level of the renewal fees June 2015, while it stated that a decision on the distribution of those fees among member states was in Autumn 2015.\n\nThe Agreement on a Unified Patent Court provides the legal basis for the Unified Patent Court: a patent court for European patents (with and without unitary effect), with jurisdiction in those countries where the Agreement is in effect. In addition to regulations regarding the court structure, it also contains substantive provisions relating to the right to prevent use of an invention and allowed use by non patent proprietors (e.g. for private non-commercial use), preliminary and permanent injunctions.\n\nThe Agreement was signed on 19 February 2013 by 24 EU member states, including all states then participating in the enhanced co-operation measures except Bulgaria and Poland, while Italy, which did not originally join the enhanced co-operation measures but subsequently signed up, did sign the UPC agreement. The agreement remains open to accession for all remaining EU member states, and Bulgaria signed the agreement on 5 March. Meanwhile, Poland decided to wait to see how the new patent system works before joining due to concerns that it would harm their economy. States which do not participate in the unitary patent regulations can still become parties to the UPC agreement, which would allow the new court to handle European patents validated in the country. Entry into force for the UPC will take place after 13 states (including Germany, France and the United Kingdom as the three states with the most patents in force) have ratified the Agreement. As of November 2015, the agreement has been ratified by 8 states (including 1 of the required ratifiers: France).\n\nThe Unified Patent Court will have exclusive jurisdiction in infringement and revocation proceedings involving European patents with unitary effect, and during a transition period non-exclusive jurisdiction (that the patent holder can be opt out from) regarding European patents without unitary effect in the states where the Agreement applies. It furthermore has jurisdiction to hear cases against decisions of the European Patent Office regarding unitary patents. As a court of several member states of the European Union it may (Court of First Instance) or must (Court of Appeal) ask prejudicial questions to the European Court of Justice when the interpretation of EU law (including the two unitary patent regulations, but excluding the UPC Agreement) is not obvious.\n\nThe court would have two divisions: a court of first instance and a court of appeal. The court of appeal and the registry would have their seats in Luxembourg, while the central division of the court of first instance would have its seat in Paris. The central division would have thematic branches in London and Munich. The court of first instance may further have local and regional divisions in all member states that wish to set up such divisions.\n\nWhile the regulations formally apply to all 26 member states participating in the enhanced cooperation for a unitary patent, from the date the UPC agreement has entered into force for the first group of ratifiers, unitary patents will only extend to the territory of those participating member states where the UPC Agreement had entered into force when the unitary effect was registered. If the unitary effect territory subsequently expands to additional participating member states for which the UPC Agreement later enters into force, this will be reflected for all subsequently registered unitary patents, but the territorial scope of the unitary effect of existing unitary patents will not be extended to these states.\n\nUnitary effect can be requested up to one month after grant of the European patent, with retroactive effect from the date of grant. However, according to the \"Draft Rules Relating to Unitary Patent Protection\", unitary effect would be registered only if the European patent has been granted with the same set of claims for all the 26 participating member states in the regulations, whether the unitary effect applies to them or not. European patents automatically become a bundle of \"national\" European patents upon grant. Upon the grant of unitary effect, the \"national\" European patents will retroactively be considered to never have existed in the territories where the unitary patent has effect. The unitary effect does not affect \"national\" European patents in states where the unitary patent does not apply. Any \"national\" European patents applying outside the \"unitary effect\" zone will co-exist with the unitary patent.\n\nAs the unitary patent is introduced by an EU regulation, it is expected to not only be valid in the mainland territory of the participating member states that are party to the UPC, but also in those of their special territories that are part of the European Union. As of April 2014, this includes the following fourteen territories:\n\nBULLET::::- Cyprus: UN Buffer Zone\nBULLET::::- Finland: Åland\nBULLET::::- France: French Guiana, Guadeloupe, Martinique, Mayotte, Réunion, Saint Martin\nBULLET::::- Germany: Büsingen am Hochrhein, Helgoland\nBULLET::::- Greece: Mount Athos\nBULLET::::- Portugal: Azores, Madeira\nBULLET::::- United Kingdom: Gibraltar\n\nIn addition to the territories above, the European Patent Convention has been extended by three member states participating in the enhanced cooperation for a unitary patent to cover some of their dependent territories outside the European Union:\n\nBULLET::::- France: French Polynesia, French Southern and Antarctic Lands, New Caledonia, Saint Barthélemy, Saint-Pierre and Miquelon, Wallis and Futuna\nBULLET::::- Netherlands: Caribbean Netherlands, Curacao, Sint Maarten\nBULLET::::- UK: Isle of Man\n\nAmong the dependencies in the second list, the Isle of Man as well as Caribbean Netherlands, Curacao and Sint Maarten intend to apply the unitary patent. Draft changes to the Dutch Patents act extends the unitary patent regulation to these territories and extends the unified patent court agreement there as well. Legislative implementation for the Isle of Man was provided in the The Patents (Isle of Man) (Amendment) Order 2017, which enters into force upon entry into force of the Unified Patent Court Agreement.\n\nThe renewal fees of the unitary patent range from 32 Euro in the second year to 4855 in the 20th year as is based on the cumulative renewal fees of Germany, France, the UK and the Netherlands, the 4 states in which most European patents are in force.\n\nTranslation requirements as well as the requirement to pay yearly patent fees in all countries in which a European patent is designated, presently renders the European patent system costly in the European Union. In an impact assessment the European Commission estimated that the costs of obtaining a patent in all 27 EU countries would drop from over 32 000 euro (mainly due to translation costs) to 6 500 euro (for the combination of an EU, Spanish and Italian patent) due to introduction of the EU patent. Per capita costs of an EU patent were estimated at just 6 euro/million in the original 25 participating countries (and 12 euro/million in the 27 EU countries for protection with an EU, Italian and Spanish patent).\n\nHow the EU Commission has presented the expected cost savings has however been sharply criticized as exaggerated and based on unrealistic assumptions. The EU Commission has notably considered the costs for validating a European patent in 27 countries while in reality only about 1% of all granted European patents are currently validated in all 27 contracting states. Based on more realistic assumptions, the cost savings are expected to be much lower than actually claimed by the Commission.\n\nWork on a Community patent started in the 1970s, but the resulting Community Patent Convention (CPC) was a failure.\n\nThe \"Luxembourg Conference on the Community Patent\" took place in 1975 and the Convention for the European Patent for the common market, or (Luxembourg) Community Patent Convention (CPC), was signed at Luxembourg on 15 December 1975, by the 9 member states of the European Economic Community at that time. However, the CPC never entered into force. It was not ratified by enough countries.\n\nFourteen years later, the Agreement relating to Community patents was made at Luxembourg on 15 December 1989. It attempted to revive the CPC project, but also failed. This Agreement consisted of an amended version of the original Community Patent Convention. Twelve states signed the Agreement: Belgium, Denmark, France, Germany, Greece, Ireland, Italy, Luxembourg, the Netherlands, Portugal, Spain, and United Kingdom. All of those states would need to have ratified the Agreement to cause it to enter into force, but only seven did so: Denmark, France, Germany, Greece, Luxembourg, the Netherlands, and United Kingdom.\n\nNevertheless, a majority of member states of the EEC at that time introduced some harmonisation into their national patent laws in anticipation of the entry in force of the CPC. A more substantive harmonisation took place at around the same time to take account of the European Patent Convention and the Strasbourg Convention.\n\nIn 2000, renewed efforts from the European Union resulted in a Community Patent Regulation proposal, sometimes abbreviated as CPR. It provides that the patent, once it has been granted by the European Patent Office (EPO) in one of its procedural languages (English, German or French) and published in that language, with a translation of the claims into the two other procedural languages, will be valid without any further translation. This proposal is aimed to achieve a considerable reduction in translation costs.\n\nNevertheless, additional translations could become necessary in legal proceedings against a suspected infringer. In such a situation, a suspected infringer who has been unable to consult the text of the patent in the official language of the Member State in which he is domiciled, is presumed, until proven otherwise, not to have knowingly infringed the patent. To protect a suspected infringer who, in such a situation, has not acted in a deliberate manner, it is provided that the proprietor of the patent will not be able to obtain damages in respect of the period prior to the translation of the patent being notified to the infringer.\n\nThe proposed Community Patent Regulation should also establish a court holding exclusive jurisdiction to invalidate issued patents; thus, a Community Patent's validity will be the same in all EU member states. This court will be attached to the present European Court of Justice and Court of First Instance through use of provisions in the Treaty of Nice.\n\nDiscussion regarding the Community patent had made clear progress in 2003 when a political agreement was reached on 3 March 2003. However, one year later in March 2004 under the Irish presidency, the Competitiveness Council failed to agree on the details of the Regulation. In particular the time delays for translating the claims and the authentic text of the claims in case of an infringement remained problematic issues throughout discussions and in the end proved insoluble.\n\nIn view of the difficulties in reaching an agreement on the community patent, other legal agreements have been proposed outside the European Union legal framework to reduce the cost of translation (of patents when granted) and litigation, namely the London Agreement, which entered into force on 1 May 2008—and which has reduced the number of countries requiring translation of European patents granted nowadays under the European Patent Convention, and the corresponding costs to obtain a European patent—and the European Patent Litigation Agreement (EPLA), a proposal that has now lapsed.\n\nAfter the council in March 2004, EU Commissioner Frits Bolkestein said that \"The failure to agree on the Community Patent I am afraid undermines the credibility of the whole enterprise to make Europe the most competitive economy in the world by 2010.\" Adding:\n\nJonathan Todd, Commission's Internal Market spokesman, declared:\nEuropean Commission President Romano Prodi, asked to evaluate his five-year term, cites as his weak point the failure of many EU governments to implement the \"Lisbon Agenda\", agreed in 2001. In particular, he cited the failure to agree on a Europewide patent, or even the languages to be used for such a patent, \"because member states did not accept a change in the rules; they were not coherent\".\n\nThere is support for the Community patent from various quarters. From the point of view of the European Commission the Community Patent is an essential step towards creating a level playing field for trade within the European Union. For smaller businesses, if the Community patent achieves its aim of providing a relatively inexpensive way of obtaining patent protection across a wide trading area, then there is also support. \n\nFor larger businesses, however, other issues come into play, which have tended to dilute overall support. In general, these businesses recognise that the current European Patent system provides the best possible protection given the need to satisfy national sovereignty requirements such as regarding translation and enforcement. The Community Patent proposal was generally supported if it would do away with both of these issues, but there was some concern about the level of competence of the proposed European Patent Court. A business would be reluctant to obtain a Europe-wide patent if it ran the risk of being revoked by an inexperienced judge. Also, the question of translations would not go away – unless the users of the system could see significant change in the position of some of the countries holding out for more of a patent specification to be translated on grant or before enforcement, it was understood that larger businesses (the bulk of the users of the patent system) would be unlikely to move away from the tried and tested European Patent.\n\nThus, in 2005, the Community patent looked unlikely to be implemented in the near future. However, on 16 January 2006 the European Commission \"launched a public consultation on how future action in patent policy to create an EU-wide system of protection can best take account of stakeholders' needs.\" The Community patent was one of the issues the consultation focused on. More than 2500 replies were received. According to the European Commission, the consultation showed that there is widespread support for the Community patent but not at any cost, and \"in particular not on the basis of the Common Political Approach reached by EU Ministers in 2003\".\n\nIn February 2007, EU Commissioner Charlie McCreevy was quoted as saying:\nThe European Commission released a white paper in April 2007 seeking to \"improve the patent system in Europe and revitalise the debate on this issue.\" On 18 April 2007, at the European Patent Forum in Munich, Germany, Günter Verheugen, Vice-President of the European Commission, said that his proposal to support the European economy was \"to have the London Agreement ratified by all member states, and to have a European patent judiciary set up, in order to achieve rapid implementation of the Community patent, which is indispensable\". He further said that he believed this could be done within five years.\n\nIn October 2007, the Portuguese presidency of the Council of the European Union proposed an EU patent jurisdiction, \"borrowing heavily from the rejected draft European Patent Litigation Agreement (EPLA)\". In November 2007, EU ministers were reported to have made some progress towards a community patent legal system, with \"some specific results\" expected in 2008.\n\nIn 2008, the idea of using machine translations to translate patents was proposed to solve the language issue, which is partially responsible for blocking progress on the community patent. Meanwhile, European Commissioner for Enterprise and Industry Günter Verheugen declared at the European Patent Forum in May 2008 that there was an \"urgent need\" for a community patent.\n\nIn December 2009, it was reported that the Swedish EU presidency had achieved a breakthrough in negotiations concerning the community patent. The breakthrough was reported to involve setting up a single patent court for the EU, however ministers conceded much work remained to be done before the community patent would become a reality.\n\nAccording to the agreed plan, the EU would accede to the European Patent Convention as a contracting state, and patents granted by the European Patent Office will, when validated for the EU, have unitary effect in the territory of the European Union. On 10 November 2010, it was announced that no agreement had been reached and that, \"in spite of the progress made, [the Competitiveness Council of the European Union had] fallen short of unanimity by a small margin,\" with commentators reporting that the Spanish representative, citing the aim to avoid any discrimination, had \"re-iterated at length the stubborn rejection of the Madrid Government of taking the 'Munich' three languages regime (English, German, French) of the European Patent Convention (EPC) as a basis for a future EU Patent.\"\n\nBULLET::::- Paris Convention for the Protection of Industrial Property\nBULLET::::- Strasbourg Convention (1963)\n\nBULLET::::- L. McDonagh, 'Exploring perspectives of the Unified Patent Court and the Unitary Patent within the Business and Legal Communities' A Report Commissioned by the Intellectual Property Office (July 2014) available at UKIPO\n\nBULLET::::- Unitary patent on the European Patent Office web site\nBULLET::::- European Commission official website (Patents, Patent reform)\nBULLET::::- Formal texts of the European Union Patent and status of adoption\nBULLET::::- Regulation 1257/2012 Implementing Enhanced co-operation in the area of the creation of unitary patent protection (published 31 December 2012)\nBULLET::::- Agreement on a Unified Patent Court (international treaty between the 25 states, not in force)\nBULLET::::- Regulation 1260/2012 implementing enhanced co-operation in the area of the creation of unitary patent protection with regard to the applicable translation arrangements (published 31 December 2012)\nBULLET::::- Rules of procedure, draft version 14 (published 31 January 2013)\nBULLET::::- Rules Relating to Unitary Patent Protection, Draft of 6 June 2014\nBULLET::::- Non implemented instruments\nBULLET::::- Amended Community Patent Convention (1989)\nBULLET::::- 1989's Agreement relating to Community patents\nBULLET::::- Implementing Regulations to the Convention for the European patent for the common market\nBULLET::::- Protocols related to Litigation, a common appeal court, its statute and modification of entry into force\nBULLET::::- Proposal for a Council Regulation on the Community patent (2000)\nBULLET::::- Proposal of the Commission (28.11.2000)\nBULLET::::- Current state of the legislation process\nBULLET::::- Positions by various organisations\nBULLET::::- April: Unitary patent: For a democratic innovation policy in Europe\nBULLET::::- EPO: on the EU patent\nBULLET::::- FFII: The Community Patent Consultation\nBULLET::::- FSFE: EU: the unitary patent\nBULLET::::- Eurolinux 2001: Appeal for a Lean and Balanced Community Patent\n"}
{"id": "6763", "url": "https://en.wikipedia.org/wiki?curid=6763", "title": "Cistron", "text": "Cistron\n\nA cistron is an alternative term to a gene. The word cistron is used to emphasize that genes exhibit a specific behavior in a cis-trans test; distinct positions (or loci) within a genome are cistronic.\n\nThe words \"cistron\" and \"gene\" were coined before the advancing state of biology made it clear that the concepts they refer to are practically equivalent. The same historical naming practices are responsible for many of the synonyms in the life sciences.\n\nThe term cistron was coined by Seymour Benzer in an article entitled \"The elementary units of heredity\". The cistron was defined by an operational test applicable to most organisms that is sometimes referred to as a cis-trans test, but more often as a complementation test.\n\nFor example, suppose a mutation at a chromosome position formula_1 is responsible for a change in recessive trait in a diploid organism (where chromosomes come in pairs). We say that the mutation is recessive because the organism will exhibit the wild type phenotype (ordinary trait) unless both chromosomes of a pair have the mutation (homozygous mutation). Similarly, suppose a mutation at another position, formula_2, is responsible for the same recessive trait. The positions formula_1 and formula_2 are said to be within the same cistron when an organism that has the mutation at formula_1 on one chromosome and has the mutation at position formula_2 on the paired chromosome exhibits the recessive trait even though the organism is not homozygous for either mutation. When instead the wild type trait is expressed, the positions are said to belong to distinct cistrons / genes. Or simply put, mutations on the same cistrons will not complement; as opposed to mutations on different cistrons may complement (see Benzer's T4 bacteriophage experiments T4 rII system).\n\nFor example, an operon is a stretch of DNA that is transcribed to create a contiguous segment of RNA, but contains more than one cistron / gene. The operon is said to be polycistronic, whereas ordinary genes are said to be monocistronic.\n"}
{"id": "6766", "url": "https://en.wikipedia.org/wiki?curid=6766", "title": "Commonwealth", "text": "Commonwealth\n\nA commonwealth is a traditional English term for a political community founded for the common good. Historically it has sometimes been synonymous with \"republic\". The noun \"commonwealth\", meaning \"public welfare general good or advantage\", dates from the 15th century. Originally a phrase (the common-wealth or the common wealth – echoed in the modern synonym \"public wealth\") it comes from the old meaning of \"wealth\", which is \"well-being\", and is itself a loose translation of the Latin res publica (republic). The term literally meant \"common well-being\". In the 17th century, the definition of \"commonwealth\" expanded from its original sense of \"public welfare\" or \"commonweal\" to mean \"a state in which the supreme power is vested in the people; a republic or democratic state\".\n\nThe term evolved to become a title to a number of political entities. Three countries – Australia, the Bahamas, and Dominica – have the official title \"Commonwealth\", as do four U.S. states and two U.S. territories. Since the early 20th century, the term has been used to name some fraternal associations of nations, most notably the Commonwealth of Nations, an organization primarily of former territories of the British Empire, which is often referred to as simply \"the Commonwealth\".\n\nTranslations of Roman writers' works to English have on occasion translated \"\"Res publica\"\", and variants thereof, to \"the commonwealth\", a term referring to the Roman state as a whole.\n\nThe Commonwealth of England was the official name of the political unit (\"de facto\" military rule in the name of parliamentary supremacy) that replaced the Kingdom of England (after the English Civil War) from 1649–53 and 1659–60, under the rule of Oliver Cromwell and his son and successor Richard. From 1653 to 1659, although still legally known as a Commonwealth, the republic, united with the former Kingdom of Scotland, operated under different institutions (at times as a \"de facto\" monarchy) and is known by historians as the Protectorate. In a British context, it is sometimes referred to as the \"Old Commonwealth\".\n\nThe Icelandic Commonwealth or the Icelandic Free State () was the state existing in Iceland between the establishment of the Althing in 930 and the pledge of fealty to the Norwegian king in 1262. It was initially established by a public consisting largely of recent immigrants from Norway who had fled the unification of that country under King Harald Fairhair.\n\nThe Commonwealth of the Philippines was the administrative body that governed the Philippines from 1935 to 1946, aside from a period of exile in the Second World War from 1942 to 1945 when Japan occupied the country. It replaced the Insular Government, a United States territorial government, and was established by the Tydings–McDuffie Act. The Commonwealth was designed as a transitional administration in preparation for the country's full achievement of independence, which was achieved in 1946. The Commonwealth of the Philippines was a founding member of the United Nations.\n\n\"Republic\" is still an alternative translation of the traditional name of the Polish–Lithuanian Commonwealth. Wincenty Kadłubek (Vincent Kadlubo, 1160–1223) used for the first time the original Latin term \"res publica\" in the context of Poland in his \"Chronicles of the Kings and Princes of Poland\". The name was used officially for the confederal country formed by Poland and Lithuania 1569–1795.\n\nIt is also often referred as \"Nobles' Commonwealth\" (1505–1795, i.e., before the union). In the contemporary political doctrine of the Polish–Lithuanian Commonwealth, \"our state is a Republic (or Commonwealth) under the presidency of the King\". The Commonwealth introduced a doctrine of religious tolerance called Warsaw Confederation, had its own parliament \"Sejm\" (although elections were restricted to nobility and elected kings, who were bound to certain contracts \"Pacta conventa\" from the beginning of the reign).\n\n\"A commonwealth of good counsaile\" was the title of the 1607 English translation of the work of Wawrzyniec Grzymała Goślicki \"De optimo senatore\" that presented to English readers many of the ideas present in the political system of the Polish–Lithuanian Commonwealth.\n\nBetween 1914 and 1925, Catalonia was an autonomous region of Spain. Its government during that time was given the title \"mancomunidad\" (Catalan: \"mancomunitat\"), which is translated into English as \"commonwealth\". The Commonwealth of Catalonia had limited powers and was formed as a federation of the four Catalan provinces. A number of Catalan-language institutions were created during its existence.\n\nBetween 1838 and 1847, Liberia was officially known as the \"Commonwealth of Liberia\". It changed its name to the \"Republic of Liberia\" when it declared independence (and adopted a new constitution) in 1847.\n\n\"Commonwealth\" was first proposed as a term for a federation of the six Australian crown colonies at the 1891 constitutional convention in Sydney. Its adoption was initially controversial, as it was associated by some with the republicanism of Oliver Cromwell (see above), but it was retained in all subsequent drafts of the constitution. The term was finally incorporated into law in the \"Commonwealth of Australia Constitution Act 1900\", which established the federation. Australia operates under a federal system, in which power is divided between the federal (national) government and the state governments (the successors of the six colonies). So, in an Australian context, the term \"Commonwealth\" (capitalized), which is often abbreviated to Cth, refers to the federal government, and \"Commonwealth of Australia\" is the official name of the country.\n\nThe Bahamas uses the official style \"Commonwealth of The Bahamas\".\n\nThe small Caribbean republic of Dominica has used the official style \"Commonwealth of Dominica\" since 1978.\n\nFour states of the United States officially designate themselves as \"commonwealths\". All four were part of Great Britain's possessions along the Atlantic coast of North America prior to the formation of the United States of America in 1776. As such, they share a strong influence of English common law in some of their laws and institutions. The four are:\nBULLET::::- Kentucky is designated a commonwealth by the Kentucky Constitution as the \"Commonwealth of Kentucky\".\nBULLET::::- Massachusetts is a commonwealth, declaring itself as such in its constitution, which states: \"[T]he body politic is formed by a voluntary association of individuals: it is a social compact, by which the whole people covenants with each citizen, and each citizen with the whole people, that all shall be governed by certain laws for the common good.\"\nBULLET::::- Pennsylvania uses the \"Commonwealth of Pennsylvania\" constitutionally and in its official title.\nBULLET::::- Virginia has been known as the \"Commonwealth of Virginia\" since before the American Revolutionary War, and is referred to as a commonwealth in its constitution.\n\nTwo organized but unincorporated U.S. territories are called commonwealths. The two are:\nBULLET::::- Commonwealth of Puerto Rico, since 1952\nBULLET::::- Commonwealth of the Northern Mariana Islands, since 1978\n\nThe Commonwealth of Nations—formerly the British Commonwealth—is a voluntary association or confederation of 53 independent sovereign states, most of which were once part of the British Empire. The Commonwealth's membership includes both republics and monarchies. The Head of the Commonwealth is Queen Elizabeth II, who also reigns as monarch directly in the 16 member states known as Commonwealth realms. \n\nThe Commonwealth of Independent States (CIS) is a loose alliance or confederation consisting of nine of the 15 former Soviet Republics, the exceptions being Turkmenistan (a CIS associate member), Lithuania, Latvia, Estonia, Ukraine, and Georgia. Georgia left the CIS in August 2008 after a clash with Russia over South Ossetia. Its creation signalled the dissolution of the Soviet Union, its purpose being to \"allow a civilised divorce\" between the Soviet Republics. The CIS has developed as a forum by which the member-states can co-operate in economics, defence, and foreign policy.\n\nLabour MP Tony Benn sponsored a \"Commonwealth of Britain Bill\" several times between 1991 and 2001, intended to abolish the monarchy and establish a British republic. It never reached second reading.\n\nBULLET::::- Confederation\nBULLET::::- Democracy\nBULLET::::- Federation\nBULLET::::- League\n\nBULLET::::- Commonwealth of Nations\nBULLET::::- The Commonwealth — UK government site\nBULLET::::- Commonwealth of Nations Secretariat*\nBULLET::::- Commonwealth Foundation\nBULLET::::- Royal Commonwealth Society\nBULLET::::- Commonwealth of Independent States\nBULLET::::- CIS Executive Committee\nBULLET::::- CIS Statistical Committee\nBULLET::::- Countries\nBULLET::::- Commonwealth of Australia\nBULLET::::- United States\nBULLET::::- Commonwealth of Kentucky\nBULLET::::- Commonwealth of Massachusetts\nBULLET::::- Commonwealth of Pennsylvania\nBULLET::::- Commonwealth of Virginia\nBULLET::::- Commonwealth of Puerto Rico\nBULLET::::- Commonwealth of the Northern Mariana Islands\nBULLET::::- Polish-Lithuanian Commonwealth\nBULLET::::- Commonwealth of Diverse Cultures: Poland's Heritage\nBULLET::::- Commonwealth New\nBULLET::::- The Commonwealth Secretariat\nBULLET::::- Commonwealth News\nBULLET::::- Commonwealth News\n"}
{"id": "6767", "url": "https://en.wikipedia.org/wiki?curid=6767", "title": "Commodore 1541", "text": "Commodore 1541\n\nThe Commodore 1541 (also known as the CBM 1541 and VIC-1541) is a floppy disk drive which was made by Commodore International for the Commodore 64 (C64), Commodore's most popular home computer. The best-known floppy disk drive for the C64, the 1541 is a single-sided 170-kilobyte drive for 5¼\" disks. The 1541 directly followed the Commodore 1540 (meant for the VIC-20).\n\nThe disk drive uses group coded recording (GCR) and contains a MOS Technology 6502 microprocessor, doubling as a disk controller and on-board disk operating system processor. The number of sectors per track varies from 17 to 21 (an early implementation of zone bit recording). The drive's built-in disk operating system is CBM DOS 2.6.\n\nThe 1541 was priced at under at its introduction. A C64 plus a 1541 cost about $900, while an Apple II with no disk drive cost $1295. The first 1541 drives produced in 1982 have a label on the front reading VIC-1541 and have an off-white case to match the VIC-20. In 1983, the 1541 was switched to having the familiar beige case and a front label reading simply \"1541\" along with rainbow stripes to match the Commodore 64.\n\nBy 1983 a 1541 sold for $300 or less. After a brutal home-computer price war that Commodore began, the C64 and 1541 together cost under $500. The drive became very popular, and became difficult to find. The company claimed that the shortage occurred because 90% of C64 owners bought the 1541 compared to its 30% expectation, but the press discussed what \"Creative Computing\" described as \"an absolutely alarming return rate\" because of defects. The magazine reported in March 1984 that it received three defective drives in two weeks, and \"Compute!'s Gazette\" reported in December 1983 that four of the magazine's seven drives had failed; \"COMPUTE! Publications sorely needs additional 1541s for in-house use, yet we can't find any to buy. After numerous phone calls over several days, we were able to locate only two units in the entire continental United States\", reportedly because of Commodore's attempt to resolve a manufacturing issue that caused the high failures.\n\nThe early (1982 to 1983) 1541s have a spring-eject mechanism (Alps drive), and the disks often fail to release. This style of drive has the popular nickname \"Toaster Drive\", because it requires the use of a knife or other hard thin object to pry out the stuck media just like a piece of toast stuck in an actual toaster (though this is inadvisable with actual toasters). This was fixed later when Commodore changed the vendor of the drive mechanism (Mitsumi) and adopted the flip-lever Newtronics mechanism, greatly improving reliability. In addition, Commodore made the drive's controller board smaller and reduced its chip count compared to the early 1541s (which had a large PCB running the length of the case, with dozens of TTL chips). The beige-case Newtronics 1541 was produced from 1984 to 1986.\n\nAll but the very earliest non-II model 1541s can use either the Alps or Newtronics mechanism. Visually, the first models, of the \"VIC-1541\" denomination, have an off-white color like the VIC-20 and VIC-1540. Then, to match the look of the C64, CBM changed the drive's color to brown-beige and the name to \"Commodore 1541\".\n\nThe 1541's numerous shortcomings opened a market for a number of third-party clones of the disk drive, a situation that continued for the lifetime of the C64. Well-known clones are the \"Oceanic OC-118\" a.k.a. \"Excelerator+\", the MSD Super Disk single and dual drives, the \"Enhancer 2000\", the \"Indus GT\", and \"CMD\" 's \"FD-2000\" and \"FD-4000\". Nevertheless, the 1541 became the first disk drive to see widespread use in the home and Commodore sold millions of the units.\n\nIn 1986, Commodore released the 1541C, a revised version that offered quieter and slightly more reliable operation and a light beige case matching the color scheme of the Commodore 64C. It was replaced in 1988 by the 1541-II, which uses an external power supply to provide cooler operation and allows the drive to have a smaller desktop footprint (the power supply \"brick\" being placed elsewhere, typically on the floor). Later ROM revisions fixed assorted problems, including a software bug that caused the save-and-replace command to corrupt data.\n\nThe Commodore 1570 is an upgrade from the 1541 for use with the Commodore 128, available in Europe. It offers MFM capability for accessing CP/M disks, improved speed, and somewhat quieter operation, but was only manufactured until Commodore got its production lines going with the 1571, the double-sided drive. Finally, the small, external-power-supply-based, MFM-based Commodore 1581 3½\" drive was made, giving 800 KB access to the C128 and C64.\n\nThe 1541 does not have DIP switches to change the device number. If a user added more than one drive to a system the user had to open the case and cut a trace in the circuit board to permanently change the drive's device number, or hand-wire an external switch to allow it to be changed externally. It was also possible to change the drive number via a software command, which was temporary and would be erased as soon as the drive was powered off.\n\n1541 drives at power up always default to device #8. If multiple drives in a chain are used, then the startup procedure is to power on the first drive in the chain, alter its device number via a software command to the highest number in the chain (if three drives were used, then the first drive in the chain would be set to device #10), then power on the next drive, alter its device number to the next lowest, and repeat the procedure until the final drive at the end of the chain was powered on and left as device #8.\n\nUnlike the Apple II, where support for two drives was normal, it was relatively uncommon for Commodore software to support this setup, and the CBM DOS copy file command was not even able to copy files between drives--a third party copy utility must be used instead.\n\nThe pre-II 1541s also have an internal power source, which generate much heat. The heat generation was a frequent source of humour. For example, \"Compute!\" stated in 1988 that \"Commodore 64s used to be a favorite with amateur and professional chefs since they could compute and cook on top of their 1500-series disk drives at the same time\". A series of humorous tips in \"MikroBitti\" in 1989 said \"When programming late, coffee and kebab keep nicely warm on top of the 1541.\" The \"MikroBitti\" review of the 1541-II said that its external power source \"should end the jokes about toasters\".\n\nThe drive-head mechanism installed in the early production years is notoriously easy to misalign. The most common cause of the 1541's drive head knocking and subsequent misalignment is copy-protection schemes on commercial software. The main cause of the problem is that the disk drive itself does not feature any means of detecting when the read/write head reaches track zero. Accordingly, when a disk is not formatted or a disk error occurs, the unit tries to move the head 40 times in the direction of track zero (although the 1541 DOS only uses 35 tracks, the drive mechanism itself is a 40-track unit, so this ensured track zero would be reached no matter where the head was before). Once track zero is reached, every further attempt to move the head in that direction would cause it to be rammed against a solid stop: for example, if the head happened to be on track 18 (where the directory is located) before this procedure, the head would be actually moved 18 times, and then rammed against the stop 22 times. This ramming gives the characteristic \"machine gun\" noise and sooner or later throws the head out of alignment.\n\nA defective head-alignment part likely caused many of the reliability issues in early 1541 drives; one dealer told \"Compute!s Gazette\" in 1983 that the part had caused all but three of several hundred drive failures that he had repaired. The drives were so unreliable that \"Info\" magazine joked, \"Sometimes it seems as if one of the original design specs ... must have said 'Mean time between failure: 10 accesses.'\". Users can realign the drive themselves with a software program and a calibration disk. What the user would do is remove the drive from its case and then loosen the screws holding the stepper motor that moved the head, then with the calibration disk in the drive gently turn the stepper motor back and forth until the program shows a good alignment. The screws are then tightened and the drive is put back into its case.\n\nA third-party fix for the 1541 appeared in which the solid head stop was replaced by a sprung stop, giving the head a much easier life. The later 1571 drive (which is 1541-compatible) incorporates track-zero detection by photo-interrupter and is thus immune to the problem. Also, a software solution, which resides in the drive controller's ROM, prevents the rereads from occurring, though this could cause problems when genuine errors did occur.\n\nDue to the alignment issues on the Alps drive mechanisms, Commodore switched suppliers to Newtronics in 1984. The Newtronics mechanism drives have a lever rather than a pull-down tab to close the drive door. Although the alignment issues were resolved after the switch, the Newtronics drives added a new reliability problem in that many of the read/write heads were improperly sealed, causing moisture to penetrate the head and short it out.\n\nThe 1541's PCB consists mainly of a 6502 CPU, two 6522 chips, and 2k of work RAM. Up to 48k of RAM can be added; this was mainly useful for defeating copy protection schemes since an entire disk track could be loaded into drive RAM, while the standard 2k only accommodated a few sectors (theoretically eight, but some of the RAM was used by CBM DOS as work space). Some Commodore users used 1541s with expanded RAM as an impromptu math coprocessor by uploading math-intensive code to the drive for background processing.\n\nThe 1541 uses a proprietary serialized derivative of the IEEE-488 parallel interface, which Commodore used on their previous disk drives for the PET/CBM range of personal and business computers, but when the VIC-20 was in development, a cheaper alternative to the expensive IEE-488 cables was sought. To ensure a ready supply of inexpensive cabling for its home computer peripherals, Commodore chose standard DIN connectors for the serial interface. Disk drives and other peripherals such as printers connected to the computer via a daisy chain setup, necessitating only a single connector on the computer itself.\n\n\"IEEE Spectrum\" in 1985 stated that:\n\nThe C-64's designers blamed the 1541's slow speed on the marketing department's insistence that the computer be compatible with the 1540, which was slow because of a flaw in the 6522 VIA interface controller. Initially, Commodore intended to use a hardware shift register (one component of the 6522) to maintain fast drive speeds with the new serial interface. However, a hardware bug with this chip prevented the initial design from working as anticipated, and the ROM code was hastily rewritten to handle the entire operation in software. According to Jim Butterfield, this causes a speed reduction by a factor of five; had 1540 compatibility not been a requirement, the disk interface would have been much faster. In any case, the C64 normally could not work with a 1540 unless the VIC-II video output was disabled via a register write, which would increase the CPU speed enough to be able to access it.\n\nAs implemented on the VIC-20 and C64, Commodore DOS transfers 300 bytes per second, compared to the Atari 810's 2,400 bytes per second, the Apple Disk II's 15,000 bytes per second, and the 300-baud data rate of the Commodore Datasette storage system. About 20 minutes are needed to copy one disk—10 minutes of reading time, and 10 minutes of writing time. However, since both the computer and the drive can easily be reprogrammed, third parties quickly wrote more efficient firmware that would speed up drive operations drastically. Without hardware modifications, some \"fast loader\" utilities (which replaced 1541's onboard ROM) managed to achieve speeds of up to 4 kB/s. The most common of these products are the Epyx FastLoad, the Final Cartridge, and the Action Replay plug-in ROM cartridges, which all have machine code monitor and disk editor software on board as well. The popular Commodore computer magazines of the era also entered the arena with type-in fast-load utilities, with \"Compute!'s Gazette\" publishing \"TurboDisk\" in 1985 and \"RUN\" publishing \"Sizzle\" in 1987.\n\nEven though each 1541 has its own on-board disk controller and disk operating system, it is not normally possible for a user to command two 1541 drives to copy a disk (one drive reading and the other writing) as with older dual drives like the 4040 and 8050 that were often found with the PET computer, and which the 1541 is backward-compatible with (it can read 4040 disks but not write to them since its internal operating system is similar enough for reading but not for writing). Unfortunately, however, the routines in the previous disk operating system to enable disk copying were removed for the 1541 as it was intended to be a stand-alone unit. Originally, to copy from drive to drive, software running on the C64 was needed and it would first read from one drive into computer memory, then write out to the other. Only later when first, Fast Hack'em, then other disk backup programs, were released, was true drive-to-drive copying possible for a pair of 1541s. The user could then unplug the C64 from the drives (i.e. from the first drive in the daisy chain) and do something else with the computer as the drives proceeded to copy the entire disk. This is not a recommended practice as disconnecting the serial lead from a powered drive and/or computer can result in destruction of one or both of the port chips in the disk drive.\n\nThe 1541 drive uses standard 5.25\" double-density floppy media; high-density media will not work due to its different magnetic coating requiring a higher magnetic coercivity. As the GCR encoding scheme does not use the index hole, the drive was also compatible with hard-sectored disks. The standard CBM DOS format is 170k with 35 tracks and 256-byte sectors. It is similar to the format used on the PET 4040 drives, but a minor difference in a header byte makes the 4040 and 1541 only read-compatible; disks formatted with one drive cannot be written to by the other.\n\nThe 4040 drives used Shugart SA-400s, which were 35-track units, thus the format there was due to physical limitations of the drive mechanism. The 1541 used 40 track mechanisms, but Commodore intentionally limited the CBM DOS format to 35 tracks because of reliability issues with the early units. It was possible via low-level programming to move the drive head to tracks 36-40 and write on them, this was sometimes done by commercial software for copy protection purposes or to get additional data on the disk.\n\nHowever, one track is reserved by DOS for directory and file allocation information (the BAM, block availability map). And since for normal files, two bytes of each physical sector are used by DOS as a pointer to the next physical track and sector of the file, only 254 out of the 256 bytes of a block are used for file contents.\n\nIf the disk side was not otherwise prepared with a custom format, (e.g. for data disks), 664 blocks would be free after formatting, giving 664 × 254 = 168,656 bytes (or almost 165 kB) for user data.\n\nBy using custom formatting and load/save routines (sometimes included in third-party DOSes, see below), all of the mechanically possible 40 tracks can be used.\n\nOwing to the drive's non-use of the index hole, it was also possible to make \"flippy\" disks by inserting the diskette upside-down and formatting the other side, and it was commonplace and normal for commercial software to be distributed on such disks.\n\n! Track !! Sectors(256 bytes) !! bits/s\nalign=center 21  16M/4/(13+0) = 307,692\nalign=center 19  16M/4/(13+1) = 285,714\nalign=center 18  16M/4/(13+2) = 266,667\nalign=center 17  16M/4/(13+3) = 250,000\nalign=center 17  16M/4/(13+3) = 250,000\n\nTracks 36-42 are non-standard. The bitrate is the raw one between the read/write head and signal circuitry so actual useful data rate is a factor 5/4 less due to GCR encoding.\n\nThe 1541 disk typically has 35 tracks. Track 18 is reserved; the remaining tracks are available for data storage. The header is on 18/0 (track 18, sector 0) along with the BAM (block availability map), and the directory starts on 18/1 (track 18, sector 1). The file interleave is 10 blocks, while the directory interleave is 3 blocks.\n\nHeader contents: The header is similar to other Commodore disk headers, the structural differences being the BAM offset ($04) and size, and the label+ID+type offset ($90).\n\nEarly copy protection schemes deliberately introduced read errors on the disk, the software refusing to load unless the correct error message is returned. The general idea was that simple disk-copy programs are incapable of copying the errors. When one of these errors is encountered, the disk drive (as do many floppy disk drives) will attempt one or more reread attempts after first resetting the head to track zero. Few of these schemes had much deterrent effect, as various software companies soon released \"nibbler\" utilities that enabled protected disks to be copied and, in some cases, the protection removed.\n\nCommodore copy protection sometimes depends on specific hardware configurations. \"Gunship\", for example, does not load if a second disk drive or printer is connected to the computer.\n\nBULLET::::- Commodore 64\nBULLET::::- Commodore 64 peripherals\nBULLET::::- 1541 Ultimate\n\nBULLET::::- CBM (1982). \"VIC-1541 Single Drive Floppy Disk User's Manual\". 2nd ed. Commodore Business Machines, Inc. P/N 1540031-02.\nBULLET::::- Neufeld, Gerald G. (1985). \"1541 User's Guide. The Complete Guide to Commodore's 1541 Disk Drive\". Second Printing, June 1985. 413 pp. Copyright © 1984 by DATAMOST, Inc. (Brady). .\nBULLET::::- Immers, Richard; Neufeld, Gerald G. (1984). \"Inside Commodore DOS. The Complete Guide to the 1541 Disk Operating System.\" DATAMOST, Inc & Reston Publishing Company, Inc. (Prentice-Hall). .\nBULLET::::- Englisch, Lothar; Szczepanowski, Norbert (1984). \"The Anatomy of the 1541 Disk Drive\". Grand Rapids, MI: Abacus Software (translated from the original 1983 German edition, Düsseldorf: Data Becker GmbH). .\nBULLET::::- Disk Preservation Project: internal drive mechanics and copy protection\nBULLET::::- Undocumented 1541 drive functions from the Project 64 website\nBULLET::::- RUN Magazine Issue 64\nBULLET::::- devili.iki.fi: Beyond the 1541, Mass Storage For The 64 And 128, COMPUTE!'s Gazette, issue 32, February 1986 (market overview)\nBULLET::::- 1541 Maintenance Guide from Bitsavers\n"}
{"id": "6769", "url": "https://en.wikipedia.org/wiki?curid=6769", "title": "Commodore 1581", "text": "Commodore 1581\n\nThe Commodore 1581 is a 3½-inch double-sided double-density floppy disk drive that was released by Commodore Business Machines (CBM) in 1987, primarily for its C64 and C128 home/personal computers. The drive stores 800 kilobytes using an MFM encoding but formats different from the MS-DOS (720 kB), Amiga (880 kB), and Mac Plus (800 kB) formats. With special software it's possible to read C1581 disks on an x86 PC system, and likewise, read MS-DOS and other formats of disks in the C1581 (using Big Blue Reader), provided that the PC or other floppy handles the size format. This capability was most frequently used to read MS-DOS disks. The drive was released in the summer of 1987 and quickly became popular with bulletin board system (BBS) operators and other users.\n\nLike the 1541 and 1571, the 1581 has an onboard MOS Technology 6502 CPU with its own ROM and RAM, and uses a serial version of the IEEE-488 interface. Inexplicably, the drive's ROM contains commands for parallel use, although no parallel interface was available. Unlike the 1571, which is nearly 100% backward-compatible with the 1541, the 1581 is only compatible with previous Commodore drives at the DOS level and cannot utilize software that performs low-level disk access (as the vast majority of Commodore 64 games do).\n\nThe version of Commodore DOS built into the 1581 added support for partitions, which could also function as fixed-allocation subdirectories. PC-style subdirectories were rejected as being too difficult to work with in terms of block availability maps, then still much in vogue, and which for some time had been the traditional way of inquiring into block availability. The 1581 supports the C128's burst mode for fast disk access, but not when connected to an older Commodore machine like the Commodore 64. The 1581 provides a total of 3160 blocks free when formatted (a block being equal to 256 bytes). The number of permitted directory entries was also increased, to 296 entries. With a storage capacity of 800 kB, the 1581 is the highest-capacity serial-bus drive that was ever made by Commodore (the 1-MB SFD-1001 uses the parallel IEEE-488), and the only 3½\" one. However, starting in 1991, Creative Micro Designs (CMD) made the FD-2000 high density (1.6 MB) and FD-4000 extra-high density (3.2 MB) 3½\" drives, both of which offered not only a 1581-emulation mode but also 1541- and 1571-compatibility modes.\n\nLike the 1541 and 1571, a nearly identical job queue is available to the user in zero page (except for job 0), providing for exceptional degrees of compatibility.\n\nUnlike the cases of the 1541 and 1571, the low-level disk format used by the 1581 is similar enough to the MS-DOS format as the 1581 is built around a WD1770 FM/MFM floppy controller chip. The 1581 disk format consists of 80 tracks and ten 512 byte sectors per track, used as 20 logical sectors of 256 bytes each. Special software is required to read 1581 disks on a PC due to the different file system. An internal floppy drive and controller are required as well; USB floppy drives operate strictly at the file system level and do not allow low-level disk access. The WD1770 controller chip, however, was the seat of some early problems with 1581 drives when the first production runs were recalled due to a high failure rate; the problem was quickly corrected. Later versions of the 1581 drive have a smaller, more streamlined-looking external power supply provided with them.\n\n! Quantity !! Value\n\nThe 1581 disk has 80 logical tracks, each with 40 logical sectors (the actual physical layout of the diskette is abstracted and managed by a hardware translation layer). The directory starts on 40/3 (track 40, sector 3). The disk header is on 40/0, and the BAM (block availability map) resides on 40/1 and 40/2.\n\nHeader Contents\n\nBAM Contents, 40/1\n\nBAM Contents, 40/2\n\nBULLET::::- Commodore 64 peripherals\nBULLET::::- Commodore 128\n\nBULLET::::- d81.de: Permanent home of 1581-Copy, A MS-Windows based Tool uses any standard x86-PC 3.5\" drive to WRITE & READ 1581 disk images (d81).\nBULLET::::- optusnet.com.au: 1581 Games, Commodore 1581 Games, D81 , CMD FD2000 & FD4000 Games, Tools & Games specifically for the 1581 disk drive.\nBULLET::::- optusnet.com.au: SEGA SF-7000 with PC 3.5\" Floppy Drive, Copy disk to PC and vice versa, How to use a PC 3.5\" floppy drive in the 1581 device\nBULLET::::- vice-emu: Commodore compatible Disk Drives, drive info\nBULLET::::- tut.fi: DCN-2692 floppy controller board, C1581 clone (complete)\n"}
{"id": "6771", "url": "https://en.wikipedia.org/wiki?curid=6771", "title": "College football", "text": "College football\n\nCollege football is gridiron football consisting of American football played by teams of student athletes fielded by American universities, colleges, and military academies, or Canadian football played by teams of student athletes fielded by Canadian universities. It was through college football play that American football rules first gained popularity in the United States.\n\nUnlike most other sports in North America, no minor league farm organizations exist in American or Canadian football. Therefore, college football is generally considered to be the second tier of American football in the United States and Canadian football in Canada; one step ahead of high school competition, and one step below professional competition. However, in some areas of the country, college football is more popular than professional football, and for much of the early 20th century, college football was seen as more prestigious than professional football.\n\nIt is in college football where a player's performance directly impacts his chances of playing professional football. The best collegiate players will typically declare for the professional draft after three to four years of collegiate competition, with the NFL holding its annual draft every spring in which 256 players are selected annually. Those not selected can still attempt to land an NFL roster spot as an undrafted free agent.\n\nEven after the emergence of the professional National Football League (NFL), college football remained extremely popular throughout the U.S. \nAlthough the college game has a much larger margin for talent than its pro counterpart, the sheer number of fans following major colleges provides a financial equalizer for the game, with Division I programs — the highest level — playing in huge stadiums, six of which have seating capacity exceeding 100,000 people. In many cases, college stadiums employ bench-style seating, as opposed to individual seats with backs and arm rests (although many stadiums do have a small number of chairback seats in addition to the bench seating). This allows them to seat more fans in a given amount of space than the typical professional stadium, which tends to have more features and comforts for fans. (Only three stadiums owned by U.S. colleges or universities — Cardinal Stadium at the University of Louisville, Georgia State Stadium at Georgia State University and FAU Stadium at Florida Atlantic University — consist entirely of chairback seating).\n\nCollege athletes, unlike players in the NFL, are not permitted by the NCAA to be paid salaries. Colleges are only allowed to provide non-monetary compensation such as athletic scholarships that provide for tuition, housing, and books.\n\nModern North American football has its origins in various games, all known as \"football\", played at public schools in Great Britain in the mid-19th century. By the 1840s, students at Rugby School were playing a game in which players were able to pick up the ball and run with it, a sport later known as rugby football. The game was taken to Canada by British soldiers stationed there and was soon being played at Canadian colleges.\n\nThe first documented gridiron football match was played at University College, a college of the University of Toronto, November 9, 1861. One of the participants in the game involving University of Toronto students was (Sir) William Mulock, later Chancellor of the school. A football club was formed at the university soon afterward, although its rules of play at this stage are unclear.\n\nIn 1864, at Trinity College, also a college of the University of Toronto, F. Barlow Cumberland and Frederick A. Bethune devised rules based on rugby football. Modern Canadian football is widely regarded as having originated with a game played in Montreal, in 1865, when British Army officers played local civilians. The game gradually gained a following, and the Montreal Football Club was formed in 1868, the first recorded non-university football club in Canada.\n\nEarly games appear to have had much in common with the traditional \"mob football\" played in Great Britain. The games remained largely unorganized until the 19th century, when intramural games of football began to be played on college campuses. Each school played its own variety of football. Princeton University students played a game called \"ballown\" as early as 1820. A Harvard tradition known as \"Bloody Monday\" began in 1827, which consisted of a mass ballgame between the freshman and sophomore classes. In 1860, both the town police and the college authorities agreed the Bloody Monday had to go. The Harvard students responded by going into mourning for a mock figure called \"Football Fightum\", for whom they conducted funeral rites. The authorities held firm and it was a dozen years before football was once again played at Harvard. Dartmouth played its own version called \"Old division football\", the rules of which were first published in 1871, though the game dates to at least the 1830s. All of these games, and others, shared certain commonalities. They remained largely \"mob\" style games, with huge numbers of players attempting to advance the ball into a goal area, often by any means necessary. Rules were simple, violence and injury were common. The violence of these mob-style games led to widespread protests and a decision to abandon them. Yale, under pressure from the city of New Haven, banned the play of all forms of football in 1860.\n\nAmerican football historian Parke H. Davis described the period between 1869 and 1875 as the 'Pioneer Period'; the years 1876–93 he called the 'Period of the American Intercollegiate Football Association'; and the years 1894–1933 he dubbed the 'Period of Rules Committees and Conferences'.\n\nOn November 6, 1869, Rutgers University faced Princeton University (then known as the College of New Jersey) in the first-ever game of intercollegiate football that resembled more the game of soccer than \"football\" as it is played today. It was played with a round ball and, like all early games, used a set of rules suggested by Rutgers captain William J. Leggett, based on The Football Association's first set of rules, which were an early attempt by the former pupils of England's public schools, to unify the rules of their public schools games and create a universal and standardized set of rules for the game of football and bore little resemblance to the American game which would be developed in the following decades. It is still usually regarded as the first game of college football. The game was played at a Rutgers field. Two teams of 25 players attempted to score by kicking the ball into the opposing team's goal. Throwing or carrying the ball was not allowed, but there was plenty of physical contact between players. The first team to reach six goals was declared the winner. Rutgers won by a score of six to four. A rematch was played at Princeton a week later under Princeton's own set of rules (one notable difference was the awarding of a \"free kick\" to any player that caught the ball on the fly, which was a feature adopted from The Football Association's rules; the fair catch kick rule has survived through to modern American game). Princeton won that game by a score of 8 – 0. Columbia joined the series in 1870, and by 1872 several schools were fielding intercollegiate teams, including Yale and Stevens Institute of Technology.\n\nColumbia University was the third school to field a team. The Lions traveled from New York City to New Brunswick on November 12, 1870, and were defeated by Rutgers 6 to 3. The game suffered from disorganization and the players kicked and battled each other as much as the ball. Later in 1870, Princeton and Rutgers played again with Princeton defeating Rutgers 6-0. This game's violence caused such an outcry that no games at all were played in 1871. Football came back in 1872, when Columbia played Yale for the first time. The Yale team was coached and captained by David Schley Schaff, who had learned to play football while attending Rugby School. Schaff himself was injured and unable to the play the game, but Yale won the game 3-0 nonetheless. Later in 1872, Stevens Tech became the fifth school to field a team. Stevens lost to Columbia, but beat both New York University and City College of New York during the following year.\n\nBy 1873, the college students playing football had made significant efforts to standardize their fledgling game. Teams had been scaled down from 25 players to 20. The only way to score was still to bat or kick the ball through the opposing team's goal, and the game was played in two 45 minute halves on fields 140 yards long and 70 yards wide. On October 20, 1873, representatives from Yale, Columbia, Princeton, and Rutgers met at the Fifth Avenue Hotel in New York City to codify the first set of intercollegiate football rules. Before this meeting, each school had its own set of rules and games were usually played using the home team's own particular code. At this meeting, a list of rules, based more on the Football Association's rules than the rules of the recently founded Rugby Football Union, was drawn up for intercollegiate football games.\n\nOld \"Football Fightum\" had been resurrected at Harvard in 1872, when Harvard resumed playing football. Harvard, however, preferred to play a rougher version of football called \"the Boston Game\" in which the kicking of a round ball was the most prominent feature though a player could run with the ball, pass it, or dribble it (known as \"babying\"). The man with the ball could be tackled, although hitting, tripping, \"hacking\" (shin-kicking) and other unnecessary roughness was prohibited. There was no limit to the number of players, but there were typically ten to fifteen per side. A player could carry the ball only when being pursued.\n\nAs a result of this, Harvard refused to attend the rules conference organized by Rutgers, Princeton and Columbia at the Fifth Avenue Hotel in New York City on October 20, 1873 to agree on a set of rules and regulations that would allow them to play a form of football that was essentially Association football; and continued to play under its own code. While Harvard's voluntary absence from the meeting made it hard for them to schedule games against other American universities, it agreed to a challenge to play the rugby team of McGill University, from Montreal, in a two-game series. It was agreed that two games would be played on Harvard's Jarvis baseball field in Cambridge, Massachusetts on May 14 and 15, 1874: one to be played under Harvard rules, another under the stricter rugby regulations of McGill. Jarvis Field was at the time a patch of land at the northern point of the Harvard campus, bordered by Everett and Jarvis Streets to the north and south, and Oxford Street and Massachusetts Avenue to the east and west. Harvard beat McGill in the \"Boston Game\" on the Thursday and held McGill to a 0-0 tie on the Friday. The Harvard students took to the rugby rules and adopted them as their own, The games featured a round ball instead of a rugby-style oblong ball. This series of games represents an important milestone in the development of the modern game of American football. In October 1874, the Harvard team once again traveled to Montreal to play McGill in rugby, where they won by three tries.\n\nInasmuch as Rugby football had been transplanted to Canada from England, the McGill team played under a set of rules which allowed a player to pick up the ball and run with it whenever he wished. Another rule, unique to McGill, was to count tries (the act of grounding the football past the opposing team's goal line; it is important to note that there was no end zone during this time), as well as goals, in the scoring. In the Rugby rules of the time, a try only provided the attempt to kick a free goal from the field. If the kick was missed, the try did not score any points itself.\n\nHarvard quickly took a liking to the rugby game, and its use of the try which, until that time, was not used in American football. The try would later evolve into the score known as the touchdown. On June 4, 1875, Harvard faced Tufts University in the first game between two American colleges played under rules similar to the McGill/Harvard contest, which was won by Tufts. The rules included each side fielding 11 men at any given time, the ball was advanced by kicking or carrying it, and tackles of the ball carrier stopped play - - actions of which have carried over to the modern version of football played today \n\nHarvard later challenged its closest rival, Yale, to which the Bulldogs accepted. The two teams agreed to play under a set of rules called the \"Concessionary Rules\", which involved Harvard conceding something to Yale's soccer and Yale conceding a great deal to Harvard's rugby. They decided to play with 15 players on each team. On November 13, 1875, Yale and Harvard played each other for the first time ever, where Harvard won 4-0. At the first The Game (as the annual contest between Harvard and Yale came to be named) the future \"father of American football\" Walter Camp was among the 2000 spectators in attendance. Walter, who would enroll at Yale the next year, was torn between an admiration for Harvard's style of play and the misery of the Yale defeat, and became determined to avenge Yale's defeat. Spectators from Princeton also carried the game back home, where it quickly became the most popular version of football.\n\nOn November 23, 1876, representatives from Harvard, Yale, Princeton, and Columbia met at the Massasoit House in Springfield, Massachusetts to standardize a new code of rules based on the rugby game first introduced to Harvard by McGill University in 1874. Three of the schools—Harvard, Columbia, and Princeton—formed the Intercollegiate Football Association, as a result of the meeting. Yale initially refused to join this association because of a disagreement over the number of players to be allowed per team (relenting in 1879) and Rutgers were not invited to the meeting. The rules that they agreed upon were essentially those of rugby union at the time with the exception that points be awarded for scoring a try, not just the conversion afterwards (extra point). Incidentally, rugby was to make a similar change to its scoring system 10 years later.\nWalter Camp is widely considered to be the most important figure in the development of American football. As a youth, he excelled in sports like track, baseball, and association football, and after enrolling at Yale in 1876, he earned varsity honors in every sport the school offered.\n\nFollowing the introduction of rugby-style rules to American football, Camp became a fixture at the Massasoit House conventions where rules were debated and changed. Dissatisfied with what seemed to him to be a disorganized mob, he proposed his first rule change at the first meeting he attended in 1878: a reduction from fifteen players to eleven. The motion was rejected at that time but passed in 1880. The effect was to open up the game and emphasize speed over strength. Camp's most famous change, the establishment of the line of scrimmage and the snap from center to quarterback, was also passed in 1880. Originally, the snap was executed with the foot of the center. Later changes made it possible to snap the ball with the hands, either through the air or by a direct hand-to-hand pass. Rugby league followed Camp's example, and in 1906 introduced the play-the-ball rule, which greatly resembled Camp's early scrimmage and center-snap rules. In 1966, rugby league introduced a four-tackle rule (changed in 1972 to a six-tackle rule) based on Camp's early down-and-distance rules.\n\nCamp's new scrimmage rules revolutionized the game, though not always as intended. Princeton, in particular, used scrimmage play to slow the game, making incremental progress towards the end zone during each down. Rather than increase scoring, which had been Camp's original intent, the rule was exploited to maintain control of the ball for the entire game, resulting in slow, unexciting contests. At the 1882 rules meeting, Camp proposed that a team be required to advance the ball a minimum of five yards within three downs. These down-and-distance rules, combined with the establishment of the line of scrimmage, transformed the game from a variation of rugby football into the distinct sport of American football.\n\nCamp was central to several more significant rule changes that came to define American football. In 1881, the field was reduced in size to its modern dimensions of 120 by 53 yards (109.7 by 48.8 meters). Several times in 1883, Camp tinkered with the scoring rules, finally arriving at four points for a touchdown, two points for kicks after touchdowns, two points for safeties, and five for field goals. Camp's innovations in the area of point scoring influenced rugby union's move to point scoring in 1890. In 1887, game time was set at two halves of 45 minutes each. Also in 1887, two paid officials—a referee and an umpire—were mandated for each game. A year later, the rules were changed to allow tackling below the waist, and in 1889, the officials were given whistles and stopwatches.\n\nAfter leaving Yale in 1882, Camp was employed by the New Haven Clock Company until his death in 1925. Though no longer a player, he remained a fixture at annual rules meetings for most of his life, and he personally selected an annual All-American team every year from 1889 through 1924. The Walter Camp Football Foundation continues to select All-American teams in his honor.\n+Historical college football scoring\n! Era !! Touchdown !! Field goal !! Conversion (kick) !! Conversion (touchdown)!! Safety !! Conversion safety !! Defensive conversion\n\nCollege football expanded greatly during the last two decades of the 19th century. Several major rivalries date from this time period.\n\nNovember 1890 was an active time in the sport. In Baldwin City, Kansas, on November 22, 1890, college football was first played in the state of Kansas. Baker beat Kansas 22–9. On the 27th, Vanderbilt played Nashville (Peabody) at Athletic Park and won 40–0. It was the first time organized football played in the state of Tennessee. The 29th also saw the first instance of the Army–Navy Game. Navy won 24–0.\n\nRutgers was first to extend the reach of the game. An intercollegiate game was first played in the state of New York when Rutgers played Columbia on November 2, 1872. It was also the first scoreless tie in the history of the fledgling sport. Yale football starts the same year and has its first match against Columbia, the nearest college to play football. It took place at Hamilton Park in New Haven and was the first game in New England. The game was essentially soccer with 20-man sides, played on a field 400 by 250 feet. Yale wins 3-0, Tommy Sherman scoring the first goal and Lew Irwin the other two.\n\nAfter the first game against Harvard, Tufts took its squad to Bates College in Lewiston, Maine for the first football game played in Maine. This occurred on November 6, 1875.\n\nPenn's Athletic Association was looking to pick \"a twenty\" to play a game of football against Columbia. This \"twenty\" never played Columbia, but did play twice against Princeton. Princeton won both games 6 to 0. The first of these happened on November 11, 1876, in Philadelphia and was the first intercollegiate game in the state of Pennsylvania.\n\nBrown enters the intercollegiate game in 1878.\n\nThe first game where one team scored over 100 points happened on October 25, 1884, when Yale routed Dartmouth 113–0. It was also the first time one team scored over 100 points and the opposing team was shut out. The next week, Princeton outscored Lafayette by 140 to 0.\n\nThe first intercollegiate game in the state of Vermont happened on November 6, 1886, between Dartmouth and Vermont at Burlington, Vermont. Dartmouth won 91 to 0.\n\nPenn State played its first season in 1887, but had no head coach for their first five years, from 1887–1891. The teams played its home games on the Old Main lawn on campus in State College, Pennsylvania. They compiled a 12–8–1 record in these seasons, playing as an independent from 1887–1890.\n\nIn 1891, the Pennsylvania Intercollegiate Football Association (PIFA) was formed. It consisted of Bucknell (University of Lewisburg), Dickinson, Franklin & Marshall, Haverford, Penn State and Swarthmore. Lafayette and Lehigh were excluded because it was felt they would dominate the Association. Penn State won the championship with a 4–1–0 record. Bucknell's record was 3–1–1 (losing to Franklin & Marshall and tying Dickinson). The Association was dissolved prior to the 1892 season.\n\nThe first nighttime football game was played in Mansfield, Pennsylvania on September 28, 1892, between Mansfield State Normal and Wyoming Seminary and ended at halftime in a 0–0 tie. The Army–Navy game of 1893 saw the first documented use of a football helmet by a player in a game. Joseph M. Reeves had a crude leather helmet made by a shoemaker in Annapolis and wore it in the game after being warned by his doctor that he risked death if he continued to play football after suffering an earlier kick to the head.\n\nIn 1879, the University of Michigan became the first school west of Pennsylvania to establish a college football team. On May 30, 1879, Michigan beat Racine College 1–0 in a game played in Chicago. The \"Chicago Daily Tribune\" called it \"the first rugby-football game to be played west of the Alleghenies.\" Other Midwestern schools soon followed suit, including the University of Chicago, Northwestern University, and the University of Minnesota. The first western team to travel east was the 1881 Michigan team, which played at Harvard, Yale and Princeton. The nation's first college football league, the Intercollegiate Conference of Faculty Representatives (also known as the Western Conference), a precursor to the Big Ten Conference, was founded in 1895.\n\nLed by coach Fielding H. Yost, Michigan became the first \"western\" national power. From 1901 to 1905, Michigan had a 56-game undefeated streak that included a 1902 trip to play in the first college football bowl game, which later became the Rose Bowl Game. During this streak, Michigan scored 2,831 points while allowing only 40.\n\nOrganized intercollegiate football was first played in the state of Minnesota on September 30, 1882, when Hamline was convinced to play Minnesota. Minnesota won 2 to 0. It was the first game west of the Mississippi River.\n\nNovember 30, 1905, saw Chicago defeat Michigan 2 to 0. Dubbed \"The First Greatest Game of the Century\", broke Michigan's 56-game unbeaten streak and marked the end of the \"Point-a-Minute\" years.\n\nOrganized intercollegiate football was first played in the state of Virginia and the south on November 2, 1873, in Lexington between Washington and Lee and VMI. Washington and Lee won 4–2. Some industrious students of the two schools organized a game for October 23, 1869, but it was rained out. Students of the University of Virginia were playing pickup games of the kicking-style of football as early as 1870, and some accounts even claim it organized a game against Washington and Lee College in 1871; but no record has been found of the score of this contest. Due to scantness of records of the prior matches some will claim Virginia v. Pantops Academy November 13, 1887, as the first game in Virginia.\n\nOn April 9, 1880, at Stoll Field, Transylvania University (then called Kentucky University) beat Centre College by the score of 13¾–0 in what is often considered the first recorded game played in the South. The first game of \"scientific football\" in the South was the first instance of the Victory Bell rivalry between North Carolina and Duke (then known as Trinity College) held on Thanksgiving Day, 1888, at the North Carolina State Fairgrounds in Raleigh, North Carolina.\n\nOn November 13, 1887 the Virginia Cavaliers and Pantops Academy fought to a scoreless tie in the first organized football game in the state of Virginia. Students at UVA were playing pickup games of the kicking-style of football as early as 1870, and some accounts even claim that some industrious ones organized a game against Washington and Lee College in 1871, just two years after Rutgers and Princeton's historic first game in 1869. But no record has been found of the score of this contest. Washington and Lee also claims a 4 to 2 win over VMI in 1873.\n\nOn October 18, 1888, the Wake Forest Demon Deacons defeated the North Carolina Tar Heels 6 to 4 in the first intercollegiate game in the state of North Carolina.\n\nOn December 14, 1889, Wofford defeated Furman 5 to 1 in the first intercollegiate game in the state of South Carolina. The game featured no uniforms, no positions, and the rules were formulated before the game.\n\nJanuary 30, 1892, saw the first football game played in the Deep South when the Georgia Bulldogs defeated Mercer 50–0 at Herty Field.\n\nThe beginnings of the contemporary Southeastern Conference and Atlantic Coast Conference start in 1894. The Southern Intercollegiate Athletic Association (SIAA) was founded on December 21, 1894, by William Dudley, a chemistry professor at Vanderbilt. The original members were Alabama, Auburn, Georgia, Georgia Tech, North Carolina, , and Vanderbilt. Clemson, Cumberland, Kentucky, LSU, Mercer, Mississippi, Mississippi A&M (Mississippi State), Southwestern Presbyterian University, Tennessee, Texas, Tulane, and the University of Nashville joined the following year in 1895 as invited charter members. The conference was originally formed for \"the development and purification of college athletics throughout the South\".\n\nIt is thought that the first forward pass in football occurred on October 26, 1895, in a game between Georgia and North Carolina when, out of desperation, the ball was thrown by the North Carolina back Joel Whitaker instead of punted and George Stephens caught the ball. On November 9, 1895, John Heisman executed a hidden ball trick utilizing quarterback Reynolds Tichenor to get Auburn's only touchdown in a 6 to 9 loss to Vanderbilt. It was the first game in the south decided by a field goal. Heisman later used the trick against Pop Warner's Georgia team. Warner picked up the trick and later used it at Cornell against Penn State in 1897. He then used it in 1903 at Carlisle against Harvard and garnered national attention.\n\nThe 1899 Sewanee Tigers are one of the all-time great teams of the early sport. The team went 12–0, outscoring opponents 322 to 10. Known as the \"Iron Men\", with just 13 men they had a six-day road trip with five shutout wins over Texas A&M; Texas; Tulane; LSU; and Ole Miss. It is recalled memorably with the phrase \"... and on the seventh day they rested.\" Grantland Rice called them \"the most durable football team I ever saw.\"\n\nOrganized intercollegiate football was first played in the state of Florida in 1901. A 7-game series between intramural teams from Stetson and Forbes occurred in 1894. The first intercollegiate game between official varsity teams was played on November 22, 1901. Stetson beat Florida Agricultural College at Lake City, one of the four forerunners of the University of Florida, 6-0, in a game played as part of the Jacksonville Fair.\n\nOn September 27, 1902, Georgetown beat Navy 4 to 0. It is claimed by Georgetown authorities as the game with the first ever \"roving center\" or linebacker when Percy Given stood up, in contrast to the usual tale of Germany Schulz. The first linebacker in the South is often considered to be Frank Juhan.\n\nOn Thanksgiving Day 1903, a game was scheduled in Montgomery, Alabama between the best teams from each region of the Southern Intercollegiate Athletic Association for an \"SIAA championship game\", pitting Cumberland against Heisman's Clemson. The game ended in an 11–11 tie causing many teams to claim the title. Heisman pressed hardest for Cumberland to get the claim of champion. It was his last game as Clemson head coach.\n\n1904 saw big coaching hires in the south: Mike Donahue at Auburn, John Heisman at Georgia Tech, and Dan McGugin at Vanderbilt were all hired that year. Both Donahue and McGugin just came from the north that year, Donahue from Yale and McGugin from Michigan, and were among the initial inductees of the College Football Hall of Fame. The undefeated 1904 Vanderbilt team scored an average of 52.7 points per game, the most in college football that season, and allowed just four points.\n\nThe first college football game in Oklahoma Territory occurred on November 7, 1895, when the 'Oklahoma City Terrors' defeated the Oklahoma Sooners 34 to 0. The Terrors were a mix of Methodist college students and high schoolers. The Sooners did not manage a single first down. By next season, Oklahoma coach John A. Harts had left to prospect for gold in the Arctic. Organized football was first played in the territory on November 29, 1894, between the Oklahoma City Terrors and Oklahoma City High School. The high school won 24 to 0.\n\nThe University of Southern California first fielded an American football team in 1888. Playing its first game on November 14 of that year against the Alliance Athletic Club, in which USC gained a 16–0 victory. Frank Suffel and Henry H. Goddard were playing coaches for the first team which was put together by quarterback Arthur Carroll; who in turn volunteered to make the pants for the team and later became a tailor. USC faced its first collegiate opponent the following year in fall 1889, playing St. Vincent's College to a 40–0 victory. In 1893, USC joined the Intercollegiate Football Association of Southern California (the forerunner of the SCIAC), which was composed of USC, Occidental College, Throop Polytechnic Institute (Caltech), and Chaffey College. Pomona College was invited to enter, but declined to do so. An invitation was also extended to Los Angeles High School.\nIn 1891, the first Stanford football team was hastily organized and played a four-game season beginning in January 1892 with no official head coach. Following the season, Stanford captain John Whittemore wrote to Yale coach Walter Camp asking him to recommend a coach for Stanford. To Whittemore's surprise, Camp agreed to coach the team himself, on the condition that he finish the season at Yale first. As a result of Camp's late arrival, Stanford played just three official games, against San Francisco's Olympic Club and rival California. The team also played exhibition games against two Los Angeles area teams that Stanford does not include in official results. Camp returned to the East Coast following the season, then returned to coach Stanford in 1894 and 1895.\n\nOn December 25, 1894, Amos Alonzo Stagg's Chicago Maroons agreed to play Camp's Stanford football team in San Francisco in the first postseason intersectional contest, foreshadowing the modern bowl game. Future president Herbert Hoover was Stanford's student financial manager. Chicago won 24 to 4. Stanford won a rematch in Los Angeles on December 29 by 12 to 0.\nThe Big Game between Stanford and California is the oldest college football rivalry in the West. The first game was played on San Francisco's Haight Street Grounds on March 19, 1892, with Stanford winning 14–10. The term \"Big Game\" was first used in 1900, when it was played on Thanksgiving Day in San Francisco. During that game, a large group of men and boys, who were observing from the roof of the nearby S.F. and Pacific Glass Works, fell into the fiery interior of the building when the roof collapsed, resulting in 13 dead and 78 injured. On December 4, 1900, the last victim of the disaster (Fred Lilly) died, bringing the death toll to 22; and, to this day, the \"Thanksgiving Day Disaster\" remains the deadliest accident to kill spectators at a U.S. sporting event.\n\nThe University of Oregon began playing American football in 1894 and played its first game on March 24, 1894, defeating Albany College 44–3 under head coach Cal Young. Cal Young left after that first game and J.A. Church took over the coaching position in the fall for the rest of the season. Oregon finished the season with two additional losses and a tie, but went undefeated the following season, winning all four of its games under head coach Percy Benson. In 1899, the Oregon football team left the state for the first time, playing the California Golden Bears in Berkeley, California.\n\nAmerican football at Oregon State University started in 1893 shortly after athletics were initially authorized at the college. Athletics were banned at the school in May 1892, but when the strict school president, Benjamin Arnold, died, President John Bloss reversed the ban. Bloss's son William started the first team, on which he served as both coach and quarterback. The team's first game was an easy 63-0 defeat over the home team, Albany College.\n\nIn May 1900, Yost was hired as the football coach at Stanford University, and, after traveling home to West Virginia, he arrived in Palo Alto, California, on August 21, 1900. Yost led the 1900 Stanford team to a 7–2–1, outscoring opponents 154 to 20. The next year in 1901, Yost was hired by Charles A. Baird as the head football coach for the Michigan Wolverines football team. On January 1, 1902, Yost's dominating 1901 Michigan Wolverines football team agreed to play a 3–1–2 team from Stanford University in the inaugural \"Tournament East-West football game\" what is now known as the \"Rose Bowl Game\" by a score of 49–0 after Stanford captain Ralph Fisher requested to quit with eight minutes remaining.\n\nThe 1905 season marked the first meeting between Stanford and USC. Consequently, Stanford is USC's oldest existing rival. The Big Game between Stanford and Cal on November 11, 1905, was the first played at Stanford Field, with Stanford winning 12–5.\n\nIn 1906, citing concerns about the violence in American Football, universities on the West Coast, led by California and Stanford, replaced the sport with rugby union. At the time, the future of American football was very much in doubt and these schools believed that rugby union would eventually be adopted nationwide. Other schools followed suit and also made the switch included Nevada, St. Mary's, Santa Clara, and USC (in 1911). However, due to the perception that West Coast football was inferior to the game played on the East Coast anyway, East Coast and Midwest teams shrugged off the loss of the teams and continued playing American football. With no nationwide movement, the available pool of rugby teams to play remained small. The schools scheduled games against local club teams and reached out to rugby union powers in Australia, New Zealand, and especially, due to its proximity, Canada. The annual Big Game between Stanford and California continued as rugby, with the winner invited by the British Columbia Rugby Union to a tournament in Vancouver over the Christmas holidays, with the winner of that tournament receiving the Cooper Keith Trophy.\n\nDuring 12 seasons of playing rugby union, Stanford was remarkably successful: the team had three undefeated seasons, three one-loss seasons, and an overall record of 94 wins, 20 losses, and 3 ties for a winning percentage of .816. However, after a few years, the school began to feel the isolation of its newly adopted sport, which was not spreading as many had hoped. Students and alumni began to clamor for a return to American football to allow wider intercollegiate competition. The pressure at rival California was stronger (especially as the school had not been as successful in the Big Game as they had hoped), and in 1915 California returned to American football. As reasons for the change, the school cited rule change back to American football, the overwhelming desire of students and supporters to play American football, interest in playing other East Coast and Midwest schools, and a patriotic desire to play an \"American\" game. California's return to American football increased the pressure on Stanford to also change back in order to maintain the rivalry. Stanford played its 1915, 1916, and 1917 \"Big Games\" as rugby union against Santa Clara and California's football \"Big Game\" in those years was against Washington, but both schools desired to restore the old traditions. The onset of American involvement in World War I gave Stanford an out: In 1918, the Stanford campus was designated as the Students' Army Training Corps headquarters for all of California, Nevada, and Utah, and the commanding officer Sam M. Parker decreed that American football was the appropriate athletic activity to train soldiers and rugby union was dropped.\n\nThe University of Colorado began playing American football in 1890. Colorado found much success in its early years, winning eight Colorado Football Association Championships (1894–97, 1901–08).\n\nThe following was taken from the \"Silver & Gold\" newspaper of December 16, 1898. It was a recollection of the birth of Colorado football written by one of CU's original gridders, John C. Nixon, also the school's second captain. It appears here in its original form:\n\nIn 1909, the Rocky Mountain Athletic Conference was founded, featuring four members: Colorado, Colorado College, Colorado School of Mines, and Colorado Agricultural College. The University of Denver and the University of Utah joined the RMAC in 1910. For its first thirty years, the RMAC was considered a major conference equivalent to today's Division I, before 7 larger members left and formed the Mountain States Conference (also called the Skyline Conference).\n\nCollege football increased in popularity through the remainder of the 19th and early 20th century. It also became increasingly violent. Between 1890 and 1905, 330 college athletes died as a direct result of injuries sustained on the football field. These deaths could be attributed to the mass formations and gang tackling that characterized the sport in its early years.\n\nThe 1894 Harvard–Yale game, known as the \"Hampden Park Blood Bath\", resulted in crippling injuries for four players; the contest was suspended until 1897. The annual Army–Navy game was suspended from 1894 to 1898 for similar reasons. One of the major problems was the popularity of mass-formations like the flying wedge, in which a large number of offensive players charged as a unit against a similarly arranged defense. The resultant collisions often led to serious injuries and sometimes even death. Georgia fullback Richard Von Albade Gammon notably died on the field from concussions received against Virginia in 1897, causing Georgia, Georgia Tech, and Mercer to suspend their football programs.\n\nThe situation came to a head in 1905 when there were 19 fatalities nationwide. President Theodore Roosevelt reportedly threatened to shut down the game if drastic changes were not made. However, the threat by Roosevelt to eliminate football is disputed by sports historians. What is absolutely certain is that on October 9, 1905, Roosevelt held a meeting of football representatives from Harvard, Yale, and Princeton. Though he lectured on eliminating and reducing injuries, he never threatened to ban football. He also lacked the authority to abolish football and was, in fact, actually a fan of the sport and wanted to preserve it. The President's sons were also playing football at the college and secondary levels at the time.\n\nMeanwhile, John H. Outland held an experimental game in Wichita, Kansas that reduced the number of scrimmage plays to earn a first down from four to three in an attempt to reduce injuries. The \"Los Angeles Times\" reported an increase in punts and considered the game much safer than regular play but that the new rule was not \"conducive to the sport\". In 1906, President Roosevelt organized a meeting among thirteen school leaders at the White House to find solutions to make the sport safer for the athletes. Because the college officials could not agree upon a change in rules, it was decided over the course of several subsequent meetings that an external governing body should be responsible. Finally, on December 28, 1905, 62 schools met in New York City to discuss rule changes to make the game safer. As a result of this meeting, the Intercollegiate Athletic Association of the United States was formed in 1906. The IAAUS was the original rule making body of college football, but would go on to sponsor championships in other sports. The IAAUS would get its current name of National Collegiate Athletic Association (NCAA) in 1910, and still sets rules governing the sport.\n\nThe rules committee considered widening the playing field to \"open up\" the game, but Harvard Stadium (the first large permanent football stadium) had recently been built at great expense; it would be rendered useless by a wider field. The rules committee legalized the forward pass instead. Though it was underutilized for years, this proved to be one of the most important rule changes in the establishment of the modern game. Another rule change banned \"mass momentum\" plays (many of which, like the infamous \"flying wedge\", were sometimes literally deadly).\n\nAs a result of the 1905–1906 reforms, mass formation plays became illegal and forward passes legal. Bradbury Robinson, playing for visionary coach Eddie Cochems at Saint Louis University, threw the first legal pass in a September 5, 1906, game against Carroll College at Waukesha. Other important changes, formally adopted in 1910, were the requirements that at least seven offensive players be on the line of scrimmage at the time of the snap, that there be no pushing or pulling, and that interlocking interference (arms linked or hands on belts and uniforms) was not allowed. These changes greatly reduced the potential for collision injuries. Several coaches emerged who took advantage of these sweeping changes. Amos Alonzo Stagg introduced such innovations as the huddle, the tackling dummy, and the pre-snap shift. Other coaches, such as Pop Warner and Knute Rockne, introduced new strategies that still remain part of the game.\n\nBesides these coaching innovations, several rules changes during the first third of the 20th century had a profound impact on the game, mostly in opening up the passing game. In 1914, the first roughing-the-passer penalty was implemented. In 1918, the rules on eligible receivers were loosened to allow eligible players to catch the ball anywhere on the field—previously strict rules were in place allowing passes to only certain areas of the field. Scoring rules also changed during this time: field goals were lowered to three points in 1909 and touchdowns raised to six points in 1912.\n\nStar players that emerged in the early 20th century include Jim Thorpe, Red Grange, and Bronko Nagurski; these three made the transition to the fledgling NFL and helped turn it into a successful league. Sportswriter Grantland Rice helped popularize the sport with his poetic descriptions of games and colorful nicknames for the game's biggest players, including Notre Dame's \"Four Horsemen\" backfield and Fordham University's linemen, known as the \"Seven Blocks of Granite\".\n\nIn 1907 at Champaign, Illinois Chicago and Illinois played in the first game to have a halftime show featuring a marching band. Chicago won 42–6. On November 25, 1911 Kansas and Missouri played the first homecoming football game. The game was \"broadcast\" play-by-play over telegraph to at least 1,000 fans in Lawrence, Kansas. It ended in a 3–3 tie. The game between West Virginia and Pittsburgh on October 8, 1921, saw the first live radio broadcast of a college football game when Harold W. Arlin announced that year's Backyard Brawl played at Forbes Field on KDKA. Pitt won 21–13. On October 28, 1922, Princeton and Chicago played the first game to be nationally broadcast on radio. Princeton won 21–18 in a hotly contested game which had Princeton dubbed the \"Team of Destiny.\"\n\nOne publication claims \"The first scouting done in the South was in 1905, when Dan McGugin and Captain Innis Brown, of Vanderbilt went to Atlanta to see Sewanee play Georgia Tech.\" Fuzzy Woodruff claims Davidson was the first in the south to throw a legal forward pass in 1906. The following season saw Vanderbilt execute a double pass play to set up the touchdown that beat Sewanee in a meeting of unbeatens for the SIAA championship. Grantland Rice cited this event as the greatest thrill he ever witnessed in his years of watching sports. Vanderbilt coach Dan McGugin in \"Spalding's Football Guide\"'s summation of the season in the SIAA wrote \"The standing. First, Vanderbilt; second, Sewanee, a might good second;\" and that Aubrey Lanier \"came near winning the Vanderbilt game by his brilliant dashes after receiving punts.\" Bob Blake threw the final pass to center Stein Stone, catching it near the goal amongst defenders. Honus Craig then ran in the winning touchdown.\n\nUtilizing the \"jump shift\" offense, John Heisman's Georgia Tech Golden Tornado won 222 to 0 over Cumberland on October 7, 1916, at Grant Field in the most lopsided victory in college football history. Tech went on a 33-game winning streak during this period. The 1917 team was the first national champion from the South, led by a powerful backfield. It also had the first two players from the Deep South selected first-team All-American in Walker Carpenter and Everett Strupper. Pop Warner's Pittsburgh Panthers were also undefeated, but declined a challenge by Heisman to a game. When Heisman left Tech after 1919, his shift was still employed by protege William Alexander.\n\nIn 1906, Vanderbilt defeated Carlisle 4 to 0, the result of a Bob Blake field goal. In 1907 Vanderbilt fought Navy to a 6 to 6 tie. In 1910 Vanderbilt held defending national champion Yale to a scoreless tie.\n\nHelping Georgia Tech's claim to a title in 1917, the Auburn Tigers held undefeated, Chic Harley-led Big Ten champion Ohio State to a scoreless tie the week before Georgia Tech beat the Tigers 68 to 7. The next season, with many players gone due to World War I, a game was finally scheduled at Forbes Field with Pittsburgh. The Panthers, led by freshman Tom Davies, defeated Georgia Tech 32 to 0. Tech center Bum Day was the first player on a Southern team ever selected first-team All-American by Walter Camp.\n\n1917 saw the rise of another Southern team in Centre of Danville, Kentucky. In 1921 Bo McMillin-led Centre upset defending national champion Harvard 6 to 0 in what is widely considered one of the greatest upsets in college football history. The next year Vanderbilt fought Michigan to a scoreless tie at the inaugural game at Dudley Field (now Vanderbilt Stadium), the first stadium in the South made exclusively for college football. Michigan coach Fielding Yost and Vanderbilt coach Dan McGugin were brothers-in-law, and the latter the protege of the former. The game featured the season's two best defenses and included a goal line stand by Vanderbilt to preserve the tie. Its result was \"a great surprise to the sporting world.\" Commodore fans celebrated by throwing some 3,000 seat cushions onto the field. The game features prominently in Vanderbilt's history. That same year, Alabama upset Penn 9 to 7.\n\nVanderbilt's line coach then was Wallace Wade, who coached Alabama to the south's first Rose Bowl victory in 1925. This game is commonly referred to as \"the game that changed the south.\" Wade followed up the next season with an undefeated record and Rose Bowl tie. Georgia's 1927 \"dream and wonder team\" defeated Yale for the first time. Georgia Tech, led by Heisman protege William Alexander, gave the dream and wonder team its only loss, and the next year were national and Rose Bowl champions. The Rose Bowl included Roy Riegels' wrong-way run. On October 12, 1929, Yale lost to Georgia in Sanford Stadium in its first trip to the south. Wade's Alabama again won a national championship and Rose Bowl in 1930.\n\nGlenn \"Pop\" Warner coached at several schools throughout his career, including the University of Georgia, Cornell University, University of Pittsburgh, Stanford University, Iowa State University, and Temple University. One of his most famous stints was at the Carlisle Indian Industrial School, where he coached Jim Thorpe, who went on to become the first president of the National Football League, an Olympic Gold Medalist, and is widely considered one of the best overall athletes in history. Warner wrote one of the first important books of football strategy, \"Football for Coaches and Players\", published in 1927. Though the shift was invented by Stagg, Warner's single wing and double wing formations greatly improved upon it; for almost 40 years, these were among the most important formations in football. As part of his single and double wing formations, Warner was one of the first coaches to effectively utilize the forward pass. Among his other innovations are modern blocking schemes, the three-point stance, and the reverse play. The youth football league, Pop Warner Little Scholars, was named in his honor.\n\nKnute Rockne rose to prominence in 1913 as an end for the University of Notre Dame, then a largely unknown Midwestern Catholic school. When Army scheduled Notre Dame as a warm-up game, they thought little of the small school. Rockne and quarterback Gus Dorais made innovative use of the forward pass, still at that point a relatively unused weapon, to defeat Army 35–13 and helped establish the school as a national power. Rockne returned to coach the team in 1918, and devised the powerful Notre Dame Box offense, based on Warner's single wing. He is credited with being the first major coach to emphasize offense over defense. Rockne is also credited with popularizing and perfecting the forward pass, a seldom used play at the time. The 1924 team featured the Four Horsemen backfield. In 1927, his complex shifts led directly to a rule change whereby all offensive players had to stop for a full second before the ball could be snapped. Rather than simply a regional team, Rockne's \"Fighting Irish\" became famous for barnstorming and played any team at any location. It was during Rockne's tenure that the annual Notre Dame-University of Southern California rivalry began. He led his team to an impressive 105–12–5 record before his premature death in a plane crash in 1931. He was so famous at that point that his funeral was broadcast nationally on radio.\n\nIn the early 1930s, the college game continued to grow, particularly in the South, bolstered by fierce rivalries such as the \"South's Oldest Rivalry\", between Virginia and North Carolina and the \"Deep South's Oldest Rivalry\", between Georgia and Auburn. Although before the mid-1920s most national powers came from the Northeast or the Midwest, the trend changed when several teams from the South and the West Coast achieved national success. Wallace William Wade's 1925 Alabama team won the 1926 Rose Bowl after receiving its first national title and William Alexander's 1928 Georgia Tech team defeated California in the 1929 Rose Bowl. College football quickly became the most popular spectator sport in the South.\n\nSeveral major modern college football conferences rose to prominence during this time period. The Southwest Athletic Conference had been founded in 1915. Consisting mostly of schools from Texas, the conference saw back-to-back national champions with Texas Christian University (TCU) in 1938 and Texas A&M in 1939. The Pacific Coast Conference (PCC), a precursor to the Pac-12 Conference (Pac-12), had its own back-to-back champion in the University of Southern California which was awarded the title in 1931 and 1932. The Southeastern Conference (SEC) formed in 1932 and consisted mostly of schools in the Deep South. As in previous decades, the Big Ten continued to dominate in the 1930s and 1940s, with Minnesota winning 5 titles between 1934 and 1941, and Michigan (1933, 1947, and 1948) and Ohio State (1942) also winning titles.\n\nAs it grew beyond its regional affiliations in the 1930s, college football garnered increased national attention. Four new bowl games were created: the Orange Bowl, Sugar Bowl, the Sun Bowl in 1935, and the Cotton Bowl in 1937. In lieu of an actual national championship, these bowl games, along with the earlier Rose Bowl, provided a way to match up teams from distant regions of the country that did not otherwise play. In 1936, the Associated Press began its weekly poll of prominent sports writers, ranking all of the nation's college football teams. Since there was no national championship game, the final version of the AP poll was used to determine who \nwas crowned the National Champion of college football.\n\nThe 1930s saw growth in the passing game. Though some coaches, such as General Robert Neyland at Tennessee, continued to eschew its use, several rules changes to the game had a profound effect on teams' ability to throw the ball. In 1934, the rules committee removed two major penalties—a loss of five yards for a second incomplete pass in any series of downs and a loss of possession for an incomplete pass in the end zone—and shrunk the circumference of the ball, making it easier to grip and throw. Players who became famous for taking advantage of the easier passing game included Alabama end Don Hutson and TCU passer \"Slingin\" Sammy Baugh.\n\nIn 1935, New York City's Downtown Athletic Club awarded the first Heisman Trophy to University of Chicago halfback Jay Berwanger, who was also the first ever NFL Draft pick in 1936. The trophy was designed by sculptor Frank Eliscu and modeled after New York University player Ed Smith. The trophy recognizes the nation's \"most outstanding\" college football player and has become one of the most coveted awards in all of American sports.\n\nDuring World War II, college football players enlisted in the armed forces, some playing in Europe during the war. As most of these players had eligibility left on their college careers, some of them returned to college at West Point, bringing Army back-to-back national titles in 1944 and 1945 under coach Red Blaik. Doc Blanchard (known as \"Mr. Inside\") and Glenn Davis (known as \"Mr. Outside\") both won the Heisman Trophy, in 1945 and 1946. On the coaching staff of those 1944–1946 Army teams was future Pro Football Hall of Fame coach Vince Lombardi.\n\nThe 1950s saw the rise of yet more dynasties and power programs. Oklahoma, under coach Bud Wilkinson, won three national titles (1950, 1955, 1956) and all ten Big Eight Conference championships in the decade while building a record 47-game winning streak. Woody Hayes led Ohio State to two national titles, in 1954 and 1957, and won three Big Ten titles. The Michigan State Spartans were known as the \"football factory\" during the 1950s, where coaches Clarence Munn and Duffy Daugherty led the Spartans to two national titles and two Big Ten titles after joining the Big Ten athletically in 1953. Wilkinson and Hayes, along with Robert Neyland of Tennessee, oversaw a revival of the running game in the 1950s. Passing numbers dropped from an average of 18.9 attempts in 1951 to 13.6 attempts in 1955, while teams averaged just shy of 50 running plays per game. Nine out of ten Heisman Trophy winners in the 1950s were runners. Notre Dame, one of the biggest passing teams of the decade, saw a substantial decline in success; the 1950s were the only decade between 1920 and 1990 when the team did not win at least a share of the national title. Paul Hornung, Notre Dame quarterback, did, however, win the Heisman in 1956, becoming the only player from a losing team ever to do so.\n\nFollowing the enormous success of the 1958 NFL Championship Game, college football no longer enjoyed the same popularity as the NFL, at least on a national level. While both games benefited from the advent of television, since the late 1950s, the NFL has become a nationally popular sport while college football has maintained strong regional ties.\nAs professional football became a national television phenomenon, college football did as well. In the 1950s, Notre Dame, which had a large national following, formed its own network to broadcast its games, but by and large the sport still retained a mostly regional following. In 1952, the NCAA claimed all television broadcasting rights for the games of its member institutions, and it alone negotiated television rights. This situation continued until 1984, when several schools brought a suit under the Sherman Antitrust Act; the Supreme Court ruled against the NCAA and schools are now free to negotiate their own television deals. ABC Sports began broadcasting a national Game of the Week in 1966, bringing key matchups and rivalries to a national audience for the first time.\n\nNew formations and play sets continued to be developed. Emory Bellard, an assistant coach under Darrell Royal at the University of Texas, developed a three-back option style offense known as the wishbone. The wishbone is a run-heavy offense that depends on the quarterback making last second decisions on when and to whom to hand or pitch the ball to. Royal went on to teach the offense to other coaches, including Bear Bryant at Alabama, Chuck Fairbanks at Oklahoma and Pepper Rodgers at UCLA; who all adapted and developed it to their own tastes. The strategic opposite of the wishbone is the spread offense, developed by professional and college coaches throughout the 1960s and 1970s. Though some schools play a run-based version of the spread, its most common use is as a passing offense designed to \"spread\" the field both horizontally and vertically. Some teams have managed to adapt with the times to keep winning consistently. In the rankings of the most victorious programs, Michigan, Ohio State, and Notre Dame are ranked first, second, and third in total wins.\n\n colspan=\"2\" style=\"text-align:center;\" Growth of bowlgames 1930–2010\n! width=90  Year\n! # of games\n2014\n\nIn 1940, for the highest level of college football, there were only five bowl games (Rose, Orange, Sugar, Sun, and Cotton). By 1950, three more had joined that number and in 1970, there were still only eight major college bowl games. The number grew to eleven in 1976. At the birth of cable television and cable sports networks like ESPN, there were fifteen bowls in 1980. With more national venues and increased available revenue, the bowls saw an explosive growth throughout the 1980s and 1990s. In the thirty years from 1950 to 1980, seven bowl games were added to the schedule. From 1980 to 2008, an additional 20 bowl games were added to the schedule. Some have criticized this growth, claiming that the increased number of games has diluted the significance of playing in a bowl game. Yet others have countered that the increased number of games has increased exposure and revenue for a greater number of schools, and see it as a positive development.\n\nWith the growth of bowl games, it became difficult to determine a national champion in a fair and equitable manner. As conferences became contractually bound to certain bowl games (a situation known as a tie-in), match-ups that guaranteed a consensus national champion became increasingly rare. In 1992, seven conferences and independent Notre Dame formed the Bowl Coalition, which attempted to arrange an annual No.1 versus No.2 matchup based on the final AP poll standings. The Coalition lasted for three years; however, several scheduling issues prevented much success; tie-ins still took precedence in several cases. For example, the Big Eight and SEC champions could never meet, since they were contractually bound to different bowl games. The coalition also excluded the Rose Bowl, arguably the most prestigious game in the nation, and two major conferences—the Pac-10 and Big Ten—meaning that it had limited success. In 1995, the Coalition was replaced by the Bowl Alliance, which reduced the number of bowl games to host a national championship game to three—the Fiesta, Sugar, and Orange Bowls—and the participating conferences to five—the ACC, SEC, Southwest, Big Eight, and Big East. It was agreed that the No.1 and No.2 ranked teams gave up their prior bowl tie-ins and were guaranteed to meet in the national championship game, which rotated between the three participating bowls. The system still did not include the Big Ten, Pac-10, or the Rose Bowl, and thus still lacked the legitimacy of a true national championship. However, one positive side effect is that if there were three teams at the end of the season vying for a national title, but one of them was a Pac-10/Big Ten team bound to the Rose Bowl, then there would be no difficulty in deciding which teams to place in the Bowl Alliance \"national championship\" bowl; if the Pac-10 / Big Ten team won the Rose Bowl and finished with the same record as whichever team won the other bowl game, they could have a share of the national title. This happened in the final year of the Bowl Alliance, with Michigan winning the 1998 Rose Bowl and Nebraska winning the 1998 Orange Bowl. Without the Pac-10/Big Ten team bound to a bowl game, it would be difficult to decide which two teams should play for the national title.\n\nIn 1998, a new system was put into place called the Bowl Championship Series. For the first time, it included all major conferences (ACC, Big East, Big 12, Big Ten, Pac-10, and SEC) and four major bowl games (Rose, Orange, Sugar and Fiesta). The champions of these six conferences, along with two \"at-large\" selections, were invited to play in the four bowl games. Each year, one of the four bowl games served as a national championship game. Also, a complex system of human polls, computer rankings, and strength of schedule calculations was instituted to rank schools. Based on this ranking system, the No.1 and No.2 teams met each year in the national championship game. Traditional tie-ins were maintained for schools and bowls not part of the national championship. For example, in years when not a part of the national championship, the Rose Bowl still hosted the Big Ten and Pac-10 champions.\n\nThe system continued to change, as the formula for ranking teams was tweaked from year to year. At-large teams could be chosen from any of the Division I-A conferences, though only one selection—Utah in 2005—came from a BCS non-AQ conference. Starting with the 2006 season, a fifth game—simply called the BCS National Championship Game—was added to the schedule, to be played at the site of one of the four BCS bowl games on a rotating basis, one week after the regular bowl game. This opened up the BCS to two additional at-large teams. Also, rules were changed to add the champions of five additional conferences (Conference USA [C-USA], the Mid-American Conference [MAC], the Mountain West Conference [MW], the Sun Belt Conference and the Western Athletic Conference [WAC]), provided that said champion ranked in the top twelve in the final BCS rankings, or was within the top 16 of the BCS rankings and ranked higher than the champion of at least one of the BCS Automatic Qualifying (AQ) conferences. Several times since this rule change was implemented, schools from non-AQ conferences have played in BCS bowl games. In 2009, Boise State played TCU in the Fiesta Bowl, the first time two schools from non-AQ conferences played each other in a BCS bowl game. The last team from the non-AQ ranks to reach a BCS bowl game in the BCS era was Northern Illinois in 2012, which played in (and lost) the 2013 Orange Bowl.\n\nThe longtime resistance to a playoff system at the FBS level finally ended with the creation of the College Football Playoff (CFP) beginning with the 2014 season. The CFP is a Plus-One system, a concept that became popular as a BCS alternative following controversies in 2003 and 2004. The CFP is a four-team tournament whose participants are chosen and seeded by a 13-member selection committee. The semifinals are hosted by two of a group of traditional bowl games known as the New Year's Six, with semifinal hosting rotating annually among three pairs of games in the following order: Rose/Sugar, Orange/Cotton, and Fiesta/Peach. The two semifinal winners then advance to the College Football Playoff National Championship, whose host is determined by open bidding several years in advance.\n\nThe establishment of the CFP followed a tumultuous period of conference realignment in Division I. The WAC, after seeing all but two of its football members leave, dropped football after the 2012 season. The Big East split into two leagues in 2013; the schools that did not play FBS football reorganized as a new non-football Big East Conference, while the FBS member schools that remained in the original structure joined with several new members and became the American Athletic Conference. The American retained the Big East's automatic BCS bowl bid for the 2013 season, but lost this status in the CFP era.\n\nThe 10 FBS conferences are formally and popularly divided into two groups:\nBULLET::::- Power Five – Five of the six AQ conferences of the BCS era, specifically the ACC, Big Ten, Big 12, Pac-12, and SEC. Each champion of these conferences is assured of a spot in a New Year's Six bowl, though not necessarily in a semifinal game. Notre Dame remains a football independent, but is counted among the Power Five because of its full but non-football ACC membership, including a football scheduling alliance with that conference. It has its own arrangement for access to the New Year's Six games should it meet certain standards.\nBULLET::::- Group of Five – The remaining five FBS conferences – American, C-USA, MAC, MW, and Sun Belt. The other five current FBS independents, Army, BYU, Liberty, Massachusetts, and New Mexico State, are also considered to be part of this group. One conference champion from this group receives a spot in a New Year's Six game. In the first five seasons of the CFP, the Group of Five has yet to place a team in a semifinal. Of the five Group of Five teams selected for New Year's Six bowls, three have won their games.\n\nAlthough rules for the high school, college, and NFL games are generally consistent, there are several minor differences. The NCAA Football Rules Committee determines the playing rules for Division I (both Bowl and Championship Subdivisions), II, and III games (the National Association of Intercollegiate Athletics (NAIA) is a separate organization, but uses the NCAA rules).\n\nBULLET::::- A pass is ruled complete if one of the receiver's feet is inbounds at the time of the catch. In the NFL both feet must be inbounds.\nBULLET::::- A player is considered down when any part of his body other than the feet or hands touches the ground or when the ball carrier is tackled or otherwise falls and loses possession of the ball as he contacts the ground with any part of his body, with the sole exception of the holder for field goal and extra point attempts. In the NFL a player is active until he is tackled or forced down by a member of the opposing team (down by contact).\nBULLET::::- The clock stops after the offense completes a first down and begins again—assuming it is following a play in which the clock would not normally stop—once the referee says the ball is ready for play. In the NFL the clock does not explicitly stop for a first down.\nBULLET::::- Overtime was introduced in 1996, eliminating most ties except in the regular season. Since 2019, during overtime, each team is given one possession from its opponent's twenty-five yard line with no game clock, despite the one timeout per period and use of play clock; the procedure repeats for next three possessions if needed; all possessions thereafter will be from the opponent's 3-yard line. The team leading after both possessions is declared the winner. If the teams remain tied, overtime periods continue, with a coin flip determining the first possession. Possessions alternate with each overtime, until one team leads the other at the end of the overtime. Starting with triple overtime, a one-point PAT field goal after a touchdown is no longer allowed, forcing teams to attempt a two-point conversion after a touchdown. After quadruple overtime, only two-point conversion attempts will be conducted thereafter. (In the NFL overtime is decided by a modified sudden-death period of 10 minutes in preseason and regular-season games and 15 minutes in playoff games, and regular-season games can still end in a tie if neither team scores. Overtime for regular-season games in the NFL began with the 1974 season; the overtime period for all games was 15 minutes until it was shortened for non-playoff games effective in . In the postseason, if the teams are still tied, teams will play additional overtime periods until either team scores.)\nBULLET::::- A tie game is still possible, per NCAA Rule 3-3-3 (c) and (d). If a game is suspended because of inclement weather while tied, typically in the second half or at the end of regulation, and the game is unable to be continued, the game ends in a tie. Similar to baseball, if one team has scored in its possession and the other team has not completed its possession, the score during the overtime can be wiped out and the game ruled a tie. Some conferences may enforce a curfew for the safety of the players. If, because of numerous overtimes or weather, the game reaches the time-certain finish imposed by the curfew tied, the game is ruled a tie.\nBULLET::::- Extra point tries are attempted from the three-yard line. Kicked tries count as one point. Teams can also go for \"the two-point conversion\" which is when a team will line up at the three-yard line and try to score. If they are successful, they receive two points, if they are not, then they receive zero points. Starting with the 2015 season, the NFL uses the 15-yard line as the line of scrimmage for placekick attempts, but the two-yard line for two-point attempts. The two-point conversion was not implemented in the NFL until 1994, but it had been previously used in the old American Football League (AFL) before it merged with the NFL in 1970.\nBULLET::::- The defensive team may score two points on a point-after touchdown attempt by returning a blocked kick, fumble, or interception into the opposition's end zone. In addition, if the defensive team gains possession, but then moves backwards into the end zone and is stopped, a one-point safety will be awarded to the offense, although, unlike a real safety, the offense kicks off, opposed to the team charged with the safety. This college rule was added in 1988. The NFL, which previously treated the ball as dead during a conversion attempt—meaning that the attempt ended when the defending team gained possession of the football—adopted the college rule in 2015.\nBULLET::::- The two-minute warning is not used in college football, except in rare cases where the scoreboard clock has malfunctioned and is not being used.\nBULLET::::- There is an option to use instant replay review of officiating decisions. Division I FBS schools use replay in virtually all games; replay is rarely used in lower division games. Every play is subject to booth review with coaches only having one challenge. In the NFL, only scoring plays, turnovers, the final 2:00 of each half and all overtime periods are reviewed, and coaches are issued two challenges (with the option for a 3rd if the first two are successful).\nBULLET::::- Since the 2012 season, the ball is placed on the 25-yard line following a touchback on either a kickoff or a free kick following a safety. The NFL adopted this rule in 2018. In all other touchback situations at all levels of the game, the ball is placed on the 20.\nBULLET::::- Among other rule changes in 2007, kickoffs were moved from the 35-yard line back five yards to the 30-yard line, matching a change that the NFL had made in 1994. Some coaches and officials questioned this rule change as it could lead to more injuries to the players as there will likely be more kickoff returns. The rationale for the rule change was to help reduce dead time in the game. The NFL returned its kickoff location to the 35-yard line effective in 2011; college football did not do so until 2012.\nBULLET::::- Several changes were made to college rules in 2011, all of which differ from NFL practice:\nBULLET::::- If a player is penalized for unsportsmanlike conduct for actions that occurred during a play ending in a touchdown by that team, but before the goal line was crossed, the touchdown will be nullified. In the NFL, the same foul would result in a penalty on the conversion attempt or ensuing kickoff, at the option of the non-penalized team.\nBULLET::::- If a team is penalized in the final minute of a half and the penalty causes the clock to stop, the opposing team now has the right to have 10 seconds run off the clock in addition to the yardage penalty. The NFL has a similar rule in the final minute of the half, but it applies only to specified violations against the offensive team. The new NCAA rule applies to penalties on both sides of the ball.\nBULLET::::- Players lined up outside the tackle box—more specifically, those lined up more than 7 yards from the center—will now be allowed to block below the waist only if they are blocking straight ahead or toward the nearest sideline.\nBULLET::::- On placekicks, no offensive lineman can now be engaged by more than two defensive players. A violation will be a 5-yard penalty.\nBULLET::::- In 2018, the NCAA made a further change to touchback rules that the NFL has yet to duplicate; a fair catch on a kickoff or a free kick following a safety that takes place between the receiving team's goal line and 25-yard lines is treated as a touchback, with the ball placed at the 25.\n\nCollege teams mostly play other similarly sized schools through the NCAA's divisional system. Division I generally consists of the major collegiate athletic powers with larger budgets, more elaborate facilities, and (with the exception of a few conferences such as the Pioneer Football League) more athletic scholarships. Division II primarily consists of smaller public and private institutions that offer fewer scholarships than those in Division I. Division III institutions also field teams, but do not offer any scholarships.\n\nFootball teams in Division I are further divided into the Bowl Subdivision (consisting of the largest programs) and the Championship Subdivision. The Bowl Subdivision has historically not used an organized tournament to determine its champion, and instead teams compete in post-season bowl games. That changed with the debut of the four-team College Football Playoff at the end of the 2014 season.\n\nTeams in each of these four divisions are further divided into various regional conferences.\n\nSeveral organizations operate college football programs outside the jurisdiction of the NCAA:\n\nBULLET::::- The National Association of Intercollegiate Athletics has jurisdiction over more than 80 college football teams, mostly in the midwest.\nBULLET::::- The National Junior College Athletic Association has jurisdiction over two-year institutions, except in California.\nBULLET::::- The California Community College Athletic Association governs sports, including football, at that state's two-year institutions. CCCAA members compete for their own championships and do not participate in the NJCAA.\nBULLET::::- Club football, a sport in which student clubs run the teams instead of the colleges themselves, is overseen by two organizations: the National Club Football Association and the Intercollegiate Club Football Federation. The two competing sanctioning bodies have some overlap, and several clubs are members of both organizations.\nBULLET::::- The Collegiate Sprint Football League governs 10 teams, all in the northeast. Its primary restriction is that all players must weigh less than the average college student (that threshold is set, , at ).\n\nA college that fields a team in the NCAA is not restricted from fielding teams in club or sprint football, and several colleges field two teams, a varsity (NCAA) squad and a club or sprint squad (no schools, , field both club \"and\" sprint teams at the same time).\n\nBULLET::::- College football national championships in NCAA Division I FBS – Overview of systems for determining national champions at the highest level of college football from 1869 to present.\nBULLET::::- College Football Playoff – 4 team playoff system for determining national champions at the highest level of college football beginning in 2014.\nBULLET::::- Bowl Championship Series – The primary method of determining the national champion at the highest level of college football from 1998–2013; preceded by the Bowl Alliance (1995–1997) and the Bowl Coalition (1992–1994).\nBULLET::::- NCAA Division I Football Championship – playoff for determining the national champion at the second highest level of college football, Division I FCS, from 1978 to present.\nBULLET::::- NCAA Division I FCS Consensus Mid-Major Football National Championship – awarded by poll from 2001- to 2007 for a subset of the second highest level of play in college football, FCS.\nBULLET::::- NCAA Division II Football Championship – playoff for determining the national champion at the third highest level of college football from 1973 to present.\nBULLET::::- NCAA Division III Football Championship – playoff for determining the national champion at the fourth highest level of college football from 1973 to present.\nBULLET::::- NAIA National Football Championship -playoff for determining the national champions of college football governed by the National Association of Intercollegiate Athletics.\nBULLET::::- NJCAA National Football Championship – playoff for determining the national champions of college football governed by the National Junior College Athletic Association.\nBULLET::::- CSFL Championship – Champions of the Collegiate Sprint Football League, a weight restricted football sport.\n\nStarted in the 2014 season, four Division I FBS teams are selected at the end of regular season to compete in a playoff for the FBS national championship. The inaugural champion was Ohio State University. The College Football Playoff replaced the Bowl Championship Series, which had been used as the selection method to determine the national championship game participants since in the 1998 season. Clemson won the 2019 national championship.\n\nAt the Division I FCS level, the teams participate in a 24-team playoff (most recently expanded from 20 teams in 2013) to determine the national championship. Under the current playoff structure, the top eight teams are all seeded, and receive a bye week in the first round. The highest seed receives automatic home field advantage. Starting in 2013, non-seeded teams can only host a playoff game if both teams involved are unseeded; in such a matchup, the schools must bid for the right to host the game. Selection for the playoffs is determined by a selection committee, although usually a team must have an 8-4 record to even be considered. Losses to an FBS team count against their playoff eligibility, while wins against a Division II opponent do not count towards playoff consideration. Thus, only Division I wins (whether FBS, FCS, or FCS non-scholarship) are considered for playoff selection. The Division I National Championship game is held in Frisco, Texas.\n\nDivision II and Division III of the NCAA also participate in their own respective playoffs, crowning national champions at the end of the season. The National Association of Intercollegiate Athletics also holds a playoff.\n\nUnlike other college football divisions and most other sports—collegiate or professional—the Football Bowl Subdivision, formerly known as Division I-A college football, has historically not employed a playoff system to determine a champion. Instead, it has a series of postseason \"bowl games\". The annual National Champion in the Football Bowl Subdivision is then instead traditionally determined by a vote of sports writers and other non-players.\n\nThis system has been challenged often, beginning with an NCAA committee proposal in 1979 to have a four-team playoff following the bowl games. However, little headway was made in instituting a playoff tournament until 2014, given the entrenched vested economic interests in the various bowls. Although the NCAA publishes lists of claimed FBS-level national champions in its official publications, it has never recognized an official FBS national championship; this policy continues even after the establishment of the College Football Playoff (which is not directly run by the NCAA) in 2014. As a result, the official Division I National Champion is the winner of the Football Championship Subdivision, as it is the highest level of football with an NCAA-administered championship tournament.\n\nThe first bowl game was the 1902 Rose Bowl, played between Michigan and Stanford; Michigan won 49-0. It ended when Stanford requested and Michigan agreed to end it with 8 minutes on the clock. That game was so lopsided that the game was not played annually until 1916, when the Tournament of Roses decided to reattempt the postseason game. The term \"bowl\" originates from the shape of the Rose Bowl stadium in Pasadena, California, which was built in 1923 and resembled the Yale Bowl, built in 1915. This is where the name came into use, as it became known as the Rose Bowl Game. Other games came along and used the term \"bowl\", whether the stadium was shaped like a bowl or not.\n\nAt the Division I FBS level, teams must earn the right to be bowl eligible by winning at least 6 games during the season (teams that play 13 games in a season, which is allowed for Hawaii and any of its home opponents, must win 7 games). They are then invited to a bowl game based on their conference ranking and the tie-ins that the conference has to each bowl game. For the 2009 season, there were 34 bowl games, so 68 of the 120 Division I FBS teams were invited to play at a bowl. These games are played from mid-December to early January and most of the later bowl games are typically considered more prestigious.\n\nAfter the Bowl Championship Series, additional all-star bowl games round out the post-season schedule through the beginning of February.\n\nPartly as a compromise between both bowl game and playoff supporters, the NCAA created the Bowl Championship Series (BCS) in 1998 in order to create a definitive national championship game for college football. The series included the four most prominent bowl games (Rose Bowl, Orange Bowl, Sugar Bowl, Fiesta Bowl), while the national championship game rotated each year between one of these venues. The BCS system was slightly adjusted in 2006, as the NCAA added a fifth game to the series, called the National Championship Game. This allowed the four other BCS bowls to use their normal selection process to select the teams in their games while the top two teams in the BCS rankings would play in the new National Championship Game.\n\nThe BCS selection committee used a complicated, and often controversial, computer system to rank all Division I-FBS teams and the top two teams at the end of the season played for the national championship. This computer system, which factored in newspaper polls, online polls, coaches' polls, strength of schedule, and various other factors of a team's season, led to much dispute over whether the two best teams in the country were being selected to play in the National Championship Game.\n\nThe BCS ended after the 2013 season and, since the 2014 season, the FBS national champion has been determined by a four-team tournament known as the College Football Playoff (CFP). A selection committee of college football experts decides the participating teams. Six major bowl games (the Rose, Sugar, Cotton, Orange, Peach, and Fiesta) rotate on a three-year cycle as semifinal games, with the winners advancing to the College Football Playoff National Championship. This arrangement is contractually locked in until the 2026 season.\n\nCollege football is a controversial institution within American higher education, where the amount of money involved—what people will pay for the entertainment provided—is a corrupting factor within universities that they are usually ill-equipped to deal with. According to William E. Kirwan, chancellor of the University of Maryland System and co-director of the Knight Commission on Intercollegiate Athletics, \"We've reached a point where big-time intercollegiate athletics is undermining the integrity of our institutions, diverting presidents and institutions from their main purpose.\" Football coaches often make more than the presidents of their universities which employ them. Athletes are alleged to receive preferential treatment both in academics and when they run afoul of the law. Although in theory football is an extra-curricular activity engaged in as a sideline by students, it is widely believed to turn a substantial profit, from which the athletes receive no direct benefit. There has been serious discussion about making student-athletes university employees to allow them to be paid. In reality, the majority of major collegiate football programs operated at a financial loss in 2014.\n\nCanadian football, which parallels American football, is played by university teams in Canada under the auspices of U Sports. (Unlike in the United States, no junior colleges play football in Canada, and the sanctioning body for junior college athletics in Canada, CCAA, does not sanction the sport.) However, amateur football outside of colleges is played in Canada, such as in the Canadian Junior Football League. Organized competition in American football also exists at the collegiate level in Mexico (ONEFA), the UK (British Universities American Football League), Japan (Japan American Football Association, Koshien Bowl), and South Korea (Korea American Football Association).\n\nBULLET::::- Heisman Trophy\nBULLET::::- Maxwell Award\nBULLET::::- Walter Camp Award\nBULLET::::- Outland Trophy\nBULLET::::- Associated Press Player of the Year\nBULLET::::- Johnny Rodgers Award\nBULLET::::- Fred Biletnikoff Award\nBULLET::::- Lou Groza Award\nBULLET::::- Lombardi Award\nBULLET::::- Bronko Nagurski Trophy\n\nBULLET::::- Dick Butkus Award\nBULLET::::- Jim Thorpe Award\nBULLET::::- Doak Walker Award\nBULLET::::- Campbell Trophy\nBULLET::::- Johnny Unitas Golden Arm Award\nBULLET::::- Home Depot Award\nBULLET::::- Ray Guy Award\nBULLET::::- John Mackey Award\nBULLET::::- Burlsworth Trophy\nBULLET::::- Jet Award\n\nBULLET::::- Walter Payton Award\nBULLET::::- Buck Buchanan Award\nBULLET::::- Jerry Rice Award\n\nBULLET::::- College Football Playoff\nBULLET::::- College athletics in the United States\nBULLET::::- College Football Hall of Fame\nBULLET::::- College football on radio\nBULLET::::- College football on television\nBULLET::::- College rugby\nBULLET::::- Helmet stickers\nBULLET::::- Homosexuality in American football\nBULLET::::- List of defunct college football teams\nBULLET::::- List of defunct college football conferences\nBULLET::::- List of historically significant college football games\nBULLET::::- List of sports attendance figures\nBULLET::::- Sports injury\n\nBULLET::::- \"The Invention Of Football\". \"Current Events\", 00113492, November 14, 2011, Vol. 111, Issue 8\nBULLET::::- Brian M. Ingrassia, \"The Rise of Gridiron University: Higher Education's Uneasy Alliance with Big-Time Football.\" Lawrence, Kansas: University Press of Kansas, 2012.\n\nBULLET::::- NCAA football official site\nBULLET::::- Composite television schedule of NCAA football games\n\nBULLET::::- College Football at d1sportsnet.com\nBULLET::::- College Football at Sports-Reference.com\nBULLET::::- Stassen College Football, comprehensive college football database\nBULLET::::- College Football Data Warehouse\nBULLET::::- Gridiron History\n\nBULLET::::- NCAA Football 2011 and 2012 Rules and Interpretations\n\nBULLET::::- Google Map of Division II Football Programs\nBULLET::::- Map of FBS Teams and Conferences\n"}
{"id": "6773", "url": "https://en.wikipedia.org/wiki?curid=6773", "title": "Ciprofloxacin", "text": "Ciprofloxacin\n\nCiprofloxacin is an antibiotic used to treat a number of bacterial infections. This includes bone and joint infections, intra abdominal infections, certain type of infectious diarrhea, respiratory tract infections, skin infections, typhoid fever, and urinary tract infections, among others. For some infections it is used in addition to other antibiotics. It can be taken by mouth, as eye drops, as ear drops, or intravenously.\nCommon side effects include nausea, vomiting, diarrhea and rash. Severe side effects include an increased risk of tendon rupture, hallucinations, and nerve damage. In people with myasthenia gravis, there is worsening muscle weakness. Rates of side effects appear to be higher than some groups of antibiotics such as cephalosporins but lower than others such as clindamycin. Studies in other animals raise concerns regarding use in pregnancy. No problems were identified, however, in the children of a small number of women who took the medication. It appears to be safe during breastfeeding. It is a second-generation fluoroquinolone with a broad spectrum of activity that usually results in the death of the bacteria.\nCiprofloxacin was patented in 1980 and introduced in 1987. It is on the World Health Organization's List of Essential Medicines, the safest and most effective medicines needed in a health system. It is available as a generic medication and is not very expensive. The wholesale cost in the developing world is between 0.03 and 0.13 a dose. In the United States it is sold for about 0.40 per dose. In 2016 it was the 102nd most prescribed medication in the United States with more than seven million prescriptions.\n\nCiprofloxacin is used to treat a wide variety of infections, including infections of bones and joints, endocarditis, gastroenteritis, malignant otitis externa, respiratory tract infections, cellulitis, urinary tract infections, prostatitis, anthrax, and chancroid.\n\nCiprofloxacin only treats bacterial infections; it does not treat viral infections such as the common cold. For certain uses including acute sinusitis, lower respiratory tract infections and uncomplicated gonorrhea, ciprofloxacin is not considered a first-line agent.\n\nCiprofloxacin occupies an important role in treatment guidelines issued by major medical societies for the treatment of serious infections, especially those likely to be caused by Gram-negative bacteria, including \"Pseudomonas aeruginosa\". For example, ciprofloxacin in combination with metronidazole is one of several first-line antibiotic regimens recommended by the Infectious Diseases Society of America for the treatment of community-acquired abdominal infections in adults. It also features prominently in treatment guidelines for acute pyelonephritis, complicated or hospital-acquired urinary tract infection, acute or chronic prostatitis, certain types of endocarditis, certain skin infections, and prosthetic joint infections.\n\nIn other cases, treatment guidelines are more restrictive, recommending in most cases that older, narrower-spectrum drugs be used as first-line therapy for less severe infections to minimize fluoroquinolone-resistance development. For example, the Infectious Diseases Society of America recommends the use of ciprofloxacin and other fluoroquinolones in urinary tract infections be reserved to cases of proven or expected resistance to narrower-spectrum drugs such as nitrofurantoin or trimethoprim/sulfamethoxazole. The European Association of Urology recommends ciprofloxacin as an alternative regimen for the treatment of uncomplicated urinary tract infections, but cautions that the potential for \"adverse events have to be considered\".\n\nAlthough approved by regulatory authorities for the treatment of respiratory infections, ciprofloxacin is not recommended for respiratory infections by most treatment guidelines due in part to its modest activity against the common respiratory pathogen \"Streptococcus pneumoniae\". \"Respiratory quinolones\" such as levofloxacin, having greater activity against this pathogen, are recommended as first line agents for the treatment of community-acquired pneumonia in patients with important co-morbidities and in patients requiring hospitalization (Infectious Diseases Society of America 2007). Similarly, ciprofloxacin is not recommended as a first-line treatment for acute sinusitis.\n\nCiprofloxacin is approved for the treatment of gonorrhea in many countries, but this recommendation is widely regarded as obsolete due to resistance development.\n\nIn the United States ciprofloxacin is pregnancy category C. This category includes drugs for which no adequate and well-controlled studies in human pregnancy exist, and for which animal studies have suggested the potential for harm to the fetus, but potential benefits may warrant use of the drug in pregnant women\ndespite potential risks. An expert review of published data on experiences with ciprofloxacin use during pregnancy by the Teratogen Information System concluded therapeutic doses during\npregnancy are unlikely to pose a substantial teratogenic risk (quantity and quality of data=fair), but the data are insufficient to state no risk exists. Exposure to quinolones, including levofloxacin, during the first-trimester is not associated with an increased risk of stillbirths, premature births, birth defects, or low birth weight.\n\nTwo small post-marketing epidemiology studies of mostly short-term, first-trimester exposure found that fluoroquinolones did not increase risk of major malformations, spontaneous abortions, premature birth, or low birth weight. The label notes, however, that these studies are insufficient to reliably evaluate the definitive safety or risk of less common defects by ciprofloxacin in pregnant women and their developing fetuses.\n\nFluoroquinolones have been reported as present in a mother's milk and thus passed on to the nursing child. The U.S. Food and Drug Administration (FDA) recommends that because of the risk of serious adverse reactions (including articular damage) in infants nursing from mothers taking ciprofloxacin, a decision should be made whether to discontinue nursing or discontinue the drug, taking into account the importance of the drug to the mother.\n\nOral and intravenous ciprofloxacin are approved by the FDA for use in children for only two indications due to the risk of permanent injury to the musculoskeletal system:\n\n1) Inhalational anthrax (postexposure)\n\n2) Complicated urinary tract infections and pyelonephritis due to \"Escherichia coli\", but never as first-line agents. Current recommendations by the American Academy of Pediatrics note the systemic use of ciprofloxacin in children should be restricted to infections caused by multidrug-resistant pathogens or when no safe or effective alternatives are available.\n\nIts spectrum of activity includes most strains of bacterial pathogens responsible for community-acquired pneumonias, bronchitis, urinary tract infections, and gastroenteritis. Ciprofloxacin is particularly effective against Gram-negative bacteria (such as \"Escherichia coli\", \"Haemophilus influenzae\", \"Klebsiella pneumoniae\", \"Legionella pneumophila\", \"Moraxella catarrhalis\", \"Proteus mirabilis\", and \"Pseudomonas aeruginosa\"), but is less effective against Gram-positive bacteria (such as methicillin-sensitive \"Staphylococcus aureus\", \"Streptococcus pneumoniae\", and \"Enterococcus faecalis\") than newer fluoroquinolones.\n\nAs a result of its widespread use to treat minor infections readily treatable with older, narrower spectrum antibiotics, many bacteria have developed resistance to this drug in recent years, leaving it significantly less effective than it would have been otherwise.\n\nResistance to ciprofloxacin and other fluoroquinolones may evolve rapidly, even during a course of treatment. Numerous pathogens, including enterococci, \"Streptococcus pyogenes\" and \"Klebsiella pneumoniae\" (quinolone-resistant) now exhibit resistance. Widespread veterinary usage of the fluoroquinolones, particularly in Europe, has been implicated. Meanwhile, some \"Burkholderia cepacia\", \"Clostridium innocuum\" and \"Enterococcus faecium\" strains have developed resistance to ciprofloxacin to varying degrees.\n\nFluoroquinolones had become the class of antibiotics most commonly prescribed to adults in 2002. Nearly half (42%) of those prescriptions in the U.S. were for conditions not approved by the FDA, such as acute bronchitis, otitis media, and acute upper respiratory tract infection, according to a study supported in part by the Agency for Healthcare Research and Quality. Additionally, they were commonly prescribed for medical conditions that were not even bacterial to begin with, such as viral infections, or those to which no proven benefit existed.\n\nContraindications include:\nBULLET::::- Taking tizanidine at the same time\nBULLET::::- Use by those who are hypersensitive to any member of the quinolone class of antimicrobial agents\n\nCiprofloxacin is also considered to be contraindicated in children (except for the indications outlined above), in pregnancy, to nursing mothers, and in people with epilepsy or other seizure disorders.\n\nAdverse effects can involve the tendons, muscles, joints, nerves, and the central nervous system.\n\nRates of adverse effects appear to be higher than with some groups of antibiotics such as cephalosporins but lower than with others such as clindamycin. Compared to other antibiotics some studies find a higher rate of adverse effects while others find no difference.\n\nIn clinical trials most of the adverse events were described as mild or moderate in severity, abated soon after the drug was discontinued, and required no treatment. Some adverse effects may be permanent. Ciprofloxacin was stopped because of an adverse event in 1% of people treated with the medication by mouth. The most frequently reported drug-related events, from trials of all formulations, all dosages, all drug-therapy durations, and for all indications, were nausea (2.5%), diarrhea (1.6%), abnormal liver function tests (1.3%), vomiting (1%), and rash (1%). Other adverse events occurred at rates of <1%.\n\nCiprofloxacin includes a black box warning in the United States due to an increased risk of tendinitis and tendon rupture, especially in people who are older than 60 years, people who also use corticosteroids, and people with kidney, lung, or heart transplants. Tendon rupture can occur during therapy or even months after discontinuation of the medication. One study found that fluoroquinolone use was associated with a 1.9-fold increase in tendon problems. The risk increased to 3.2 in those over 60 years of age and to 6.2 in those over the age of 60 who were also taking corticosteroids. Among the 46,766 quinolone users in the study, 38 (0.08%) cases of Achilles tendon rupture were identified.\n\nThe fluoroquinolones, including ciprofloxacin, are associated with an increased risk of cardiac toxicity, including QT interval prolongation, torsades de pointes, ventricular arrhythmia, and sudden death. \n\nThe 2013 FDA label warns of nervous system effects. Ciprofloxacin, like other fluoroquinolones, is known to trigger seizures or lower the seizure threshold, and may cause other central nervous system adverse effects. Headache, dizziness, and insomnia have been reported as occurring fairly commonly in postapproval review articles, along with a much lower incidence of serious CNS adverse effects such as tremors, psychosis, anxiety, hallucinations, paranoia, and suicide attempts, especially at higher doses. Like other fluoroquinolones, it is also known to cause peripheral neuropathy that may be irreversible, such as weakness, burning pain, tingling or numbness.\n\nCiprofloxacin is active in six of eight \"in vitro\" assays used as rapid screens for the detection of genotoxic effects, but is not active in \"in vivo\" assays of genotoxicity. Long-term carcinogenicity studies in rats and mice resulted in no carcinogenic or tumorigenic effects due to ciprofloxacin at daily oral dose levels up to 250 and 750 mg/kg to rats and mice, respectively (about 1.7 and 2.5 times the highest recommended therapeutic dose based upon mg/m). Results from photo co-carcinogenicity testing indicate ciprofloxacin does not reduce the time to appearance of UV-induced skin tumors as compared to vehicle control.\n\nThe other black box warning is that ciprofloxacin should not be used in people with myasthenia gravis due to possible exacerbation of muscle weakness which may lead to breathing problems resulting in death or ventilator support. Fluoroquinolones are known to block neuromuscular transmission. There are concerns that fluoroquinolones including ciprofloxacin can affect cartilage in young children.\n\n\"Clostridium difficile\"-associated diarrhea is a serious adverse effect of ciprofloxacin and other fluoroquinolones; it is unclear whether the risk is higher than with other broad-spectrum antibiotics.\n\nA wide range of rare but potentially fatal adverse effects reported to the U.S. FDA or the subject of case reports includes aortic dissection, toxic epidermal necrolysis, Stevens-Johnson syndrome, low blood pressure, allergic pneumonitis, bone marrow suppression, hepatitis or liver failure, and sensitivity to light. The medication should be discontinued if a rash, jaundice, or other sign of hypersensitivity occurs.\n\nChildren and the elderly are at a much greater risk of experiencing adverse reactions.\n\nOverdose of ciprofloxacin may result in reversible renal toxicity. Treatment of overdose includes emptying of the stomach by induced vomiting or gastric lavage, as well as administration of antacids containing magnesium, aluminum, or calcium to reduce drug absorption. Renal function and urinary pH should be monitored. Important support includes adequate hydration and urine acidification if necessary to prevent crystalluria. Hemodialysis or peritoneal dialysis can only remove less than 10% of ciprofloxacin. Ciprofloxacin may be quantified in plasma or serum to monitor for drug accumulation in patients with hepatic dysfunction or to confirm a diagnosis of poisoning in acute overdose victims.\n\nCiprofloxacin interacts with certain foods and several other drugs leading to undesirable increases or decreases in the serum levels or distribution of one or both drugs.\n\nCiprofloxacin should not be taken with antacids containing magnesium or aluminum, highly buffered drugs (sevelamer, lanthanum carbonate, sucralfate, didanosine), or with supplements containing calcium, iron, or zinc. It should be taken two hours before or six hours after these products. Magnesium or aluminum antacids turn ciprofloxacin into insoluble salts that are not readily absorbed by the intestinal tract, reducing peak serum concentrations by 90% or more, leading to therapeutic failure. Additionally, it should not be taken with dairy products or calcium-fortified juices alone, as peak serum concentration and the area under the serum concentration-time curve can be reduced up to 40%. However, ciprofloxacin may be taken with dairy products or calcium-fortified juices as part of a meal.\n\nCiprofloxacin inhibits the drug-metabolizing enzyme CYP1A2 and thereby can reduce the clearance of drugs metabolized by that enzyme. CYP1A2 substrates that exhibit increased serum levels in ciprofloxacin-treated patients include tizanidine, theophylline, caffeine, methylxanthines, clozapine, olanzapine, and ropinirole. Co-administration of ciprofloxacin with the CYP1A2 substrate tizanidine (Zanaflex) is contraindicated due to a 583% increase in the peak serum concentrations of tizanidine when administered with ciprofloxacin as compared to administration of tizanidine alone. Use of ciprofloxacin is cautioned in patients on theophylline due to its narrow therapeutic index. The authors of one review recommended that patients being treated with ciprofloxacin reduce their caffeine intake. Evidence for significant interactions with several other CYP1A2 substrates such as cyclosporine is equivocal or conflicting.\n\nThe Committee on Safety of Medicines and the FDA warn that central nervous system adverse effects, including seizure risk, may be increased when NSAIDs are combined with quinolones. The mechanism for this interaction may involve a synergistic increased antagonism of GABA neurotransmission.\n\nAltered serum levels of the antiepileptic drugs phenytoin and carbamazepine (increased and decreased) have been reported in patients receiving concomitant ciprofloxacin.\n\nCiprofloxacin is a potent inhibitor of CYP1A2, CYP2D6, and CYP3A4.\n\nCiprofloxacin is a broad-spectrum antibiotic of the fluoroquinolone class. It is active against some Gram-positive and many Gram-negative bacteria. It functions by inhibiting DNA gyrase, and a type II topoisomerase, topoisomerase IV, necessary to separate bacterial DNA, thereby inhibiting cell division.\n\nCiprofloxacin for systemic administration is available as immediate-release tablets, extended-release tablets, an oral suspension, and as a solution for intravenous administration. When administered over one hour as an intravenous infusion, ciprofloxacin rapidly distributes into the tissues, with levels in some tissues exceeding those in the serum. Penetration into the central nervous system is relatively modest, with cerebrospinal fluid levels normally less than 10% of peak serum concentrations. The serum half-life of ciprofloxacin is about 4–6 hours, with 50-70% of an administered dose being excreted in the urine as unmetabolized drug. An additional 10% is excreted in urine as metabolites. Urinary excretion is virtually complete 24 hours after administration. Dose adjustment is required in the elderly and in those with renal impairment.\n\nCiprofloxacin is weakly bound to serum proteins (20-40%), but is an inhibitor of the drug-metabolizing enzyme cytochrome P450 1A2, which leads to the potential for clinically important drug interactions with drugs metabolized by that enzyme.\n\nCiprofloxacin is about 70% orally available when administered orally, so a slightly higher dose is needed to achieve the same exposure when switching from IV to oral administration\n\nThe extended release oral tablets allow once-daily administration by releasing the drug more slowly in the gastrointestinal tract. These tablets contain 35% of the administered dose in an immediate-release form and 65% in a slow-release matrix. Maximum serum concentrations are achieved between 1 and 4 hours after administration. Compared to the 250- and 500-mg immediate-release tablets, the 500-mg and 1000-mg XR tablets provide higher C, but the 24‑hour AUCs are equivalent.\n\nCiprofloxacin immediate-release tablets contain ciprofloxacin as the hydrochloride salt, and the XR tablets contain a mixture of the hydrochloride salt as the free base.\n\nCiprofloxacin is 1-cyclopropyl-6-fluoro-1,4-dihydro-4-oxo-7-(1-piperazinyl)-3-quinolinecarboxylic acid. Its empirical formula is CHFNO and its molecular weight is 331.4 g/mol. It is a faintly yellowish to light yellow crystalline substance.\n\nCiprofloxacin hydrochloride (USP) is the monohydrochloride monohydrate salt of ciprofloxacin. It is a faintly yellowish to light yellow crystalline substance with a molecular weight of 385.8 g/mol. Its empirical formula is CHFNOHCl•HO.\n\nCiprofloxacin is the most widely used of the second-generation quinolones. In 2010, over 20 million prescriptions were written, making it the 35th-most commonly prescribed generic drug and the 5th-most commonly prescribed antibacterial in the U.S.\n\nThe first members of the quinolone antibacterial class were relatively low-potency drugs such as nalidixic acid, used mainly in the treatment of urinary tract infections owing to their renal excretion and propensity to be concentrated in urine. In 1979, the publication of a patent filed by the pharmaceutical arm of Kyorin Seiyaku Kabushiki Kaisha disclosed the discovery of norfloxacin, and the demonstration that certain structural modifications including the attachment of a fluorine atom to the quinolone ring leads to dramatically enhanced antibacterial potency. In the aftermath of this disclosure, several other pharmaceutical companies initiated research and development programs with the goal of discovering additional antibacterial agents of the fluoroquinolone class.\n\nThe fluoroquinolone program at Bayer focused on examining the effects of very minor changes to the norfloxacin structure. In 1983, the company published \"in vitro\" potency data for ciprofloxacin, a fluoroquinolone antibacterial having a chemical structure differing from that of norfloxacin by the presence of a single carbon atom. This small change led to a two- to 10-fold increase in potency against most strains of Gram-negative bacteria. Importantly, this structural change led to a four-fold improvement in activity against the important Gram-negative pathogen \"Pseudomonas aeruginosa\", making ciprofloxacin one of the most potent known drugs for the treatment of this intrinsically antibiotic-resistant pathogen.\n\nThe oral tablet form of ciprofloxacin was approved in October 1987, just one year after the approval of norfloxacin. In 1991, the intravenous formulation was introduced. Ciprofloxacin sales reached a peak of about 2 billion euros in 2001, before Bayer's patent expired in 2004, after which annual sales have averaged around €200 million.\n\nThe name probably originates from the International Scientific Nomenclature: ci- (alteration of cycl-) + propyl + fluor- + ox- + az- + -mycin.\n\nIt is available as a generic medication and not very expensive. Wholesale it costs between 0.03 and 0.13 a dose. In the United States it is sold for about 0.40 per dose.\n\nOn 24 October 2001, the Prescription Access Litigation (PAL) project filed suit to dissolve an agreement between Bayer and three of its competitors which produced generic versions of drugs (Barr Laboratories, Rugby Laboratories, and Hoechst-Marion-Roussel) that PAL claimed was blocking access to adequate supplies and cheaper, generic versions of ciprofloxacin. The plaintiffs charged that Bayer Corporation, a unit of Bayer AG, had unlawfully paid the three competing companies a total of $200 million to prevent cheaper, generic versions of ciprofloxacin from being brought to the market, as well as manipulating its price and supply. Numerous other consumer advocacy groups joined the lawsuit. On 15 October 2008, five years after Bayer's patent had expired, the United States District Court for the Eastern District of New York granted Bayer's and the other defendants' motion for summary judgment, holding that any anticompetitive effects caused by the settlement agreements between Bayer and its codefendants were within the exclusionary zone of the patent and thus could not be redressed by federal antitrust law, in effect upholding Bayer's agreement with its competitors.\n\nCiprofloxacin for systemic administration is available as immediate-release tablets, as extended-release tablets, as an oral suspension, and as a solution for intravenous infusion. It is also available for local administration as eye drops and ear drops.\n\nA class action was filed against Bayer AG on behalf of employees of the Brentwood Post Office in Washington, D.C., and workers at the U.S. Capitol, along with employees of American Media, Inc. in Florida and postal workers in general who alleged they suffered serious adverse effects from taking ciprofloxacin in the aftermath of the anthrax attacks in 2001. The action alleged Bayer failed to warn class members of the potential side effects of the drug, thereby violating the Pennsylvania Unfair Trade Practices and Consumer Protection Laws. The class action was defeated and the litigation abandoned by the plaintiffs. A similar action was filed in 2003 in New Jersey by four New Jersey postal workers but was withdrawn for lack of grounds, as workers had been informed of the risks of ciprofloxacin when they were given the option of taking the drug.\n\nAs resistance to ciprofloxacin has grown since its introduction, research has been conducted to discover and develop analogs that can be effective against resistant bacteria; some have been looked at in antiviral models as well.\n\nBULLET::::- Ciprofloxacin Ophthalmic\n"}
{"id": "6774", "url": "https://en.wikipedia.org/wiki?curid=6774", "title": "Consubstantiation", "text": "Consubstantiation\n\nConsubstantiation is a Christian theological doctrine that (like transubstantiation) describes the real presence in the Eucharist. It holds that during the sacrament, the substance of the body and blood of Christ are present \"alongside\" the substance of the bread and wine, which remain present. \nIt was part of the doctrines of Lollardy and considered a heresy by the Roman Catholic Church.\n\nIn the early church it was common for the bread to be regarded as bread yet also as the Body of Christ, and the wine regarded as wine yet also as the Blood of Christ; indeed, some argued that to deny that both bread and wine, and Body and Blood, were present was to deny the Incarnation.\n\nIn about 150, Justin Martyr, referring to the Eucharist, wrote: \"Not as common bread and common drink do we receive these; but in like manner as Jesus Christ our Savior, having been made flesh by the Word of God, had both flesh and blood for our salvation, so likewise have we been taught that the food which is blessed by the prayer of His word, and from which our blood and flesh by transmutation are nourished, is the flesh and blood of that Jesus who was made flesh.\"\n\nJustin Martyr wrote, in the \"Dialogue with Trypho\", ch 70: \"Now it is evident, that in this prophecy [allusion is made] to the bread which our Christ gave us to eat, in remembrance of His being made flesh for the sake of His believers, for whom also He suffered; and to the cup which He gave us to drink, in remembrance of His own blood, with giving of thanks.\"\n\nIrenaeus of Lyons wrote, \"But what consistency is there in those who hold that the bread over which thanks have been given is the body of their Lord, and the cup his blood, if they do not acknowledge that He is the Son of the Creator… How can they say that the flesh which has been nourished by the Body of the Lord and by his blood gives way to corruption and does not partake of life? …For as the bread from the earth, receiving the invocation of God, is no longer common bread but the Eucharist, consisting of two elements, earthly and heavenly…”\n\nThe doctrine of consubstantiation is often held in contrast to the doctrine of transubstantiation. While some Lutherans use the term \"consubstantiation\" to describe their doctrine, many reject it as not accurately reflecting the eucharistic doctrine of Martin Luther, the sacramental union. They reject the concept of consubstantiation because it replaces what they believe to be the biblical doctrine with a philosophical construct, denotes a mixing of substances (bread and wine with body and blood), and denotes a \"gross, Capernaitic, carnal\" presence of the body and blood of Christ.\n\nIn England in the late 14th century, there was a political and religious movement known as Lollardy. Among much broader goals, the Lollards affirmed a form of consubstantiation—that the Eucharist remained physically bread and wine, while becoming spiritually the body and blood of Christ. Lollardy survived up until the time of the English Reformation.\n\nLiterary critic Kenneth Burke's dramatism takes this concept and utilizes it in secular rhetorical theory to look at the dialectic of unity and difference within the context of logology.\n\nBULLET::::- Eucharistic theology\nBULLET::::- Impanation\nBULLET::::- Real Presence\nBULLET::::- Transignification\n"}
{"id": "6775", "url": "https://en.wikipedia.org/wiki?curid=6775", "title": "Chlorophyta", "text": "Chlorophyta\n\nChlorophyta or Prasinophyta is a taxon of green algae informally called chlorophytes. The name is used in two very different senses, so care is needed to determine the use by a particular author. In older classification systems, it refers to a highly paraphyletic group of \"all\" the green algae within the green plants (Viridiplantae) and thus includes about 7,000 species of mostly aquatic photosynthetic eukaryotic organisms. In newer classifications, it refers to the sister of the streptophytes/charophytes. The clade Streptophyta consists of the Charophyta in which the Embryophyta emerged. In this sense the Chlorophyta includes only about 4,300 species. About 90% of all known species live in freshwater.\nLike the land plants (bryophytes and tracheophytes), green algae contain chlorophyll a and chlorophyll b and store food as starch in their plastids.\n\nWith the exception of Palmophyllophyceae, Trebouxiophyceae, Ulvophyceae and Chlorophyceae, which show various degrees of multicellularity, all the Chlorophyta lineages are unicellular. Some members of the group form symbiotic relationships with protozoa, sponges, and cnidarians. Others form symbiotic relationships with fungi to form lichens, but the majority of species are free-living. Some conduct sexual reproduction, which is oogamous or isogamous. All members of the clade have motile flagellated swimming cells. While most species live in freshwater habitats and a large number in marine habitats, other species are adapted to a wide range of land environments. For example, \"Chlamydomonas nivalis\", which causes Watermelon snow, lives on summer alpine snowfields. Others, such as \"Trentepohlia\" species, live attached to rocks or woody parts of trees. \"Monostroma kuroshiense\", an edible green alga cultivated worldwide and most expensive among green algae, belongs to this group.\n\nSpecies of Chlorophyta (treated as what is now considered one of the two main clades of Viridiplantae) are common inhabitants of marine, freshwater and terrestrial environments. Several species have adapted to specialised and extreme environments, such as deserts, arctic environments, hypersaline habitats, marine deep waters, deep-sea hydrothermal vents and habitats that experiences extreme changes in temperature, light and salinity. Some groups, such as the Trentepohliales are exclusively found on land. Several species of Chlorophyta live in symbiosis with a diverse range of eukaryotes, including fungi (to form lichens), ciliates, forams, cnidarians and molluscs.\n\nCharacteristics used for the classification of Chlorophyta are: type of zoid, mitosis (karyokynesis), cytokinesis, organization level, life cycle, type of gametes, cell wall polysaccharides and more recently genetic data.\n\nA newer proposed classification follows Leliaert et al. 2011 and modified with Silar 2016, Leliaert 2016 and Lopes dos Santos et al. 2017 for the green algae clades and Novíkov & Barabaš-Krasni 2015 for the land plants clade. Sánchez-Baracaldo et al. is followed for the basal clades.\n\nSimplified phylogeny of the Chlorophyta, according to Leliaert \"et al\". 2012. Note that many algae previously classified in Chlorophyta are placed here in Streptophyta.\nBULLET::::- Viridiplantae\nBULLET::::- Chlorophyta\nBULLET::::- core chlorophytes\nBULLET::::- Ulvophyceae\nBULLET::::- Cladophorales\nBULLET::::- Dasycladales\nBULLET::::- Bryopsidales\nBULLET::::- Trentepohliales\nBULLET::::- Ulvales-Ulotrichales\nBULLET::::- Oltmannsiellopsidales\nBULLET::::- Chlorophyceae\nBULLET::::- Oedogoniales\nBULLET::::- Chaetophorales\nBULLET::::- Chaetopeltidiales\nBULLET::::- Chlamydomonadales\nBULLET::::- Sphaeropleales\nBULLET::::- Trebouxiophyceae\nBULLET::::- Chlorellales\nBULLET::::- Oocystaceae\nBULLET::::- Microthamniales\nBULLET::::- Trebouxiales\nBULLET::::- \"Prasiola\" clade\nBULLET::::- Chlorodendrophyceae\nBULLET::::- prasinophytes (paraphyletic)\nBULLET::::- Pyramimonadales\nBULLET::::- Mamiellophyceae\nBULLET::::- Pycnococcaceae\nBULLET::::- Nephroselmidophyceae\nBULLET::::- Prasinococcales\nBULLET::::- Palmophyllales\nBULLET::::- Streptophyta\nBULLET::::- charophytes\nBULLET::::- Mesostigmatophyceae\nBULLET::::- Chlorokybophyceae\nBULLET::::- Klebsormidiophyceae\nBULLET::::- Charophyceae\nBULLET::::- Zygnematophyceae\nBULLET::::- Coleochaetophyceae\nBULLET::::- Embryophyta (land plants)\n\nA possible classification when Chlorophyta refers to one of the two clades of the Viridiplantae is shown below.\nBULLET::::- Class Prasinophyceae\nBULLET::::- Class Chlorophyceae\nBULLET::::- Class Trebouxiophyceae\nBULLET::::- Class Ulvophyceae\n\nBULLET::::- Division Chlorophyta (green algae sensu stricto)\nBULLET::::- Subdivision Chlorophytina\nBULLET::::- Class Chlorophyceae (chlorophytes)\nBULLET::::- Order Chlamydomonadales (+ some Chlorococcales + some Tetrasporales + some Chlorosarcinales)\nBULLET::::- Order Sphaeropleales (sensu Deason, plus \"Bracteacoccus, Schroederia, Scenedesmaceae, Selanastraceae\")\nBULLET::::- Order Oedogoniales\nBULLET::::- Order Chaetopeltidales\nBULLET::::- Order Chaetophorales\nBULLET::::- Incertae Sedis (\"Cylindrocapsa\" clade, \"Mychonastes\" clade)\nBULLET::::- Class Ulvophyceae (ulvophytes)\nBULLET::::- Order Ulotrichales\nBULLET::::- Order Ulvales\nBULLET::::- Order Siphoncladales/Cladophorales\nBULLET::::- Order Caulerpales\nBULLET::::- Order Dasycladales\nBULLET::::- Class Trebouxiophyceae (trebouxiophytes)\nBULLET::::- Order Trebouxiales\nBULLET::::- Order Microthamniales\nBULLET::::- Order Prasiolales\nBULLET::::- Order Chlorellales\nBULLET::::- Class Prasinophyceae (prasinophytes)\nBULLET::::- Order Pyramimonadales\nBULLET::::- Order Mamiellales\nBULLET::::- Order Pseudoscourfieldiales\nBULLET::::- Order Chlorodendrales\nBULLET::::- Incertae sedis (Unnamed clade of coccoid taxa)\nBULLET::::- Division Charophyta (charophyte algae and embryophytes)\nBULLET::::- Class Mesostigmatophyceae (mesostigmatophytes)\nBULLET::::- Class Chlorokybophyceae (chlorokybophytes)\nBULLET::::- Class Klebsormidiophyceae (klebsormidiophytes)\nBULLET::::- Class Zygnemophyceae (conjugates)\nBULLET::::- Order Zygnematales (filamentous conjugates and saccoderm desmids)\nBULLET::::- Order Desmidiales (placoderm desmids)\nBULLET::::- Class Coleochaetophyceae (coleochaetophytes)\nBULLET::::- Order Coleochaetales\nBULLET::::- Subdivision Streptophytina\nBULLET::::- Class Charophyceae (reverts to use of GM Smith)\nBULLET::::- Order Charales (charophytes sensu stricto)\nBULLET::::- Class Embryophyceae (embryophytes)\n\nClassification of the Chlorophyta, treated as all green algae, according to Hoek, Mann and Jahns 1995.\nBULLET::::- Class Prasinophyceae (orders Mamiellales, Pseudocourfeldiales, Pyramimonadales, Chlorodendrales)\nBULLET::::- Class Chlorophyceae (orders Volvocales [including the Tetrasporales], Chlorococcales, Chaetophorales, Oedogoniales)\nBULLET::::- Class Ulvophyceae (orders Codiolales, Ulvales)\nBULLET::::- Class Cladophorophyceae (order Cladophorales)\nBULLET::::- Class Bryopsidophyceae (orders Bryopsidales, Halimedales)\nBULLET::::- Class Dasycladophyceae (order Dasycladales)\nBULLET::::- Class Trentepohliophyceae (order Trentepohliales)\nBULLET::::- Class Pleurastrophyceae (order Pleurastrales)\nBULLET::::- \"Incertae sedis\" (order Prasiolales)\nBULLET::::- Class Klebsormidiophyceae (orders Klebsormidiales, Coleochaetales)\nBULLET::::- Class Zygnematophyceae (order Zygnematales, Desmidiales)\nBULLET::::- Class Charophyceae (order Charales)\n\nIn a note added in proof, an alternative classification is presented for the algae of the class Chlorophyceae:\nBULLET::::- Class Chlamydophyceae (orders Volvocales, Chlorococcales, Chaetophorales)\nBULLET::::- Class Oedogoniophyceae (order Oedogoniales)\nBULLET::::- Class Chlorophyceae (order Chlorellales)\n\nClassification of the Chlorophyta and Charophyta according to Bold and Wynne 1985.\n\nBULLET::::- Chlorophyta, Chlorophyceae (16 orders)\nBULLET::::- Volvocales\nBULLET::::- Tetrasporales\nBULLET::::- Chlorococcales\nBULLET::::- Chlorosarcinales\nBULLET::::- Ulotrichales\nBULLET::::- Sphaeropleales\nBULLET::::- Chaetophorales\nBULLET::::- Trentepohliales\nBULLET::::- Oedogoniales\nBULLET::::- Ulvales\nBULLET::::- Cladophorales\nBULLET::::- Acrosiphoniales\nBULLET::::- Caulerpales\nBULLET::::- Siphonocladales\nBULLET::::- Dasycladales\nBULLET::::- Zygnematales\nBULLET::::- Charophyta, Charophyceae (1 order)\nBULLET::::- Charales\n\nClassification of the Chlorophyta according to Mattox & Stewart 1984:\n\nBULLET::::- Micromonadophyceae (similar to Prasinophyceae; Tetraselmidiales transferred to Pleurastrophyceae)\nBULLET::::- Charophyceae Rabenhorst\nBULLET::::- Chlorokybales\nBULLET::::- Klebsormidiales\nBULLET::::- Zygnematales\nBULLET::::- Coleochaetales\nBULLET::::- Charales\nBULLET::::- Ulvophyceae\nBULLET::::- Pleurastrophyceae\nBULLET::::- Tetraselmidiales\nBULLET::::- Pleurastrales\nBULLET::::- Chlorophyceae Wille in Warming\nBULLET::::- Chlamydomonadales\nBULLET::::- Volvocales\nBULLET::::- Chlorococcales\nBULLET::::- Sphaeropleales\nBULLET::::- Chlorosarcinales\nBULLET::::- Chaetophorales\nBULLET::::- Oedogoniales\n\nClassification of the Chlorophyta according to Fott 1971.\nBULLET::::- Class Chlorophyceae\nBULLET::::- Class Conjugatophyceae\nBULLET::::- Class Charophyceae\n\nClassification of the Chlorophyta and related algae according to Round 1971.\n\nBULLET::::- \"green algae\"\nBULLET::::- Euglenophyta\nBULLET::::- Prasinophyta\nBULLET::::- Charophyta\nBULLET::::- Chlorophyta\nBULLET::::- Zygnemaphyceae (= Conjugatophyceae; orders Mesotaeniales, Zygnematales, Gonatozygales, Desmidiales)\nBULLET::::- Oedogoniophyceae (order Oedogoniales)\nBULLET::::- Bryopsidophyceae\nBULLET::::- Hemisiphoniidae (orders Cladophorales, Sphaeropleales, Acrosiphoniales\nBULLET::::- Cystosiphoniidae (orders Dasycladales, Siphonocladales, Chlorochytriales)\nBULLET::::- Eusiphoniidae (orders Derbesiales, Codiales, Caulerpales, Dichotomosiphonales, Phyllosiphonales)\nBULLET::::- Chlorophyceae\nBULLET::::- orders Chlamydomonadales, Volvocales, Polyblepharidales, Tetrasporales, Chlorodendrales, Chlorosarcinales, Chlorococcales\nBULLET::::- orders Ulotrichales, Codiolales, Ulvales, Prasiolales, Cylindrocapsales, Microsporales\nBULLET::::- orders Chaetophorales, Coleochaetales, Trentepohliales, Pleurococcales, Ulvellales\n\nClassification of the Chlorophyta according to Smith 1938:\n\nBULLET::::- Class 1. Chlorophyceae\nBULLET::::- Class 2. Charophyceae\n\n"}
{"id": "6776", "url": "https://en.wikipedia.org/wiki?curid=6776", "title": "Capybara", "text": "Capybara\n\nThe capybara (\"Hydrochoerus hydrochaeris\") is a mammal native to South America. It is the largest living rodent in the world. Also called chigüire, chigüiro (in Colombia and Venezuela) and carpincho, it is a member of the genus \"Hydrochoerus\", of which the only other extant member is the lesser capybara (\"Hydrochoerus isthmius\"). Its close relatives include guinea pigs and rock cavies, and it is more distantly related to the agouti, the chinchilla, and the coypu. The capybara inhabits savannas and dense forests and lives near bodies of water. It is a highly social species and can be found in groups as large as 100 individuals, but usually lives in groups of 10–20 individuals. The capybara is not a threatened species but it is hunted for its meat and hide and also for grease from its thick fatty skin, which is used in the pharmaceutical trade.\n\nIts common name is derived from Tupi ', a complex agglutination of ' (leaf) + ' (slender) + ' (eat) + \"\" (a suffix for agent nouns), meaning \"one who eats slender leaves\", or \"grass-eater\". Capybaras were called several times \"Cuartins\" in Colombia in 2018: in Eje Cafetero (Alcalà) and near Barranquilla where the meat was offered as \"Cuartin Asado\".\n\nThe scientific name, both \"hydrochoerus\" and \"hydrochaeris\", comes from Greek (' \"water\") and (' \"pig, hog\").\n\nThe capybara and the lesser capybara belong to the subfamily Hydrochoerinae along with the rock cavies. The living capybaras and their extinct relatives were previously classified in their own family Hydrochoeridae. Since 2002, molecular phylogenetic studies have recognized a close relationship between \"Hydrochoerus\" and \"Kerodon\", the rock cavies, supporting placement of both genera in a subfamily of Caviidae.\n\nPaleontological classifications previously used Hydrochoeridae for all capybaras, while using Hydrochoerinae for the living genus and its closest fossil relatives, such as \"Neochoerus\", but more recently have adopted the classification of Hydrochoerinae within Caviidae. The taxonomy of fossil hydrochoerines is also in a state of flux. In recent years, the diversity of fossil hydrochoerines has been substantially reduced. This is largely due to the recognition that capybara molar teeth show strong variation in shape over the life of an individual. In one instance, material once referred to four genera and seven species on the basis of differences in molar shape is now thought to represent differently aged individuals of a single species, \"Cardiatherium paranense\".\nAmong fossil species, the name \"capybara\" can refer to the many species of Hydrochoerinae that are more closely related to the modern \"Hydrochoerus\" than to the \"cardiomyine\" rodents like \"Cardiomys\". The fossil genera \"Cardiatherium\", \"Phugatherium\", \"Hydrochoeropsis\", and \"Neochoerus\" are all capybaras under that concept.\n\nThe capybara has a heavy, barrel-shaped body and short head, with reddish-brown fur on the upper part of its body that turns yellowish-brown underneath. Its sweat glands can be found in the surface of the hairy portions of its skin, an unusual trait among rodents. The animal lacks down hair, and its guard hair differs little from over hair.\n\nAdult capybaras grow to in length, stand tall at the withers, and typically weigh , with an average in the Venezuelan llanos of . Females are slightly heavier than males. The top recorded weights are for a wild female from Brazil and for a wild male from Uruguay. Also, an 81 kg individual was reported in São Paulo in 2001 or 2002. The dental formula is . Capybaras have slightly webbed feet and vestigial tails. Their hind legs are slightly longer than their forelegs; they have three toes on their rear feet and four toes on their front feet. Their muzzles are blunt, with nostrils, and the eyes and ears are near the top of their heads.\n\nIts karyotype has 2n = 66 and FN = 102.\n\nCapybaras are semiaquatic mammals found throughout almost all countries of South America except Chile. They live in densely forested areas near bodies of water, such as lakes, rivers, swamps, ponds, and marshes, as well as flooded savannah and along rivers in the tropical rainforest. They are superb swimmers and can hold their breath underwater for up to five minutes at a time. Capybara have flourished in cattle ranches. They roam in home ranges averaging 10 hectares (25 acres) in high-density populations.\n\nMany escapees from captivity can also be found in similar watery habitats around the world. Sightings are fairly common in Florida, although a breeding population has not yet been confirmed. In 2011, one specimen was spotted on the Central Coast of California.\n\nCapybaras are herbivores, grazing mainly on grasses and aquatic plants, as well as fruit and tree bark. They are very selective feeders and feed on the leaves of one species and disregard other species surrounding it. They eat a greater variety of plants during the dry season, as fewer plants are available. While they eat grass during the wet season, they have to switch to more abundant reeds during the dry season. Plants that capybaras eat during the summer lose their nutritional value in the winter, so are not consumed at that time. The capybara's jaw hinge is not perpendicular, so they chew food by grinding back-and-forth rather than side-to-side. Capybaras are autocoprophagous, meaning they eat their own feces as a source of bacterial gut flora, to help digest the cellulose in the grass that forms their normal diet, and to extract the maximum protein and vitamins from their food. They may also regurgitate food to masticate again, similar to cud-chewing by cattle. As is the case with other rodents, the front teeth of capybaras grow continually to compensate for the constant wear from eating grasses; their cheek teeth also grow continuously.\n\nLike its relative the guinea pig, the capybara does not have the capacity to synthesize vitamin C, and capybaras not supplemented with vitamin C in captivity have been reported to develop gum disease as a sign of scurvy.\n\nThey can have a lifespan of 8–10 years, but live less than four years in the wild, because they are \"a favourite food of jaguar, puma, ocelot, eagle, and caiman\". The capybara is also the preferred prey of the anaconda.\n\nCapybaras are known to be gregarious. While they sometimes live solitarily, they are more commonly found in groups of around 10–20 individuals, with two to four adult males, four to seven adult females, and the remainder juveniles. Capybara groups can consist of as many as 50 or 100 individuals during the dry season when the animals gather around available water sources. Males establish social bonds, dominance, or general group consensus. They can make dog-like barks when threatened or when females are herding young.\n\nCapybaras have two types of scent glands; a morillo (), located on the snout, and anal glands. Both sexes have these glands, but males have much larger morillos and use their anal glands more frequently. The anal glands of males are also lined with detachable hairs. A crystalline form of scent secretion is coated on these hairs and is released when in contact with objects such as plants. These hairs have a longer-lasting scent mark and are tasted by other capybaras. Capybaras scent-mark by rubbing their morillos on objects, or by walking over scrub and marking it with their anal glands. Capybaras can spread their scent further by urinating; however, females usually mark without urinating and scent-mark less frequently than males overall. Females mark more often during the wet season when they are in estrus. In addition to objects, males also scent-mark females.\n\nWhen in estrus, the female's scent changes subtly and nearby males begin pursuit. In addition, a female alerts males she is in estrus by whistling through her nose. During mating, the female has the advantage and mating choice. Capybaras mate only in water, and if a female does not want to mate with a certain male, she either submerges or leaves the water. Dominant males are highly protective of the females, but they usually cannot prevent some of the subordinates from copulating. The larger the group, the harder it is for the male to watch all the females. Dominant males secure significantly more matings than each subordinate, but subordinate males, as a class, are responsible for more matings than each dominant male. The lifespan of the capybara's sperm is longer than that of other rodents.\n\nCapybara gestation is 130–150 days, and produces a litter of four capybara young on average, but may produce between one and eight in a single litter. Birth is on land and the female rejoins the group within a few hours of delivering the newborn capybaras, which join the group as soon as they are mobile. Within a week, the young can eat grass, but continue to suckle—from any female in the group—until weaned around 16 weeks. The young form a group within the main group. Alloparenting has been observed in this species. Breeding peaks between April and May in Venezuela and between October and November in Mato Grosso, Brazil.\n\nThough quite agile on land, capybaras are equally at home in the water. They are excellent swimmers, and can remain completely submerged for up to five minutes, an ability they use to evade predators. Capybaras can sleep in water, keeping only their noses out of the water. As temperatures increase during the day, they wallow in water and then graze during the late afternoon and early evening. They also spend time wallowing in mud. They rest around midnight and then continue to graze before dawn.\n\nCapybaras are not considered a threatened species; their population is stable throughout most of their South American range, though in some areas hunting has reduced their numbers.\n\nCapybaras are hunted for their meat and pelts in some areas, and otherwise killed by humans who see their grazing as competition for livestock. In some areas, they are farmed, which has the effect of ensuring the wetland habitats are protected. Their survival is aided by their ability to breed rapidly.\n\nCapybaras have adapted well to urbanization in South America. They can be found in many areas in zoos and parks, and may live for 12 years in captivity. Capybaras are gentle and usually allow humans to pet and hand-feed them, but physical contact is normally discouraged, as their ticks can be vectors to Rocky Mountain spotted fever.\n\nThe European Association of Zoos and Aquaria asked Drusillas Park in Alfriston, Sussex, England to keep the studbook for capybaras, to monitor captive populations in Europe. The studbook includes information about all births, deaths and movements of capybaras, as well as how they are related.\n\nCapybaras are farmed for meat and skins in South America. The meat is considered unsuitable to eat in some areas, while in other areas it is considered an important source of protein. In parts of South America, especially in Venezuela, capybara meat is popular during Lent and Holy Week as the Catholic Church previously issued special dispensation to allow it to be eaten while other meats are generally forbidden.\n\nAlthough it is illegal in some states, capybaras are occasionally kept as pets in the United States.\n\nThe image of a capybara features on the 2-peso coin of Uruguay.\n\nIn Japan, following the lead of Izu Shaboten Park in 1982, multiple establishments in Japan that raise capybaras have adopted the practice of having them relax in onsen during the winter.\n\nBULLET::::- \"Josephoartigasia monesi\", an extinct species identified as the largest rodent ever\nBULLET::::- Kurloff cell, a type of cell found in capybaras and guinea pigs\n\nBULLET::::- Animal Diversity Web \"Hydrochoerus hydrochaeris\"\nBULLET::::- Capybara information\n"}
{"id": "6777", "url": "https://en.wikipedia.org/wiki?curid=6777", "title": "Computer animation", "text": "Computer animation\n\nComputer animation is the process used for digitally generating animated images. The more general term computer-generated imagery (CGI) encompasses both static scenes and dynamic images, while computer animation \"only\" refers to moving images. Modern computer animation usually uses 3D computer graphics, although 2D computer graphics are still used for stylistic, low bandwidth, and faster real-time renderings. Sometimes, the target of the animation is the computer itself, but sometimes film as well.\n\nComputer animation is essentially a digital successor to stop motion techniques, but using 3D models, and traditional animation techniques using frame-by-frame animation of 2D illustrations. Computer-generated animations are more controllable than other, more physically based processes, like constructing miniatures for effects shots, or hiring extras for crowd scenes, because it allows the creation of images that would not be feasible using any other technology. It can also allow a single graphic artist to produce such content without the use of actors, expensive set pieces, or props. To create the illusion of movement, an image is displayed on the computer monitor and repeatedly replaced by a new image that is similar to it but advanced slightly in time (usually at a rate of 24, 25, or 30 frames/second). This technique is identical to how the illusion of movement is achieved with television and motion pictures.\n\nFor 3D animations, objects (models) are built on the computer monitor (modeled) and 3D figures are rigged with a virtual skeleton. For 2D figure animations, separate objects (illustrations) and separate transparent layers are used with or without that virtual skeleton. Then the limbs, eyes, mouth, clothes, etc. of the figure are moved by the animator on key frames. The differences in appearance between key frames are automatically calculated by the computer in a process known as tweening or morphing. Finally, the animation is rendered.\n\nFor 3D animations, all frames must be rendered after the modeling is complete. For 2D vector animations, the rendering process is the key frame illustration process, while tweened frames are rendered as needed. For pre-recorded presentations, the rendered frames are transferred to a different format or medium, like digital video. The frames may also be rendered in real time as they are presented to the end-user audience. Low bandwidth animations transmitted via the internet (e.g. Adobe Flash, X3D) often use software on the end-users computer to render in real time as an alternative to streaming or pre-loaded high bandwidth animations.\n\nTo trick the eye and the brain into thinking they are seeing a smoothly moving object, the pictures should be drawn at around 12 frames per second or faster. (A frame is one complete image.) With rates above 75-120 frames per second, no improvement in realism or smoothness is perceivable due to the way the eye and the brain both process images. At rates below 12 frames per second, most people can detect jerkiness associated with the drawing of new images that detracts from the illusion of realistic movement. Conventional hand-drawn cartoon animation often uses 15 frames per second in order to save on the number of drawings needed, but this is usually accepted because of the stylized nature of cartoons. To produce more realistic imagery, computer animation demands higher frame rates.\n\nFilms seen in theaters in the United States run at 24 frames per second, which is sufficient to create the illusion of continuous movement. For high resolution, adapters are used.\n\nEarly digital computer animation was developed at Bell Telephone Laboratories in the 1960s by Edward E. Zajac, Frank W. Sinden, Kenneth C. Knowlton, and A. Michael Noll. Other digital animation was also practiced at the Lawrence Livermore National Laboratory.\n\nIn 1967, a computer animation named \"Hummingbird\" was created by Charles Csuri and James Shaffer.\n\nIn 1968, a computer animation called \"\" was created with BESM-4 by Nikolai Konstantinov, depicting a cat moving around.\n\nIn 1971, a computer animation called \"Metadata\" was created, showing various shapes.\n\nAn early step in the history of computer animation was the sequel to the 1973 film \"Westworld,\" a science-fiction film about a society in which robots live and work among humans. The sequel, \"Futureworld\" (1976), used the 3D wire-frame imagery, which featured a computer-animated hand and face both created by University of Utah graduates Edwin Catmull and Fred Parke. This imagery originally appeared in their student film \"A Computer Animated Hand\", which they completed in 1972.\n\nDevelopments in CGI technologies are reported each year at SIGGRAPH, an annual conference on computer graphics and interactive techniques that is attended by thousands of computer professionals each year. Developers of computer games and 3D video cards strive to achieve the same visual quality on personal computers in real-time as is possible for CGI films and animation. With the rapid advancement of real-time rendering quality, artists began to use game engines to render non-interactive movies, which led to the art form Machinima.\n\nThe very first full length computer animated television series was \"ReBoot\", which debuted in September 1994; the series followed the adventures of characters who lived inside a computer. The first feature-length computer animated film was \"Toy Story\" (1995), which was made by Pixar. It followed an adventure centered around toys and their owners. This groundbreaking film was also the first of many fully computer-animated movies.\n\nIn most 3D computer animation systems, an animator creates a simplified representation of a character's anatomy, which is analogous to a skeleton or stick figure. They are by default arranged into a default position known as a bind pose. The position of each segment of the skeletal model is defined by animation variables, or Avars for short. In human and animal characters, many parts of the skeletal model correspond to the actual bones, but skeletal animation is also used to animate other things, with facial features (though other methods for facial animation exist). The character \"Woody\" in \"Toy Story\", for example, uses 700 Avars (100 in the face alone). The computer doesn't usually render the skeletal model directly (it is invisible), but it does use the skeletal model to compute the exact position and orientation of that certain character, which is eventually rendered into an image. Thus by changing the values of Avars over time, the animator creates motion by making the character move from frame to frame.\n\nThere are several methods for generating the Avar values to obtain realistic motion. Traditionally, animators manipulate the Avars directly. Rather than set Avars for every frame, they usually set Avars at strategic points (frames) in time and let the computer interpolate or tween between them in a process called \"keyframing\". Keyframing puts control in the hands of the animator and has roots in hand-drawn traditional animation.\n\nIn contrast, a newer method called \"motion capture\" makes use of live action footage. When computer animation is driven by motion capture, a real performer acts out the scene as if they were the character to be animated. His/her motion is recorded to a computer using video cameras and markers and that performance is then applied to the animated character.\n\nEach method has its advantages and as of 2007, games and films are using either or both of these methods in productions. Keyframe animation can produce motions that would be difficult or impossible to act out, while motion capture can reproduce the subtleties of a particular actor. For example, in the 2006 film \"\", Bill Nighy provided the performance for the character Davy Jones. Even though Nighy doesn't appear in the movie himself, the movie benefited from his performance by recording the nuances of his body language, posture, facial expressions, etc. Thus motion capture is appropriate in situations where believable, realistic behavior and action is required, but the types of characters required exceed what can be done throughout the conventional costuming.\n\n3D computer animation combines 3D models of objects and programmed or hand \"keyframed\" movement. These models are constructed out of geometrical vertices, faces, and edges in a 3D coordinate system. Objects are sculpted much like real clay or plaster, working from general forms to specific details with various sculpting tools. Unless a 3D model is intended to be a solid color, it must be painted with \"textures\" for realism. A bone/joint animation system is set up to deform the CGI model (e.g., to make a humanoid model walk). In a process known as \"rigging\", the virtual marionette is given various controllers and handles for controlling movement. Animation data can be created using motion capture, or keyframing by a human animator, or a combination of the two.\n\n3D models rigged for animation may contain thousands of control points — for example, \"Woody\" from \"Toy Story\" uses 700 specialized animation controllers. Rhythm and Hues Studios labored for two years to create Aslan in the movie \"\", which had about 1,851 controllers (742 in the face alone). In the 2004 film \"The Day After Tomorrow\", designers had to design forces of extreme weather with the help of video references and accurate meteorological facts. For the 2005 remake of \"King Kong\", actor Andy Serkis was used to help designers pinpoint the gorilla's prime location in the shots and used his expressions to model \"human\" characteristics onto the creature. Serkis had earlier provided the voice and performance for Gollum in J. R. R. Tolkien's \"The Lord of the Rings\" trilogy.\n\nComputer animation can be created with a computer and an animation software. Some impressive animation can be achieved even with basic programs; however, the rendering can take a lot of time on an ordinary home computer. Professional animators of movies, television and video games could make photorealistic animation with high detail. This level of quality for movie animation would take hundreds of years to create on a home computer. Instead, many powerful workstation computers are used. Graphics workstation computers use two to four processors, and they are a lot more powerful than an actual home computer and are specialized for rendering. A large number of workstations (known as a \"\"render farm\"\") are networked together to effectively act as a giant computer. The result is a computer-animated movie that can be completed in about one to five years (however, this process is not composed solely of rendering). A workstation typically costs $2,000-16,000 with the more expensive stations being able to render much faster due to the more technologically-advanced hardware that they contain. Professionals also use digital movie cameras, motion/performance capture, bluescreens, film editing software, props, and other tools used for movie animation. Programs like Blender allow for people who cant afford expensive animation and rendering software to be able to work in a similar manner to those who use the commercial grade equipment.\n\nThe realistic modeling of human facial features is both one of the most challenging and sought after elements in computer-generated imagery. Computer facial animation is a highly complex field where models typically include a very large number of animation variables. Historically speaking, the first SIGGRAPH tutorials on \"State of the art in Facial Animation\" in 1989 and 1990 proved to be a turning point in the field by bringing together and consolidating multiple research elements and sparked interest among a number of researchers.\n\nThe Facial Action Coding System (with 46 \"action units\", \"lip bite\" or \"squint\"), which had been developed in 1976, became a popular basis for many systems. As early as 2001, MPEG-4 included 68 Face Animation Parameters (FAPs) for lips, jaws, etc., and the field has made significant progress since then and the use of facial microexpression has increased.\n\nIn some cases, an affective space, the PAD emotional state model, can be used to assign specific emotions to the faces of avatars. In this approach, the PAD model is used as a high level emotional space and the lower level space is the MPEG-4 Facial Animation Parameters (FAP). A mid-level Partial Expression Parameters (PEP) space is then used to in a two-level structure – the PAD-PEP mapping and the PEP-FAP translation model.\n\nRealism in computer animation can mean making each frame look photorealistic, in the sense that the scene is rendered to resemble a photograph or make the characters' animation believable and lifelike. Computer animation can also be realistic with or without the photorealistic rendering.\n\nOne of the greatest challenges in computer animation has been creating human characters that look and move with the highest degree of realism. Part of the difficulty in making pleasing, realistic human characters is the uncanny valley, the concept where the human audience (up to a point) tends to have an increasingly negative, emotional response as a human replica looks and acts more and more human. Films that have attempted photorealistic human characters, such as \"The Polar Express\", \"Beowulf\", and \"A Christmas Carol\"\nhave been criticized as \"creepy\" and \"disconcerting\".\n\nThe goal of computer animation is not always to emulate live action as closely as possible, so many animated films instead feature characters who are anthropomorphic animals, legendary creatures and characters, superheroes, or otherwise have non-realistic, cartoon-like proportions. Computer animation can also be tailored to mimic or substitute for other kinds of animation, like traditional stop-motion animation (as shown in \"Flushed Away\" or \"The Lego Movie\"). Some of the long-standing basic principles of animation, like squash & stretch, call for movement that is not strictly realistic, and such principles still see widespread application in computer animation.\n\nCGI short films have been produced as independent animation since 1976. An early example of an animated feature film to incorporate CGI animation was the 1983 Japanese anime film \"\". The popularity of computer animation (especially in the field of special effects) skyrocketed during the modern era of U.S. animation. The first completely computer-animated movie was \"Toy Story\" (1995), but \"VeggieTales\" is the first American fully 3D computer animated series sold directly (made in 1993); its success inspired other animation series, such as \"ReBoot\" in 1994. While other films like \"Avatar\" used CGI for a majority of the movie, it still incorporated human actors into the mix.\n\nSome notable producers of computer-animated feature films include:\n\nBULLET::::- Animal Logic – Films include \"Happy Feet\" (2006), \"\" (2010), \"Walking with Dinosaurs\" (2013) and \"The Lego Movie\" (2014)\nBULLET::::- Aardman Animations  – Films include \"Flushed Away\" (2006) and \"Arthur Christmas\" (2011)\nBULLET::::- Blue Sky Studios – Films include \"Ice Age\" (2002), \"Robots\" (2005), \"Horton Hears a Who!\" (2008), \"Rio\" (2011), \"The Peanuts Movie\" (2015)\nBULLET::::- DreamWorks Animation – Films include \"Shrek\" (2001), \"Madagascar\" (2005), \"Kung Fu Panda\" (2008), \"How to Train Your Dragon\" (2010), \"The Croods\" (2013), \"Trolls\" (2016), \"The Boss Baby\" (2017)\nBULLET::::- ImageMovers  – Films include \"The Polar Express\" (2004), \"Monster House\" (2006), \"A Christmas Carol\" (2009), \"Mars Needs Moms\" (2009)\nBULLET::::- Ilion Animation Studios — Films include \"Planet 51\" (2009), \"\" (2014) \"Wonder Park\" (2019)\nBULLET::::- Illumination — Films include \"Despicable Me\" (2010), \"The Lorax\" (2012), \"Minions\" (2015), \"The Secret Life of Pets\" (2016), \"Sing\" (2016), \"The Grinch\" (2018)\nBULLET::::- Industrial Light & Magic – Films include \"Rango\" (2011) and \"Strange Magic\" (2015)\nBULLET::::- Pacific Data Images – Films include \"Antz\" (1998), \"Shrek\" (2001), \"Madagascar\" (2005), \"Megamind\" (2010), \"Mr. Peabody and Sherman\" (2014)\nBULLET::::- Pixar Animation Studios – Films include \"Toy Story\" (1995), \"A Bug's Life\" (1998), \"Monsters, Inc.\" (2001), \"Finding Nemo\" (2003), \"The Incredibles\" (2004), \"Ratatouille\" (2007), \"WALL-E\" (2008), \"Up\" (2009), \"Inside Out\" (2015), \"The Good Dinosaur\" (2015), \" Coco\" (2017)\nBULLET::::- Rainmaker Studios – Films include \"Escape from Planet Earth\" (2013) and \"Ratchet & Clank\" (2016)\nBULLET::::- Reel FX Animation Studios – Films include \"Free Birds\" (2013) and \"The Book of Life\" (2014)\nBULLET::::- Sony Pictures Animation – Films include \"Open Season\" (2006), \"Surf's Up\" (2007), \"Cloudy with a Chance of Meatballs\" (2009), \"The Smurfs\" (2011), \"Hotel Transylvania\" (2012), \"Cloudy with a Chance of Meatballs 2\" (2013), \"Hotel Transylvania 2\" (2015), \"The Emoji Movie\" (2017), \"\" (2018)\nBULLET::::- Sony Pictures Imageworks  – Films include \"The Angry Birds Movie\" (2016)\nBULLET::::- Triggerfish Animation Studios – Films include \"Zambezia\" (2013), \"Khumba\" (2014)\nBULLET::::- Walt Disney Animation Studios – Films include \"Bolt\" (2008), \"Tangled\" (2010), \"Wreck-It Ralph\" (2012), \"Frozen\" (2013), \"Big Hero 6\" (2014), \"Zootopia\" (2016), \"Moana\" (2016)\nBULLET::::- Warner Animation Group – Films include \"The Lego Movie\" (2014), \"Storks\" (2016), \"The Lego Batman Movie\" (2017), \"Smallfoot\" (2018)\nBULLET::::- Wizart Animation – Films include \"The Snow Queen\" (2012), \"Sheep and Wolves\" (2016)\n\nThe popularity of websites that allow members to upload their own movies for others to view has created a growing community of amateur computer animators. With utilities and programs often included free with modern operating systems, many users can make their own animated movies and shorts. Several free and open-source animation software applications exist as well. The ease at which these animations can be distributed has attracted professional animation talent also. Companies such as PowToon and GoAnimate attempt to bridged the gap by giving amateurs access to professional animations as clip art.\n\nThe oldest (most backward compatible) web-based animations are in the animated GIF format, which can be uploaded and seen on the web easily. However, the raster graphics format of GIF animations slows the download and frame rate, especially with larger screen sizes. The growing demand for higher quality web-based animations was met by a vector graphics alternative that relied on the use of a plugin. For decades, Flash animations were the most popular format, until the web development community abandoned support for the Flash player plugin. Web browsers on mobile devices and mobile operating systems never fully supported the Flash plugin.\n\nBy this time, internet bandwidth and download speeds increased, making raster graphic animations more convenient. Some of the more complex vector graphic animations had a slower frame rate due to complex rendering than some of the raster graphic alternatives. Many of the GIF and Flash animations were already converted to digital video formats, which were compatible with mobile devices and reduced file sizes via video compression technology. However, compatibility was still problematic as some of the popular video formats such as Apple's QuickTime and Microsoft Silverlight required plugins. YouTube, the most popular video viewing website, was also relying on the Flash plugin to deliver digital video in the Flash Video format.\n\nThe latest alternatives are HTML5 compatible animations. Technologies such as JavaScript and CSS animations made sequencing the movement of images in HTML5 web pages more convenient. SVG animations offered a vector graphic alternative to the original Flash graphic format, SmartSketch. YouTube offers an HTML5 alternative for digital video. APNG (Animated PNG) offered a raster graphic alternative to animated GIF files that enables multi-level transparency not available in GIFs\n\nIn 2D computer animation, moving objects are often referred to as \"sprites.\" A sprite is an image that has a location associated with it. The location of the sprite is changed slightly, between each displayed frame, to make the sprite appear to move. The following pseudocode makes a sprite move from left to right:\n\nComputer animation uses different techniques to produce animations. Most frequently, sophisticated mathematics is used to manipulate complex three-dimensional polygons, apply \"textures\", lighting and other effects to the polygons and finally rendering the complete image. A sophisticated graphical user interface may be used to create the animation and arrange its choreography. Another technique called constructive solid geometry defines objects by conducting boolean operations on regular shapes, and has the advantage that animations may be accurately produced at any resolution.\n\n\"To animate means, figuratively, to \"give life to\". There are two basic methods that animators commonly use to accomplish this.\"\n\nComputer-assisted animation is usually classed as two-dimensional (2D) animation. Drawings are either hand drawn (pencil to paper) or interactively drawn (on the computer) using different assisting appliances and are positioned into specific software packages. Within the software package, the creator places drawings into different key frames which fundamentally create an outline of the most important movements. The computer then fills in the \"in-between frames\", a process commonly known as Tweening. Computer-assisted animation employs new technologies to produce content faster than is possible with traditional animation, while still retaining the stylistic elements of traditionally drawn characters or objects.\n\nExamples of films produced using computer-assisted animation are \"The Little Mermaid\", \"The Rescuers Down Under\", \"Beauty and the Beast\", \"Aladdin\", \"The Lion King\", \"Pocahontas\", \"The Hunchback of Notre Dame\", \"Hercules\", \"Mulan\", \"The Road to El Dorado\" and \"Tarzan\".\n\nComputer-generated animation is known as three-dimensional (3D) animation. Creators design an object or character with an X, a Y and a Z axis. No pencil-to-paper drawings create the way computer generated animation works. The object or character created will then be taken into a software, key framing and tweening are also carried out in computer generated animation but are also a lot of techniques used that do not relate to traditional animation. Animators can break physical laws by using mathematical algorithms to cheat mass, force and gravity rulings. Fundamentally, time scale and quality could be said to be a preferred way to produce animation as they are two major things that are enhanced by using computer generated animation. Another positive aspect of CGA is the fact one can create a flock of creatures to act independently when created as a group. An animal's fur can be programmed to wave in the wind and lie flat when it rains instead of programming each strand of hair separately.\n\nA few examples of computer-generated animation movies are \"Toy Story\", \"Frozen\", and \"Shrek\". \n\nBULLET::::- Animation\nBULLET::::- Animation database\nBULLET::::- Autodesk\nBULLET::::- Avar (animation variable)\nBULLET::::- Computer-generated imagery (CGI)\nBULLET::::- New York Institute of Technology Computer Graphics Lab\nBULLET::::- Computer representation of surfaces\nBULLET::::- Hand-Over\nBULLET::::- Humanoid animation\nBULLET::::- List of animation studios\nBULLET::::- List of computer-animated films\nBULLET::::- List of computer-animated television series\nBULLET::::- Medical animation\nBULLET::::- Morph target animation\nBULLET::::- Machinima (recording video from games and virtual worlds)\nBULLET::::- Motion capture\nBULLET::::- Procedural animation\nBULLET::::- Ray tracing\nBULLET::::- Rich Representation Language\nBULLET::::- Skeletal animation\nBULLET::::- Timeline of computer animation in film and television\nBULLET::::- Virtual artifact\nBULLET::::- Wire-frame model\nBULLET::::- 12 basic principles of animation\n\n\nBULLET::::- Galería 3D, Half a century of 3D Computer Animations (1962-2002).\n"}
{"id": "6778", "url": "https://en.wikipedia.org/wiki?curid=6778", "title": "Ceawlin of Wessex", "text": "Ceawlin of Wessex\n\nCeawlin (also spelled Ceaulin and Caelin, died \"ca.\" 593) was a King of Wessex. He may have been the son of Cynric of Wessex and the grandson of Cerdic of Wessex, whom the \"Anglo-Saxon Chronicle\" represents as the leader of the first group of Saxons to come to the land which later became Wessex. Ceawlin was active during the last years of the Anglo-Saxon expansion, with little of southern England remaining in the control of the native Britons by the time of his death.\n\nThe chronology of Ceawlin's life is highly uncertain. The historical accuracy and dating of many of the events in the later \"Anglo-Saxon Chronicle\" have been called into question, and his reign is variously listed as lasting seven, seventeen, or thirty-two years. The \"Chronicle\" records several battles of Ceawlin's between the years 556 and 592, including the first record of a battle between different groups of Anglo-Saxons, and indicates that under Ceawlin Wessex acquired significant territory, some of which was later to be lost to other Anglo-Saxon kingdoms. Ceawlin is also named as one of the eight \"\"bretwaldas\"\", a title given in the \"Chronicle\" to eight rulers who had overlordship over southern Britain, although the extent of Ceawlin's control is not known.\n\nCeawlin died in 593, having been deposed the year before, possibly by his successor, Ceol. He is recorded in various sources as having two sons, Cutha and Cuthwine, but the genealogies in which this information is found are known to be unreliable.\n\nThe history of the sub-Roman period in Britain is poorly sourced and the subject of a number of important disagreements among historians. It appears, however, that in the fifth century raids on Britain by continental peoples developed into migrations. The newcomers included Angles, Saxons, Jutes, and Frisians. These peoples captured territory in the east and south of England, but at about the end of the fifth century, a British victory at the battle of Mons Badonicus halted the Anglo-Saxon advance for fifty years. Near the year 550, however, the British began to lose ground once more, and within twenty-five years, it appears that control of almost all of southern England was in the hands of the invaders.\n\nThe peace following the battle of Mons Badonicus is attested partly by Gildas, a monk, who wrote \"De Excidio et Conquestu Britanniae\" or \"On the Ruin and Conquest of Britain\" during the middle of the sixth century. This essay is a polemic against corruption and Gildas provides little in the way of names and dates. He appears, however, to state that peace had lasted from the year of his birth to the time he was writing. The \"Anglo-Saxon Chronicle\" is the other main source that bears on this period, in particular in an entry for the year 827 that records a list of the kings who bore the title \"\"bretwalda\"\", or \"Britain-ruler\". That list shows a gap in the early sixth century that matches Gildas's version of events.\n\nCeawlin's reign belongs to the period of Anglo-Saxon expansion at the end of the sixth century. Though there are many unanswered questions about the chronology and activities of the early West Saxon rulers, it is clear that Ceawlin was one of the key figures in the final Anglo-Saxon conquest of southern Britain.\n\nThe two main written sources for early West Saxon history are the \"Anglo-Saxon Chronicle\" and the West Saxon Genealogical Regnal List. The \"Chronicle\" is a set of annals which were compiled near the year 890, during the reign of King Alfred the Great of Wessex. They record earlier material for the older entries, which were assembled from earlier annals that no longer survive, as well as from saga material that might have been transmitted orally. The \"Chronicle\" dates the arrival of the future \"West Saxons\" in Britain to 495, when Cerdic and his son, Cynric, land at \"Cerdices ora\", or Cerdic's shore. Almost twenty annals describing Cerdic's campaigns and those of his descendants appear interspersed through the next hundred years of entries in the \"Chronicle\". Although these annals provide most of what is known about Ceawlin, the historicity of many of the entries is uncertain.\n\nThe West Saxon Genealogical Regnal List is a list of rulers of Wessex, including the lengths of their reigns. It survives in several forms, including as a preface to the [B] manuscript of the \"Chronicle\". As with the \"Chronicle\", the list was compiled during the reign of Alfred the Great, and both the list and the \"Chronicle\" are influenced by the desire of their writers to use a single line of descent to trace the lineage of the Kings of Wessex through Cerdic to Gewis, the legendary eponymous ancestor of the West Saxons, who is made to descend from Woden. The result served the political purposes of the scribe, but is riddled with contradictions for historians.\n\nThe contradictions may be seen clearly by calculating dates by different methods from the various sources. The first event in West Saxon history, the date of which can be regarded as reasonably certain, is the baptism of Cynegils, which occurred in the late 630s, perhaps as late as 640. The \"Chronicle\" dates Cerdic's arrival to 495, but adding up the lengths of the reigns as given in the West Saxon Genealogical Regnal List leads to the conclusion that Cerdic's reign might have started in 532, a difference of 37 years. Neither 495 nor 532 may be treated as reliable, however--the latter date relies on the presumption that the Regnal List is correct in presenting the Kings of Wessex as having succeeded one another, with no omitted kings, and no joint kingships, and that the durations of the reigns are correct as given. None of these presumptions may be made safely.\n\nThe sources also are inconsistent on the length of Ceawlin's reign. The \"Chronicle\" gives it as thirty-two years, from 560 to 592, but the Regnal Lists disagree: different versions give it as seven or seventeen years. A recent detailed study of the Regnal List dates the arrival of the West Saxons in England to 538, and favours seven years as the most likely length of Ceawlin's reign, with dates of 581–588 proposed. The sources do agree that Ceawlin is the son of Cynric and he usually is named as the father of Cuthwine. There is one discrepancy in this case: the entry for 685 in the [A] version of the \"Chronicle\" assigns Ceawlin a son, Cutha, but in the 855 entry in the same manuscript, Cutha is listed as the son of Cuthwine. Cutha also is named as Ceawlin's brother in the [E] and [F] versions of the \"Chronicle\", in the 571 and 568 entries, respectively.\n\nWhether Ceawlin is a descendant of Cerdic is a matter of debate. Subgroupings of different West Saxon lineages give the impression of separate groups, of which Ceawlin's line is one. Some of the problems in the Wessex genealogies may have come about because of efforts to integrate Ceawlin's line with the other lineages: it was very important to the West Saxons to be able to trace their ancestors back to Cerdic. Another reason for doubting the literal nature of these early genealogies is that the etymology of the names of several early members of the dynasty do not appear to be Germanic, as would be expected in the names of leaders of an apparently Anglo-Saxon dynasty. The name Ceawlin is one of the names that do not have convincing Anglo-Saxon etymologies; it seems more likely to be of native British origin.\n\nThe earliest sources do not use the term \"West Saxon\". According to Bede's \"Ecclesiastical History of the English People\", the term is interchangeable with the Gewisse. The term \"West Saxon\" appears only in the late seventh century, after the reign of Cædwalla.\n\nUltimately, the kingdom of Wessex occupied the southwest of England, but the initial stages in this expansion are not apparent from the sources. Cerdic's landing, whenever it is to be dated, seems to have been near the Isle of Wight, and the annals record the conquest of the island in 530. In 534, according to the \"Chronicle\", Cerdic died and his son Cynric took the throne; the \"Chronicle\" adds that \"they gave the Isle of Wight to their nephews, Stuf and Wihtgar\". These records are in direct conflict with Bede, who states that the Isle of Wight was settled by Jutes, not Saxons; the archaeological record is somewhat in favour of Bede on this.\n\nSubsequent entries in the \"Chronicle\" give details of some of the battles by which the West Saxons won their kingdom. Ceawlin's campaigns are not given as near the coast. They range along the Thames valley and beyond, as far as Surrey in the east and the mouth of the Severn in the west. Ceawlin clearly is part of the West Saxon expansion, but the military history of the period is difficult to understand. In what follows the dates are as given in the \"Chronicle\", although, as noted above, these are earlier than now thought accurate.\n\nThe first record of a battle fought by Ceawlin is in 556, when he and his father, Cynric, fought the native Britons at \"\", or Bera's Stronghold. This now is identified as Barbury Castle, an Iron Age hill fort in Wiltshire, near Swindon. Cynric would have been king of Wessex at this time.\n\nThe first battle Ceawlin fought as king is dated by the \"Chronicle\" to 568, when he and Cutha fought with Æthelberht, the king of Kent. The entry says \"Here Ceawlin and Cutha fought against Aethelberht and drove him into Kent; and they killed two ealdormen, Oslaf and Cnebba, on Wibbandun.\" The location of \"Wibbandun\", which can be translated as \"Wibba's Mount\", has not been identified definitely; it was at one time thought to be Wimbledon, but this now is known to be incorrect. This battle is notable as the first recorded conflict between the invading peoples: previous battles recorded in the \"Chronicle\" are between the Anglo-Saxons and the native Britons.\n\nThere are multiple examples of joint kingship in Anglo-Saxon history, and this may be another: it is not clear what Cutha's relationship to Ceawlin is, but it certainly is possible he was also a king. The annal for 577, below, is another possible example.\n\nThe annal for 571 reads: \"Here Cuthwulf fought against the Britons at Bedcanford, and took four settlements: Limbury and Aylesbury, Benson and Eynsham; and in the same year he passed away.\" Cuthwulf's relationship with Ceawlin is unknown, but the alliteration common to Anglo-Saxon royal families suggests Cuthwulf may be part of the West Saxon royal line. The location of the battle itself is unidentified. It has been suggested that it was Bedford, but what is known of the early history of Bedford's names does not support this. This battle is of interest because it is surprising that an area so far east should still be in Briton hands this late: there is ample archaeological evidence of early Saxon and Anglian presence in the Midlands, and historians generally have interpreted Gildas's \"De Excidio\" as implying that the Britons had lost control of this area by the mid-sixth century. One possible explanation is that this annal records a reconquest of land that was lost to the Britons in the campaigns ending in the battle of Mons Badonicus.\n\nThe annal for 577 reads \"Here Cuthwine and Ceawlin fought against the Britons, and they killed three kings, Coinmail and Condidan and Farinmail, in the place which is called Dyrham, and took three cities: Gloucester and Cirencester and Bath.\" This entry is all that is known of these Briton kings; their names are in an archaic form that makes it very likely that this annal derives from a much older written source. The battle itself has long been regarded as a key moment in the Saxon advance, since in reaching the Bristol Channel, the West Saxons divided the Britons west of the Severn from land communication with those in the peninsula to the south of the Channel. Wessex almost certainly lost this territory to Penda of Mercia in 628, when the \"Chronicle\" records that \"Cynegils and Cwichelm fought against Penda at Cirencester and then came to an agreement.\"\n\nIt is possible that when Ceawlin and Cuthwine took Bath, they found the Roman baths still operating to some extent. Nennius, a ninth-century historian, mentions a \"Hot Lake\" in the land of the Hwicce, which was along the Severn, and adds \"It is surrounded by a wall, made of brick and stone, and men may go there to bathe at any time, and every man can have the kind of bath he likes. If he wants, it will be a cold bath; and if he wants a hot bath, it will be hot\". Bede also describes hot baths in the geographical introduction to the \"Ecclesiastical History\" in terms very similar to those of Nennius.\n\nWansdyke, an early medieval defensive linear earthwork, runs from south of Bristol to near Marlborough, Wiltshire, passing not far from Bath. It probably was built in the fifth or sixth centuries, perhaps by Ceawlin.\n\nCeawlin's last recorded victory is in 584. The entry reads \"Here Ceawlin and Cutha fought against the Britons at the place which is named Fethan leag, and Cutha was killed; and Ceawlin took many towns and countless war-loot, and in anger he turned back to his own [territory].\" There is a wood named \"Fethelée\" mentioned in a twelfth-century document that relates to Stoke Lyne, in Oxfordshire, and it now is thought that the battle of Fethan leag must have been fought in this area.\n\nThe phrase \"in anger he turned back to his own\" probably indicates that this annal is drawn from saga material, as perhaps are all of the early Wessex annals. It also has been used to argue that perhaps, Ceawlin did not win the battle and that the chronicler chose not to record the outcome fully – a king does not usually come home \"in anger\" after taking \"many towns and countless war-loot\". It may be that Ceawlin's overlordship of the southern Britons came to an end with this battle.\n\nAbout 731, Bede, a Northumbrian monk and chronicler, wrote a work called the \"Ecclesiastical History of the English People\". The work was not primarily a secular history, but Bede provides much information about the history of the Anglo-Saxons, including a list early in the history of seven kings who, he said, held \"imperium\" over the other kingdoms south of the Humber. The usual translation for \"imperium\" is \"overlordship\". Bede names Ceawlin as the second on the list, although he spells it \"Caelin\", and adds that he was \"known in the speech of his own people as Ceaulin\". Bede also makes it clear that Ceawlin was not a Christian—Bede mentions a later king, Æthelberht of Kent, as \"the first to enter the kingdom of heaven\".\n\nThe \"Anglo-Saxon Chronicle,\" in an entry for the year 827, repeats Bede's list, adds Egbert of Wessex, and also mentions that they were known as \"bretwalda\", or \"Britain-ruler\". A great deal of scholarly attention has been given to the meaning of this word. It has been described as a term \"of encomiastic poetry\", but there also is evidence that it implied a definite role of military leadership.\n\nBede says that these kings had authority \"south of the Humber\", but the span of control, at least of the earlier bretwaldas, likely was less than this. In Ceawlin's case the range of control is hard to determine accurately, but Bede's inclusion of Ceawlin in the list of kings who held \"imperium\", and the list of battles he is recorded as having won, indicate an energetic and successful leader who, from a base in the upper Thames valley, dominated much of the surrounding area and held overlordship over the southern Britons for some period. Despite Ceawlin's military successes, the northern conquests he made could not always be retained: Mercia took much of the upper Thames valley, and the north-eastern towns won in 571 were among territory subsequently under the control of Kent and Mercia at different times.\n\nBede's concept of the power of these overlords also must be regarded as the product of his eighth-century viewpoint. When the \"Ecclesiastical History\" was written, Æthelbald of Mercia dominated the English south of the Humber, and Bede's view of the earlier kings was doubtless strongly coloured by the state of England at that time. For the earlier \"bretwaldas\", such as Ælle and Ceawlin, there must be some element of anachronism in Bede's description. It also is possible that Bede only meant to refer to power over Anglo-Saxon kingdoms, not the native Britons.\n\nCeawlin is the second king in Bede's list. All the subsequent bretwaldas followed more or less consecutively, but there is a long gap, perhaps fifty years, between Ælle of Sussex, the first bretwalda, and Ceawlin. The lack of gaps between the overlordships of the later bretwaldas has been used to make an argument for Ceawlin's dates matching the later entries in the \"Chronicle\" with reasonable accuracy. According to this analysis, the next bretwalda, Æthelberht of Kent, must have been already a dominant king by the time Pope Gregory the Great wrote to him in 601, since Gregory would have not written to an underking. Ceawlin defeated Æthelberht in 568 according to the \"Chronicle\". Æthelberht's dates are a matter of debate, but recent scholarly consensus has his reign starting no earlier than 580. The 568 date for the battle at Wibbandun is thought to be unlikely because of the assertion in various versions of the West Saxon Genealogical Regnal List that Ceawlin's reign lasted either seven or seventeen years. If this battle is placed near the year 590, before Æthelberht had established himself as a powerful king, then the subsequent annals relating to Ceawlin's defeat and death may be reasonably close to the correct date. In any case, the battle with Æthelberht is unlikely to have been more than a few years on either side of 590. The gap between Ælle and Ceawlin, on the other hand, has been taken as supporting evidence for the story told by Gildas in \"De Excidio\" of a peace lasting a generation or more following a Briton victory at Mons Badonicus.\n\nÆthelberht of Kent succeeds Ceawlin on the list of bretwaldas, but the reigns may overlap somewhat: recent evaluations give Ceawlin a likely reign of 581–588, and place Æthelberht's accession near to the year 589, but these analyses are no more than scholarly guesses. Ceawlin's eclipse in 592, probably by Ceol, may have been the occasion for Æthelberht to rise to prominence; Æthelberht very likely was the dominant Anglo-Saxon king by 597. Æthelberht's rise may have been earlier: the 584 annal, even if it records a victory, is the last victory of Ceawlin's in the \"Chronicle\", and the period after that may have been one of Æthelberht's ascent and Ceawlin's decline.\n\nCeawlin lost the throne of Wessex in 592. The annal for that year reads, in part: \"Here there was great slaughter at Woden's Barrow, and Ceawlin was driven out.\" Woden's Barrow is a tumulus, now called Adam's Grave, at Alton Priors, Wiltshire. No details of his opponent are given. The medieval chronicler William of Malmesbury, writing in about 1120, says that it was \"the Angles and the British conspiring together\". Alternatively, it may have been Ceol, who is supposed to have been the next king of Wessex, ruling for six years according to the West Saxon Genealogical Regnal List. According to the \"Anglo-Saxon Chronicle\", Ceawlin died the following year. The relevant part of the annal reads: \"Here Ceawlin and Cwichelm and Crida perished.\" Nothing more is known of Cwichelm and Crida, although they may have been members of the Wessex royal house – their names fit the alliterative pattern common to royal houses of the time.\n\nAccording to the Regnal List, Ceol was a son of Cutha, who was a son of Cynric; and Ceolwulf, his brother, reigned for seventeen years after him. It is possible that some fragmentation of control among the West Saxons occurred at Ceawlin's death: Ceol and Ceolwulf may have been based in Wiltshire, as opposed to the upper Thames valley. This split also may have contributed to Æthelberht's ability to rise to dominance in southern England. The West Saxons remained influential in military terms, however: the \"Chronicle\" and Bede record continued military activity against Essex and Sussex within twenty or thirty years of Ceawlin's death.\n\nBULLET::::- House of Wessex family tree\n\nPrimary sources\n\nSecondary sources\nBULLET::::- (2003 edition: )\n\nBULLET::::- —separate PASE entry for \"Celm\" (Celin ?), a variant for Ceawlin found in the genealogical preface of Anglo-Saxon Chronicle texts A and G\n"}
{"id": "6779", "url": "https://en.wikipedia.org/wiki?curid=6779", "title": "Christchurch (disambiguation)", "text": "Christchurch (disambiguation)\n\nChristchurch is the largest city in the South Island of New Zealand. \nChristchurch may also refer to:\nBULLET::::- Christchurch, New Zealand\nBULLET::::- Christchurch (New Zealand electorate), a former electorate in New Zealand, also called Town (or City) of Christchurch\nBULLET::::- Christchurch Central, the current electorate of Christchurch in New Zealand\nBULLET::::- Christchurch, Cambridgeshire, in England\nBULLET::::- Christchurch, Dorset, town on the south coast of England\nBULLET::::- RAF Christchurch, a WW II airfield near the town\nBULLET::::- Christchurch (UK Parliament constituency), England, centred on the town\nBULLET::::- Christchurch (Dorset) railway station, a railway station serving the town\nBULLET::::- Christchurch, Gloucestershire, hamlet in the west of the Forest of Dean, Gloucestershire, England\nBULLET::::- Christchurch, Newport, in Wales\nBULLET::::- Christchurch, Virginia, United States\nBULLET::::- Christchurch Mansion, a stately home in Ipswich, Suffolk\nBULLET::::- Christchurch Park, a park surrounding Christchurch Mansion\nBULLET::::- Christ Church, Barbados, Barbados\n\nBULLET::::- Christchurch School, Christchurch, Virginia, U.S.\nBULLET::::- Christchurch Boys' High School, Christchurch, New Zealand\nBULLET::::- Christchurch Girls' High School, Christchurch, New Zealand\nBULLET::::- Christ Church, Oxford\nBULLET::::- University of Otago Christchurch School of Medicine, one of three medical schools of University of Otago, New Zealand\nBULLET::::- Christchurch Anglo-Indian Higher Secondary School, Christchurch, Chennai, India\n\nBULLET::::- Christchurch F.C., England\nBULLET::::- Christchurch United, New Zealand\nBULLET::::- Christchurch Technical, New Zealand\nBULLET::::- Christchurch High School Old Boys, New Zealand\n\nBULLET::::- Christchurch-Campbell, an automobile made in 1922\nBULLET::::- ChristChurch London, an evangelic church in London, UK\nBULLET::::- \"Christchurch the Music\", 2005 compilation album from New Zealand\n\nBULLET::::- Christ Church (disambiguation)\nBULLET::::- Christ Church Cathedral (disambiguation)\nBULLET::::- Church of Christ (disambiguation)\nBULLET::::- Christian Church (disambiguation)\nBULLET::::- Christchurch railway station (disambiguation)\n"}
{"id": "6780", "url": "https://en.wikipedia.org/wiki?curid=6780", "title": "CD-R", "text": "CD-R\n\nCD-R (Compact Disc-Recordable) is a digital optical disc storage format. A CD-R disc is a compact disc that can be written once and read arbitrarily many times.\n\nCD-R discs (CD-Rs) are readable by most plain CD readers, i.e., CD readers manufactured prior to the introduction of CD-R. This is an advantage over CD-RW, which can be re-written but cannot be played on many plain CD readers.\n\nOriginally named CD Write-Once (WO), the CD-R specification was first published in 1988 by Philips and Sony in the 'Orange Book'. The Orange Book consists of several parts, furnishing details of the CD-WO, CD-MO (Magneto-Optic), and CD-RW (ReWritable). The latest editions have abandoned the use of the term \"CD-WO\" in favor of \"CD-R\", while \"CD-MO\" were used very little. Written CD-Rs and CD-RWs are, in the aspect of low-level encoding and data format, fully compatible with the audio CD (\"Red Book\" CD-DA) and data CD (\"Yellow Book\" CD-ROM) standards. (Note that the Yellow Book standard for CD-ROM only specifies a high-level data format and refers to the Red Book for all physical format and low-level code details, such as track pitch, linear bit density, and bitstream encoding.) This means they use Eight-to-Fourteen Modulation, CIRC error correction, and, for CD-ROM, the third error correction layer defined in the Yellow Book. Properly written CD-R discs on blanks of less than 80 minutes length are fully compatible with the audio CD and CD-ROM standards in all details including physical specifications. 80 minute CD-R discs marginally violate the Red Book physical format specifications, and longer discs are noncompliant. CD-RW discs have lower reflectivity than CD-R or pressed (non-writable) CDs and for this reason cannot meet the Red Book standard (or come close). Some hardware compatible with Red Book CDs may have difficulty reading CD-Rs and, because of their lower reflectivity, especially CD-RWs. To the extent that CD hardware can read extended-length discs or CD-RW discs, it is because that hardware has capability beyond the minimum required by the Red Book and Yellow Book standards (the hardware is more capable than it needs to be to bear the Compact Disc logo).\n\nCD-R recording systems available in 1990 were similar to the washing machine-sized Meridian CD Publisher, based on the two-piece rack mount Yamaha PDS audio recorder costing $35,000, not including the required external ECC circuitry for data encoding, SCSI hard drive subsystem, and MS-DOS control computer. By 1992, the cost of typical recorders was down to $10,000–12,000, and in September 1995, Hewlett-Packard introduced its model 4020i manufactured by Philips, which, at $995, was the first recorder to cost less than $1000.\n\nThe dye materials developed by Taiyo Yuden made it possible for CD-R discs to be compatible with Audio CD and CD-ROM discs.\n\nInitially, in the United States, there was a market separation between \"music\" CD-Rs and \"data\" CD-Rs, the former being several times more expensive than the latter due to industry copyright arrangements with the RIAA. Physically, there is no difference between the discs save for the Disc Application Flag that identifies their type: standalone audio recorders will only accept \"music\" CD-Rs to enforce the RIAA arrangement, while computer CD-R drives can use either type of media to burn either type of content.\n\nA standard CD-R is a thick disc made of polycarbonate about 120 mm in diameter. The 120 mm disc has a storage capacity of 74 minutes of audio or 650 Megabytes of data. CD-R/RWs are available with capacities of 80 minutes of audio or 737,280,000 bytes (700 MiB), which they achieve by molding the disc at the tightest allowable tolerances specified in the Orange Book CD-R/CD-RW standards. The engineering margin that was reserved for manufacturing tolerance has been used for data capacity instead, leaving no tolerance for manufacturing; for these discs to be truly compliant with the Orange Book standard, the manufacturing process must be perfect .\n\nDespite the foregoing, most CD-Rs on the market have an 80-minute capacity. There are also 90 minute/790 MiB and 99 minute/870 MiB discs, although they are less common (and depart from the Orange Book standard outright). Also, due to the limitations of the data structures in the ATIP (see below), 90 and 99 minute blanks will identify as 80 minute ones. (As the ATIP is part of the Orange Book standard, it is natural that its design does not support some nonstandard disc configurations.) Therefore, in order to use the additional capacity, these discs have to be burned using \"overburn\" options in the CD recording software. (Overburning itself is so named because it is outside the written standards, but, due to market demand, it has nonetheless become a de facto standard function in most CD writing drives and software for them.)\n\nSome drives use special techniques, such as Plextor's GigaRec or Sanyo's HD-BURN, to write more data onto a given disc; these techniques are inherently deviations from the Compact Disc (Red, Yellow, and/or Orange Book) standards, making the recorded discs proprietary-formatted and not fully compatible with standard CD players and drives. However, in certain applications where discs will not be distributed or exchanged outside a private group and will not be archived for a long time, a proprietary format may be an acceptable way to obtain greater capacity (up to 1.2 GiB with GigaRec or 1.8 GiB with HD-BURN on 99 minute media). The greatest risk in using such a proprietary data storage format, assuming that it works reliably as designed, is that it may be difficult or impossible to repair or replace the hardware used to read the media if it fails, is damaged, or is lost after its original vendor discontinues it.\n\nNothing in the Red, Yellow or Orange Book standards prohibits disc reading/writing devices from having the capacity to read or write discs beyond the Compact Disc standards. The standards do require discs to meet precise requirements in order to be called Compact Discs, but the other discs may be called by other names; if this were not true, no DVD drive could legally bear the Compact Disc logo. While disc players and drives may have capabilities beyond the standards, enabling them to read and write nonstandard discs, there is no assurance, in the absence of explicit additional manufacturer specifications beyond normal Compact Disc logo certification, that any particular player or drive will perform beyond the standards at all or consistently. Furthermore, if the same device with no explicit performance specs beyond the Compact Disc logo initially handles nonstandard discs reliably, there is no assurance that it will not later stop doing so, and in that case, there is no assurance that it can be made to do so again by service or adjustment. Therefore, discs with capacities larger than 650 MB, and especially those larger than 700 MB, are less interchangeable among players/drives than standard discs and are not very suitable for archival use, as their readability on future equipment, or even on the same equipment at a future time, is not assured, even under the assumption that the discs will not degrade at all.\nThe polycarbonate disc contains a spiral groove, called the \"pregroove\" (because it is molded in before data are written to the disc), to guide the laser beam upon writing and reading information. The pregroove is molded into the top side of the polycarbonate disc, where the pits and lands would be molded if it were a pressed (nonrecordable) Red Book CD; the bottom side, which faces the laser beam in the player or drive, is flat and smooth. The polycarbonate disc is coated on the pregroove side with a very thin layer of organic dye. Then, on top of the dye is coated a thin, reflecting layer of silver, a silver alloy, or gold. Finally, a protective coating of a photo-polymerizable lacquer is applied on top of the metal reflector and cured with UV-light.\n\nA blank CD-R is not \"empty\"; the pregroove has a wobble (the ATIP), which helps the writing laser to stay on track and to write the data to the disc at a constant rate. Maintaining a constant rate is essential to ensure proper size and spacing of the pits and lands burned into the dye layer. As well as providing timing information, the ATIP (absolute time in pregroove) is also a data track containing information about the CD-R manufacturer, the dye used and media information (disc length and so on). The pregroove is not destroyed when the data are written to the CD-R, a point which some copy protection schemes use to distinguish copies from an original CD.\nThere are three basic formulations of dye used in CD-Rs:\nBULLET::::1. Cyanine dye CD-Rs were the earliest ones developed, and their formulation is patented by Taiyo Yuden. CD-Rs based on this dye are mostly green in color. The earlier models were very chemically unstable and this made cyanine based discs unsuitable for archival use; they could fade and become unreadable in a few years. Many manufacturers like Taiyo Yuden use proprietary chemical additives to make more stable cyanine discs (\"metal stabilized Cyanine\", \"Super Cyanine\"). Older cyanine dye based CD-Rs, as well as all the hybrid dyes based on cyanine, were very sensitive to UV-rays and could have become unreadable after only a few days if they were exposed to direct sunlight. Although the additives used have made cyanine more stable, it is still the most sensitive of the dyes in UV rays (showing signs of degradation within a week of direct sunlight exposure). A common mistake users make is to leave the CD-Rs with the \"clear\" (recording) surface upwards, in order to protect it from scratches, as this lets the sun hit the recording surface directly.\nBULLET::::2. Phthalocyanine dye CD-Rs are usually silver, gold or light green. The patents on phthalocyanine CD-Rs are held by Mitsui and Ciba Specialty Chemicals. Phthalocyanine is a natively stable dye (has no need for stabilizers) and CD-Rs based on this are often given a rated lifetime of hundreds of years. Unlike cyanine, phthalocyanine is less resistant to UV rays and CD-Rs based on this dye show signs of degradation only after two weeks of direct sunlight exposure. However, phthalocyanine is more sensitive than cyanine to writing laser power calibration, meaning that the power level used by the writing laser has to be more accurately adjusted for the disc in order to get a good recording; this may erode the benefits of dye stability, as marginally written discs (with higher correctable error rates) will lose data (i.e. have uncorrectable errors) after less dye degradation than well written discs (with lower correctable error rates).\nBULLET::::3. Azo dye CD-Rs are dark blue in color, and their formulation is patented by Mitsubishi Chemical Corporation. Azo dyes are also chemically stable, and Azo CD-Rs are typically rated with a lifetime of decades. Azo is the most resistant dye against UV light and begins to degrade only after the third or fourth week of direct sunlight exposure. More modern implementations of this kind of dye include Super Azo which is not as deep blue as the earlier Metal Azo. This change of composition was necessary in order to achieve faster writing speeds.\n\nThere are many hybrid variations of the dye formulations, such as Formazan by Kodak (a hybrid of cyanine and phthalocyanine).\n\nUnfortunately, many manufacturers have added additional coloring to disguise their unstable cyanine CD-Rs in the past, so the formulation of a disc cannot be determined based purely on its color. Similarly, a gold reflective layer does not guarantee use of phthalocyanine dye. The quality of the disc is also not only dependent on the dye used, it is also influenced by sealing, the top layer, the reflective layer, and the polycarbonate. Simply choosing a disc based on its dye type may be problematic. Furthermore, correct power calibration of the laser in the writer, as well as correct timing of the laser pulses, stable disc speed, and so on, is critical to not only the immediate readability but the longevity of the recorded disc, so for archiving it is important to have not only a high quality disc but a high quality writer. In fact, a high quality writer may produce adequate results with medium quality media, but high quality media cannot compensate for a mediocre writer, and discs written by such a writer cannot achieve their maximum potential archival lifetime.\n\n!Data writing speed\n!Data writing rate\n!Write time for 80 minute/700 MiB CD-R\n1×150 kB/s80 minutes\n2×300 kB/s40 minutes\n4×600 kB/s20 minutes\n8×1.2 MB/s10 minutes\n12×1.8 MB/s7.5 minutes\n16×2.4 MB/s5 minutes\n20×3.0 MB/s4 minutes\n24×3.6 MB/s3.4 minutes (see below)\n32×4.8 MB/s2.5 minutes (see below)\n40×6.0 MB/s2 minutes (see below)\n48×7.2 MB/s1.7 minutes (see below)\n52×7.8 MB/s1.5 minutes (see below)\n\nThese times only include the actual optical writing pass over the disc. For most disc recording operations, additional time is used for overhead processes, such as organizing the files and tracks, which adds to the theoretical minimum total time required to produce a disc. (An exception might be making a disc from a prepared ISO image, for which the overhead would likely be trivial.) At the lowest write speeds, this overhead takes so much less time than the actual disc writing pass that it may be negligible, but at higher write speeds, the overhead time becomes a larger proportion of the overall time taken to produce a finished disc and may add significantly to it.\n\nAlso, above 20× speed, drives use a Zoned-CLV or CAV strategy, where the advertised maximum speed is only reached near the outer rim of the disc. This is not taken into account by the above table. (If this were not done, the faster rotation that would be required at the inner tracks could cause the disc to fracture and/or could cause excessive vibration which would make accurate and successful writing impossible.)\n\nThe blank disc has a pre-groove track onto which the data are written. The pre-groove track, which also contains timing information, ensures that the recorder follows the same spiral path as a conventional CD. A CD recorder writes data to a CD-R disc by pulsing its laser to heat areas of the organic dye layer. The writing process does not produce indentations (pits); instead, the heat permanently changes the optical properties of the dye, changing the reflectivity of those areas. Using a low laser power, so as not to further alter the dye, the disc is read back in the same way as a CD-ROM. However, the reflected light is modulated not by pits, but by the alternating regions of heated and unaltered dye. The change of the intensity of the reflected laser radiation is transformed into an electrical signal, from which the digital information is recovered (\"decoded\"). Once a section of a CD-R is written, it cannot be erased or rewritten, unlike a CD-RW. A CD-R can be recorded in multiple sessions.\nA CD recorder can write to a CD-R using several methods including:\nBULLET::::1. Disc At Once – the whole CD-R is written in one session with no gaps and the disc is \"closed\" meaning no more data can be added and the CD-R effectively becomes a standard read-only CD. With no gaps between the tracks the Disc At Once format is useful for \"live\" audio recordings.\nBULLET::::2. Track At Once – data are written to the CD-R one track at a time but the CD is left \"open\" for further recording at a later stage. It also allows data and audio to reside on the same CD-R.\nBULLET::::3. Packet Writing – used to record data to a CD-R in \"packets\", allowing extra information to be appended to a disc at a later time, or for information on the disc to be made \"invisible\". In this way, CD-R can emulate CD-RW; however, each time information on the disc is altered, more data has to be written to the disc. There can be compatibility issues with this format and some CD drives.\n\nWith careful examination, the written and unwritten areas can be distinguished by the naked eye. CD-Rs are written from the center outwards, so the written area appears as an inner band with slightly different shading.\n\nReal-life (not accelerated aging) tests have revealed that some CD-Rs degrade quickly even if stored normally. The quality of a CD-R disc has a large and direct influence on longevity—low quality discs should not be expected to last very long. According to research conducted by J. Perdereau, CD-Rs are expected to have an average life expectancy of 10 years. Branding isn't a reliable guide to quality, because many brands (major as well as no name) do not manufacture their own discs. Instead they are sourced from different manufacturers of varying quality. For best results, the actual manufacturer and material components of each batch of discs should be verified.\n\nBurned CD-Rs suffer from material degradation, just like most writable media. CD-R media have an internal layer of dye used to store data. In a CD-RW disc, the recording layer is made of an alloy of silver and other metals—indium, antimony, and tellurium. In CD-R media, the dye itself can degrade, causing data to become unreadable.\n\nAs well as degradation of the dye, failure of a CD-R can be due to the reflective surface. While silver is less expensive and more widely used, it is more prone to oxidation resulting in a non-reflecting surface. Gold on the other hand, although more expensive and no longer widely used, is an inert material, so gold-based CD-Rs do not suffer from this problem. Manufacturers have estimated the longevity of gold-based CD-Rs to be as high as 100 years.\n\nIt is recommended if using adhesive-backed paper labels that the labels be specially made for CD-Rs. A balanced CD vibrates only slightly when rotated at high speed. Bad or improperly made labels, or labels applied off-center, unbalance the CD and can cause it to vibrate when it spins, which causes read errors and even risks damaging the drive.\n\nA professional alternative to CD labels is pre-printed CDs using a 5-color silkscreen or offset press. Using a permanent marker pen is also a common practice. However, solvents from such pens can affect the dye layer.\n\nSince CD-Rs in general cannot be logically erased to any degree, the disposal of CD-Rs presents a possible security issue if they contain sensitive / private data. Destroying the data requires physically destroying the disc or data layer. Heating the disc in a microwave oven for 10–15 seconds effectively destroys the data layer by causing arcing in the metal reflective layer, but this same arcing may cause damage or excessive wear to the microwave oven. Many office paper shredders are also designed to shred CDs.\n\nSome recent burners (Plextor, LiteOn) support erase operations on -R media, by \"overwriting\" the stored data with strong laser power, although the erased area cannot be overwritten with new data.\n\nThe polycarbonate material and possible gold or silver in the reflective layer would make CD-Rs highly recyclable. However, the polycarbonate is of very little value and the quantity of precious metals is so small that it is not profitable to recover them. Consequently, recyclers that accept CD-Rs typically do not offer compensation for donating or transporting the materials.\n\nBULLET::::- Absolute Time In Pregroove\nBULLET::::- Blu-ray Disc\nBULLET::::- CD recorder\nBULLET::::- CD-R caddy\nBULLET::::- CD-ROM, GD-ROM\nBULLET::::- CD-RW, DVD-RW\nBULLET::::- DVD, DVD-R, DVD+R, DVD+R DL\nBULLET::::- HD DVD\nBULLET::::- Labelflash\nBULLET::::- LightScribe\nBULLET::::- MultiLevel Recording, an obsolete technology (with non-binary modulation)\nBULLET::::- Optical disc authoring\nBULLET::::- Rainbow Books\nBULLET::::- GD-ROM\nBULLET::::- MIL-CD\n\nBULLET::::- ECMA-394: Recordable Compact Disc Systems CD-R Multi-Speed (standardized Orange Book, Part II, Volume 2)\nBULLET::::- The CD-R FAQ\nBULLET::::- Understanding CD-R & CD-RW at the Optical Storage Technology Association site.\n"}
{"id": "6781", "url": "https://en.wikipedia.org/wiki?curid=6781", "title": "Cytosol", "text": "Cytosol\n\nThe cytosol, also known as intracellular fluid (ICF) or cytoplasmic matrix, or groundplasm, is the liquid found inside cells. It is separated into compartments by membranes. For example, the mitochondrial matrix separates the mitochondrion into many compartments.\n\nIn the eukaryotic cell, the cytosol is surrounded by the cell membrane and is part of the cytoplasm, which also comprises the mitochondria, plastids, and other organelles (but not their internal fluids and structures); the cell nucleus is separate. The cytosol is thus a liquid matrix around the organelles. In prokaryotes, most of the chemical reactions of metabolism take place in the cytosol, while a few take place in membranes or in the periplasmic space. In eukaryotes, while many metabolic pathways still occur in the cytosol, others take place within organelles.\n\nThe cytosol is a complex mixture of substances dissolved in water. Although water forms the large majority of the cytosol, its structure and properties within cells is not well understood. The concentrations of ions such as sodium and potassium are different in the cytosol than in the extracellular fluid; these differences in ion levels are important in processes such as osmoregulation, cell signaling, and the generation of action potentials in excitable cells such as endocrine, nerve and muscle cells. The cytosol also contains large amounts of macromolecules, which can alter how molecules behave, through macromolecular crowding.\n\nAlthough it was once thought to be a simple solution of molecules, the cytosol has multiple levels of organization. These include concentration gradients of small molecules such as calcium, large complexes of enzymes that act together and take part in metabolic pathways, and protein complexes such as proteasomes and carboxysomes that enclose and separate parts of the cytosol.\n\nThe term \"cytosol\" was first introduced in 1965 by H. A. Lardy, and initially referred to the liquid that was produced by breaking cells apart and pelleting all the insoluble components by ultracentrifugation. Such a soluble cell extract is not identical to the soluble part of the cell cytoplasm and is usually called a cytoplasmic fraction.\n\nThe term \"cytosol\" is now used to refer to the liquid phase of the cytoplasm in an intact cell. This excludes any part of the cytoplasm that is contained within organelles. Due to the possibility of confusion between the use of the word \"cytosol\" to refer to both extracts of cells and the soluble part of the cytoplasm in intact cells, the phrase \"aqueous cytoplasm\" has been used to describe the liquid contents of the cytoplasm of living cells.\n\nPrior to this, other terms, including hyaloplasm, were used for the cell fluid, not always synonymously, as its nature was not very clear (see protoplasm).\n\nThe proportion of cell volume that is cytosol varies: for example while this compartment forms the bulk of cell structure in bacteria, in plant cells the main compartment is the large central vacuole. The cytosol consists mostly of water, dissolved ions, small molecules, and large water-soluble molecules (such as proteins). The majority of these non-protein molecules have a molecular mass of less than 300 Da. This mixture of small molecules is extraordinarily complex, as the variety of molecules that are involved in metabolism (the metabolites) is immense. For example, up to 200,000 different small molecules might be made in plants, although not all these will be present in the same species, or in a single cell. Estimates of the number of metabolites in single cells such as \"E. coli\" and baker's yeast predict that under 1,000 are made.\n\nMost of the cytosol is water, which makes up about 70% of the total volume of a typical cell. The pH of the intracellular fluid is 7.4. while human cytosolic pH ranges between 7.0 - 7.4, and is usually higher if a cell is growing. The viscosity of cytoplasm is roughly the same as pure water, although diffusion of small molecules through this liquid is about fourfold slower than in pure water, due mostly to collisions with the large numbers of macromolecules in the cytosol. Studies in the brine shrimp have examined how water affects cell functions; these saw that a 20% reduction in the amount of water in a cell inhibits metabolism, with metabolism decreasing progressively as the cell dries out and all metabolic activity halting when the water level reaches 70% below normal.\n\nAlthough water is vital for life, the structure of this water in the cytosol is not well understood, mostly because methods such as nuclear magnetic resonance spectroscopy only give information on the average structure of water, and cannot measure local variations at the microscopic scale. Even the structure of pure water is poorly understood, due to the ability of water to form structures such as water clusters through hydrogen bonds.\n\nThe classic view of water in cells is that about 5% of this water is strongly bound in by solutes or macromolecules as water of solvation, while the majority has the same structure as pure water. This water of solvation is not active in osmosis and may have different solvent properties, so that some dissolved molecules are excluded, while others become concentrated. However, others argue that the effects of the high concentrations of macromolecules in cells extend throughout the cytosol and that water in cells behaves very differently from the water in dilute solutions. These ideas include the proposal that cells contain zones of low and high-density water, which could have widespread effects on the structures and functions of the other parts of the cell. However, the use of advanced nuclear magnetic resonance methods to directly measure the mobility of water in living cells contradicts this idea, as it suggests that 85% of cell water acts like that pure water, while the remainder is less mobile and probably bound to macromolecules.\n\nThe concentrations of the other ions in cytosol are quite different from those in extracellular fluid and the cytosol also contains much higher amounts of charged macromolecules such as proteins and nucleic acids than the outside of the cell structure.\n\n+ Typical ion concentrations in mammalian cytosol and blood.\n!Ion\n! Concentration in cytosol (millimolar) \n! Concentration in blood (millimolar) \n Potassium \nalign=\"center\"  139 \nalign=\"center\"  4 \n Sodium \nalign=\"center\"  12 \nalign=\"center\"  145 \n Chloride \nalign=\"center\"  4 \nalign=\"center\"  116 \n Bicarbonate \nalign=\"center\"  12 \nalign=\"center\"  29 \n Amino acids in proteins \nalign=\"center\"  138 \nalign=\"center\"  9 \n Magnesium \nalign=\"center\"  0.8 \nalign=\"center\"  1.5 \n Calcium \nalign=\"center\"  <0.0002 \nalign=\"center\"  1.8 \n\nIn contrast to extracellular fluid, cytosol has a high concentration of potassium ions and a low concentration of sodium ions. This difference in ion concentrations is critical for osmoregulation, since if the ion levels were the same inside a cell as outside, water would enter constantly by osmosis - since the levels of macromolecules inside cells are higher than their levels outside. Instead, sodium ions are expelled and potassium ions taken up by the Na⁺/K⁺-ATPase, potassium ions then flow down their concentration gradient through potassium-selection ion channels, this loss of positive charge creates a negative membrane potential. To balance this potential difference, negative chloride ions also exit the cell, through selective chloride channels. The loss of sodium and chloride ions compensates for the osmotic effect of the higher concentration of organic molecules inside the cell.\n\nCells can deal with even larger osmotic changes by accumulating osmoprotectants such as betaines or trehalose in their cytosol. Some of these molecules can allow cells to survive being completely dried out and allow an organism to enter a state of suspended animation called cryptobiosis. In this state the cytosol and osmoprotectants become a glass-like solid that helps stabilize proteins and cell membranes from the damaging effects of desiccation.\n\nThe low concentration of calcium in the cytosol allows calcium ions to function as a second messenger in calcium signaling. Here, a signal such as a hormone or an action potential opens calcium channel so that calcium floods into the cytosol. This sudden increase in cytosolic calcium activates other signalling molecules, such as calmodulin and protein kinase C. Other ions such as chloride and potassium may also have signaling functions in the cytosol, but these are not well understood.\n\nProtein molecules that do not bind to cell membranes or the cytoskeleton are dissolved in the cytosol. The amount of protein in cells is extremely high, and approaches 200 mg/ml, occupying about 20-30% of the volume of the cytosol. However, measuring precisely how much protein is dissolved in cytosol in intact cells is difficult, since some proteins appear to be weakly associated with membranes or organelles in whole cells and are released into solution upon cell lysis. Indeed, in experiments where the plasma membrane of cells were carefully disrupted using saponin, without damaging the other cell membranes, only about one quarter of cell protein was released. These cells were also able to synthesize proteins if given ATP and amino acids, implying that many of the enzymes in cytosol are bound to the cytoskeleton. However, the idea that the majority of the proteins in cells are tightly bound in a network called the microtrabecular lattice is now seen as unlikely.\n\nIn prokaryotes the cytosol contains the cell's genome, within a structure known as a nucleoid. This is an irregular mass of DNA and associated proteins that control the transcription and replication of the bacterial chromosome and plasmids. In eukaryotes the genome is held within the cell nucleus, which is separated from the cytosol by nuclear pores that block the free diffusion of any molecule larger than about 10 nanometres in diameter.\n\nThis high concentration of macromolecules in cytosol causes an effect called macromolecular crowding, which is when the effective concentration of other macromolecules is increased, since they have less volume to move in. This crowding effect can produce large changes in both the rates and the position of chemical equilibrium of reactions in the cytosol. It is particularly important in its ability to alter dissociation constants by favoring the association of macromolecules, such as when multiple proteins come together to form protein complexes, or when DNA-binding proteins bind to their targets in the genome.\n\nAlthough the components of the cytosol are not separated into regions by cell membranes, these components do not always mix randomly and several levels of organization can localize specific molecules to defined sites within the cytosol.\n\nAlthough small molecules diffuse rapidly in the cytosol, concentration gradients can still be produced within this compartment. A well-studied example of these are the \"calcium sparks\" that are produced for a short period in the region around an open calcium channel. These are about 2 micrometres in diameter and last for only a few milliseconds, although several sparks can merge to form larger gradients, called \"calcium waves\". Concentration gradients of other small molecules, such as oxygen and adenosine triphosphate may be produced in cells around clusters of mitochondria, although these are less well understood.\n\nProteins can associate to form protein complexes, these often contain a set of proteins with similar functions, such as enzymes that carry out several steps in the same metabolic pathway. This organization can allow substrate channeling, which is when the product of one enzyme is passed directly to the next enzyme in a pathway without being released into solution. Channeling can make a pathway more rapid and efficient than it would be if the enzymes were randomly distributed in the cytosol, and can also prevent the release of unstable reaction intermediates. Although a wide variety of metabolic pathways involve enzymes that are tightly bound to each other, others may involve more loosely associated complexes that are very difficult to study outside the cell. Consequently, the importance of these complexes for metabolism in general remains unclear.\n\nSome protein complexes contain a large central cavity that is isolated from the remainder of the cytosol. One example of such an enclosed compartment is the proteasome. Here, a set of subunits form a hollow barrel containing proteases that degrade cytosolic proteins. Since these would be damaging if they mixed freely with the remainder of the cytosol, the barrel is capped by a set of regulatory proteins that recognize proteins with a signal directing them for degradation (a ubiquitin tag) and feed them into the proteolytic cavity.\n\nAnother large class of protein compartments are bacterial microcompartments, which are made of a protein shell that encapsulates various enzymes. These compartments are typically about 100-200 nanometres across and made of interlocking proteins. A well-understood example is the carboxysome, which contains enzymes involved in carbon fixation such as RuBisCO.\n\nNon-membrane bound organelles can form as Biomolecular condensates, which arise by clustering, oligomerisation, or polymerisation of macromolecules to drive colloidal phase separation of the cytoplasm or nucleus.\n\nAlthough the cytoskeleton is not part of the cytosol, the presence of this network of filaments restricts the diffusion of large particles in the cell. For example, in several studies tracer particles larger than about 25 nanometres (about the size of a ribosome) were excluded from parts of the cytosol around the edges of the cell and next to the nucleus. These \"excluding compartments\" may contain a much denser meshwork of actin fibres than the remainder of the cytosol. These microdomains could influence the distribution of large structures such as ribosomes and organelles within the cytosol by excluding them from some areas and concentrating them in others.\n\nThe cytosol has no single function and is instead the site of multiple cell processes. Examples of these processes include signal transduction from the cell membrane to sites within the cell, such as the cell nucleus, or organelles. This compartment is also the site of many of the processes of cytokinesis, after the breakdown of the nuclear membrane in mitosis. Another major function of cytosol is to transport metabolites from their site of production to where they are used. This is relatively simple for water-soluble molecules, such as amino acids, which can diffuse rapidly through the cytosol. However, hydrophobic molecules, such as fatty acids or sterols, can be transported through the cytosol by specific binding proteins, which shuttle these molecules between cell membranes. Molecules taken into the cell by endocytosis or on their way to be secreted can also be transported through the cytosol inside vesicles, which are small spheres of lipids that are moved along the cytoskeleton by motor proteins.\n\nThe cytosol is the site of most metabolism in prokaryotes, and a large proportion of the metabolism of eukaryotes. For instance, in mammals about half of the proteins in the cell are localized to the cytosol. The most complete data are available in yeast, where metabolic reconstructions indicate that the majority of both metabolic processes and metabolites occur in the cytosol. Major metabolic pathways that occur in the cytosol in animals are protein biosynthesis, the pentose phosphate pathway, glycolysis and gluconeogenesis. The localization of pathways can be different in other organisms, for instance fatty acid synthesis occurs in chloroplasts in plants and in apicoplasts in apicomplexa.\n"}
{"id": "6782", "url": "https://en.wikipedia.org/wiki?curid=6782", "title": "Compound", "text": "Compound\n\nCompound may refer to:\n\nBULLET::::- Compound (enclosure), a cluster of buildings having a shared purpose, usually inside a fence or wall\nBULLET::::- Compound (fortification), a version of the above fortified with defensive structures\nBULLET::::- Compound (migrant labour), a hostel for migrant workers such as those historically connected with mines in South Africa\nBULLET::::- The Compound, an area of Palm Bay, Florida, US\nBULLET::::- Komboni or compound, a type of slum in Zambia\n\nBULLET::::- Compound interest, unpaid interest that is added to the principal so that subsequent interest is calculated on the grossed amount\n\nBULLET::::- Composition (fine), a legal procedure in use after the English Civil War\nBULLET::::- Committee for Compounding with Delinquents, an English Civil War institution that allowed Parliament to compound the estates of Royalists\nBULLET::::- Compounding treason, an offence under the common law of England\nBULLET::::- Compounding a felony, a previous offense under the common law of England\n\nBULLET::::- Compound (linguistics), a word that consists of more than one radical element\nBULLET::::- Compound sentence (linguistics), a type of sentence made up of two or more independent clauses and no subordinate (dependent) clauses\n\nBULLET::::- Compounding, the mixing of drugs in pharmacy\nBULLET::::- Compound fracture, a complete fractures of bone where at least one fragment has damaged the skin, soft tissue or surrounding body cavity\nBULLET::::- Compound leaf, a type of leaf being divided into smaller leaflets\n\nBULLET::::- Chemical compound, combination of two or more elements\nBULLET::::- Compounding, the mixing of drugs in pharmacy\nBULLET::::- Plastic compounding, a method of preparing plastic formulations\n\nBULLET::::- Compound engine, a steam engine in which steam is expanded through a series of two or three cylinders before exhaust\nBULLET::::- Turbo-compound engine, an internal combustion engine where exhaust gases expand through power-turbines\nBULLET::::- Compounding pressure, a method in which pressure in a steam turbine is made to drop in a number of stages\nBULLET::::- Eisenhuth Horseless Vehicle Company, or Compound, a former US automobile make with a unique compound gasoline engine; exhausts of 2 cylinders were expanded in a larger third one\n\nBULLET::::- Compound bow, a type of bow for archery\nBULLET::::- Polyhedral compound, a polyhedron composed of multiple polyhedra sharing the same centre\n\nBULLET::::- \"The Compound\" (book), a 2008 young adult novel by S. A. Bodeen\nBULLET::::- Compound (company), a Venture capital firm previously known as Metamorphic Ventures\nBULLET::::- Compound (music), an attribute of a time signature\nBULLET::::- Compound chocolate, an inexpensive chocolate substitute that uses cocoa but excludes cocoa butter\nBULLET::::- Composite (disambiguation)\n"}
{"id": "6784", "url": "https://en.wikipedia.org/wiki?curid=6784", "title": "Citizenship", "text": "Citizenship\n\nCitizen is the status of a person recognized under the custom or law as being a legal member of a sovereign state or belonging to a nation. The idea of citizenship has been defined as the capacity of individuals to defend their rights in front of the governmental authority.\n\nA person may have multiple citizenships. A person who does not have citizenship of any state is said to be stateless, while one who lives on state borders whose territorial status is uncertain is a border-lander.\n\nNationality is often used as a synonym for citizenship in English – notably in international law – although the term is sometimes understood as denoting a person's membership of a nation (a large ethnic group). In some countries, e.g. the United States, the United Kingdom, nationality and citizenship can have different meanings (for more information, see Nationality versus citizenship).\n\nEach country has its own policies, regulations and criteria as to who is entitled to its citizenship. A person can be recognized or granted citizenship on a number of bases. Usually citizenship based on circumstances of birth is automatic, but in other cases an application may be required.\n\nBULLET::::- Citizenship by birth (\"jus sanguinis\"). If one or both of a person's parents are citizens of a given state, then the person may have the right to be a citizen of that state as well. Formerly this might only have applied through the paternal line, but sex equality became common since the late twentieth century. Citizenship is granted based on ancestry or ethnicity and is related to the concept of a nation state common in Europe. Where \"jus sanguinis\" holds, a person born outside a country, one or both of whose parents are citizens of the country, is also a citizen. Some states (United Kingdom, Canada) limit the right to citizenship by descent to a certain number of generations born outside the state; others (Germany, Ireland) grant citizenship only if each new generation is registered with the relevant foreign mission within a specified deadline; while others (France, Switzerland, Italy) have no limitation on the number of generations born abroad who can claim citizenship of their ancestors' country. This form of citizenship is common in civil law countries.\nBULLET::::- Born within a country (\"jus soli\"). Some people are automatically citizens of the state in which they are born. This form of citizenship originated in England, where those who were born within the realm were subjects of the monarch (a concept pre-dating citizenship) and is common in common law countries. Most countries in the Americas grant unconditional jus soli citizenship, while it has been limited or abolished in almost all other countries.\nBULLET::::- In many cases, both \"jus soli\" and \"jus sanguinis\" hold citizenship either by place or parentage (or both).\nBULLET::::- Citizenship by marriage (\"jus matrimonii\"). Many countries fast-track naturalization based on the marriage of a person to a citizen. Countries which are destinations for such immigration often have regulations to try to detect sham marriages, where a citizen marries a non-citizen typically for payment, without them having the intention of living together. Many countries (United Kingdom, Germany, United States, Canada) allow citizenship by marriage only if the foreign spouse is a permanent resident of the country in which citizenship is sought; others (Switzerland, Luxembourg) allow foreign spouses of expatriate citizens to obtain citizenship after a certain period of marriage, and sometimes also subject to language skills and proof of cultural integration (e.g. regular visits to the spouse's country of citizenship).\nBULLET::::- Naturalization. States normally grant citizenship to people who have entered the country legally and been granted permit to stay, or been granted political asylum, and also lived there for a specified period. In some countries, naturalization is subject to conditions which may include passing a test demonstrating reasonable knowledge of the language or way of life of the host country, good conduct (no serious criminal record) and moral character (such as drunkenness, or gambling), vowing allegiance to their new state or its ruler and renouncing their prior citizenship. Some states allow dual citizenship and do not require naturalized citizens to formally renounce any other citizenship.\nBULLET::::- Citizenship by investment or Economic Citizenship. Wealthy people invest money in property or businesses, buy government bonds or simply donate cash directly, in exchange for citizenship and a passport. Whilst legitimate and usually limited in quota, the schemes are controversial. Costs for citizenship by investment range from as little as $100,000 (£74,900) to as much as €2.5m (£2.19m)\nBULLET::::- Excluded categories. In the past there have been exclusions on entitlement to citizenship on grounds such as skin color, ethnicity, sex, and free status (not being a slave). Most of these exclusions no longer apply in most places. Modern examples include some Arab countries which rarely grant citizenship to non-Muslims, e.g. Qatar is known for granting citizenship to foreign athletes, but they all have to profess the Islamic faith in order to receive citizenship. The United States grants citizenship to those born as a result of reproductive technologies, and internationally adopted children born after February 27, 1983. Some exclusions still persist for internationally adopted children born before February 27, 1983 even though their parents meet citizenship criteria.\n\nMany thinkers point to the concept of citizenship beginning in the early city-states of ancient Greece, although others see it as primarily a modern phenomenon dating back only a few hundred years and, for humanity, that the concept of citizenship arose with the first laws. \"Polis\" meant both the political assembly of the city-state as well as the entire society. Citizenship concept has generally been identified as a western phenomenon. There is a general view that citizenship in ancient times was a simpler relation than modern forms of citizenship, although this view has come under scrutiny. The relation of citizenship has not been a fixed or static relation, but constantly changed within each society, and that according to one view, citizenship might \"really have worked\" only at select periods during certain times, such as when the Athenian politician Solon made reforms in the early Athenian state.\n\nHistorian Geoffrey Hosking in his 2005 \"Modern Scholar\" lecture course suggested that citizenship in ancient Greece arose from an appreciation for the importance of freedom. Hosking explained:\n\nSlavery permitted slave-owners to have substantial free time, and enabled participation in public life. Polis citizenship was marked by exclusivity. Inequality of status was widespread; citizens (πολίτης \"politēs\" < πόλις 'city') had a higher status than non-citizens, such as women, slaves, and resident foreigners (metics). The first form of citizenship was based on the way people lived in the ancient Greek times, in small-scale organic communities of the polis. Citizenship was not seen as a separate activity from the private life of the individual person, in the sense that there was not a distinction between public and private life. The obligations of citizenship were deeply connected into one's everyday life in the polis. These small-scale organic communities were generally seen as a new development in world history, in contrast to the established ancient civilizations of Egypt or Persia, or the hunter-gatherer bands \nelsewhere. From the viewpoint of the ancient Greeks, a person's public life was not separated from their private life, and Greeks did not distinguish between the two worlds according to the modern western conception. The obligations of citizenship were deeply connected with everyday life. To be truly human, one had to be an active citizen to the community, which Aristotle famously expressed: \"To take no part in the running of the community's affairs is to be either a beast or a god!\" This form of citizenship was based on obligations of citizens towards the community, rather than rights given to the citizens of the community. This was not a problem because they all had a strong affinity with the polis; their own destiny and the destiny of the community were strongly linked. Also, citizens of the polis saw obligations to the community as an opportunity to be virtuous, it was a source of honour and respect. In Athens, citizens were both ruler and ruled, important political and judicial offices were rotated and all citizens had the right to speak and vote in the political assembly.\n\nIn the Roman Empire, citizenship expanded from small-scale communities to the entirety of the empire. Romans realized that granting citizenship to people from all over the empire legitimized Roman rule over conquered areas. Roman citizenship was no longer a status of political agency, as it had been reduced to a judicial safeguard and the expression of rule and law. Rome carried forth Greek ideas of citizenship such as the principles of equality under the law, civic participation in government, and notions that \"no one citizen should have too much power for too long\", but Rome offered relatively generous terms to its captives, including chances for lesser forms of citizenship. If Greek citizenship was an \"emancipation from the world of things\", the Roman sense increasingly reflected the fact that citizens could act upon material things as well as other citizens, in the sense of buying or selling property, possessions, titles, goods. One historian explained:\n\nRoman citizenship reflected a struggle between the upper-class patrician interests against the lower-order working groups known as the plebeian class. A citizen came to be understood as a person \"free to act by law, free to ask and expect the law's protection, a citizen of such and such a legal community, of such and such a legal standing in that community\". Citizenship meant having rights to have possessions, immunities, expectations, which were \"available in many kinds and degrees, available or unavailable to many kinds of person for many kinds of reason\". The law itself was a kind of bond uniting people. Roman citizenship was more impersonal, universal, multiform, having different degrees and applications.\n\nDuring the European Middle Ages, citizenship was usually associated with cities and towns, and applied mainly to middle class folk. Titles such as burgher, grand burgher (German \"Großbürger\") and bourgeoisie denoted political affiliation and identity in relation to a particular locality, as well as membership in a mercantile or trading class; thus, individuals of respectable means and socioeconomic status were interchangeable with citizens.\n\nDuring this era, members of the nobility had a range of privileges above commoners (see aristocracy), though political upheavals and reforms, beginning most prominently with the French Revolution, abolished privileges and created an egalitarian concept of citizenship.\n\nDuring the Renaissance, people transitioned from being subjects of a king or queen to being citizens of a city and later to a nation. Each city had its own law, courts, and independent administration. And being a citizen often meant being subject to the city's law in addition to having power in some instances to help choose officials. City dwellers who had fought alongside nobles in battles to defend their cities were no longer content with having a subordinate social status, but demanded a greater role in the form of citizenship. Membership in guilds was an indirect form of citizenship in that it helped their members succeed financially. The rise of citizenship was linked to the rise of republicanism, according to one account, since independent citizens meant that kings had less power. Citizenship became an idealized, almost abstract, concept, and did not signify a submissive relation with a lord or count, but rather indicated the bond between a person and the state in the rather abstract sense of having rights and duties.\n\nThe modern idea of citizenship still respects the idea of political participation, but it is usually done through \"elaborate systems of political representation at a distance\" such as representative democracy. Modern citizenship is much more passive; action is delegated to others; citizenship is often a constraint on acting, not an impetus to act. Nevertheless, citizens are usually aware of their obligations to authorities, and are aware that these bonds often limit what they can do.\n\nFrom 1790 until the mid-twentieth century, United States law used racial criteria to establish citizenship rights and regulate who was eligible to become a naturalized citizen. The Naturalization Act of 1790, the first law in U.S. history to establish rules for citizenship and naturalization, barred citizenship to all people who were not of European descent, stating that \"any alien being a free white person, who shall have resided within the limits and under the jurisdiction of the United States for the term of two years, may be admitted to become a citizen thereof.\"\n\nUnder early U.S. laws, African Americans were not eligible for citizenship. In 1857, these laws were upheld in the US Supreme Court case \"Dred Scott v. Sandford\", which ruled that \"a free negro of the African race, whose ancestors were brought to this country and sold as slaves, is not a 'citizen' within the meaning of the Constitution of the United States,\" and that \"the special rights and immunities guarantied to citizens do not apply to them.\"\n\nIt was not until the abolition of slavery following the American Civil War that African Americans were granted citizenship rights. The 14th Amendment to the U.S. Constitution, ratified on July 9, 1868, stated that \"all persons born or naturalized in the United States, and subject to the jurisdiction thereof, are citizens of the United States and of the State wherein they reside.\" Two years later, the Naturalization Act of 1870 would extend the right to become a naturalized citizen to include \"aliens of African nativity and to persons of African descent\".\n\nDespite the gains made by African Americans after the Civil War, Native Americans, Asians, and others not considered \"free white persons\" were still denied the ability to become citizens. The 1882 Chinese Exclusion Act explicitly denied naturalization rights to all people of Chinese origin, while subsequent acts passed by the US Congress, such as laws in 1906, 1917, and 1924, would include clauses that denied immigration and naturalization rights to people based on broadly defined racial categories. Supreme Court cases such as \"Ozawa v. United States\" (1922) and \"U.S. v. Bhagat Singh Thind\" (1923), would later clarify the meaning of the phrase \"free white persons,\" ruling that ethnically Japanese, Indian, and other non-European people were not \"white persons\", and were therefore ineligible for naturalization under U.S. law.\n\nNative Americans were not granted full US citizenship until the passage of the Indian Citizenship Act in 1924. However, even well into the 1960s some state laws prevented Native Americans from exercising their full rights as citizens, such as the right to vote. In 1962, New Mexico became the last state to enfranchise Native Americans.\n\nIt was not until the passage of the Immigration and Nationality Act of 1952 that the racial and gender restrictions for naturalization were explicitly abolished. However, the act still contained restrictions regarding who was eligible for US citizenship, and retained a national quota system which limited the number of visas given to immigrants based on their national origin, to be fixed \"at a rate of one-sixth of one percent of each nationality's population in the United States in 1920\". It was not until the passage of the Immigration and Nationality Act of 1965 that these immigration quota systems were drastically altered in favor of a less discriminatory system.\n\nThe 1918 constitution of revolutionary Russia granted citizenship to any foreigners who were living within Russia, so long as they were \"engaged in work and [belonged] to the working class.\" It recognized \"the equal rights of all citizens, irrespective of their racial or national connections\" and declared oppression of any minority group or race \"to be contrary to the fundamental laws of the Republic.\" The 1918 constitution also established the right to vote and be elected to soviets for both men and women \"irrespective of religion, nationality, domicile, etc. [...] who shall have completed their eighteenth year by the day of election.\" The later constitutions of the USSR would grant universal Soviet citizenship to the citizens of all member republics in concord with the principles of non-discrimination laid out in the original 1918 constitution of Russia.\n\nNational Socialism or \"Nazism\", the German variant of twentieth century fascism whose precepts were laid out in Adolf Hitler's Mein Kampf, classified inhabitants of the nation into three main hierarchical categories, each of which would have different rights and duties in relation to the state: citizens, subjects, and aliens. The first category, citizens, were to possess full civic rights and responsibilities. Citizenship would be conferred only on males of German (or so-called \"Aryan\") heritage who had completed military service, and could be revoked at any time by the state. The Reich Citizenship Law of 1935 established racial criteria for citizenship in the German Reich, and because of this law Jews and others who could not prove \"German\" racial heritage were stripped of their citizenship.\n\nThe second category, subjects, referred to all others who were born within the nation's boundaries who did not fit the racial criteria for citizenship. Subjects would have no voting rights, could not hold any position within the state, and possessed none of the other rights and civic responsibilities conferred on citizens. All women were to be conferred \"subject\" status upon birth, and could only obtain \"citizen\" status if they worked independently or if they married a German citizen (see women in Nazi Germany).\n\nThe final category, aliens, referred to those who were citizens of another state, who also had no rights.\n\nCitizenship status, under social contract theory, carries with it both rights and duties. In this sense, citizenship was described as \"a bundle of rights -- primarily, political participation in the life of the community, the right to vote, and the right to receive certain protection from the community, as well as obligations.\" Citizenship is seen by most scholars as culture-specific, in the sense that the meaning of the term varies considerably from culture to culture, and over time. In China, for example, there is a cultural politics of citizenship which could be called \"peopleship\".\n\nHow citizenship is understood depends on the person making the determination. The relation of citizenship has never been fixed or static, but constantly changes within each society. While citizenship has varied considerably throughout history, and within societies over time, there are some common elements but they vary considerably as well. As a bond, citizenship extends beyond basic kinship ties to unite people of different genetic backgrounds. It usually signifies membership in a political body. It is often based on, or was a result of, some form of military service or expectation of future service. It usually involves some form of political participation, but this can vary from token acts to active service in government.\n\nCitizenship is a status in society. It is an ideal state as well. It generally describes a person with legal rights within a given political order. It almost always has an element of exclusion, meaning that some people are not citizens, and that this distinction can sometimes be very important, or not important, depending on a particular society. Citizenship as a concept is generally hard to isolate intellectually and compare with related political notions, since it relates to many other aspects of society such as the family, military service, the individual, freedom, religion, ideas of right and wrong, ethnicity, and patterns for how a person should behave in society. When there are many different groups within a nation, citizenship may be the only real bond which unites everybody as equals without discrimination—it is a \"broad bond\" linking \"a person with the state\" and gives people a universal identity as a legal member of a specific nation.\n\nModern citizenship has often been looked at as two competing underlying ideas:\n\nBULLET::::- The liberal-individualist or sometimes liberal conception of citizenship suggests that citizens should have entitlements necessary for human dignity. It assumes people act for the purpose of enlightened self-interest. According to this viewpoint, citizens are sovereign, morally autonomous beings with duties to pay taxes, obey the law, engage in business transactions, and defend the nation if it comes under attack, but are essentially passive politically, and their primary focus is on economic betterment. This idea began to appear around the seventeenth and eighteenth centuries, and became stronger over time, according to one view. According to this formulation, the state exists for the benefit of citizens and has an obligation to respect and protect the rights of citizens, including civil rights and political rights. It was later that so-called social rights became part of the obligation for the state.\n\nBULLET::::- The civic-republican or sometimes classical or civic humanist conception of citizenship emphasizes man's political nature, and sees citizenship as an active process, not a passive state or legal marker. It is relatively more concerned that government will interfere with popular places to practice citizenship in the public sphere. Citizenship means being active in government affairs. According to one view, most people today live as citizens according to the liberal-individualist conception but wished they lived more according to the civic-republican ideal. An ideal citizen is one who exhibits \"good civic behavior\". Free citizens and a republic government are \"mutually interrelated.\" Citizenship suggested a commitment to \"duty and civic virtue\".\nScholars suggest that the concept of citizenship contains many unresolved issues, sometimes called tensions, existing within the relation, that continue to reflect uncertainty about what citizenship is supposed to mean. Some unresolved issues regarding citizenship include questions about what is the proper balance between duties and rights. Another is a question about what is the proper balance between political citizenship versus social citizenship. Some thinkers see benefits with people being absent from public affairs, since too much participation such as revolution can be destructive, yet too little participation such as total apathy can be problematic as well. Citizenship can be seen as a special elite status, and it can also be seen as a democratizing force and something that everybody has; the concept can include both senses. According to sociologist Arthur Stinchcombe, citizenship is based on the extent that a person can control one's own destiny within the group in the sense of being able to influence the government of the group. One last distinction within citizenship is the so-called consent descent distinction, and this issue addresses whether citizenship is a fundamental matter determined by a person choosing to belong to a particular nation––by their consent––or is citizenship a matter of where a person was born––that is, by their descent.\n\nSome intergovernmental organizations have extended the concept and terminology associated with citizenship to the international level, where it is applied to the totality of the citizens of their constituent countries combined. Citizenship at this level is a secondary concept, with rights deriving from national citizenship.\n\nThe Maastricht Treaty introduced the concept of citizenship of the European Union. Article 17 (1) of the Treaty on European Union stated that: Citizenship of the Union is hereby established. Every person holding the nationality of a Member State shall be a citizen of the Union. Citizenship of the Union shall be additional to and not replace national citizenship.\n\nAn agreement known as the amended EC Treaty established certain minimal rights for European Union citizens. Article 12 of the amended EC Treaty guaranteed a general right of non-discrimination within the scope of the Treaty. Article 18 provided a limited right to free movement and residence in Member States other than that of which the European Union citizen is a national. Articles 18-21 and 225 provide certain political rights.\n\nUnion citizens have also extensive rights to move in order to exercise economic activity in any of the Member States which predate the introduction of Union citizenship.\n\nCitizenship of the Mercosur is granted to eligible citizens of the Southern Common Market member states. It was approved in 2010 through the Citizenship Statute and should be fully implemented by the member countries in 2021, when the program will be transformed in an international treaty incorporated into the national legal system of the countries, under the concept of \"Mercosur Citizen\".\n\nThe concept of \"Commonwealth Citizenship\" has been in place ever since the establishment of the Commonwealth of Nations. As with the EU, one holds Commonwealth citizenship only by being a citizen of a Commonwealth member state. This form of citizenship offers certain privileges within some Commonwealth countries:\nBULLET::::- Some such countries do not require tourist visas of citizens of other Commonwealth countries, or allow some Commonwealth citizens to stay in the country for tourism purposes without a visa for longer than citizens of other countries.\nBULLET::::- In some Commonwealth countries, resident citizens of other Commonwealth countries are entitled to political rights, e.g., the right to vote in local and national elections and in some cases even the right to stand for election.\nBULLET::::- In some instances the right to work in any position (including the civil service) is granted, except for certain specific positions, such as in the defense departments, Governor-General or President or Prime Minister.\nBULLET::::- In the United Kingdom, all Commonwealth citizens legally residing in the country can vote and stand for office at all elections.\n\nAlthough Ireland was excluded from the Commonwealth in 1949 because it declared itself a republic, Ireland is generally treated as if it were still a member. Legislation often specifically provides for equal treatment between Commonwealth countries and Ireland and refers to \"Commonwealth countries and Ireland\". Ireland's citizens are not classified as foreign nationals in the United Kingdom.\n\nCanada departed from the principle of nationality being defined in terms of allegiance in 1921. In 1935 the Irish Free State was the first to introduce its own citizenship. However, Irish citizens were still treated as subjects of the Crown, and they are still not regarded as foreign, even though Ireland is not a member of the Commonwealth. The Canadian Citizenship Act of 1947 provided for a distinct Canadian Citizenship, automatically conferred upon most individuals born in Canada, with some exceptions, and defined the conditions under which one could become a naturalized citizen. The concept of Commonwealth citizenship was introduced in 1948 in the British Nationality Act 1948. Other dominions adopted this principle such as New Zealand, by way of the British Nationality and New Zealand Citizenship Act of 1948.\n\nCitizenship most usually relates to membership of the nation state, but the term can also apply at the subnational level. Subnational entities may impose requirements, of residency or otherwise, which permit citizens to participate in the political life of that entity, or to enjoy benefits provided by the government of that entity. But in such cases, those eligible are also sometimes seen as \"citizens\" of the relevant state, province, or region. An example of this is how the fundamental basis of Swiss citizenship is citizenship of an individual commune, from which follows citizenship of a canton and of the Confederation. Another example is Åland where the residents enjoy a special provincial citizenship within Finland, \"hembygdsrätt\".\n\nThe United States has a federal system in which a person is a citizen of their specific state of residence, such as New Jersey or California, as well as a citizen of the United States. State constitutions may grant certain rights above and beyond what are granted under the United States Constitution and may impose their own obligations including the sovereign right of taxation and military service; each state maintains at least one military force subject to national militia transfer service, the state's national guard, and some states maintain a second military force not subject to nationalization.\n\n\"Active citizenship\" is the philosophy that citizens should work towards the betterment of their community through economic participation, public, volunteer work, and other such efforts to improve life for all citizens. In this vein, citizenship education is taught in schools, as an academic subject in some countries. By the time children reach secondary education there is an emphasis on such unconventional subjects to be included in academic curriculum. While the diagram on citizenship to the right is rather facile and depth-less, it is simplified to explain the general model of citizenship that is taught to many secondary school pupils. The idea behind this model within education is to instill in young pupils that their actions (i.e. their vote) affect collective citizenship and thus in turn them.\n\nIt is taught in the Republic of Ireland as an exam subject for the Junior Certificate. It is known as Civic, Social and Political Education (CSPE). A new Leaving Certificate exam subject with the working title 'Politics & Society' is being developed by the National Council for Curriculum and Assessment (NCCA) and is expected to be introduced to the curriculum sometime after 2012.\n\nCitizenship is offered as a General Certificate of Secondary Education (GCSE) course in many schools in the United Kingdom. As well as teaching knowledge about democracy, parliament, government, the justice system, human rights and the UK's relations with the wider world, students participate in active citizenship, often involving a social action or social enterprise in their local community.\nBULLET::::- Citizenship is a compulsory subject of the National Curriculum in state schools in England for all pupils aged 11–16. Some schools offer a qualification in this subject at GCSE and A level. All state schools have a statutory requirement to teach the subject, assess pupil attainment and report student's progress in citizenship to parents.\nBULLET::::- In Wales the model used is Personal and Social Education.\nBULLET::::- Citizenship is not taught as a discrete subject in Scottish schools, but is a cross-curricular strand of the Curriculum for Excellence. However they do teach a subject called \"Modern Studies\" which covers the social, political and economic study of local, national and international issues.\nBULLET::::- Citizenship is taught as a standalone subject in all state schools in Northern Ireland and most other schools in some forms from year 8 to 10 prior to GCSEs. Components of Citizenship are then also incorporated into GCSE courses such as 'Learning for Life and Work'.\n\nThere are two kinds of criticism of citizenship education in schools. Firstly, some philosophers of education argue that most governments and mainstream policies stimulate and advocate questionable approaches of citizenship education. These approaches aim to develop specific dispositions in students, dispositions conducive to political participation and solidarity. But there are radically different views on the nature of good citizenship and education should involve and develop autonomy and open-mindedness. Therefore, it requires a more critical approach than is possible when political participation and solidarity are conceived of as goals of education. Secondly, some educationalists argue that merely teaching children about the theory of citizenship is ineffective, unless schools themselves reflect democratic practices by giving children the opportunity to have a say in decision making. They suggest that schools are fundamentally undemocratic institutions, and that such a setting cannot instill in children the commitment and belief in democratic values that is necessary for citizenship education to have a proper impact. Some educationalists relate this criticism to John Dewey (see critical comments on this interpretation of Dewey: Van der Ploeg, 2016).\n\nBULLET::::- Citizenship in Canada\nBULLET::::- Citizenship in Germany\nBULLET::::- Citizenship of the United States\nBULLET::::- Citizen in uniform\nBULLET::::- Global citizenship\nBULLET::::- History of citizenship\nBULLET::::- Honorary citizenship\nBULLET::::- Honorary Canadian citizenship\nBULLET::::- Honorary citizen of the United States\nBULLET::::- Nationalism\nBULLET::::- Non-citizens (Latvia)\nBULLET::::- Spatial citizenship\nBULLET::::- Transnational citizenship\n\n\nBULLET::::- Beaven, Brad, and John Griffiths. \"Creating the Exemplary Citizen: The Changing Notion of Citizenship in Britain 1870–1939,\" \"Contemporary British History\" (2008) 22#2 pp 203–225\n\nBULLET::::- BBC PSHE & Citizenship\nBULLET::::- The Life in the UK Citizenship Test Report by Thom Brooks\n"}
{"id": "6787", "url": "https://en.wikipedia.org/wiki?curid=6787", "title": "Chiapas", "text": "Chiapas\n\nChiapas (), officially the Free and Sovereign State of Chiapas (), is one of the 31 states that, along with Mexico City, make up the 32 federal entities of Mexico. It is divided into 124 municipalities as of September 2017 and its capital city is Tuxtla Gutiérrez. Other important population centers in Chiapas include Ocosingo, Tapachula, San Cristóbal de las Casas, Comitán and Arriaga. It is the southernmost state in Mexico. It is located in Southeastern Mexico, and it borders the states of Oaxaca to the west, Veracruz to the northwest and Tabasco to the north, and by the Petén, Quiché, Huehuetenango and San Marcos departments of Guatemala to the east and southeast. Chiapas has a coastline along the Pacific Ocean to the south.\n\nIn general, Chiapas has a humid, tropical climate. In the north, in the area bordering Tabasco, near Teapa, rainfall can average more than per year. In the past, natural vegetation in this region was lowland, tall perennial rainforest, but this vegetation has been almost completely cleared to allow agriculture and ranching. Rainfall decreases moving towards the Pacific Ocean, but it is still abundant enough to allow the farming of bananas and many other tropical crops near Tapachula. On the several parallel \"sierras\" or mountain ranges running along the center of Chiapas, climate can be quite temperate and foggy, allowing the development of cloud forests like those of the Reserva de la Biosfera el Triunfo, home to a handful of resplendent quetzals and horned guans.\n\nChiapas is home to the ancient Mayan ruins of Palenque, Yaxchilán, Bonampak, Chinkultic and Toniná. It is also home to one of the largest indigenous populations in the country with twelve federally recognized ethnicities.\n\nThe official name of the state is Chiapas. It is believed to have come from the ancient city of Chiapan, which in Náhuatl means \"the place where the chia sage grows.\" After the Spanish arrived (1522), they established two cities called Chiapas delos Indios and Chiapas delos Españoles (1528), with the name of Provincia de Chiapas for the area around the cities. The first coat of arms of the region dates from 1535 as that of the Ciudad Real (San Cristóbal de las Casas). Chiapas painter Javier Vargas Ballinas designed the modern coat of arms.\n\nHunter gatherers began to occupy the central valley of the state around 7000 BCE, but little is known about them. The oldest archaeological remains in the seat are located at the Santa Elena Ranch in Ocozocoautla whose finds include tools and weapons made of stone and bone. It also includes burials. In the pre Classic period from 1800 BCE to 300 CE, agricultural villages appeared all over the state although hunter gather groups would persist for long after the era.\n\nRecent excavations in the Soconusco region of the state indicate that the oldest civilization to appear in what is now modern Chiapas is that of the Mokaya, which were cultivating corn and living in houses as early as 1500 BCE, making them one of the oldest in Mesoamerica. There is speculation that these were the forefathers of the Olmec, migrating across the Grijalva Valley and onto the coastal plain of the Gulf of Mexico to the north, which was Olmec territory. One of these people's ancient cities is now the archeological site of Chiapa de Corzo, in which was found the oldest calendar known on a piece of ceramic with a date of 36 BCE. This is three hundred years before the Mayans developed their calendar. The descendants of Mokaya are the Mixe-Zoque.\n\nDuring the pre Classic era, it is known that most of Chiapas was not Olmec, but had close relations with them, especially the Olmecs of the Isthmus of Tehuantepec. Olmec-influenced sculpture can be found in Chiapas and products from the state including amber, magnetite, and ilmenite were exported to Olmec lands. The Olmecs came to what is now the northwest of the state looking for amber with one of the main pieces of evidence for this called the Simojovel Ax.\nMayan civilization began in the pre-Classic period as well, but did not come into prominence until the Classic period (300–900CE). Development of this culture was agricultural villages during the pre-Classic period with city building during the Classic as social stratification became more complex. The Mayans built cities on the Yucatán Peninsula and west into Guatemala. In Chiapas, Mayan sites are concentrated along the state's borders with Tabasco and Guatemala, near Mayan sites in those entities. Most of this area belongs to the Lacandon Jungle.\n\nMayan civilization in the Lacandon area is marked by rising exploitation of rain forest resources, rigid social stratification, fervent local identity, waging war against neighboring peoples. At its height, it had large cities, a writing system, and development of scientific knowledge, such as mathematics and astronomy. Cities were centered on large political and ceremonial structures elaborately decorated with murals and inscriptions. Among these cities are Palenque, Bonampak, Yaxchilan, Chinkultic, Toniná and Tenón. The Mayan civilization had extensive trade networks and large markets trading in goods such as animal skins, indigo, amber, vanilla and quetzal feathers. It is not known what ended the civilization but theories range from over population size, natural disasters, disease, and loss of natural resources through over exploitation or climate change.\n\nNearly all Mayan cities collapsed around the same time, 900CE. From then until 1500 CE, social organization of the region fragmented into much smaller units and social structure became much less complex. There was some influence from the rising powers of central Mexico but two main indigenous groups emerged during this time, the Zoques and the various Mayan descendants. The Chiapans, for whom the state is named, migrated into the center of the state during this time and settled around Chiapa de Corzo, the old Mixe–Zoque stronghold. There is evidence that the Aztecs appeared in the center of the state around Chiapa de Corza in the 15thcentury, but were unable to displace the native Chiapa tribe. However, they had enough influence so that the name of this area and of the state would come from Nahuatl.\n\nWhen the Spanish arrived in the 16th century, they found the indigenous peoples divided into Mayan and non-Mayan, with the latter dominated by the Zoques and Chiapa. The first contact between Spaniards and the people of Chiapas came in 1522, when Hernán Cortés sent tax collectors to the area after Aztec Empire was subdued. The first military incursion was headed by Luis Marín, who arrived in 1523. After three years, Marín was able to subjugate a number of the local peoples, but met with fierce resistance from the Tzotzils in the highlands. The Spanish colonial government then sent a new expedition under Diego de Mazariegos. Mazariegos had more success than his predecessor, but many natives preferred to commit suicide rather than submit to the Spanish. One famous example of this is the Battle of Tepetchia, where many jumped to their deaths in the Sumidero Canyon.\n\nIndigenous resistance was weakened by continual warfare with the Spaniards and disease. By 1530 almost all of the indigenous peoples of the area had been subdued with the exception of the Lacandons in the deep jungles who actively resisted until 1695. However, the main two groups, the Tzotzils and Tzeltals of the central highlands were subdued enough to establish the first Spanish city, today called San Cristóbal de las Casas, in 1528. It was one of two settlements initially called Villa Real de Chiapa de los Españoles and the other called Chiapa de los Indios.\nSoon after, the encomienda system was introduced, which reduced most of the indigenous population to serfdom and many even as slaves as a form of tribute and way of locking in a labor supply for tax payments. The conquistadors brought previously unknown diseases. This, as well as overwork on plantations, dramatically decreased the indigenous population. The Spanish also established missions, mostly under the Dominicans, with the Diocese of Chiapas established in 1538 by Pope Paul III. The Dominican evangelizers became early advocates of the indigenous' people's plight, with Bartolomé de las Casas winning a battle with the passing of a law in 1542 for their protection. This order also worked to make sure that communities would keep their indigenous name with a saint's prefix leading to names such as San Juan Chamula and San Lorenzo Zinacantán. He also advocated adapting the teaching of Christianity to indigenous language and culture. The encomienda system that had perpetrated much of the abuse of the indigenous peoples declined by the end of the 16th century, and was replaced by haciendas. However, the use and misuse of Indian labor remained a large part of Chiapas politics into modern times. Maltreatment and tribute payments created an undercurrent of resentment in the indigenous population that passed on from generation to generation. One uprising against high tribute payments occurred in the Tzeltal communities in the Los Alto region in 1712. Soon, the Tzoltzils and Ch'ols joined the Tzeltales in rebellion, but within a year the government was able to extinguish the rebellion.\n\nAs of 1778, Thomas Kitchin described Chiapas as \"the metropolis of the original Mexicans,\" with a population of approximately 20,000, and consisting mainly of indigenous peoples. The Spanish introduced new crops such as sugar cane, wheat, barley and indigo as main economic staples along native ones such as corn, cotton, cacao and beans. Livestock such as cattle, horses and sheep were introduced as well. Regions would specialize in certain crops and animals depending on local conditions and for many of these regions, communication and travel were difficult. Most Europeans and their descendants tended to concentrate in cities such as Ciudad Real, Comitán, Chiapa and Tuxtla. Intermixing of the races was prohibited by colonial law but by the end of the 17th century there was a significant mestizo population. Added to this was a population of African slaves brought in by the Spanish in the middle of the 16th century due to the loss of native workforce.\n\nInitially, \"Chiapas\" referred to the first two cities established by the Spanish in what is now the center of the state and the area surrounding them. Two other regions were also established, the Soconusco and Tuxtla, all under the regional colonial government of Guatemala. Chiapas, Soconusco and Tuxla regions were united to the first time as an \"intendencia\" during the Bourbon Reforms in 1790 as an administrative region under the name of Chiapas. However, within this intendencia, the division between Chiapas and Soconusco regions would remain strong and have consequences at the end of the colonial period.\n\nFrom the colonial period Chiapas was relatively isolated from the colonial authorities in Mexico City and regional authorities in Guatemala. One reason for this was the rugged terrain. Another was that much of Chiapas was not attractive to the Spanish. It lacked mineral wealth, large areas of arable land, and easy access to markets. This isolation spared it from battles related to Independence. José María Morelos y Pavón did enter the city of Tonalá but incurred no resistance. The only other insurgent activity was the publication of a newspaper called \"El Pararrayos\" by Matías de Córdova in San Cristóbal de las Casas.\n\nFollowing the end of Spanish rule in New Spain, it was unclear what new political arrangements would emerge. The isolation of Chiapas from centers of power, along with the strong internal divisions in the intendencia caused a political crisis after royal government collapsed in Mexico City in 1821, ending the Mexican War of Independence. During this war, a group of influential Chiapas merchants and ranchers sought the establishment of the Free State of Chiapas. This group became known as the \"La Familia Chiapaneca\". However, this alliance did not last with the lowlands preferring inclusion among the new republics of Central America and the highlands annexation to Mexico. In 1821, a number of cities in Chiapas, starting in Comitán, declared the state's separation from the Spanish empire. In 1823, Guatemala became part of the United Provinces of Central America, which united to form a federal republic that would last from 1823 to 1839. With the exception of the pro-Mexican Ciudad Real (San Cristóbal) and some others, many Chiapanecan towns and villages favored a Chiapas independent of Mexico and some favored unification with Guatemala.\n\nElites in highland cities pushed for incorporation into Mexico. In 1822, then-Emperor Agustín de Iturbide decreed that Chiapas was part of Mexico. In 1823, the Junta General de Gobierno was held and Chiapas declared independence again. In July 1824, the Soconusco District of southwestern Chiapas split off from Chiapas, announcing that it would join the Central American Federation. In September of the same year, a referendum was held on whether the intendencia would join Central America or Mexico, with many of the elite endorsing union with Mexico. This referendum ended in favor of incorporation with Mexico (allegedly through manipulation of the elite in the highlands), but the Soconusco region maintained a neutral status until 1842, when Oaxacans under General Antonio López de Santa Anna occupied the area, and declared it reincorporated into Mexico. Elites of the area would not accept this until 1844. Guatemala would not recognize Mexico's annexation of the Soconusco region until 1895 even though a final border between Chiapas and the country was finalized until 1882. The State of Chiapas was officially declared in 1824, with its first constitution in 1826. Ciudad Real was renamed San Cristóbal de las Casas in 1828.\n\nIn the decades after the official end of the war, the provinces of Chiapas and Soconusco unified, with power concentrated in San Cristóbal de las Casas. The state's society evolved into three distinct spheres: indigenous peoples, mestizos from the farms and haciendas and the Spanish colonial cities. Most of the political struggles were between the latter two groups especially over who would control the indigenous labor force. Economically, the state lost one of its main crops, indigo, to synthetic dyes. There was a small experiment with democracy in the form of \"open city councils\" but it was short lived because voting was heavily rigged.\n\nThe Universidad Pontificia y Literaria de Chiapas was founded in 1826, with Mexico's second teacher's college founded in the state in 1828.\n\nWith the ouster of conservative Antonio López de Santa Anna, Mexican liberals came to power. The Reform War (1858–1861) fought between Liberals, who favored federalism and sought economic development, decreased power of the Roman Catholic Church, and Mexican army, and Conservatives, who favored centralized autocratic government, retention of elite privileges, did not lead to any military battles in the state. Despite that it strongly affected Chiapas politics. In Chiapas, the Liberal-Conservative division had its own twist. Much of the division between the highland and lowland ruling families was for whom the Indians should work for and for how long as the main shortage was of labor. These families split into Liberals in the lowlands, who wanted further reform and Conservatives in the highlands who still wanted to keep some of the traditional colonial and church privileges. For most of the early and mid 19th century, Conservatives held most of the power and were concentrated in the larger cities of San Cristóbal de las Casas, Chiapa (de Corzo), Tuxtla and Comitán. As Liberals gained the upper hand nationally in the mid-19th century, one Liberal politician Ángel Albino Corzo gained control of the state. Corzo became the primary exponent of Liberal ideas in the southeast of Mexico and defended the Palenque and Pichucalco areas from annexation by Tabasco. However, Corzo's rule would end in 1875, when he opposed the regime of Porfirio Díaz.\n\nLiberal land reforms would have negative effects on the state's indigenous population unlike in other areas of the country. Liberal governments expropriated lands that were previously held by the Spanish Crown and Catholic Church in order to sell them into private hands. This was not only motivated by ideology, but also due to the need to raise money. However, many of these lands had been in a kind of \"trust\" with the local indigenous populations, who worked them. Liberal reforms took away this arrangement and many of these lands fell into the hands of large landholders who when made the local Indian population work for three to five days a week just for the right to continue to cultivate the lands. This requirement caused many to leave and look for employment elsewhere. Most became \"free\" workers on other farms, but they were often paid only with food and basic necessities from the farm shop. If this was not enough, these workers became indebted to these same shops and then unable to leave.\n\nThe opening up of these lands also allowed many whites and mestizos (often called Ladinos in Chiapas) to encroach on what had been exclusively indigenous communities in the state. These communities had had almost no contact with the Ladino world, except for a priest. The new Ladino landowners occupied their acquired lands as well as others, such as shopkeepers, opened up businesses in the center of Indian communities. In 1848, a group of Tzeltals plotted to kill the new mestizos in their midst, but this plan was discovered, and was punished by th removal of large number of the community's male members. The changing social order had severe negative effects on the indigenous population with alcoholism spreadings, leading to more debts as it was expensive. The struggles between Conservatives and Liberals nationally disrupted commerce and confused power relations between Indian communities and Ladino authorities. It also resulted in some brief respites for Indians during times when the instability led to uncollected taxes.\n\nOne other effect that Liberal land reforms had was the start of coffee plantations, especially in the Soconusco region. One reason for this push in this area was that Mexico was still working to strengthen its claim on the area against Guatemala's claims on the region. The land reforms brought colonists from other areas of the country as well as foreigners from England, the United States and France. These foreign immigrants would introduce coffee production to the areas, as well as modern machineray and professional administration of coffee plantations. Eventually, this production of coffee would become the state's most important crop.\n\nAlthough the Liberals had mostly triumphed in the state and the rest of the country by the 1860s, Conservatives still held considerable power in Chiapas. Liberal politicians sought to solidify their power among the indigenous groups by weakening the Roman Catholic Church. The more radical of these even allowed indigenous groups the religious freedoms to return to a number of native rituals and beliefs such as pilgrimages to natural shrines such as mountains and waterfalls.\n\nThis culminated in the Chiapas \"caste war\", which was an uprising the Tzotzils beginning in 1868. The basis of the uprising was the establishment of the \"three stones cult\" in Tzajahemal. Agustina Gómez Checheb was a girl tending her father's sheep when three stones fell from the sky. Collecting them, she put them on her father's altar and soon claimed that the stone communicated with her. Word of this soon spread and the \"talking stones\" of Tzajahemel soon became a local indigenous pilgrimage site. The cult was taken over by one pilgrim, Pedro Díaz Cuzcat, who also claimed to be able to communicate with the stones, and had knowledge of Catholic ritual, becoming a kind of priest. However, this challenged the traditional Catholic faith and non Indians began to denounce the cult. Stories about the cult include embellishments such as the crucifixion of a young Indian boy.\n\nThis led to the arrest of Checheb and Cuzcat in December 1868. This caused resentment among the Tzotzils. Although the Liberals had earlier supported the cult, Liberal landowners had also lost control of much of their Indian labor and Liberal politicians were having a harder time collecting taxes from indigenous communities. An Indian army gathered at Zontehuitz then attacked various villages and haciendas. By the following June the city of San Cristóbal was surrounded by several thousand Indians, who offered the exchanged of several Ladino captives for their religious leaders and stones. Chiapas governor Dominguéz come to San Cristóbal with about three hundred heavily armed men, who then attacked the Indian force armed only with sticks and machetes. The indigenous force was quickly dispersed and routed with government troops pursuing pockets of guerrilla resistance in the mountains until 1870. The event effectively returned control of the indigenous workforce back to the highland elite.\n\nThe Porfirio Díaz era at the end of the 19th century and beginning of the 20th was initially thwarted by regional bosses called caciques, bolstered by a wave of Spanish and mestizo farmers who migrated to the state and added to the elite group of wealthy landowning families. There was some technological progress such as a highway from San Cristóbal to the Oaxaca border and the first telephone line in the 1880s, but Porfirian era economic reforms would not begin until 1891 with Governor Emilio Rabasa. This governor took on the local and regional caciques and centralized power into the state capital, which he moved from San Cristóbal de las Casas to Tuxtla in 1892. He modernized public administration, transportation and promoted education. Rabasa also introduced telegraph, limited public schooling, sanitation and road construction, including a route from San Cristóbal to Tuxtla then Oaxaca, which signaled the beginning of favoritism of development in the central valley over the highlands. He also changed state policies to favor foreign investment, favored large land mass consolidation for the production of cash crops such as henequen, rubber, guayule, cochineal and coffee. Agricultural production boomed, especially coffee, which induced the construction of port facilities in Tonalá. The economic expansion and investment in roads also increased access to tropical commodities such as hardwoods, rubber and chicle.\n\nThese still required cheap and steady labor to be provided by the indigenous population. By the end of the 19th century, the four main indigenous groups, Tzeltals, Tzotzils, Tojolabals and Ch’ols were living in \"reducciones\" or reservations, isolated from one another. Conditions on the farms of the Porfirian era was serfdom, as bad if not worse than for other indigenous and mestizo populations leading to the Mexican Revolution. While this coming event would affect the state, Chiapas did not follow the uprisings in other areas that would end the Porfirian era.\n\nJapanese immigration to Mexico began in 1897 when the first thirty five migrants arrived in Chiapas to work on coffee farms, so that Mexico was the first Latin American country to receive organized Japanese immigration. Although this colony ultimately failed, there remains a small Japanese community in Acacoyagua, Chiapas.\n\nIn the early 20th century and into the Mexican Revolution, the production of coffee was particularly important but labor-intensive. This would lead to a practice called \"enganche\" (hook), where recruiter would lure workers with advanced pay and other incentives such as alcohol and then trap them with debts for travel and other items to be worked off. This practice would lead to a kind of indentured servitude and uprisings in areas of the state, although they never led to large rebel armies as in other parts of Mexico.\n\nA small war broke out between Tuxtla Gutiérrez and San Cristobal in 1911. San Cristóbal, allied with San Juan Chamula, tried to regain the state's capital but the effort failed. San Cristóbal de las Casas, who had a very limited budget, to the extent that it had to ally with San Juan Chamula, and Tuxtla Gutierrez, which was enough only a small ragtag army to beat overwhelmingly the army helped by chamulas from San Cristóbal. There were three years of peace after that until troops allied with \"First Chief\" of the revolutionary Constitutionalist forces, Venustiano Carranza, entered in 1914 taking over the government, with the aim of imposing the \"Ley de Obreros\" (Workers' Law) to address injustices against the state's mostly indigenous workers. Conservatives responded violently months later when they were certain the Carranza forces would take their lands. This was mostly by way of guerrilla actions headed by farm owners who called themselves the \"Mapaches\". This action continued for six years, until President Carranza was assassinated in 1920 and revolutionary general Álvaro Obregón became president of Mexico. This allowed the Mapaches to gain political power in the state and effectively stop many of the social reforms occurring in other parts of Mexico.\n\nThe Mapaches continued to fight against socialists and communists in Mexico from 1920 to 1936, to maintain their control over the state. In general, elite landowners also allied with the nationally dominant party founded by Plutarco Elías Calles following the assassination of president-elect Obregón in 1928; that party was renamed the Institutional Revolutionary Party in 1946. Through that alliance, they could block land reform in this way as well. The Mapaches were first defeated in 1925 when an alliance of socialists and former Carranza loyalists had Carlos A. Vidal selected as governor, although he was assassinated two years later. The last of the Mapache resistance was overcome in the early 1930s by Governor Victorico Grajales, who pursued President Lázaro Cárdenas' social and economic policies including persecution of the Catholic Church. These policies would have some success in redistributing lands and organizing indigenous workers but the state would remain relatively isolated for the rest of the 20thcentury. The territory was reorganized into municipalities in 1916. The current state constitution was written in 1921.\n\nThere was political stability from the 1940s to the early 1970s; however, regionalism regained with people thinking of themselves as from their local city or municipality over the state. This regionalism impeded the economy as local authorities restrained outside goods. For this reason, construction of highways and communications were pushed to help with economic development. Most of the work was done around Tuxtla Gutiérrez and Tapachula. This included the Sureste railroad connecting northern municipalities such as Pichucalco, Salto de Agua, Palenque, Catazajá and La Libertad. The Cristobal Colon highway linked Tuxtla to the Guatemalan border. Other highways included El Escopetazo to Pichucalco, a highway between San Cristóbal and Palenque with branches to Cuxtepeques and LaFrailesca. This helped to integrate the state's economy, but it also permitted the political rise of communal land owners called ejidatarios.\n\nIn the mid-20th century, the state experienced a significant rise in population, which outstripped local resources, especially land in the highland areas. Since the 1930s, many indigenous and mestizos have migrated from the highland areas into the Lacandon Jungle with the populations of Altamirano, Las Margaritas, Ocosingo and Palenque rising from less than 11,000 in 1920 to over 376,000 in 2000. These migrants came to the jungle area to clear forest and grow crops and raise livestock, especially cattle. Economic development in general raised the output of the state, especially in agriculture, but it had the effect of deforesting many areas, especially the Lacandon. Added to this was there were still serf like conditions for many workers and insufficient educational infrastructure. Population continued to increase faster than the economy could absorb. There were some attempts to resettle peasant farmers onto non cultivated lands, but they were met with resistance. President Gustavo Díaz Ordaz awarded a land grant to the town of Venustiano Carranza in 1967, but that land was already being used by cattle-ranchers who refused to leave. The peasants tried to take over the land anyway, but when violence broke out, they were forcibly removed. In Chiapas poor farmland and severe poverty afflict the Mayan Indians which led to unsuccessful non violent protests and eventually armed struggle started by the Zapatista National Liberation Army in January 1994.\n\nThese events began to lead to political crises in the 1970s, with more frequent land invasions and takeovers of municipal halls. This was the beginning of a process that would lead to the emergence of the Zapatista movement in the 1990s. Another important factor to this movement would be the role of the Catholic Church from the 1960s to the 1980s. In 1960, Samuel Ruiz became the bishop of the Diocese of Chiapas, centered in San Cristóbal. He supported and worked with Marist priests and nuns following an ideology called liberation theology. In 1974, he organized a statewide \"Indian Congress\" with representatives from the Tzeltal, Tzotzil, Tojolabal and Ch'ol peoples from 327 communities as well as Marists and the Maoist People's Union. This congress was the first of its kind with the goal of uniting the indigenous peoples politically. These efforts were also supported by leftist organizations from outside Mexico, especially to form unions of ejido organizations. These unions would later form the base of the EZLN organization. One reason for the Church's efforts to reach out to the indigenous population was that starting in the 1970s, a shift began from traditional Catholic affiliation to Protestant, Evangelical and other Christian sects.\n\nThe 1980s saw a large wave of refugees coming into the state from Central America as a number of these countries, especially Guatemala, were in the midst of violent political turmoil. The Chiapas/Guatemala border had been relatively porous with people traveling back and forth easily in the 19th and 20thcenturies, much like the Mexico/U.S. border around the same time. This is in spite of tensions caused by Mexico's annexation of the Soconusco region in the 19thcentury. The border between Mexico and Guatemala had been traditionally poorly guarded, due to diplomatic considerations, lack of resources and pressure from landowners who need cheap labor sources.\n\nThe arrival of thousands of refugees from Central America stressed Mexico's relationship with Guatemala, at one point coming close to war as well as a politically destabilized Chiapas. Although Mexico is not a signatory to the UN Convention Relating to the Status of Refugees, international pressure forced the government to grant official protection to at least some of the refugees. Camps were established in Chiapas and other southern states, and mostly housed Mayan peoples. However, most Central American refugees from that time never received any official status, estimated by church and charity groups at about half a million from El Salvador alone. The Mexican government resisted direct international intervention in the camps, but eventually relented somewhat because of finances. By 1984, there were 92 camps with 46,000 refugees in Chiapas, concentrated in three areas, mostly near the Guatemalan border. To make matters worse, the Guatemalan army conducted raids into camps on Mexican territories with significant casualties, terrifying the refugees and local populations. From within Mexico, refugees faced threats by local governments who threatened to deport them, legally or not, and local paramilitary groups funded by those worried about the political situation in Central American spilling over into the state. The official government response was to militarize the areas around the camps, which limited international access and migration into Mexico from Central America was restricted. By 1990, it was estimated that there were over 200,000 Guatemalans and half a million from El Salvador, almost all peasant farmers and most under age twenty.\n\nIn the 1980s, the politization of the indigenous and rural populations of the state that began in the 1960s and 1970s continued. In 1980, several ejido (communal land organizations) joined to form the Union of Ejidal Unions and United Peasants of Chiapas, generally called the Union of Unions, or UU. It had a membership of 12,000 families from over 180 communities. By 1988, this organization joined with other to form the ARIC-Union of Unions (ARIC-UU) and took over much of the Lacandon Jungle portion of the state. Most of the members of these organization were from Protestant and Evangelical sects as well as \"Word of God\" Catholics affiliated with the political movements of the Diocese of Chiapas. What they held in common was indigenous identity vis-à-vis the non-indigenous, using the old 19th century \"caste war\" word \"Ladino\" for them.\n\nThe adoption of liberal economic reforms by the Mexican federal government clashed with the leftist political ideals of these groups, notably as the reforms were believed to have begun to have negative economic effects on poor farmers, especially small-scale indigenous coffee-growers. Opposition would coalesce into the Zapatista movement in the 1990s. Although the Zapatista movement couched its demands and cast its role in response to contemporary issues, especially in its opposition to neoliberalism, it operates in the tradition of a long line of peasant and indigenous uprisings that have occurred in the state since the colonial era. This is reflected in its indigenous vs. Ladino character. However, the movement was an economic one as well. Although the area has extensive resources, much of the local population of the state, especially in rural areas, did not benefit from this bounty. In the 1990s, two thirds of the state's residents did not have sewage service, only a third had electricity and half did not have potable water. Over half of the schools offered education only to the third grade and most pupils dropped out by the end of first grade. Grievances, strongest in the San Cristóbal and Lacandon Jungle areas, were taken up by a small leftist guerrilla band led by a man called only \"Subcomandante Marcos.\"\n\nThis small band, called the Zapatista Army of National Liberation (Ejército Zapatista de Liberación Nacional, EZLN), came to the world's attention when on January 1, 1994 (the day the NAFTA treaty went into effect) EZLN forces occupied and took over the towns of San Cristobal de las Casas, Las Margaritas, Altamirano, Ocosingo and three others. They read their proclamation of revolt to the world and then laid siege to a nearby military base, capturing weapons and releasing many prisoners from the jails. This action followed previous protests in the state in opposition to neoliberal economic policies.\n\nAlthough it has been estimated as having no more than 300 armed guerrilla members, the EZLN paralyzed the Mexican government, which balked at the political risks of direct confrontation. The major reason for this was that the rebellion caught the attention of the national and world press, as Marcos made full use of the then-new Internet to get the group's message out, putting the spotlight on indigenous issues in Mexico in general. Furthermore, the opposition press in Mexico City, especially \"La Jornada\", actively supported the rebels. These factors encouraged the rebellion to go national. Many blamed the unrest on infiltration of leftists among the large Central American refugee population in Chiapas, and the rebellion opened up splits in the countryside between those supporting and opposing the EZLN. Zapatista sympathizers have included mostly Protestants and Word of God Catholics, opposing those \"traditionalist\" Catholics who practiced a syncretic form of Catholicism and indigenous beliefs. This split had existed in Chiapas since the 1970s, with the latter group supported by the caciques and others in the traditional power-structure. Protestants and Word of God Catholics (allied directly with the bishopric in San Cristóbal) tended to oppose traditional power structures.\n\nThe Bishop of Chiapas, Samuel Ruiz, and the Diocese of Chiapas reacted by offering to mediate between the rebels and authorities. However, because of this diocese's activism since the 1960s, authorities accused the clergy of being involved with the rebels. There was some ambiguity about the relationship between Ruiz and Marcos and it was a constant feature of news coverage, with many in official circles using such to discredit Ruiz. Eventually, the activities of the Zapatistas began to worry the Roman Catholic Church in general and to upstage the diocese's attempts to re establish itself among Chiapan indigenous communities against Protestant evangelization. This would lead to a breach between the Church and the Zapatistas.\n\nThe Zapatista story remained in headlines for a number of years. One reason for this was the December 1997 massacre of forty-five Tzotzil peasants, mostly women and children in the Zapatista-controlled village of Acteal in the Chenhaló municipality just north of San Cristóbal. This allowed many media outlets in Mexico to step up their criticisms of the government. However, the massacre was not carried out by the government but by civilians, demonstrating how the emergence of the Zapatista movement had divided indigenous groups.\n\nDespite this, the armed conflict was brief, mostly because the Zapatistas, unlike many other guerilla movements, did not try to gain traditional political power. It focused more on trying to manipulate public opinion in order to obtain concessions from the government. This has linked the Zapatistas to other indigenous and identity-politics movements that arose in the late-20th century. The main concession that the group received was the San Andrés Accords (1996), also known as the Law on Indian Rights and Culture. The Accords appear to grant certain indigenous zones autonomy, but this is against the Mexican constitution, so its legitimacy has been questioned. Zapatista declarations since the mid-1990s have called for a new constitution. the government had not found a solution to this problem. The revolt also pressed the government to institute anti-poverty programs such as \"Progresa\" (later called \"Oportunidades\") and the \"Puebla-Panama Plan\" – aiming to increase trade between southern Mexico and Central America.\n\nAs of the first decade of the 2000s the Zapatista movement remained popular in many indigenous communities. The uprising gave indigenous peoples a more active role in the state's politics. However, it did not solve the economic issues that many peasant farmers face, especially the lack of land to cultivate. This problem has been at crisis proportions since the 1970s, and the government's reaction has been to encourage peasant farmers—mostly indigenous—to migrate into the sparsely populated Lacandon Jungle, a trend since earlier in the century.\n\nFrom the 1970s on, some 100,000 people set up homes in this rainforest area, with many being recognized as \"ejidos\", or communal land-holding organizations. These migrants included Tzeltals, Tojolabals, Ch'ols and mestizos, mostly farming corn and beans and raising livestock. However, the government changed policies in the late 1980s with the establishment of the Montes Azules Biosphere Reserve, as much of the Lacandon Jungle had been destroyed or severely damaged. While armed resistance has wound down, the Zapatistas have remained a strong political force, especially around San Cristóbal and the Lacandon Jungle, its traditional bases. Since the Accords, they have shifted focus in gaining autonomy for the communities they control.\n\nSince the 1994 uprising, migration into the Lacandon Jungle has significantly increased, involving illegal settlements and cutting in the protected biosphere reserve. The Zapatistas support these actions as part of indigenous rights, but that has put them in conflict with international environmental groups and with the indigenous inhabitants of the rainforest area, the Lacandons. Environmental groups state that the settlements pose grave risks to what remains of the Lacandon, while the Zapatistas accuse them of being fronts for the government, which wants to open the rainforest up to multinational corporations. Added to this is the possibility that significant oil and gas deposits exist under this area.\n\nThe Zapatista movement has had some successes. The agricultural sector of the economy now favors \"ejidos\" and other commonly-owned land. There have been some other gains economically as well. In the last decades of the 20th century, Chiapas's traditional agricultural economy has diversified somewhat with the construction of more roads and better infrastructure by the federal and state governments. Tourism has become important in some areas of the state, especially in San Cristóbal de las Casas and Palenque.\nIts economy is important to Mexico as a whole as well, producing coffee, corn, cacao, tobacco, sugar, fruit, vegetable and honey for export. It is also a key state for the nation's petrochemical and hydroelectric industries. A significant percentage of PEMEX's drilling and refining takes place in Chiapas and Tabasco, and Chiapas produces fifty-five percent of Mexico's hydroelectric energy.\n\nHowever, Chiapas remains one of the poorest states in Mexico. Ninety-four of its 111 municipalities have a large percentage of the population living in poverty. In areas such as Ocosingo, Altamirano and Las Margaritas, the towns where the Zapatistas first came into prominence in 1994, 48% of the adults are illiterate. Chiapas is still considered isolated and distant from the rest of Mexico, both culturally and geographically. It has significantly underdeveloped infrastructure compared to the rest of the country, and its significant indigenous population with isolationist tendencies keep the state distinct culturally. Cultural stratification, neglect and lack of investment by the Mexican federal government has exacerbated this problem.\n\nChiapas is located in the south east of Mexico, bordering the states of Tabasco, Veracruz and Oaxaca with the Pacific Ocean to the south and Guatemala to the east. It has a territory of 74,415 km, the eighth largest state in Mexico. The state consists of 118 municipalities organized into nine political regions called Center, Altos, Fronteriza, Frailesca, Norte, Selva, Sierra, Soconusco and Istmo-Costa. There are 18 cities, twelve towns (villas) and 111 pueblos (villages). Major cities include Tuxtla Gutiérrez, San Cristóbal de las Casas, Tapachula, Palenque, Comitán, and Chiapa de Corzo.\n\nThe state has a complex geography with seven distinct regions according to the Mullerried classification system. These include the Pacific Coast Plains, the Sierra Madre de Chiapas, the Central Depression, the Central Highlands, the Eastern Mountains, the Northern Mountains and the Gulf Coast Plains. The Pacific Coast Plains is a strip of land parallel to the ocean. It is composed mostly of sediment from the mountains that border it on the northern side. It is uniformly flat, and stretches from the Bernal Mountain south to Tonalá. It has deep salty soils due to its proximity to the sea. It has mostly deciduous rainforest although most has been converted to pasture for cattle and fields for crops. It has numerous estuaries with mangroves and other aquatic vegetation.\n\nThe Sierra Madre de Chiapas runs parallel to the Pacific coastline of the state, northwest to southeast as a continuation of the Sierra Madre del Sur. This area has the highest altitudes in Chiapas including the Tacaná Volcano, which rises above sea level. Most of these mountains are volcanic in origin although the nucleus is metamorphic rock. It has a wide range of climates but little arable land. It is mostly covered in middle altitude rainforest, high altitude rainforest, and forests of oaks and pines. The mountains partially block rain clouds from the Pacific, a process known as Orographic lift, which creates a particularly rich coastal region called the Soconusco. The main commercial center of the sierra is the town of Motozintla, also near the Guatemalan border.\n\nThe Central Depression is in the center of the state. It is an extensive semi flat area bordered by the Sierra Madre de Chiapas, the Central Highlands and the Northern Mountains. Within the depression there are a number of distinct valleys. The climate here can be very hot and humid in the summer, especially due to the large volume of rain received in July and August. The original vegetation was lowland deciduous rainforest with some rainforest of middle altitudes and some oaks above above sea level.\n\nThe Central Highlands, also referred to as Los Altos, are mountains oriented from northwest to southeast with altitudes ranging from twelve to sixteen hundred meters above sea level. The western highlands are displaced faults, while the eastern highlands are mainly folds of sedimentary formationsmainly limestone, shale, and sandstone. These mountains, along the Sierra Madre of Chiapas become the Cuchumatanes where they extend over the border into Guatemala. Its topography is mountainous with many narrow valleys and karst formations called uvalas or poljés, depending on the size. Most of the rock is limestone allowing for a number of formations such as caves and sinkholes. There are also some isolated pockets of volcanic rock with the tallest peaks being the Tzontehuitz and Huitepec volcanos. There are no significant surface water systems as they are almost all underground. The original vegetation was forest of oak and pine but these have been heavily damaged. The highlands climate in the Koeppen modified classification system for Mexico is humid temperate C(m) and subhumid temperate C (w 2 ) (w). This climate exhibits a summer rainy season and a dry winter, with possibilities of frost from December to March. The Central Highlands have been the population center of Chiapas since the Conquest. European epidemics were hindered by the tierra fría climate, allowing the indigenous peoples in the highlands to retain their large numbers.\n\nThe Eastern Mountains (Montañas del Oriente) are in the east of the state, formed by various parallel mountain chains mostly made of limestone and sandstone. Its altitude varies from . This area receives moisture from the Gulf of Mexico with abundant rainfall and exuberant vegetation, which creates the Lacandon Jungle, one of the most important rainforests in Mexico. The Northern Mountains (Montañas del Norte) are in the north of the state. They separate the flatlands of the Gulf Coast Plains from the Central Depression. Its rock is mostly limestone. These mountains also receive large amounts of rainfall with moisture from the Gulf of Mexico giving it a mostly hot and humid climate with rains year round. In the highest elevations around , temperatures are somewhat cooler and do experience a winter. The terrain is rugged with small valleys whose natural vegetation is high altitude rainforest.\n\nThe Gulf Coast Plains (Llanura Costera del Golfo) stretch into Chiapas from the state of Tabasco, which gives it the alternate name of the Tabasqueña Plains. These plains are found only in the extreme north of the state. The terrain is flat and prone to flooding during the rainy season as it was built by sediments deposited by rivers and streams heading to the Gulf.\n\nThe Lacandon Jungle is situated in north eastern Chiapas, centered on a series of canyonlike valleys called the Cañadas, between smaller mountain ridges oriented from northwest to southeast. The ecosystem covers an area of approximately 1.9 million hectares extending from Chiapas into northern Guatemala and southern Yucatán Peninsula and into Belize. This area contains as much as 25% of Mexico's total species diversity, most of which has not been researched. It has a predominantly hot and humid climate (Am w\" i g) with most rain falling from summer to part of fall, with an average of between 2300 and 2600 mm per year. There is a short dry season from March to May. The predominate wild vegetation is perennial high rainforest. The Lacandon comprises a biosphere reserve (Montes Azules); four natural protected areas (Bonampak, Yaxchilan, Chan Kin, and Lacantum); and the communal reserve (La Cojolita), which functions as a biological corridor with the area of Petén in Guatemala. Flowing within the Rainforest is the Usumacinta River, considered to be one of the largest rivers in Mexico and seventh largest in the world based on volume of water.\nDuring the 20th century, the Lacandon has had a dramatic increase in population and along with it, severe deforestation. The population of municipalities in this area, Altamirano, Las Margaritas, Ocosingo and Palenque have risen from 11,000 in 1920 to over 376,000 in 2000. Migrants include Ch'ol, Tzeltal, Tzotzil, Tojolabal indigenous peoples along with mestizos, Guatemalan refugees and others. Most of these migrants are peasant farmers, who cut forest to plant crops. However, the soil of this area cannot support annual crop farming for more than three or four harvents. The increase in population and the need to move on to new lands has pitted migrants against each other, the native Lacandon people, and the various ecological reserves for land. It is estimated that only ten percent of the original Lacandon rainforest in Mexico remains, with the rest strip-mined, logged and farmed. It once stretched over a large part of eastern Chiapas but all that remains is along the northern edge of the Guatemalan border. Of this remaining portion, Mexico is losing over five percent each year.\n\nThe best preserved portion of the Lacandon is within the Montes Azules Biosphere Reserve. It is centered on what was a commercial logging grant by the Porfirio Díaz government, which the government later nationalized. However, this nationalization and conversion into a reserve has made it one of the most contested lands in Chiapas, with the already existing ejidos and other settlements within the park along with new arrivals squatting on the land.\n\nThe Soconusco region encompasses a coastal plain and a mountain range with elevations of up to 2000 meters above sea levels paralleling the Pacific Coast. The highest peak in Chiapas is the Tacaná Volcano at 4,800 meters above sea level. In accordance with an 1882 treaty, the dividing line between Mexico and Guatemala goes right over the summit of this volcano. The climate is tropical, with a number of rivers and evergreen forests in the mountains. This is Chiapas’ major coffee-producing area, as it has the best soils and climate for coffee.\nBefore the arrival of the Spanish, this area was the principal source of cocoa seeds in the Aztec empire, which they used as currency, and for the highly prized quetzal feathers used by the nobility. It would become the first area to produce coffee, introduced by an Italian entrepreneur on the La Chacara farm. Coffee is cultivated on the slopes of these mountains mostly between 600 and 1200 masl. Mexico produces about 4 million sacks of green coffee each year, fifth in the world behind Brazil, Colombia, Indonesia and Vietnam. Most producers are small with plots of land under five hectares. From November to January, the annual crop is harvested and processed employing thousands of seasonal workers. Lately, a number of coffee haciendas have been developing tourism infrastructure as well.\n\nChiapas is located in the tropical belt of the planet, but the climate is moderated in many areas by altitude. For this reason, there are hot, semi-hot, temperate and even cold climates. Some areas have abundant rainfall year-round and others receive most of their rain between May and October, with a dry season from November to April. The mountain areas affect wind and moisture flow over the state, concentrating moisture in certain areas of the state. They also are responsible for some cloud-covered rainforest areas in the Sierra Madre.\n\nChiapas' rainforests are home to thousands of animals and plants, some of which cannot be found anywhere else in the world. Natural vegetation varies from lowland to highland tropical forest, pine and oak forests in the highest altitudes and plains area with some grassland. Chiapas is ranked second in forest resources in Mexico with valued woods such as pine, cypress, \"Liquidambar\", oak, cedar, mahogany and more. The Lacandon Jungle is one of the last major tropical rainforests in the northern hemisphere with an extension of . It contains about sixty percent of Mexico's tropical tree species, 3,500 species of plants, 1,157 species of invertebrates and over 500 of vertebrate species. Chiapas has one of the greatest diversities in wildlife in the Americas. There are more than 100 species of amphibians, 700 species of birds, fifty of mammals and just over 200 species of reptiles. In the hot lowlands, there are armadillos, monkeys, pelicans, wild boar, jaguars, crocodiles, iguanas and many others. In the temperate regions there are species such as bobcats, salamanders, a large red lizard Abronia lythrochila, weasels, opossums, deer, ocelots and bats. The coastal areas have large quantities of fish, turtles, and crustaceans, with many species in danger of extinction or endangered as they are endemic only to this area. The total biodiversity of the state is estimated at over 50,000 species of plants and animals. The diversity of species is not limited to the hot lowlands. The higher altitudes also have mesophile forests, oak/pine forests in the Los Altos, Northern Mountains and Sierra Madre and the extensive estuaries and mangrove wetlands along the coast.\n\nChiapas has about thirty percent of Mexico's fresh water resources. The Sierra Madre divides them into those that flow to the Pacific and those that flow to the Gulf of Mexico. Most of the first are short rivers and streams; most longer ones flow to the Gulf. Most Pacific side rivers do not drain directly into this ocean but into lagoons and estuaries. The two largest rivers are the Grijalva and the Usumacinta, with both part of the same system. The Grijalva has four dams built on it the Belisario Dominguez (La Angostura); Manuel Moreno Torres (Chicoasén); Nezahualcóyotl (Malpaso); and Angel Albino Corzo (Peñitas). The Usumacinta divides the state from Guatemala and is the longest river in Central America. In total, the state has of surface waters, of coastline, control of of ocean, of estuaries and ten lake systems. Laguna Miramar is a lake in the Montes Azules reserve and the largest in the Lacandon Jungle at 40 km in diameter. The color of its waters varies from indigo to emerald green and in ancient times, there were settlements on its islands and its caves on the shoreline. The Catazajá Lake is 28 km north of the city of Palenque. It is formed by rainwater captured as it makes it way to the Usumacinta River. It contains wildlife such as manatees and iguanas and it is surrounded by rainforest. Fishing on this lake is an ancient tradition and the lake has an annual bass fishing tournament. The Welib Já Waterfall is located on the road between Palenque and Bonampak.\n\nThe state has thirty-six protected areas at the state and federal levels along with 67 areas protected by various municipalities. The Sumidero Canyon National Park was decreed in 1980 with an extension of . It extends over two of the regions of the state, the Central Depression and the Central Highlands over the municipalities of Tuxtla Gutiérrez, Nuevo Usumacinta, Chiapa de Corzo and San Fernando. The canyon has steep and vertical sides that rise to up to 1000 meters from the river below with mostly tropical rainforest but some areas with xerophile vegetation such as cactus can be found. The river below, which has cut the canyon over the course of twelve million years, is called the Grijalva. The canyon is emblematic for the state as it is featured in the state seal. The Sumidero Canyon was once the site of a battle between the Spaniards and Chiapanecan Indians. Many Chiapanecans chose to throw themselves from the high edges of the canyon rather than be defeated by Spanish forces. Today, the canyon is a popular destination for ecotourism. Visitors can take boat trips down the river that runs through the canyon and see the area's many birds and abundant vegetation.\n\nThe Montes Azules Integral Biosphere Reserve was decreed in 1978. It is located in the northeast of the state in the Lacandon Jungle. It covers in the municipalities of Maravilla Tenejapa, Ocosingo and Las Margaritas. It conserves highland perennial rainforest. The jungle is in the Usumacinta River basin east of the Chiapas Highlands. It is recognized by the United Nations Environment Programme for its global biological and cultural significance. In 1992, the Lacantun Reserve, which includes the Classic Maya archaeological sites of Yaxchilan and Bonampak, was added to the biosphere reserve.\n\nAgua Azul Waterfall Protection Area is in the Northern Mountains in the municipality of Tumbalá. It covers an area of of rainforest and pine-oak forest, centered on the waterfalls it is named after. It is located in an area locally called the \"Mountains of Water\", as many rivers flow through there on their way to the Gulf of Mexico. The rugged terrain encourages waterfalls with large pools at the bottom, that the falling water has carved into the sedimentary rock and limestone. Agua Azul is one of the best known in the state. The waters of the Agua Azul River emerge from a cave that forms a natural bridge of thirty meters and five small waterfalls in succession, all with pools of water at the bottom. In addition to Agua Azul, the area has other attractions—such as the Shumuljá River, which contains rapids and waterfalls, the Misol Há Waterfall with a thirty-meter drop, the Bolón Ajau Waterfall with a fourteen-meter drop, the Gallito Copetón rapids, the Blacquiazules Waterfalls, and a section of calm water called the Agua Clara.\n\nThe El Ocote Biosphere Reserve was decreed in 1982 located in the Northern Mountains at the boundary with the Sierra Madre del Sur in the municipalities of Ocozocoautla, Cintalapa and Tecpatán. It has a surface area of and preserves a rainforest area with karst formations. The Lagunas de Montebello National Park was decreed in 1959 and consists of near the Guatemalan border in the municipalities of La Independencia and La Trinitaria. It contains two of the most threatened ecosystems in Mexico the \"cloud rainforest\" and the Soconusco rainforest. The El Triunfo Biosphere Reserve, decreed in 1990, is located in the Sierra Madre de Chiapas in the municipalities of Acacoyagua, Ángel Albino Corzo, Montecristo de Guerrero, La Concordia, Mapastepec, Pijijiapan, Siltepec and Villa Corzo near the Pacific Ocean with . It conserves areas of tropical rainforest and many freshwater systems endemic to Central America. It is home to around 400 species of birds including several rare species such as the horned guan, the quetzal and the azure-rumped tanager. The Palenque National Forest is centered on the archaeological site of the same name and was decreed in 1981. It is located in the municipality of Palenque where the Northern Mountains meet the Gulf Coast Plain. It extends over of tropical rainforest. The Laguna Bélgica Conservation Zone is located in the north west of the state in the municipality of Ocozocoautla. It covers forty-two hectares centered on the Bélgica Lake. The El Zapotal Ecological Center was established in 1980. Nahá – Metzabok is an area in the Lacandon Jungle whose name means \"place of the black lord\" in Nahuatl. It extends over and in 2010, it was included in the World Network of Biosphere Reserves. Two main communities in the area are called Nahá and Metzabok. They were established in the 1940s, but the oldest communities in the area belong to the Lacandon people. The area has large numbers of wildlife including endangered species such as eagles, quetzals and jaguars.\n\nAs of 2010, the population is 4,796,580, the eighth most populous state in Mexico. The 20th century saw large population growth in Chiapas. From fewer than one million inhabitants in 1940, the state had about two million in 1980, and over 4 million in 2005. Overcrowded land in the highlands was relieved when the rainforest to the east was subject to land reform. Cattle ranchers, loggers, and subsistence farmers migrated to the rain forest area. The population of the Lacandon was only one thousand people in 1950, but by the mid-1990s this had increased to 200 thousand. As of 2010, 78% lives in urban communities with 22% in rural communities. While birthrates are still high in the state, they have come down in recent decades from 7.4 per woman in 1950. However, these rates still mean significant population growth in raw numbers. About half of the state's population is under age 20, with an average age of 19. In 2005, there were 924,967 households, 81% headed by men and the rest by women. Most households were nuclear families (70.7%) with 22.1% consisting of extended families.\n\nMore migrate out of Chiapas than migrate in, with emigrants leaving for Tabasco, Oaxaca, Veracruz, State of Mexico and the Federal District primarily.\n\nWhile Catholics remain the majority, their numbers have dropped as many have converted to Protestant denominations in recent decades. The National Presbyterian Church in Mexico has a large following in Chiapas; some estimate that 40% of the population are followers of the Presbyterian church.\n\nThere are a number of people in the state with African features. These are the descendants of slaves brought to the state in the 16th century. There are also those with predominantly European features who are the descendants of the original Spanish colonizers as well as later immigrants to Mexico. The latter mostly came at the end of the 19th and early 20th century under the Porfirio Díaz regime to start plantations.\n\nOver the history of Chiapas, there have been 3 main indigenous groups: the Mixes-Zoques, the Mayas and the Chiapa. Today, there are an estimated fifty-six linguistic groups. As of the 2005 Census, there were 957,255 people who spoke an indigenous language out of a total population of about 3.5 million. Of this one million, one third do not speak Spanish. Out of Chiapas' 111 municipios, ninety-nine have significant indigenous populations. 22 municipalities have indigenous populations over 90%, and 36 municipalities have native populations exceeding 50%. However, despite population growth in indigenous villages, the percentage of indigenous to non indigenous continues to fall with less than 35% indigenous. Indian populations are concentrated in a few areas, with the largest concentration of indigenous-language-speaking individuals is living in 5 of Chiapas's 9 economic regions: Los Altos, Selva, Norte, Fronteriza, and Sierra. The remaining four regions, Centro, Frailesca, Soconusco, and Costa, have populations that are considered to be dominantly mestizo.\n\nThe state has about 13.5% of all of Mexico's indigenous population, and it has been ranked among the ten \"most indianized\" states, with only Campeche, Oaxaca, Quintana Roo and Yucatán having been ranked above it between 1930 and the present. These indigenous peoples have been historically resistant to assimilation into the broader Mexican society, with it best seen in the retention rates of indigenous languages and the historic demands for autonomy over geographic areas as well as cultural domains. Much of the latter has been prominent since the Zapatista uprising in 1994.\nMost of Chiapas' indigenous groups are descended from the Mayans, speaking languages that are closely related to one another, belonging to the Western Maya language group. The state was part of a large region dominated by the Mayans during the Classic period. The most numerous of these Mayan groups include the Tzeltal, Tzotzil, Ch'ol, Zoque, Tojolabal, Lacandon and Mam, which have traits in common such as syncretic religious practices, and social structure based on kinship. The most common Western Maya languages are Tzeltal and Tzotzil along with Chontal, Ch’ol, Tojolabal, Chuj, Kanjobal, Acatec, Jacaltec and Motozintlec.\n\n12 of Mexico's officially recognized native peoples live in the state have conserved their language, customs, history, dress and traditions to a significant degree. The primary groups include the Tzeltal, Tzotzil, Ch'ol, Tojolabal, Zoque, Chuj, Kanjobal, Mam, Jacalteco, Mochó Cakchiquel and Lacandon. Most indigenous communities are found in the municipalities of the Centro, Altos, Norte and Selva regions, with many having indigenous populations of over fifty percent. These include Bochil, Sitalá, Pantepec, Simojovel to those with over ninety percent indigenous such as San Juan Cancuc, Huixtán, Tenejapa, Tila, Oxchuc, Tapalapa, Zinacantán, Mitontic, Ocotepec, Chamula, and Chalchihuitán. The most numerous indigenous communities are the Tzeltal and Tzotzil peoples, who number about 400,000 each, together accounting for about half of the state's indigenous population. The next most numerous are the Ch’ol with about 200,000 people and the Tojolabal and Zoques, who number about 50,000 each. The top 3 municipalities in Chiapas with indigenous language speakers 3 years of age and older are: Ocosingo (133,811), Chilon (96,567), and San Juan Chamula (69,475). These 3 municipalities accounted for 24.8% (299,853) of all indigenous language speakers 3 years or older in the state of Chiapas, out of a total of 1,209,057 indigenous language speakers 3 years or older.\n\nAlthough most indigenous language speakers are bilingual, especially in the younger generations, many of these languages have shown resilience. 4 of Chiapas' indigenous languages Tzeltal, Tzotzil, Tojolabal and Chol are high-vitality languages, meaning that a high percentage of these ethnicities speak the language and that there is a high rate of monolingualism in it. It is used in over 80% of homes. Zoque is considered to be of medium-vitality with a rate of bilingualism of over 70% and home use somewhere between 65% and 80%. Maya is considered to be of low-vitality with almost all of its speakers bilingual with Spanish. The most spoken indigenous languages as of 2010 are Tzeltal with 461,236 speakers, Tzotzil with 417,462, Ch’ol with 191,947 and Zoque with 53,839. In total, there are 1,141,499 who speak an indigenous language or 27% of the total population. Of these 14% do not speak Spanish. Studies done between 1930 and 2000 have indicated that Spanish is not dramatically displacing these languages. In raw number, speakers of these languages are increasing, especially among groups with a long history of resistance to Spanish/Mexican domination. Language maintenance has been strongest in areas related to where the Zapatista uprising took place such as the municipalities of Altamirano, Chamula, Chanal, Larráinzar, Las Margaritas, Ocosingo, Palenque, Sabanilla, San Cristóbal de Las Casas and Simojovel.\n\nThe state's rich indigenous tradition along with its associated political uprisings, especially that of 1994, has great interest from other parts of Mexico and abroad. It has been especially appealing to a variety of academics including many anthropologists, archeologists, historians, psychologists and sociologists. The concept of \"mestizo\" or mixed indigenous European heritage became important to Mexico's identity by the time of Independence, but Chiapas has kept its indigenous identity to the present day. Since the 1970s, this has been supported by the Mexican government as it has shifted from cultural policies that favor a \"multicultural\" identity for the country. One major exception to the separatist, indigenous identity has been the case of the Chiapa people, from whom the state's name comes, who have mostly been assimilated and intermarried into the mestizo population.\n\nMost Indigenous communities have economies based primarily on traditional agriculture such as the cultivation and processing of corn, beans and coffee as a cash crop and in the last decade, many have begun producing sugarcane and jatropha for refinement into biodiesel and ethanol for automobile fuel. The raising of livestock, particularly chicken and turkey and to a lesser extent beef and farmed fish is also a major economic activity. Many indigenous, in particular the Maya are employed in the production of traditional clothing, fabrics, textiles, wood items, artworks and traditional goods such as jade and amber works. Tourism has provided a number of a these communities with markets for their handcrafts and works, some of which are very profitable.\n\nSan Cristóbal de las Casas and San Juan Chamula maintain a strong indigenous identity. On market day, many indigenous people from rural areas come into San Cristóbal to buy and sell mostly items for everyday use such as fruit, vegetables, animals, cloth, consumer goods and tools. San Juan Chamula is considered to be a center of indigenous culture, especially its elaborate festivals of Carnival and Day of Saint John. It was common for politicians, especially during Institutional Revolutionary Party's dominance to visit here during election campaigns and dress in indigenous clothing and carry a carved walking stick, a traditional sign of power. Relations between the indigenous ethnic groups is complicated. While there have been inter ethnic political activism such as that promoted by the Diocese of Chiapas in the 1970s and the Zapatista movement in the 1990s, there has been inter-indigenous conflict as well. Much of this has been based on religion, pitting those of the traditional Catholic/indigenous beliefs who support the traditional power structure against Protestants, Evangelicals and Word of God Catholics (directly allied with the Diocese) who tend to oppose it. This is particularly significant problem among the Tzeltals and Tzotzils. Starting in the 1970s, traditional leaders in San Juan Chamula began expelling dissidents from their homes and land, amounting to about 20,000 indigenous forced to leave over a thirty-year period. It continues to be a serious social problem although authorities downplay it. Recently there has been political, social and ethnic conflict between the Tzotzil who are more urbanized and have a significant number of Protestant practitioners and the Tzeltal who are predominantly Catholic and live in smaller farming communities. Many Protestant Tzotzil have accused the Tzeltal of ethnic discrimination and intimidation due to their religious beliefs and the Tzeltal have in return accused the Tzotzil of singling them out for discrimination.\n\nClothing, especially women's clothing, varies by indigenous group. For example, women in Ocosingo tend to wear a blouse with a round collar embroidered with flowers and a black skirt decorated with ribbons and tied with a cloth belt. The Lacandon people tend to wear a simple white tunic. They also make a ceremonial tunic from bark, decorated with astronomy symbols. In Tenejapa, women wear a huipil embroidered with Mayan fretwork along with a black wool rebozo. Men wear short pants, embroidered at the bottom.\n\nThe Tzeltals call themselves Winik atel, which means \"working men.\" This is the largest ethnicity in the state, mostly living southeast of San Cristóbal with the largest number in Amatenango. Today, there are about 500,000 Tzeltal Indians in Chiapas. Tzeltal Mayan, part of the Mayan language family, today is spoken by about 375,000 people making it the fourth-largest language group in Mexico. There are two main dialects; highland (or Oxchuc) and lowland (or Bachajonteco). This language, along with Tzotzil, is from the Tzeltalan subdivision of the Mayan language family. Lexico-statistical studies indicate that these two languages probably became differentiated from one another around 1200 Most children are bilingual in the language and Spanish although many of their grandparents are monolingual Tzeltal speakers.\nEach Tzeltal community constitutes a distinct social and cultural unit with its own well-defined lands, wearing apparel, kinship system, politico-religious organization, economic resources, crafts, and other cultural features. Women are distinguished by a black skirt with a wool belt and an undyed cotton bloused embroidered with flowers. Their hair is tied with ribbons and covered with a cloth. Most men do not use traditional attire. Agriculture is the basic economic activity of the Tzeltal people. Traditional Mesoamerican crops such as maize, beans, squash, and chili peppers are the most important, but a variety of other crops, including wheat, manioc, sweet potatoes, cotton, chayote, some fruits, other vegetables, and coffee.\n\nTzotzil speakers number just slightly less than theTzeltals at 226,000, although those of the ethnicity are probably higher. Tzotzils are found in the highlands or Los Altos and spread out towards the northeast near the border with Tabasco. However, Tzotzil communities can be found in almost every municipality of the state. They are concentrated in Chamula, Zinacantán, Chenalhó, and Simojovel. Their language is closely related to Tzeltal and distantly related to Yucatec Mayan and Lacandon. Men dress in short pants tied with a red cotton belt and a shirt that hangs down to their knees. They also wear leather huaraches and a hat decorated with ribbons. The women wear a red or blue skirt, a short huipil as a blouse, and use a chal or rebozo to carry babies and bundles. Tzotzil communities are governed by a katinab who is selected for life by the leaders of each neighborhood. The Tzotzils are also known for their continued use of the temazcal for hygiene and medicinal purposes.\n\nThe Ch’ols of Chiapas migrated to the northwest of the state starting about 2,000 years ago, when they were concentrated in Guatemala and Honduras. Those Ch’ols who remained in the south are distinguished by the name Chortís. Chiapas Ch’ols are closely related to the Chontal in Tabasco as well. Choles are found in Tila, Tumbalá, Sabanilla, Palenque, and Salto de Agua, with an estimated population of about 115,000 people. The Ch’ol language belongs to the Maya family and is related to Tzeltal, Tzotzil, Lacandon, Tojolabal, and Yucatec Mayan. There are three varieties of Chol (spoken in Tila, Tumbalá, and Sabanilla), all mutually intelligible. Over half of speakers are monolingual in the Chol language. Women wear a long navy blue or black skirt with a white blouse heavily embroidered with bright colors and a sash with a red ribbon. The men only occasionally use traditional dress for events such as the feast of the Virgin of Guadalupe. This dress usually includes pants, shirts and huipils made of undyed cotton, with leather huaraches, a carrying sack and a hat. The fundamental economic activity of the Ch’ols is agriculture. They primarily cultivate corn and beans, as well as sugar cane, rice, coffee, and some fruits. They have Catholic beliefs strongly influenced by native ones. Harvests are celebrated on the Feast of Saint Rose on 30 August.\n\nThe Totolabals are estimated at 35,000 in the highlands. According to oral tradition, the Tojolabales came north from Guatemala. The largest community is Ingeniero González de León in the La Cañada region, an hour outside the municipal seat of Las Margaritas. Tojolabales are also found in Comitán, Trinitaria, Altamirano and La Independencia. This area is filled with rolling hills with a temperate and moist climate. There are fast moving rivers and jungle vegetation. Tojolabal is related to Kanjobal, but also to Tzeltal and Tzotzil. However, most of the youngest of this ethnicity speak Spanish. Women dress traditionally from childhood with brightly colored skirts decorated with lace or ribbons and a blouse decorated with small ribbons, and they cover their heads with kerchiefs. They embroider many of their own clothes but do not sell them. Married women arrange their hair in two braids and single women wear it loose decorated with ribbons. Men no longer wear traditional garb daily as it is considered too expensive to make.\n\nThe Zoques are found in 3,000 square kilometers the center and west of the state scattered among hundreds of communities. These were one of the first native peoples of Chiapas, with archeological ruins tied to them dating back as far as 3500 BCE. Their language is not Mayan but rather related to Mixe, which is found in Oaxaca and Veracruz. By the time the Spanish arrived, they had been reduced in number and territory. Their ancient capital was Quechula, which was covered with water by the creation of the Malpaso Dam, along with the ruins of Guelegas, which was first buried by an eruption of the Chichonal volcano. There are still Zoque ruins at Janepaguay, the Ocozocuautla and La Ciénega valleys.\n\nThe Lacandons are one of the smallest native indigenous groups of the state with a population estimated between 600 and 1,000. They are mostly located in the communities of Lacanjá Chansayab, Najá, and Mensabak in the Lacandon Jungle. They live near the ruins of Bonampak and Yaxchilan and local lore states that the gods resided here when they lived on Earth. They inhabit about a million hectares of rainforest but from the 16th century to the present, migrants have taken over the area, most of which are indigenous from other areas of Chiapas. This dramatically altered their lifestyle and worldview. Traditional Lacandon shelters are huts made with fonds and wood with an earthen floor, but this has mostly given way to modern structures.\n\nThe Mochós or Motozintlecos are concentrated in the municipality of Motozintla on the Guatemalan border. According to anthropologists, these people are an \"urban\" ethnicity as they are mostly found in the neighborhoods of the municipal seat. Other communities can be found near the Tacaná volcano, and in the municipalities of Tuzantán and Belisario Dominguez. The name \"Mochó\" comes from a response many gave the Spanish whom they could not understand and means \"I don't know.\" This community is in the process of disappearing as their numbers shrink.\n\nThe Mams are a Mayan ethnicity that numbers about 20,000 found in thirty municipalities, especially Tapachula, Motozintla, El Porvenir, Cacahoatán and Amatenango in the southeastern Sierra Madre of Chiapas. The Mame language is one of the most ancient Mayan languages with 5,450 Mame speakers were tallied in Chiapas in the 2000 census. These people first migrated to the border region between Chiapas and Guatemala at the end of the nineteenth century, establishing scattered settlements. In the 1960s, several hundred migrated to the Lacandon rain forest near the confluence of the Santo Domingo and Jataté Rivers. Those who live in Chiapas are referred to locally as the \"Mexican Mam (or Mame)\" to differientiate them from those in Guatemala. Most live around the Tacaná volcano, which the Mams call \"our mother\" as it is considered to be the source of the fertility of the area's fields. The masculine deity is the Tajumulco volcano, which is in Guatemala.\n\nIn the last decades of the 20th century, Chiapas received a large number of indigenous refugees, especially from Guatemala, many of whom remain in the state. These have added ethnicities such as the Kekchi, Chuj, Ixil, Kanjobal, K'iche' and Cakchikel to the population. The Kanjobal mainly live along the border between Chiapas and Guatemala, with almost 5,800 speakers of the language tallied in the 2000 census. It is believed that a significant number of these Kanjobal-speakers may have been born in Guatemala and immigrated to Chiapas, maintaining strong cultural ties to the neighboring nation.\n\nChiapas accounts for 1.73% of Mexico's GDP. The primary sector, agriculture, produces 15.2% of the state's GDP. The secondary sector, mostly energy production, but also commerce, services and tourism, accounts for 21.8%. The share of the GDP coming from services is rising while that of agriculture is falling. The state is divided into nine economic regions. These regions were established in the 1980s in order to facilitate statewide economic planning. Many of these regions are based on state and federal highway systems. These include Centro, Altos, Fronteriza, Frailesca, Norte, Selva, Sierra, Soconusco and Istmo-Costa.\n\nDespite being rich in resources, Chiapas, along with Oaxaca and Guerrero, lags behind the rest of the country in almost all socioeconomic indicators. , there were 889,420 residential units; 71% had running water, 77.3% sewerage, and 93.6% electricity. Construction of these units varies from modern construction of block and concrete to those constructed of wood and laminate.\n\nBecause of its high rate of economic marginalization, more people migrate from Chiapas than migrate to it. Most of its socioeconomic indicators are the lowest in the country including income, education, health and housing. It has a significantly higher percentage of illiteracy than the rest of the country, although that situation has improved since the 1970s when over 45% were illiterate and 1980s, about 32%. The tropical climate presents health challenges, with most illnesses related to the gastro-intestinal tract and parasites. As of 2005, the state has 1,138 medical facilities: 1098 outpatient and 40 inpatient. Most are run by IMSS and ISSSTE and other government agencies. The implementation of NAFTA had negative effects on the economy, particularly by lowering prices for agricultural products. It made the southern states of Mexico poorer in comparison to those in the north, with over 90% of the poorest municipalities in the south of the country. As of 2006, 31.8% work in communal services, social services and personal services. 18.4% work in financial services, insurance and real estate, 10.7% work in commerce, restaurants and hotels, 9.8% work in construction, 8.9% in utilities, 7.8% in transportation, 3.4% in industry (excluding handcrafts), and 8.4% in agriculture.\n\nAlthough until the 1960s, many indigenous communities were considered by scholars to be autonomous and economically isolated, this was never the case. Economic conditions began forcing many to migrate to work, especially in agriculture for non-indigenous. However, unlike many other migrant workers, most indigenous in Chiapas have remained strongly tied to their home communities. A study as early as the 1970s showed that 77 percent of heads of household migrated outside of the Chamula municipality as local land did not produce sufficiently to support families. In the 1970s, cuts in the price of corn forced many large landowners to convert their fields into pasture for cattle, displacing many hired laborers, cattle required less work. These agricultural laborers began to work for the government on infrastructure projects financed by oil revenue. It is estimated that in the 1980s to 1990s as many as 100,000 indigenous people moved from the mountain areas into cities in Chiapas, with some moving out of the state to Mexico City, Cancún and Villahermosa in search of employment.\n\nAgriculture, livestock, forestry and fishing employ over 53% of the state's population; however, its productivity is considered to be low. Agriculture includes both seasonal and perennial plants. Major crops include corn, beans, sorghum, soybeans, peanuts, sesame seeds, coffee, cacao, sugar cane, mangos, bananas, and palm oil. These crops take up 95% of the cultivated land in the state and 90% of the agricultural production. Only four percent of fields are irrigated with the rest dependent on rainfall either seasonally or year round. Chiapas ranks second among the Mexican states in the production of cacao, the product used to make chocolate, and is responsible for about 60 percent of Mexico's total coffee output. The production of bananas, cacao and corn make Chiapas Mexico's second largest agricultural producer overall.\n\nCoffee is the state's most important cash crop with a history from the 19th century. The crop was introduced in 1846 by Jeronimo Manchinelli who brought 1,500 seedlings from Guatemala on his farm La Chacara. This was followed by a number of other farms as well. Coffee production intensified during the regime of Porfirio Díaz and the Europeans who came to own many of the large farms in the area. By 1892, there were 22 coffee farms in the region, among them Nueva Alemania, Hamburgo, Chiripa, Irlanda, Argovia, San Francisco, and Linda Vista in the Soconusco region. Since then coffee production has grown and diversified to include large plantations, the use and free and forced labor and a significant sector of small producers. While most coffee is grown in the Soconusco, other areas grow it, including the municipalities of Oxchuc, Pantheló, El Bosque, Tenejapa, Chenalhó, Larráinzar, and Chalchihuitán, with around six thousand producers. It also includes organic coffee producers with 18 million tons grown annually 60,000 producers. One third of these producers are indigenous women and other peasant farmers who grow the coffee under the shade of native trees without the use of agro chemicals. Some of this coffee is even grown in environmentally protected areas such as the El Triunfo reserve, where ejidos with 14,000 people grow the coffee and sell it to cooperativers who sell it to companies such as Starbucks, but the main market is Europe. Some growers have created cooperatives of their own to cut out the middleman.\n\nRanching occupies about three million hectares of natural and induced pasture, with about 52% of all pasture induced. Most livestock is done by families using traditional methods. Most important are meat and dairy cattle, followed by pigs and domestic fowl. These three account for 93% of the value of production. Annual milk production in Chiapas totals about 180 million liters per year. The state's cattle production, along with timber from the Lacandon Jungle and energy output gives it a certain amount of economic clouts compared to other states in the region.\n\nForestry is mostly based on conifers and common tropical species producing 186,858 m per year at a value of 54,511,000 pesos. Exploited non-wood species include the Camedor palm tree for its fronds. The fishing industry is underdeveloped but includes the capture of wild species as well as fish farming. Fish production is generated both from the ocean as well as the many freshwater rivers and lakes. In 2002, 28,582 tons of fish valued at 441.2 million pesos was produced. Species include tuna, shark, shrimp, mojarra and crab.\n\nThe state's abundant rivers and streams have been dammed to provide about fifty-five percent of the country's hydroelectric energy. Much of this is sent to other states accounting for over six percent of all of Mexico's energy output. Main power stations are located at Malpaso, La Angostura, Chicoasén and Peñitas, which produce about eight percent of Mexico's hydroelectric energy. Manuel Moreno Torres plant on the Grijalva River the most productive in Mexico. All of the hydroelectric plants are owned and operated by the Federal Electricity Commission (Comisión Federal de Electricidad, CFE).\n\nChiapas is rich in petroleum reserves. Oil production began during the 1980s and Chiapas has become the fourth largest producer of crude oil and natural gas among the Mexican states. Many reserves are yet untapped, but between 1984 and 1992, PEMEX drilled nineteen oil wells in the Lacandona Jungle. Currently, petroleum reserves are found in the municipalities of Juárez, Ostuacán, Pichucalco and Reforma in the north of the state with 116 wells accounting for about 6.5% of the country's oil production. It also provides about a quarter of the country's natural gas. This production equals of natural gas and 17,565,000 barrels of oil per year.\n\nIndustry is limited to small and micro enterprises and include auto parts, bottling, fruit packing, coffee and chocolate processing, production of lime, bricks and other construction materials, sugar mills, furniture making, textiles, printing and the production of handcrafts. The two largest enterprises is the Comisión Federal de Electricidad and a Petróleos Mexicanos refinery. Chiapas opened its first assembly plant in 2002, a fact that highlights the historical lack of industry in this area.\n\nChiapas is one of the states that produces a wide variety of handcrafts and folk art in Mexico. One reason for this is its many indigenous ethnicities who produce traditional items out of identity as well as commercial reasons. One commercial reason is the market for crafts provided by the tourism industry. Another is that most indigenous communities can no longer provide for their own needs through agriculture. The need to generate outside income has led to many indigenous women producing crafts communally, which has not only had economic benefits but also involved them in the political process as well. Unlike many other states, Chiapas has a wide variety of wood resources such as cedar and mahogany as well as plant species such as reeds, ixtle and palm. It also has minerals such as obsidian, amber, jade and several types of clay and animals for the production of leather, dyes from various insects used to create the colors associated with the region. Items include various types of handcrafted clothing, dishes, jars, furniture, roof tiles, toys, musical instruments, tools and more.\n\nChiapas’ most important handcraft is textiles, most of which is cloth woven on a backstrap loom. Indigenous girls often learn how to sew and embroider before they learn how to speak Spanish. They are also taught how to make natural dyes from insects, and weaving techniques. Many of the items produced are still for day-to-day use, often dyed in bright colors with intricate embroidery. They include skirts, belts, rebozos, blouses, huipils and shoulder wraps called chals. Designs are in red, yellow, turquoise blue, purple, pink, green and various pastels and decorated with designs such as flowers, butterflies, and birds, all based on local flora and fauna. Commercially, indigenous textiles are most often found in San Cristóbal de las Casas, San Juan Chamula and Zinacantán. The best textiles are considered to be from Magdalenas, Larráinzar, Venustiano Carranza and Sibaca.\n\nOne of the main minerals of the state is amber, much of which is 25 million years old, with quality comparable to that found in the Dominican Republic. Chiapan amber has a number of unique qualities, including much that is clear all the way through and some with fossilized insects and plants. Most Chiapan amber is worked into jewelry including pendants, rings and necklaces. Colors vary from white to yellow/orange to a deep red, but there are also green and pink tones as well. Since pre-Hispanic times, native peoples have believed amber to have healing and protective qualities. The largest amber mine is in Simojovel, a small village 130 km from Tuxtla Gutiérrez, which produces 95% of Chiapas' amber. Other mines are found in Huitiupán, Totolapa, El Bosque, Pueblo Nuevo Solistahuacán, Pantelhó and San Andrés Duraznal. According to the Museum of Amber in San Cristóbal, almost 300 kg of amber is extracted per month from the state. Prices vary depending on quality and color.\n\nThe major center for ceramics in the state is the city of Amatenango del Valle, with its barro blanco (white clay) pottery. The most traditional ceramic in Amatenango and Aguacatenango is a type of large jar called a cantaro used to transport water and other liquids. Many pieces created from this clay are ornamental as well as traditional pieces for everyday use such as comals, dishes, storage containers and flowerpots. All pieces here are made by hand using techniques that go back centuries. Other communities that produce ceramics include Chiapa de Corzo, Tonalá, Ocuilpa, Suchiapa and San Cristóbal de las Casas.\n\nWood crafts in the state center on furniture, brightly painted sculptures and toys. The Tzotzils of San Juan de Chamula are known for their sculptures as well as for their sturdy furniture. Sculptures are made from woods such as cedar, mahogany and strawberry tree. Another town noted for their sculptures is Tecpatán. The making lacquer to use in the decoration of wooden and other items goes back to the colonial period. The best-known area for this type of work, called \"laca\" is Chiapa de Corzo, which has a museum dedicated to it. One reason this type of decoration became popular in the state was that it protected items from the constant humidity of the climate. Much of the laca in Chiapa de Corzo is made in the traditional way with natural pigments and sands to cover gourds, dipping spoons, chests, niches and furniture. It is also used to create the Parachicos masks.\n\nTraditional Mexican toys, which have all but disappeared in the rest of Mexico, are still readily found here and include the cajita de la serpiente, yo yos, ball in cup and more. Other wooden items include masks, cooking utensils, and tools. One famous toy is the \"muñecos zapatistas\" (Zapatista dolls), which are based on the revolutionary group that emerged in the 1990s.\n\nNinety-four percent of the state's commercial outlets are small retail stores with about 6% wholesalers. There are 111 municipal markets, 55 tianguis, three wholesale food markets and 173 large vendors of staple products. The service sector is the most important to the economy, with mostly commerce, warehousing and tourism.\n\nTourism brings large numbers of visitors to the state each year. Most of Chiapas' tourism is based on its culture, colonial cities and ecology. The state has a total of 491 ranked hotels with 12,122 rooms. There are also 780 other establishments catering primarily to tourism, such as services and restaurants.\n\nThere are three main tourist routes: the Maya Route, the Colonial Route and the Coffee Route. The Maya Route runs along the border with Guatemala in the Lacandon Jungle and includes the sites of Palenque, Bonampak, Yaxchilan along with the natural attractions of Agua Azul Waterfalls, Misol-Há Waterfall, and the Catazajá Lake. Palenque is the most important of these sites, and one of the most important tourist destinations in the state. Yaxchilan was a Mayan city along the Usumacinta River. It developed between 350 and 810 CE. Bonampak is known for its well preserved murals. These Mayan sites have made the state an attraction for international tourism. These sites contain a large number of structures, most of which date back thousands of years, especially to the sixth century. In addition to the sites on the Mayan Route, there are others within the state away from the border such as Toniná, near the city of Ocosingo.\n\nThe Colonial Route is mostly in the central highlands with a significant number of churches, monasteries and other structures from the colonial period along with some from the 19th century and even into the early 20th. The most important city on this route is San Cristóbal de las Casas, located in the Los Altos region in the Jovel Valley. The historic center of the city is filled with tiled roofs, patios with flowers, balconies, Baroque facades along with Neoclassical and Moorish designs. It is centered on a main plaza surrounded by the cathedral, the municipal palace, the Portales commercial area and the San Nicolás church. In addition, it has museums dedicated to the state's indigenous cultures, one to amber and one to jade, both of which have been mined in the state. Other attractions along this route include Comitán de Domínguez and Chiapa de Corzo, along with small indigenous communities such as San Juan Chamula. The state capital of Tuxtla Gutiérrez does not have many colonial era structures left, but it lies near the area's most famous natural attraction of the Sumidero Canyon. This canyon is popular with tourists who take boat tours into it on the Grijalva River to see such features such as caves (La Cueva del Hombre, La Cueva del Silencio) and the Christmas Tree, which is a rock and plant formation on the side of one of the canyon walls created by a seasonal waterfall.\n\nThe Coffee Route begins in Tapachula and follows a mountainous road into the Suconusco regopm. The route passes through Puerto Chiapas, a port with modern infrastructure for shipping exports and receiving international cruises. The route visits a number of coffee plantations, such as Hamburgo, Chiripa, Violetas, Santa Rita, Lindavista, Perú-París, San Antonio Chicarras and Rancho Alegre. These haciendas provide visitors with the opportunity to see how coffee is grown and initially processed on these farms. They also offer a number of ecotourism activities such as mountain climbing, rafting, rappelling and mountain biking. There are also tours into the jungle vegetation and the Tacaná Volcano. In addition to coffee, the region also produces most of Chiapas’ soybeans, bananas and cacao.\n\nThe state has a large number of ecological attractions most of which are connected to water. The main beaches on the coastline include Puerto Arista, Boca del Cielo, Playa Linda, Playa Aventuras, Playa Azul and Santa Brigida. Others are based on the state's lakes and rivers. Laguna Verde is a lake in the Coapilla municipality. The lake is generally green but its tones constantly change through the day depending on how the sun strikes it. In the early morning and evening hours there can also be blue and ochre tones as well. The El Chiflón Waterfall is part of an ecotourism center located in a valley with reeds, sugarcane, mountains and rainforest. It is formed by the San Vicente River and has pools of water at the bottom popular for swimming. The Las Nubes Ecotourism center is located in the Las Margaritas municipality near the Guatemalan border. The area features a number of turquoise blue waterfalls with bridges and lookout points set up to see them up close.\n\nStill others are based on conservation, local culture and other features. The Las Guacamayas Ecotourism Center is located in the Lacandon Jungle on the edge of the Montes Azules reserve. It is centered on the conservation of the red macaw, which is in danger of extinction. The Tziscao Ecotourism Center is centered on a lake with various tones. It is located inside the Lagunas de Montebello National Park, with kayaking, mountain biking and archery. Lacanjá Chansayab is located in the interior of the Lacandon Jungle and a major Lacandon people community. It has some activities associated with ecotourism such as mountain biking, hiking and cabins. The Grutas de Rancho Nuevo Ecotourism Center is centered on a set of caves in which appear capricious forms of stalagmite and stalactites. There is horseback riding as well.\n\nArchitecture in the state begins with the archeological sites of the Mayans and other groups who established color schemes and other details that echo in later structures. After the Spanish subdued the area, the building of Spanish style cities began, especially in the highland areas.\n\nMany of the colonial-era buildings are related to Dominicans who came from Seville. This Spanish city had much Arabic influence in its architecture, and this was incorporated into the colonial architecture of Chiapas, especially in structures dating from the 16th to 18th centuries. However, there are a number of architectural styles and influences present in Chiapas colonial structures, including colors and patterns from Oaxaca and Central America along with indigenous ones from Chiapas.\n\nThe main colonial structures are the cathedral and Santo Domingo church of San Cristóbal, the Santo Domingo monastery and La Pila in Chiapa de Corzo. The San Cristóbal cathedral has a Baroque facade that was begun in the 16th century but by the time it was finished in the 17th, it had a mix of Spanish, Arabic, and indigenous influences. It is one of the most elaborately decorated in Mexico.\n\nThe churches and former monasteries of Santo Domingo, La Merced and San Francisco have ornamentation similar to that of the cathedral. The main structures in Chiapa de Corzo are the Santo Domingo monastery and the La Pila fountain. Santo Domingo has indigenous decorative details such as double headed eagles as well as a statue of the founding monk. In San Cristóbal, the Diego de Mazariegos house has a Plateresque facade, while that of Francisco de Montejo, built later in the 18th century has a mix of Baroque and Neoclassical. Art Deco structures can be found in San Cristóbal and Tapachula in public buildings as well as a number of rural coffee plantations from the Porfirio Díaz era.\n\nArt in Chiapas is based on the use of color and has strong indigenous influence. This dates back to cave paintings such as those found in Sima de las Cotorras near Tuxtla Gutiérrez and the caverns of Rancho Nuevo where human remains and offerings were also found. The best-known pre-Hispanic artwork is the Maya murals of Bonampak, which are the only Mesoamerican murals to have been preserved for over 1500 years. In general, Mayan artwork stands out for its precise depiction of faces and its narrative form. Indigenous forms derive from this background and continue into the colonial period with the use of indigenous color schemes in churches and modern structures such as the municipal palace in Tapachula. Since the colonial period, the state has produced a large number of painters and sculptors. Noted 20th-century artists include Lázaro Gómez, Ramiro Jiménez Chacón, Héctor Ventura Cruz, Máximo Prado Pozo, and Gabriel Gallegos Ramos.\n\nThe two best-known poets from the state are Jaime Sabines and Rosario Castellanos, both from prominent Chiapan families. The first was a merchant and diplomat and the second was a teacher, diplomat, theatre director and the director of the Instituto Nacional Indigenista. Jaime Sabines is widely regarded as Mexico's most influential contemporary poet. His work celebrates everyday people in common settings.\n\nThe most important instrument in the state is the marimba. In the pre-Hispanic period, indigenous peoples had already been producing music with wooden instruments. The marimba was introduced by African slaves brought to Chiapas by the Spanish. However, it achieved its widespread popularity in the early 20th century due to the formation of the Cuarteto Marimbistico de los Hermanos Gómez in 1918, who popularized the instrument and the popular music that it plays not only in Chiapas but in various parts of Mexico and into the United States. Along with Cuban Juan Arozamena, they composed the piece \"Las chiapanecas\" considered to be the unofficial anthem of the state. In the 1940s, they were also featured in a number of Mexican films. Marimbas are constructed in Venustiano Carranza, Chiapas de Corzo and Tuxtla Gutiérrez.\n\nLike the rest of Mesoamerica, the basic diet has been based on corn and Chiapas cooking retains strong indigenous influence. One important ingredient is chipilin, a fragrant and strongly flavored herb and hoja santa, the large anise-scented leaves used in much of southern Mexican cuisine. Chiapan dishes do not incorporate many chili peppers as part of their dishes. Rather, chili peppers are most often found in the condiments. One reason for that is that a local chili pepper, called the simojovel, is far too hot to use except very sparingly. Chiapan cuisine tends to rely more on slightly sweet seasonings in their main dishes such as cinnamon, plantains, prunes and pineapple are often found in meat and poultry dishes.\n\nTamales are a major part of the diet and often include chipilín mixed into the dough and hoja santa, within the tamale itself or used to wrap it. One tamale native to the state is the \"picte\", a fresh sweet corn tamale. Tamales juacanes are filled with a mixture of black beans, dried shrimp, and pumpkin seeds.\n\nMeats are centered on the European introduced beef, pork and chicken as many native game animals are in danger of extinction. Meat dishes are frequently accompanied by vegetables such as squash, chayote and carrots. Black beans are the favored type. Beef is favored, especially a thin cut called tasajo usually served in a sauce. Pepita con tasajo is a common dish at festivals especially in Chiapa de Corzo. It consists of a squash seed based sauced over reconstituted and shredded dried beef. As a cattle raising area, beef dishes in Palenque are particularly good. Pux-Xaxé is a stew with beef organ meats and mole sauce made with tomato, chili bolita and corn flour. Tzispolá is a beef broth with chunks of meat, chickpeas, cabbage and various types of chili peppers. Pork dishes include cochito, which is pork in an adobo sauce. In Chiapa de Corzo, their version is cochito horneado, which is a roast suckling pig flavored with adobo. Seafood is a strong component in many dishes along the coast. Turula is dried shrimp with tomatoes. Sausages, ham and other cold cuts are most often made and consumed in the highlands.\n\nIn addition to meat dishes, there is chirmol, a cooked tomato sauced flavored with chili pepper, onion and cilantro and zats, butterfly caterpillars from the Altos de Chiapas that are boiled in salted water, then sautéed in lard and eaten with tortillas, limes, and green chili pepper.\n\nSopa de pan consists of layers of bread and vegetables covered with a broth seasoned with saffron and other flavorings. A Comitán speciality is hearts of palm salad in vinaigrette and Palenque is known for many versions of fried plaintains, including filled with black beans or cheese.\n\nCheese making is important, especially in the municipalities of Ocosingo, Rayon and Pijijiapan. Ocosingo has its own self-named variety, which is shipped to restaurants and gourmet shops in various parts of the country. Regional sweets include crystallized fruit, coconut candies, flan and compotes. San Cristobal is noted for its sweets, as well as chocolates, coffee and baked goods.\n\nWhile Chiapas is known for good coffee, there are a number of other local beverages. The oldest is pozol, originally the name for a fermented corn dough. This dough has its origins in the pre-Hispanic period. To make the beverage, the dough is dissolved in water and usually flavored with cocoa and sugar, but sometimes it is left to ferment further. It is then served very cold with lots of ice. Taxcalate is a drink made from a powder of toasted corn, achiote, cinnamon and sugar prepared with milk or water. Pumbo is a beverage made with pineapple, club soda, vodka, sugar syrup and lots of ice. Posh is a drink distilled from sugar cane.\n\nLike in the rest of Mexico, Christianity was introduced to the native populations of Chiapas by the Spanish conquistadors. However, Catholic beliefs were mixed with indigenous ones to form what is now called \"traditionalist\" Catholic belief. The Diocese of Chiapas comprises almost the entire state, and centered on San Cristobal de las Casas. It was founded in 1538 by Pope Paul III to evangelize the area with its most famous bishop of that time Bartolomé de las Casas. Evangelization focused on grouping indigenous peoples into communities centered on a church. This bishop not only graciously evangelized the people in their own language, he worked to introduce many of the crafts still practiced today. While still a majority, only sixty-eight percent of Chiapas residents profess the Catholic faith as of 2010, compared to 83% of the rest of the country.\n\nSome indigenous people mix Christianity with Indian beliefs. One particular area where this is strong is the central highlands in small communities such as San Juan Chamula. In one church in San Cristobal, Mayan rites including the sacrifice of animals is permitted inside the church to ask for good health or to \"ward off the evil eye.\"\n\nStarting in the 1970s, there has been a shift away from traditional Catholic affiliation to Protestant, Evangelical and other Christian denominations. Presbyterians and Pentecostals attracted a large number of converts, with percentages of Protestants in the state rising from five percent in 1970 to twenty-one percent in 2000. This shift has had a political component as well, with those making the switch tending to identify across ethnic boundaries, especially across indigenous ethnic boundaries and being against the traditional power structure. The National Presbyterian Church in Mexico is particularly strong in Chiapas, the state can be described as one of the strongholds of the denomination.\n\nBoth Protestants and Word of God Catholics tend to oppose traditional cacique leadership and often worked to prohibit the sale of alcohol. The latter had the effect of attracting many women to both movements.\n\nThe growing number of Protestants, Evangelicals and Word of God Catholics challenging traditional authority has caused religious strife in a number of indigenous communities. Tensions have been strong, at times, especially in rural areas such as San Juan Chamula. Tension among the groups reached its peak in the 1990s with a large number of people injured during open clashes. In the 1970s, caciques began to expel dissidents from their communities for challenging their power, initially with the use of violence. By 2000, more than 20,000 people had been displaced, but state and federal authorities did not act to stop the expulsions. Today, the situation has quieted but the tension remains, especially in very isolated communities.\n\nThe Spanish Murabitun community, the \"Comunidad Islámica en España\", based in Granada in Spain, and one of its missionaries, Muhammad Nafia (formerly Aureliano Pérez), now emir of the Comunidad Islámica en México, arrived in the state of Chiapas shortly after the Zapatista uprising and established a commune in the city of San Cristóbal. The group, characterized as anti-capitalistic, entered an ideological pact with the socialist Zapatistas group. President Vicente Fox voiced concerns about the influence of the fundamentalism and possible connections to the Zapatistas and the Basque terrorist organization Euskadi Ta Askatasuna (ETA), but it appeared that converts had no interest in political extremism. By 2015, many indigenous Mayans and more than 700 Tzotzils have converted to Islam. In San Cristóbal, the Murabitun established a pizzeria, a carpentry workshop and a Quranic school (madrasa) where children learned Arabic and prayed five times a day in the backroom of a residential building, and women in head scarves have become a common sight. Nowadays, most of the Mayan Muslims have left the Murabitun and established ties with the CCIM, now following the orthodox Sunni school of Islam. They built the Al-Kausar Mosque in San Cristobal de las Casas. Nevertheless, the vast majority of Native Mexicans today are Non-muslims.\n\nThe earliest population of Chiapas was in the coastal Soconusco region, where the Chantuto peoples appeared, going back to 5500 BC. This was the oldest Mesoamerican culture discovered to date.\n\nThe largest and best-known archaeological sites in Chiapas belong to the Mayan civilization. Apart from a few works by Franciscan friars, knowledge of Maya civilisation largely disappeared after the Spanish Conquest. In the mid-19th century, John Lloyd Stephens and Frederick Catherwood traveled though the sites in Chiapas and other Mayan areas and published their writings and illustrations. This led to serious work on the culture including the deciphering of its hieroglyphic writing.\n\nIn Chiapas, principal Mayan sites include Palenque, Toniná, Bonampak, Chinkoltic and Tenam Puentes, all or near in the Lacandon Jungle. They are technically more advanced than earlier Olmec sites, which can best be seen in the detailed sculpting and novel construction techniques, including structures of four stories in height. Mayan sites are not only noted for large numbers of structures, but also for glyphs, other inscriptions, and artwork that has provided a relatively complete history of many of the sites.\n\nPalenque is the most important Mayan and archaeological site. Though much smaller than the huge sites at Tikal or Copán, Palenque contains some of the finest architecture, sculpture and stucco reliefs the Mayans ever produced. The history of the Palenque site begins in 431 with its height under Pakal I (615–683), Chan-Bahlum II (684–702) and Kan-Xul who reigned between 702 and 721. However, the power of Palenque would be lost by the end of the century. Pakal's tomb was not discovered inside the Temple of Inscriptions until 1949. Today, Palenque is a World Heritage Site and one of the best-known sites in Mexico.\n\nYaxchilan flourished in the 8th and 9th centuries. The site contains impressive ruins, with palaces and temples bordering a large plaza upon a terrace above the Usumacinta River. The architectural remains extend across the higher terraces and the hills to the south of the river, overlooking both the river itself and the lowlands beyond. Yaxchilan is known for the large quantity of excellent sculpture at the site, such as the monolithic carved stelae and the narrative stone reliefs carved on lintels spanning the temple doorways. Over 120 inscriptions have been identified on the various monuments from the site. The major groups are the Central Acropolis, the West Acropolis and the South Acropolis. The South Acropolis occupies the highest part of the site. The site is aligned with relation to the Usumacinta River, at times causing unconventional orientation of the major structures, such as the two ballcourts.\n\nThe city of Bonampak features some of the finest remaining Maya murals. The realistically rendered paintings depict human sacrifices, musicians and scenes of the royal court. In fact the name means “painted murals.” It is centered on a large plaza and has a stairway that leads to the Acropolis. There are also a number of notable steles.\n\nToniná is near the city of Ocosingo with its main features being the Casa de Piedra (House of Stone) and Acropolis. The latter is a series of seven platforms with various temples and steles. This site was a ceremonial center that flourished between 600 and 900 CE.\n\nWhile the Mayan sites are the best-known, there are a number of other important sites in the state, including many older than the Maya civilization.\n\nThe oldest sites are in the coastal Soconusco region. This includes the Mokaya culture, the oldest ceramic culture of Mesoamerica. Later, Paso de la Amada became important. Many of these sites are in Mazatan, Chiapas area.\n\nIzapa became an important pre-Mayan site as well.\n\nThere are also other ancient sites including Tapachula and Tepcatán, and Pijijiapan. These sites contain numerous embankments and foundations that once lay beneath pyramids and other buildings. Some of these buildings have disappeared and others have been covered by jungle for about 3,000 years, unexplored.\n\nPijijiapan and Izapa are on the Pacific coast and were the most important pre Hispanic cities for about 1,000 years, as the most important commercial centers between the Mexican Plateau and Central America. Sima de las Cotorras is a sinkhole 140 meters deep with a diameter of 160 meters in the municipality of Ocozocoautla. It contains ancient cave paintings depicting warriors, animals and more. It is best known as a breeding area for parrots, thousands of which leave the area at once at dawn and return at dusk. The state as its Museo Regional de Antropologia e Historia located in Tuxtla Gutiérrez focusing on the pre Hispanic peoples of the state with a room dedicated to its history from the colonial period.\n\nThe average number of years of schooling is 6.7, which is the beginning of middle school, compared to the Mexico average of 8.6. 16.5% have no schooling at all, 59.6% have only primary school/secondary school, 13.7% finish high school or technical school and 9.8% go to university. Eighteen out of every 100 people 15 years or older cannot read or write, compared to 7/100 nationally. Most of Chiapas’ illiterate population are indigenous women, who are often prevented from going to school. School absenteeism and dropout rates are highest among indigenous girls.\n\nThere are an estimated 1.4 million students in the state from preschool on up. The state has about 61,000 teachers and just over 17,000 centers of educations. Preschool and primary schools are divided into modalities called general, indigenous, private and community educations sponsored by CONAFE. Middle school is divided into technical, telesecundaria (distance education) and classes for working adults. About 98% of the student population of the state is in state schools. Higher levels of education include \"professional medio\" (vocational training), general high school and technology-focused high school. At this level, 89% of students are in public schools. There are 105 universities and similar institutions with 58 public and 47 private serving over 60,500 students.\n\nThe state university is the Universidad Autónoma de Chiapas (UNACH). It was begun when an organization to establish a state level institution was formed in 1965, with the university itself opening its doors ten years later in 1975. The university project was partially supported by UNESCO in Mexico. It integrated older schools such as the Escuela de Derecho (Law School), which originated in 1679; the Escuela de Ingeniería Civil (School of Civil Engineering), founded in 1966; and the Escuela de Comercio y Administración, which was located in Tuxtla Gutiérrez.\n\nThe state has approximately 22,517 km of highway with 10,857 federally maintained and 11,660 maintained by the state. Almost all of these kilometers are paved. Major highways include the Las Choapas-Raudales-Ocozocoautla, which links the state to Oaxaca, Veracruz, Puebla and Mexico City. Major airports include Llano San Juan in Ocozocoautla, Francisco Sarabia National Airport (which was replaced by Ángel Albino Corzo International Airport) in Tuxtla Gutiérrez and Corazón de María Airport (which closed in 2010) in San Cristóbal de las Casas. These are used for domestic flights with the airports in Palenque and Tapachula providing international service into Guatemala. There are 22 other airfields in twelve other municipalities. Rail lines extend over 547.8 km. There are two major lines: one in the north of the state that links the center and southeast of the country, and the Costa Panamericana route, which runs from Oaxaca to the Guatemalan border.\n\nThere are thirty six AM radio stations and sixteen FM stations. There are thirty seven local television stations and sixty six repeaters.\n\nChiapas' main port is just outside the city of Tapachula called the Puerto Chiapas. It faces 3,361 meters of ocean, with 3,060 m of warehouse space. Next to it there is an industrial park that covers 2,340,000 m. Puerto Chiapas has 60,000 m of area with a capacity to receive 1,800 containers as well as refrigerated containers. The port serves the state of Chiapas and northern Guatemala. Puerto Chiapas serves to import and export products across the Pacific to Asia, the United States, Canada and South America. It also has connections with the Panama Canal. A marina serves yachts in transit.\nThere is an international airport located eleven km away as well as a railroad terminal ending at the port proper. Over the past five years the port has grown with its newest addition being a terminal for cruise ships with tours to the Izapa site, the Coffee Route, the city of Tapachula, Pozuelos Lake and an Artesanal Chocolate Tour. Principal exports through the port include banana and banana trees, corn, fertilizer and tuna.\n\nBULLET::::- 2017 Chiapas earthquake\nBULLET::::- Benjamin, Thomas. \"A Rich Land, a Poor People: Politics and Society in Modern Chiapas\". Albuquerque: University of New Mexico Press. 1996.\nBULLET::::- Benjamin, Thomas. \"A Time of Reconquest: History, the Maya Revival, and the Zapatista Rebellion.\" \"The American Historical Review\", Vol. 105, no. 2 (April 2000): pp. 417–450.\nBULLET::::- Collier, George A, and Elizabeth Lowery Quaratiello. \"Basta! Land and the Zapatista Rebellion in Chiapas\". Oakland: The Institute for Food and Development Policy, 1994.\nBULLET::::- Collier, George A. \"The Rebellion in Chiapas and the Legacy of Energy Development.\" \"Mexican Studies/Estudios Mexicanos\", Vol. 10, no. 2 (Summer 1994): pp. 371–382\nBULLET::::- García, María Cristina. \"Seeking Refuge: Central American Migration to Mexico, the United States, and Canada\". Berkeley and Los Angeles: University of California Press 2006\nBULLET::::- Hamnett, Brian R. \"Concise History of Mexico\". Cambridge: Cambridge University Press 1999.\nBULLET::::- Hidalgo, Margarita G. (Editor). \"Contributions to the Sociology of Language: Mexican Indigenous Languages at the Dawn of the Twenty-First Century\". Berlin: DEU: Walter de Gruyter & Co. kg Publishers, Berlin, 2009.\nBULLET::::- Higgins, Nicholas P. \"Understanding the Chiapas Rebellion: Modernist Visions and the Invisible Indian\". Austin: University of Texas Press, 2004,\nBULLET::::- Jiménez González, Victor Manuel (Editor). \"Chiapas: Guía para descubrir los encantos del estado\". Mexico City: Editorial Océano de México, SA de CV 2009.\nBULLET::::- Lowe, G. W., “Chiapas de Corzo”, in Evans, Susan, ed., \"Archaeology of Ancient Mexico and Central America\", Taylor & Francis, London.\nBULLET::::- Whitmeyer, Joseph M. and Hopcroft, Rosemary L. \"Community, Capitalism, and Rebellion in Chiapas.\" \"Sociological Perspectives\" Vol. 39, no. 4 (Winter 1996): pp. 517–538.\n\nBULLET::::- Chiapas State Government\nBULLET::::- Zapatista National Army of Liberation\nBULLET::::- brief history of the conflict in Chiapas (1994–2007)\nBULLET::::- Acosta et al., 2018. \"Climate change and peopling of the Neotropics during the Pleistocene-Holocene transition\". \"Boletín de la Sociedad Geológica Mexicana\".\n"}
{"id": "6788", "url": "https://en.wikipedia.org/wiki?curid=6788", "title": "Chrysler Building", "text": "Chrysler Building\n\nThe Chrysler Building is an Art Deco–style skyscraper located in the Turtle Bay neighborhood on the East Side of Manhattan, New York City, at the intersection of 42nd Street and Lexington Avenue near Midtown Manhattan. At , the structure was the world's tallest building for 11 months before it was surpassed by the Empire State Building in 1931. It is the tallest brick building in the world with a steel framework. , the Chrysler is the eighth-tallest building in the city, tied with The New York Times Building.\n\nOriginally, a project of real estate developer and former New York State Senator William H. Reynolds, the building was constructed by Walter Chrysler, the head of the Chrysler Corporation, and served as the corporation's headquarters from 1930 until the mid-1950s. The Chrysler Building's construction was characterized by a competition with 40 Wall Street and the Empire State Building to become the world's tallest building. Although the Chrysler Building was built and designed specifically for the car manufacturer, the corporation did not pay for its construction and never owned it; rather, Walter Chrysler decided to pay for it himself so that his children could inherit it.\n\nWhen the Chrysler Building opened, there were mixed reviews of the building's design, ranging from its being inane and unoriginal to that it was modernist and iconic. Perceptions of the building have slowly evolved into its now being seen as a paragon of the Art Deco architectural style; and in 2007, it was ranked ninth on the \"List of America's Favorite Architecture\" by the American Institute of Architects.\n\nIn the mid-1920s, New York's metropolitan area surpassed London's as the world's most populous metropolitan area and its population exceeded ten million by the early 1930s. The era was characterized by profound social and technological changes. Consumer goods such as radio, cinema, and the automobile—whose use grew exponentially in the 1920s—became widespread. In 1927, Walter Chrysler's automotive company, the Chrysler Corporation, became the third-largest car manufacturer in the United States, behind Ford and General Motors. The following year, Chrysler was named \"Time\" magazine's \"Person of the Year\".\n\nThe economic boom of the 1920s and speculation in the real estate market fostered a wave of new skyscraper projects in New York City. The Chrysler Building was built as part of an ongoing building boom that resulted in the city having the world's tallest building from 1908 to 1974. Following the end of World War I, European and American architects came to see simplified design as the epitome of the modern era and Art Deco skyscrapers as symbolizing progress, innovation, and modernity. The 1916 Zoning Resolution restricted the height that street-side exterior walls of New York City buildings could rise before needing to be setback from the street. This led to the construction of Art Deco structures in New York City with significant setbacks, large volumes, and striking silhouettes that were often elaborately decorated. Art Deco buildings were constructed for only a short period of time; but because that period was during the city's late-1920s real estate boom, the numerous skyscrapers built in the Art Deco style predominated in the city skyline, giving it the romantic quality seen in films and plays. The Chrysler Building project was shaped by these circumstances.\n\nThe land on which the Chrysler Building stands was donated to The Cooper Union for the Advancement of Science and Art in 1902. The site is roughly a trapezoid with a frontage on Lexington Avenue; a frontage on 42nd Street; and a frontage on 43rd Street. The site bordered the old Boston Post Road, which predated, and ran aslant of, the Manhattan street grid established by the Commissioners' Plan of 1811. As a result, the east side of the building's base is similarly aslant.\n\nOriginally, the Chrysler Building was to be the Reynolds Building, a project of real estate developer and former New York State Senator William H. Reynolds. Prior to his involvement in planning the building, Reynolds was best known for developing Coney Island's Dreamland amusement park. When the amusement park was destroyed by fire in 1911, Reynolds turned his attention to Manhattan real estate, where he set out to build the tallest building in the world.\n\nIn 1921, Reynolds rented a large plot of land at the corner of Lexington Avenue and 42nd Street with the intention of building a tall building on the site. In 1927, after several years of delays, Reynolds hired the architect William Van Alen to build a forty-story building there.\n\nVan Alen was respected in his field for his work on the Albemarle Building at Broadway and 24th Street, designing it in collaboration with his partner H. Craig Severance. Van Alen and Severance complemented each other, with Van Alen being an original, imaginative architect and Severance being a shrewd businessperson who handled the firm's finances. However, the relationship between them became tense over disagreements on how best to run the firm. The breaking point came after a 1924 article, in the \"Architectural Review\", that praised the Albemarle Building's design, which the article attributed to Van Alen, while ignoring Severance's role altogether. The architects' partnership dissolved acrimoniously several months later, with lawsuits over the firm's clients and assets lasting over a year. This ended up being decisive for the design of the future Chrysler Building, since Severance's more traditional architectural style would otherwise have restrained Van Alen's more modern outlook.\n\nBy February 2, 1928, the proposed building's height had been increased to 54 stories, which would have made it the tallest building in Midtown. The proposal was changed again two weeks later, with official plans for a 63-story building. A little more than a week after that, the plan was changed for the third time, with two additional stories added. By this time, 42nd Street and Lexington Avenue were both hubs for construction activity, due to the removal of the Third Avenue Elevated's 42nd Street spur, which was seen as a blight on the area. The 52-story Chanin Building, diagonally across the intersection from Reynolds's proposed building, was also under construction. Because of the elevated spur's removal, real estate speculators believed that Lexington Avenue would become the \"Broadway of the East Side\", causing a ripple effect that would spur developments farther east.\n\nIn April 1928, Reynolds signed a 67-year lease for the plot and finalized the details of his ambitious project. Van Alen's original design for the skyscraper called for a base whose first-floor showroom windows would be triple-height, and above would be 12 stories with glass-wrapped corners, to create the impression that the tower was floating in mid-air. Reynolds's main contribution to the building's design was his insistence that it have a metallic crown, despite Van Alen's initial opposition; the metal-and-crystal crown would have looked like \"a jeweled sphere\" at night. Originally, the skyscraper would have risen , with 67 floors. These plans were approved in June 1928. Van Alen's drawings were unveiled in the following August and published in a magazine run by the American Institute of Architects (AIA).\n\nEventually, this design would prove too advanced and expensive for Reynolds. He instead devised an alternate design for the Reynolds Building, which was published in August 1928. The new design was much more conservative, with an Italianate dome that a critic compared to Governor Al Smith's bowler hat, and a brick arrangement on the upper floors that simulated windows in the corners, a detail that remains in the current Chrysler Building. This design almost exactly reflected the shape, setbacks, and the layout of the windows of the current building, but with a different dome.\n\nWith the design complete, groundbreaking for the Reynolds Building took place on September 19, 1928, but Reynolds did not have the means to carry on construction. Reynolds sold the plot, lease, plans, and architect's services to Walter Chrysler for $2 million on October 15, 1928. That same day, the Goodwin Construction Company began demolition of what had been built. A contract was awarded on October 28, and demolition was completed on November 9. Chrysler's initial plans for the building were similar to Reynolds's, but with the 808-foot building having 68 floors instead of 67. The plans entailed a ground-floor pedestrian arcade, a facade of stone below the fifth floor, a brick-and-terracotta facade above, and a \"three-story observation dome\" with \"bronze and glass\" at the top. However, Chrysler wanted a more progressive design, and he worked with Van Alen to redesign the skyscraper to be tall. At the new height, Chrysler's building would be taller than the Woolworth Building, a building in lower Manhattan that was the world's tallest at the time.\nFrom late 1928 to early 1929, modifications to the design of the dome continued. In March 1929, the press published details of an \"artistic dome\" that had the shape of a giant thirty-pointed star, which would be crowned by a sculpture five meters high. The final design of the dome included several arches and triangular windows. Lower down, the design was affected by Walter Chrysler's intention to make the building the Chrysler Corporation's headquarters, and as such, various architectural details were modeled after Chrysler automobile products, such as the hood ornaments of the Plymouth (see ). The building's gargoyles on the 31st floor and the eagles on the 61st floor, were designed to signify flight, and to exemplify the machine age of the 1920s. Even the topmost needle was built using a process similar to one Chrysler used to manufacture his cars, with precise \"hand craftmanship\". In his autobiography, Chrysler says he suggested that his building be taller than the Eiffel Tower.\n\nMeanwhile, excavation of the new building's foundation began in mid-November 1928 and was completed in mid-January 1929, when bedrock was reached. A total of of rock and of soil was excavated for the foundation, equal to 63% of the future building's weight. Construction of the building proper began on January 21, 1929. The Carnegie Steel Company provided the steel beams, the first of which was installed on March 27; and by April 9, the first upright beams had been set into place. The steel structure was \"a few floors\" high by June 1929, 35 floors high by early August, and completed by September. Despite a frantic steelwork construction pace of about four floors per week, no workers died during the construction of the skyscraper's steelwork. Chrysler lauded this achievement, saying, \"It is the first time that any structure in the world has reached such a height, yet the entire steel construction was accomplished without loss of life\". In total, 391,881 rivets were used, and approximately 3,826,000 bricks were manually laid to create the non-loadbearing walls of the skyscraper. Walter Chrysler personally financed the construction with his income from his car company. The Chrysler Building's height officially surpassed the Woolworth's on October 16, 1929, thereby becoming the world's tallest structure.\n\nThe same year that the Chrysler Building's construction started, banker George L. Ohrstrom proposed the construction of a 47-story office building at 40 Wall Street downtown. Shortly thereafter Ohrstrom modified his project to have 60 floors, but it was still below Woolworth and the 808-foot Chrysler Building project as announced in 1928. H. Craig Severance, Van Alen's former partner and the architect of 40 Wall Street, increased 40 Wall's height to with 62 floors in April of that year. It would thus exceed the Woolworth's height by and the Chrysler's by . 40 Wall Street and the Chrysler Building started competing for the distinction of \"world's tallest building\". The Empire State Building, on 34th Street and Fifth Avenue, entered the competition in 1929. The \"Race into the Sky\", as popular media called it at the time, was representative of the country's optimism in the 1920s, which helped fuel the building boom in major cities. The 40 Wall Street tower was revised from to 925 feet in April 1929, which would make it the world's tallest. Severance increased the height of his project and then publicly claimed the title of the world's tallest building. Construction of 40 Wall Street began in May 1929 at a frantic pace, and it was completed twelve months later.\n\nIn response, Van Alen obtained permission for a spire and had it secretly constructed inside the frame of his building. The spire was delivered to the site in four different sections. On October 23, 1929, one week after surpassing the Woolworth Building's height and one day before the catastrophic Wall Street Crash of 1929 started, the spire was assembled. According to one account, \"the bottom section of the spire was hoisted to the top of the building's dome and lowered into the 66th floor of the building.\" Then, within 90 minutes the rest of the spire's pieces were raised and riveted in sequence, helping raise the tower's height to 1,046 feet. Van Alen, who witnessed the process from the street along with its engineers and Walter Chrysler, compared the experience to watching a butterfly leaving its cocoon.\n\nIn \"The Structure and Metal Work of the Chrysler Building\", an article published in the October 1930 edition of \"Architectural Forum\", Van Alen explained the design and construction of the crown and needle:\n\nThe steel tip brought the Chrysler Building to a height of , greatly exceeding 40 Wall Street's height. However, contemporary news media did not write of the spire's erection, nor were there any press releases celebrating the spire's erection. Even the \"New York Herald Tribune\", which had virtually continuous coverage of the tower's construction, did not report on the spire's installation until days after the spire had been raised.\n\nChrysler realized that his tower's height would exceed the Empire State Building's as well, having ordered Van Alen to change the Chrysler's original roof from a stubby Romanesque dome to the narrow steel spire. However, the Empire State's developer John J. Raskob reviewed the plans and realized that he could add five more floors and a spire of his own to his 80-story building, and subsequently acquired the nearby plots needed to support that building's height extension. Two days later, the Empire State Building's co-developer, former Governor Al Smith, announced the updated plans for that skyscraper, with an observation deck on the 86th-floor roof at a height of , higher than the Chrysler's 71st-floor observation deck at .\n\nIn January 1930, it was announced that the Chrysler Corporation would maintain offices in the Chrysler Building during Automobile Show Week, and the first leases by outside tenants were announced in April 1930, before the building was officially completed. The building was formally opened on May 27, 1930, in a ceremony that coincided with the 42nd Street Property Owners and Merchants Association's meeting that year. In the lobby of the building, a bronze plaque that read \"in recognition of Mr. Chrysler's contribution to civic advancement\" was unveiled. Former Governor Smith, former Assemblyman Martin G. McCue, and 42nd Street Association president George W. Sweeney were among those in attendance. By June, it was reported that 65% of the available space had been leased. By August, the building was declared complete, but the New York City Department of Construction did not mark it as finished until February 1932.\n\nThe added height of the spire allowed the Chrysler Building to surpass 40 Wall Street as the tallest building in the world and the Eiffel Tower as the tallest structure. The Chrysler Building was thus the first man-made structure to be taller than ; and as one newspaper noted, the tower was also taller than the highest points of five states. The Chrysler Building was appraised at $14 million, but was exempt from city taxes per an 1859 law that gave tax exemptions to sites owned by the Cooper Union. The city had attempted to repeal the tax exemption, but Cooper Union had opposed that measure. Because the Chrysler Building retains the tax exemption, it has paid Cooper Union for the use of their land since opening.\n\nVan Alen's satisfaction at these accomplishments was likely muted by Walter Chrysler's later refusal to pay the balance of his architectural fee. Chrysler alleged that Van Alen had received bribes from suppliers, and Van Alen had not signed any contracts with Walter Chrysler when he took over the project. Van Alen sued and the courts ruled in his favor, requiring Chrysler to pay Van Allen $840,000, or 6% of the total budget of the building. However, the lawsuit against Chrysler markedly diminished Van Alen's reputation as an architect, which, along with the effects of the Great Depression and negative criticism, ended up ruining his career. Van Alen ended his career as professor of sculpture at the nearby Beaux-Arts Institute of Design and died in 1954. According to author Neal Bascomb, \"The Chrysler Building was his greatest accomplishment, and the one that guaranteed his obscurity.\"\n\nThe completed Chrysler Building garnered mixed reviews in the press. Van Alen was hailed as the \"Doctor of Altitude\" by \"Architect\" magazine, while architect Kenneth Murchison called Van Alen the \"Ziegfeld of his profession\", comparing him to popular Broadway producer Florenz Ziegfeld Jr.. The building was praised for being \"an expression of the intense activity and vibrant life of our day\", and for \"teem[ing] with the spirit of modernism, ... the epitome of modern business life, stand[ing] for progress in architecture and in modern building methods.\" An anonymous critic wrote in \"Architectural Forum\" October 1930 issue: \"The Chrysler...stands by itself, something apart and alone. It is simply the realization, the fulfillment in metal and masonry, of a one-man dream, a dream of such ambitions and such magnitude as to defy the comprehension and the criticism of ordinary men or by ordinary standards.\" Negative critics included journalist George S. Chappell, who called the Chrysler's design \"distinctly a stunt design, evolved to make the man in the street look up\", and Douglas Haskell, who said that the building \"embodies no compelling, organic idea.\" Others compared the Chrysler Building to \"an upended swordfish\", or claimed it had a \"Little Nemo\"-like design. Lewis Mumford, a supporter of the International Style and one of the foremost architectural critics of the United States at the time, despised the building for its \"inane romanticism, meaningless voluptuousness, [and] void symbolism\".\n\nThe Chrysler Building's distinction as the world's tallest building was short-lived. John Raskob realized the 1,050-foot Empire State Building would only be taller than the Chrysler Building, and Raskob was afraid that Walter Chrysler might try to \"pull a trick like hiding a rod in the spire and then sticking it up at the last minute.\" Another revision brought the Empire State Building's roof to , making it the tallest building in the world by far when it opened on May 1, 1931. However, the Chrysler Building is still the world's tallest steel-supported brick building. The Chrysler Building fared better commercially than the Empire State Building did: by 1935, the Chrysler had already rented 70% of its floor area, while the Empire State had only leased 23% of its area and was popularly derided as the \"Empty State Building\".\n\nContrary to popular belief, the Chrysler Corporation was never involved in the construction or ownership of the Chrysler Building, although it was built and designed for the corporation and served as its headquarters until the mid-1950s. It was a project of Walter P. Chrysler for his children. In his autobiography, Chrysler wrote that he wanted to erect the building \"so that his sons would have something to be responsible for\".\n\nThe Chrysler family inherited the property after the death of Walter Chrysler in 1940, with the property being under the ownership of W.P. Chrysler Building Corporation. In 1944, the corporation filed plans to build a 38-story annex to the east of the building, at 666 Third Avenue. In 1949, this was revised to a 32-story annex costing $9 million. The annex building, designed by Reinhard, Hofmeister & Walquist, had a facade similar to that of the original Chrysler Building. The stone for the original building was no longer manufactured, and had to be specially replicated. Construction started on the annex in June 1950, and the first tenants started leasing in June 1951. The building itself was completed by 1952, and a sky bridge connecting the two buildings' seventh floors was built in 1959.\n\nThe family sold the building in 1953 to William Zeckendorf for its assessed price of $18 million. The 1953 deal included the annex and the nearby Graybar Building, which along with the Chrysler Building sold for a combined $52 million. The new owners were Zeckendorf's company Webb and Knapp, who held a 75% interest in the sale, and the Graysler Corporation, who held a 25% stake. At the time, it was reported to be the largest real estate sale in New York City's history. In 1957, the Chrysler Building, its annex, and the Graybar Building were sold for $66 million to Lawrence Wien's realty syndicate, setting a new record for the largest sale in the city. In 1960, the complex was purchased by Sol Goldman and Alex DiLorenzo, who received a mortgage from the Massachusetts Mutual Life Insurance Company. In 1961, the building's stainless steel elements, including the needle, crown, gargoyles, and entrance doors, were polished for the first time. A group of ten workers steam-cleaned the facade below the 30th floor, and manually cleaned the portion of the tower above the 30th floor, for a cost of about $200,000.\n\nMassachusetts Mutual obtained outright ownership in 1975 after Goldman and DiLorenzo defaulted on the mortgage. The company purchased the building for $35 million. In 1978, they devised plans to renovate the facade, heating, ventilation, air‐conditioning, elevators, lobby murals, and Cloud Club headquarters in a $23 million project. This renovation was completed in 1979. They delegated the leasing of the building's space to the Edward S. Gordon Company, which leased of vacant space within five years. During Massachusetts Mutual's ownership of the Chrysler Building, the tower received two historic designations. The building was designated as a National Historic Landmark in 1976, and as a New York City Landmark in 1978, although the city only landmarked the lobby and facade. Massachusetts Mutual had opposed the city landmark designation because it \"would cause 'inevitable delay' in moving new tenants into the skyscraper\". At the time, the building had of vacant floor space, representing 40% of the total floor area. In September 1979, the building was sold again, this time to entrepreneur and Washington Redskins owner Jack Kent Cooke, in a deal that also transferred ownership of the Los Angeles Kings and Lakers to Jerry Buss.\n\nThe spire underwent a restoration that was completed in 1995. The joints in the now-closed observation deck were polished, and the facade restored, as part of a $1.5 million project. Some damaged steel strips of the needle were replaced and several parts of the gargoyles were re-welded together. The cleaning received the New York Landmarks Conservancy's Lucy G. Moses Preservation Award for 1997. Cooke died in 1997, and creditors moved to foreclose on the estate's unpaid fees soon after. Tishman Speyer Properties and the Travelers Insurance Group bought the Chrysler Center in 1997–1998 for about $220 million (equal to $ million in ) from a consortium of banks and the estate of Jack Kent Cooke. Tishman Speyer Properties had negotiated a 150-year lease from the Cooper Union, and the college continues to own the land under the Chrysler Building. Cooper Union's name is on the deed.\n\nIn 2001, a 75% stake in the building was sold, for US$300 million (equal to $ million in ), to TMW, the German arm of an Atlanta-based investment fund. In June 2008, it was reported that the Abu Dhabi Investment Council was in negotiations to buy TMW's 75% economic interest, a 15% interest from Tishman Speyer Properties in the building, and a share of the Trylons retail structure next door for US$800 million. In July 2008, it was announced that the transaction had been completed, and that the Abu Dhabi Investment Council was now 90% owner of the building, with Tishman Speyer retaining 10%.\n\nFrom 2010 to 2011, the building's energy, plumbing, and waste management systems were renovated. This resulted in a 21% decrease in the building's total energy consumption, a 64% decrease in water consumption, and an 81% rate of waste being recycled. In 2012, the building received a LEED Gold accreditation from the U.S. Green Building Council, which recognized the building's environmental sustainability and energy efficiency.\n\nThe Abu Dhabi Investment Council and Tishman Speyer put the Chrysler Building on sale again in January 2019. It was reported in March 2019 that Aby Rosen's RFR Holding LLC, in a joint venture with the Austrian SIGNA Group, had reached an agreement to purchase the Chrysler Building, albeit at a steeply discounted price.\n\nThe Chrysler Building is considered a leading example of Art Deco architecture. It is constructed of a steel frame in-filled with masonry, with areas of decorative metal cladding. The structure contains 3,862 exterior windows. Approximately fifty metal ornaments protrude at the building's corners on five floors reminiscent of gargoyles on Gothic cathedrals. The 31st-floor contains gargoyles and replicas of the 1929 Chrysler radiator caps, the 61st eagles, a nod to America's national bird.\n\nThe Chrysler Building uses bright \"Nirosta\" stainless steel extensively in its design, an austenitic alloy developed in Germany by Krupp (a German acronym for \"nichtrostender Stahl\", meaning \"non-rusting steel\"). It was the first use of this \"18-8 stainless steel\" in an American project, composed of 18% chromium and 8% nickel. Nirosta was used in the exterior ornaments, the window frames, the crown, and the needle. The steel was an integral part of Van Alen's design, as E.E. Thum explains: \"The use of permanently bright metal was of greatest aid in the carrying of rising lines and the diminishing circular forms in the roof treatment, so as to accentuate the gradual upward swing until it literally dissolves into the sky...\" Stainless steel producers used the Chrysler Building to evaluate the durability of the product in architecture. In 1929, the American Society for Testing Materials created an inspection committee to study its performance, which regarded the Chrysler Building as the best location to do so; a subcommittee examined the building's panels every five years until 1960, when the inspections were canceled because the panels had shown minimal deterioration.\n\nThe Chrysler Building's height and legally mandated setbacks influenced Van Alen in his design. The walls of the lowermost sixteen floors rise directly from the sidewalk property lines, except for a recess on one side that gives the building a \"U\"-shaped floor plan above the fourth floor. There are setbacks on floors 16, 18, 23, 28, and 31, making the building compliant with the Zoning Law of 1916. This gives the building the appearance of a ziggurat on one side and a U-shaped palazzo on the other. Above the 31st floor, there are no more setbacks until the 60th floor, above which the structure is funneled into a Maltese cross shape that \"blends the square shaft to the finial\", according to author and photographer Cervin Robinson.\n\nThe floor plans of the first sixteen floors were made as large as possible to optimize the amount of rental space nearest ground level, which was seen as most desirable. The U-shaped cut above the fourth floor served as a shaft for air flow and illumination. The area between floors 28 and 31 added \"visual interest to the middle of the building, preventing it from being dominated by the heavy detail of the lower floors and the eye-catching design of the finial. They provide a base to the column of the tower, effecting a transition between the blocky lower stories and the lofty shaft.\"\n\nThe ground floor exterior is covered in polished black granite from Shastone, while the three floors above it are done in white marble from Georgia. There are two main entrances, on Lexington Avenue and on 42nd Street, each three floors high with Shastone granite surrounding each proscenium-shaped entryway. At some distance into each main entryway, there are revolving doors located \"beneath intricately patterned metal and glass screens\", designed so as to embody the Art Deco tenet of amplifying the entrance's visual impact. A smaller side entrance on 43rd Street is only one story high. There are storefronts consisting of large Nirosta-steel-framed windows at ground level, with office windows on the second through fourth floors.\n\nThe west and east elevations of the building contain the air shafts above the fourth floor, while the north and south sides contain the receding setbacks. Below the 16th floor, the facade is clad with white brick interrupted by white-marble bands in a manner similar to a basket weaving. The windows, arranged in grids, do not have window sills, the frames being flush with the facade. Between the 16th and 24th floors, the exterior exhibits vertical white brick columns that are separated by windows on each floor. This visual effect is made possible by the presence of aluminum spandrels between the columns of windows on each floor. There are abstract reliefs on the 20th through 22nd-floor spandrels, while the 24th floor contains decorative pineapples.\n\nAbove the third setback, consisting of the 24th through 27th floors, the facade contains horizontal bands and zigzagged gray-and-black brick motifs. Above the fourth setback, between the 27th and 31st floors, the shaft starts to appear. At each corner of the 31st floor, large car-hood ornaments made of Nirosta steel serve as visually striking objects that make the base look larger. These corner extensions help counter a common optical illusion seen in tall buildings with horizontal bands, whose taller floors would normally look larger. The 31st floor also contains a gray and white frieze of hubcaps and fenders, which symbolizes both the Chrysler Corporation and serves as a visual signature of the building's Art Deco design. The bonnet embellishments take the shape of Mercury's winged helmet and resemble hood ornaments installed on Chrysler vehicles at the time.\n\nThe shaft of the tower was designed to emphasize both the horizontal and vertical: each of the tower's four sides contains three columns of windows, each framed by bricks and an unbroken marble pillar that rises along the entirety of each side. The spandrels separating the windows contain \"alternating vertical stripes in gray and white brick\", while each corner contains horizontal rows of black brick.\n\nThe interior of the building contains several innovative elements. The partitions between the offices are soundproofed and divided into interchangeable sections, so that the layout of any could be changed quickly and comfortably. Pipes under the floors carry both telephone and electricity cables.\n\nThe triangular-shaped lobby is regarded as a paragon of the Art Deco style, with clear influences of German Expressionism. Chrysler wanted the design to impress other architects and automobile magnates, so he imported various materials without giving consideration to the extra costs incurred. He covered the walls with huge slabs of African red granite. On the floor, he marked a path from the entrances to the elevators using travertine from Siena. Originally, Van Alen's plans for the lobby included four large supporting columns, but they were removed after Chrysler objected on the grounds that the columns made the lobby appear \"cramped\".\n\nThe lobby has dim lighting that gives it a somewhat subdued quality, although the appliqués of the lamps are striking and iconic. Both combine to create an intimate atmosphere and act to highlight the place. Vertical bars of fluorescent light are covered with Belgian blue marble and Mexican amber onyx, which soften and diffuse the light, to both illuminate and blend with the red marble walls. The lobby also contains four elevator banks, each with a different design.\n\nThe ceiling contains a mural named \"Transport and Human Endeavor\", commissioned by Edward Trumbull in 1930. The mural's theme is \"energy and man's application of it to the solution of his problems\", and it pays homage to the Golden Age of Aviation and the Machine Age. The mural is painted in the shape of a \"Y\" with ocher and golden tones. The central image of the mural is a \"muscled giant whose brain directs his boundless energy to the attainment of the triumphs of this mechanical era\", according to a 1930 pamphlet that advertised the building. The mural's Art Deco style is manifested in characteristic triangles, sharp angles, slightly curved lines, chrome ornaments, and numerous patterns. The mural depicts several silver planes, including the \"Spirit of St. Louis\", as well as furnaces of incandescent steel and the building itself. There is a wall panel dedicated to the work of clinchers, surveyors, masons, carpenters, plasterers, and builders. Fifty different figures were modeled after workers who participated in its construction. In 1999, the mural was returned to its original state after a restoration that removed the polyurethane coating and filled-in holes added in the 1970s.\n\nPresently, the lobby is the only publicly accessible part of the Chrysler Building. When the building opened, the first and second floors housed a public exhibition of Chrysler vehicles. This exhibition was closed before World War II.\n\nThere are 32 elevators in the skyscraper, clustered into groups of six or eight. At the time of opening, 28 of these elevators were for passenger use. Each bank serves different floors within the building, with several \"express\" elevators going from the lobby to a few landings in between, while \"local\" elevators connect the landings with the floors above these intermediate landings. As per Walter Chrysler's wishes, the elevators were designed to run at a rate of , despite the speed restriction enforced in all city elevators at the time. This restriction was loosened soon after the Empire State Building opened in 1931, as that building had also been equipped with high-speed elevators. The Chrysler Building also had three of the longest elevator shafts in the world at the time of completion.\n\nOver the course of a year, Van Alen painstakingly designed these elevators with the assistance of L.T.M. Ralston, who was in charge of developing the elevator cabs' mechanical parts. The cabs were manufactured by the Otis Elevator Company, while the doors were made by the Tyler Company. The dimensions of each elevator were deep by wide. The doors are made of metal and covered with eight types of exotic woods. When the doors are closed, they resemble \"tall fans set off by metallic palm fronds rising through a series of silver parabolas, whose edges were set off by curved lilies\" from the outside, as noted by Curcio. However, when a set of doors is open, the cab behind the doors resembles \"an exquisite Art Deco room\". These elements were influenced by Egyptian designs, which significantly impacted the Art Deco style. According to Vincent Curcio, \"these elevator interiors were perhaps the single most beautiful and, next to the dome, the most important feature of the entire building.\"\n\nEven though the woods in the elevator cabs were arranged in four basic patterns, each cab had a unique combination of woods. One writer stated that \"if anything the building is based on patterned fabrics, [the elevators] certainly are. Three of the designs could be characterized as having 'geometric', 'Mexican' and vaguely 'art nouveau' motifs, which reflect the various influences on the design of the entire building.\" The roof of each elevator was covered with a metal plate whose design was unique to that cab, which in turn was placed on a polished wooden pattern that was also customized to the cab. Hidden behind these plates were ceiling fans. Curcio wrote that these elevators \"are among the most beautiful small enclosed spaces in New York, and it is fair to say that no one who has seen or been in them has forgotten them\". Curcio compared the elevators to the curtains of a Ziegfeld production, noting that each lobby contains lighting that peaks in the middle and slopes down on either side. The decoration of the cabs' interiors was also a nod to the Chrysler Corporation's vehicles: cars built during the building's early years had dashboards with wooden moldings. Both the doors and cab interiors were considered to be works of extraordinary marquetry.\n\nOn the 42nd Street side of the Chrysler Building, a staircase from the street leads directly under the building to the New York City Subway's at Grand Central–42nd Street station. It is part of the structure's original design. The Interborough Rapid Transit Company, which at the time was the operator of all the routes serving the 42nd Street station, originally sued to block construction of the new entrance because it would cause crowding, but the New York City Board of Transportation pushed to allow the corridor anyway. Chrysler eventually built and paid for the building's subway entrance. Work on the new entrance started in March 1930 and it opened along with the Chrysler Building two months later.\n\nThe basement also had a \"hydrozone water bottling unit\" that would filter tap water into drinkable water for the building's tenants. The drinkable water would then be bottled and shipped to higher floors.\n\nThe private Cloud Club formerly occupied the 66th through 68th floors. It opened in July 1930 with some three hundred members, all wealthy males who formed the city's elite. Its creation was spurred by Texaco's wish for a proper restaurant for its executives prior to renting fourteen floors in the building. The Cloud Club was a compromise between William van Alen's modern style and Walter Chrysler's stately and traditional tastes. A member had to be elected, and if accepted, paid an initial fee of $200, plus a $150 to $300 annual fee.\n\nThere was a Tudor-style foyer on the 66th floor with oak paneling, and an old English-style grill room with wooden floors, wooden beams, wrought-iron chandeliers, and glass and lead doors. The main dining room, located on the 67th floor, was connected to the 66th floor by a Renaissance-style marble and bronze staircase and had a futuristic appearance, with polished granite columns and etched glass appliqués in Art Deco style. There was a mural of a cloud on the ceiling, and a mural of Manhattan on the dining room's north side. It is believed that the dining room was an inspiration for the Rainbow Room and the Rockefeller Center Luncheon Club, both located at 30 Rockefeller Center. On the same floor, Walter Chrysler and Texaco both had private dining rooms. The 68th floor mainly contained service spaces.\n\nIn the 1950s and 1960s, members left the Cloud Club for other clubs. Texaco, whose executives comprised most of the Cloud Club's membership, moved to Westchester County in 1977, and the club closed two years later. Although there have been several projects to rehabilitate the club or transform it into a disco or a gastronomic club, these plans have never materialized, as then-owner Cooke reportedly did not want a \"conventional\" restaurant operating within the old club. Tishman Speyer rented the top two floors of the old Cloud Club. The old staircase has been removed, as have many of the original decorations, which prompted objections from the Art Deco Society of New York.\n\nOriginally, Walter Chrysler had a two-story apartment on the 69th and 70th floors with a fireplace and a private office. The office also contained a gymnasium and the loftiest bathrooms in the city. Chrysler also had a unit on the 58th through 60th floors, which served as his residence. However, Chrysler did not use his gym much, instead choosing to stay at the Chrysler Corporation's headquarters in Detroit. Later, the 69th and 70th floors were converted into a dental clinic. In 2005, a \"New York Times\" report found that one of the dentists, Charles Weiss, had operated at the clinic's current rooftop location since 1969. The office still had the suite's original bathroom and gymnasium.\n\nFrom the building's opening until 1945 it contained a observation deck on the 71st floor, called \"Celestial\". For fifty cents visitors could transit its circumference through a corridor with vaulted ceilings painted with celestial motifs and bedecked with small hanging glass planets. The center of the observatory contained the toolbox that Walter P. Chrysler used at the beginning of his career as a mechanic; it was later preserved at the Chrysler Technology Center in Auburn Hills, Michigan. An image of the building resembling a rocket hung above it. According to a contemporary brochure views of up to were possible on a clear day; but the small triangular windows of the observatory created strange angles that made viewing difficult, depressing traffic. When the Empire State Building opened in 1931 with two observatories at a higher elevation the Chrysler observatory lost its clientele.\n\nAfter the observatory closed it was used to house radio and television broadcasting equipment. Since 1986 the old observatory has housed the office of architects Harvey Morse and Cowperwood Interests.\n\nThe Chrysler Building is renowned for, and recognized by, its terraced crown, which is an extension of the main tower. Composed of seven radiating terraced arches, Van Alen's design of the crown is a cruciform groin vault of seven concentric members with transitioning setbacks, mounted one behind another. The entire crown is clad with Nirosta steel, ribbed and riveted in a radiating sunburst pattern with many triangular vaulted windows, transitioning into smaller segments of the seven narrow setbacks of the terraced crown. Due to the curved shape of the dome, the Nirosta sheets had to be measured on site, so most of the work was carried out in workshops on the building's 67th and 75th floors.\n\nAccording to Robinson, \"Its 'dormers', each smaller and higher than the previous one, continue the wedding-cake layering of the building itself. This concept is carried forward from the 61st floor, whose eagle gargoyles echo the treatment of the 31st, to the spire, which extends the concept of 'higher and narrower' forward to infinite height and infinitesimal width. This unique treatment emphasizes the building's height, giving it an other worldly atmosphere reminiscent of the fantastic architecture of Coney Island or the Far East.\"\n\nAbove the 71st floor, the stories of the building are designed mostly for exterior appearance, functioning mainly as landings for the stairway to the spire and do not contain office space. They are very narrow, have low and sloping roofs, and are only used to house radio transmitters and other mechanical and electrical equipment. For example, the 73rd floor houses the motors of the elevators and a water tank, of which are reserved for extinguishing fires.\n\nTelevision station WCBS-TV (Channel 2) originated its transmission from the top of the Chrysler Building in 1938. WCBS-TV transmissions were shifted to the Empire State Building in 1960 in response to competition from RCA's transmitter on that building. For many years WPAT-FM and WTFM (now WKTU) also transmitted from the Chrysler Building, but their move to the Empire State Building by the 1970s ended commercial broadcasting from the structure.\n\nThe crown and spire are illuminated by a combination of fluorescent lights framing the crown's distinctive triangular windows and colored floodlights that face toward the building, allowing it to be lit in a variety of schemes for special occasions. The V-shaped fluorescent \"tube lighting\" – hundreds of 480V 40W bulbs framing 120 window openings – was added in 1981, although it had been part of the original design. Until 1998 the lights were turned off at 2 a.m., but \"New York Observer\" columnist Ron Rosenbaum convinced Tishman Speyer to keep the lights on until 6 a.m. Since 2015, the Chrysler Building and other city skyscrapers have been part of the Audubon Society's Lights Out program, turning off their lights during bird migration seasons.\n\nChrysler Center is the name of the building complex consisting of the Chrysler Building, Chrysler Building East, and the commercial pavilion located between the two, called Chrysler Trylons. In 1998, Tishman Speyer acquired the entire complex and proceeded to renovate it completely over the next two years.\n\nThe Chrysler Building annex at 666 Third Avenue, also known as the Kent Building at the time, was renovated and renamed Chrysler Building East. This International Style building, built in 1952, is high and has 32 floors. The mechanical systems were modernized and the interior was modified. Renowned architect Philip Johnson replaced the glass facade with darker glass and added a extension. After the addition, the total area of this building was .\n\nFinally, a new building, which was also designed by Philip Johnson, was built between the original skyscraper and the annex. This became the Chrysler Trylons, a commercial pavilion three stories high with a retail area of . Its design, consisting of three triangular glass pyramids that intersect each other, was inspired by the triangular windows of the Chrysler Building's crown. The building's design was so complex that a replica was built at Rimouski, Quebec. Johnson designed Chrysler Trylons as \"a monument for 42nd Street [...] to give you the top of the Chrysler Building at street level.\"\n\nAfter these modifications, the total leasable area of the complex was . The total cost of this project was about one hundred million dollars. This renovation has won several awards and commendations, including an Energy Star rating from the Environmental Protection Agency; a LEED Gold designation; and the Skyscraper Museum Outstanding Renovation Award of 2001.\n\nGeorge H. Douglas writes that the building \"remains one of the most appealing and awe-inspiring of skyscrapers\". Architect Le Corbusier called the building \"hot jazz in stone and steel\". Ada Louise Huxtable, an architectural critic, noted that the building had \"a wonderful, decorative, evocative aesthetic\", while another architectural critic, Paul Goldberger, noted the \"compressed, intense energy\" of the lobby, the \"magnificent\" elevators, and the \"magical\" view from the crown. The city's Landmarks Preservation Commission said that the tower \"embodies the romantic essence of the New York City skyscraper\". The travel guide \"Frommer's\" gave the building an \"exceptional\" recommendation, with author Pauline Frommer writing, \"In the Chrysler Building we see the roaring-twenties version of what Alan Greenspan called 'irrational exuberance'—a last burst of corporate headquarter building before stocks succumbed to the thudding crash of 1929.\"\n\nThe Chrysler Building appears in several films set in New York and is widely considered one of the most positively acclaimed buildings in the city. A 1996 survey of New York architects revealed it as their favorite, and the \"New York Times\" described it in 2005 as \"the single most important emblem of architectural imagery on the New York skyline\". In the summer of 2005, the Skyscraper Museum in Lower Manhattan asked 100 architects, builders, critics, engineers, historians, and scholars, among others, to choose their 10 favorites among 25 of the city's towers. The Chrysler Building came in first place, with 90% of respondents placing it on their ballots. In 2007, the building ranked ninth among 150 buildings in the AIA's \"List of America's Favorite Architecture\".\n\nThe Chrysler Building is widely heralded as an Art Deco icon. \"Fodor's New York City 2010\" described the building as being \"one of the great art deco masterpieces\" which \"wins many a New Yorker's vote for the city's most iconic and beloved skyscraper\". \"Frommer's\" states that the Chrysler was \"one of the most impressive Art Deco buildings ever constructed\". \"Insight Guides\" 2016 edition maintains that the Chrysler Building is considered among the city's \"most beautiful\" buildings. Its distinctive profile has inspired similar skyscrapers worldwide including One Liberty Place in Philadelphia and the Al Kazim Towers in Dubai.\n\nWhile seen in many films, the Chrysler Building almost never appears as a main setting in them, prompting architect and author James Sanders to quip it should win \"the Award for Best Supporting Skyscraper\". The building was supposed to be featured in the 1933 film \"King Kong\", but only makes a cameo at the end thanks to its producers opting for the Empire State Building in a central role. The Chrysler Building notably appears in the background of \"The Wiz\" (1978); as the setting of much of \"Q - The Winged Serpent\" (1982); in the initial credits of \"The Shadow of the Witness\" (1987); and during or after apocalyptic events in \"Independence Day\" (1996), \"Armageddon\" (1998), \"Deep Impact\" (1998), \"Godzilla\" (1998), and \"A.I. Artificial Intelligence\" (2001). The building also appears in other films, such as \"Spider-Man\" (2002), \"\" (2007), \"Two Weeks Notice\" (2002), \"The Sorcerer's Apprentice\" (2010) and \"Men in Black 3\" (2012).\n\nThe Chrysler Building is frequently the subject of photographers. In December 1929, Walter Chrysler hired the famed Margaret Bourke-White to capture it for publicity purposes. She took the images from a scaffold high and worked in a studio at ground level until she was evicted in 1934. According to one account, Bourke-White wanted to live in the building for the duration of the photo shoot, but the only person able to do so was the janitor, so she was instead relegated to co-leasing a studio with Time Inc. In 1930, several of her photographs were used in a special report on skyscrapers in the then-new \"Fortune\" magazine. In 1934, Bourke-White's partner Oscar Graubner took a famous photo called \"Margaret Bourke-White atop the Chrysler Building\", which depicts her taking a photo of the city's skyline while sitting on one of the 61st-floor eagle ornaments. On October 5, 1998, Christie's auctioned the photograph for $96,000. In addition, during a January 1931 dance organized by the Society of Beaux-Arts, six architects, including Van Alen, were photographed while wearing costumes resembling the buildings that each architect designed.\n\nThe building is also mentioned in the lyrics of several songs, as well as in the number \"It's the Hard Knock Life\" for the musical \"Annie\".\n\nThe Chrysler Corporation moved into the building as an anchor tenant in 1930, using its space as its divisional headquarters until the 1950s. Time, Inc. and Texaco oil were also original tenants. Needing more office space, Time moved to Rockefeller Center in 1937. \nTexaco relocated to Purchase, New York in 1977 in favor of a more suburban workplace.\n\nNotable modern tenants include:\nBULLET::::- Creative Artists Agency\nBULLET::::- Clyde & Co\nBULLET::::- InterMedia Partners\nBULLET::::- Regus\nBULLET::::- Troutman Sanders\nBULLET::::- YES Network\n\nBULLET::::- Architecture of New York City\nBULLET::::- List of buildings and structures\nBULLET::::- List of New York City Landmarks\nBULLET::::- List of tallest buildings and structures in the world\nBULLET::::- List of tallest buildings in the United States\nBULLET::::- List of tallest buildings in New York City\nBULLET::::- List of tallest freestanding structures in the world\nBULLET::::- National Register of Historic Places listings in New York County, New York\n\n\nBULLET::::- Tishman Speyer website\n"}
{"id": "6790", "url": "https://en.wikipedia.org/wiki?curid=6790", "title": "Cape Breton (disambiguation)", "text": "Cape Breton (disambiguation)\n\nCape Breton Island is an island in the Canadian province of Nova Scotia, in Canada.\n\nCape Breton may also refer to:\nBULLET::::- Cape Breton (Nova Scotia), a cape at the eastern tip of Cape Breton Island, Canada\nBULLET::::- Cape Breton (electoral district)\nBULLET::::- Cape Breton (provincial electoral district)\nBULLET::::- Cape Breton Highlands, a mountain range in the north of Cape Breton Island, Canada\nBULLET::::- Cape Breton Regional Municipality, a regional municipality in Nova Scotia\n\nBULLET::::- HMCS \"Cape Breton\" (ARE 100)\nBULLET::::- HMCS \"Cape Breton\" (K350)\n\nBULLET::::- Cape Breton Breakers, a former team of the defunct National Basketball League\nBULLET::::- Cape Breton Crush, a former team of the defunct Canadian Elite Hockey League\nBULLET::::- Cape Breton Oilers, a former team of the American Hockey League\nBULLET::::- Cape Breton Screaming Eagles, a team in the Quebec Major Junior Hockey League\n\nBULLET::::- Cape Breton Development Corporation\nBULLET::::- Cape Breton University\n\nBULLET::::- Capbreton or Cap Berton, a commune of the Landes département in southwestern France\nBULLET::::- Cape Breton accent\nBULLET::::- Cape Breton and Central Nova Scotia Railway\nBULLET::::- Cape Breton—Canso\nBULLET::::- Cape Breton fiddling\nBULLET::::- Cape Breton Highlands National Park\nBULLET::::- Cape Breton Labour Party\nBULLET::::- Cape Breton North and Victoria\nBULLET::::- Cape Breton Nova\nBULLET::::- \"Cape Breton Post\"\nBULLET::::- Cape Breton South (federal electoral district)\nBULLET::::- Cape Breton South (provincial electoral district)\nBULLET::::- Enterprise Cape Breton Corporation\nBULLET::::- Province of Cape Breton Island, a political movement\n"}
{"id": "6794", "url": "https://en.wikipedia.org/wiki?curid=6794", "title": "Comet Shoemaker–Levy 9", "text": "Comet Shoemaker–Levy 9\n\nComet Shoemaker–Levy 9 (formally designated D/1993 F2) was a comet that broke apart in July 1992 and collided with Jupiter in July 1994, providing the first direct observation of an extraterrestrial collision of Solar System objects. This generated a large amount of coverage in the popular media, and the comet was closely observed by astronomers worldwide. The collision provided new information about Jupiter and highlighted its possible role in reducing space debris in the inner Solar System.\n\nThe comet was discovered by astronomers Carolyn and Eugene M. Shoemaker and David Levy in 1993. Shoemaker–Levy 9 had been captured by Jupiter and was orbiting the planet at the time. It was located on the night of March 24 in a photograph taken with the Schmidt telescope at the Palomar Observatory in California. It was the first active comet observed to be orbiting a planet, and had probably been captured by Jupiter around 20–30 years earlier.\n\nCalculations showed that its unusual fragmented form was due to a previous closer approach to Jupiter in July 1992. At that time, the orbit of Shoemaker–Levy 9 passed within Jupiter's Roche limit, and Jupiter's tidal forces had acted to pull apart the comet. The comet was later observed as a series of fragments ranging up to in diameter. These fragments collided with Jupiter's southern hemisphere between July 16 and 22, 1994 at a speed of approximately (Jupiter's escape velocity) or . The prominent scars from the impacts were more easily visible than the Great Red Spot and persisted for many months.\n\nWhile conducting a program of observations designed to uncover near-Earth objects, the Shoemakers and Levy discovered Comet Shoemaker–Levy 9 on the night of March 24, 1993 in a photograph taken with the Schmidt telescope at the Palomar Observatory in California. The comet was thus a serendipitous discovery, but one that quickly overshadowed the results from their main observing program.\n\nComet Shoemaker–Levy 9 was the ninth periodic comet (a comet whose orbital period is 200 years or less) discovered by the Shoemakers and Levy, hence its name. It was their eleventh comet discovery overall including their discovery of two non-periodic comets, which use a different nomenclature. The discovery was announced in IAU Circular 5725 on March 26, 1993.\n\nThe discovery image gave the first hint that comet Shoemaker–Levy 9 was an unusual comet, as it appeared to show multiple nuclei in an elongated region about 50 arcseconds long and 10 arcseconds wide. Brian G. Marsden of the Central Bureau for Astronomical Telegrams noted that the comet lay only about 4 degrees from Jupiter as seen from Earth, and that although this could of course be a line of sight effect, its apparent motion in the sky suggested that it was physically close to it.\n\nOrbital studies of the new comet soon revealed that it was orbiting Jupiter rather than the Sun, unlike all other comets known at the time. Its orbit around Jupiter was very loosely bound, with a period of about 2 years and an apoapsis (the point in the orbit farthest from the planet) of . Its orbit around the planet was highly eccentric (\"e\" = 0.9986).\n\nTracing back the comet's orbital motion revealed that it had been orbiting Jupiter for some time. It is likely that it was captured from a solar orbit in the early 1970s, although the capture may have occurred as early as the mid-1960s. Several other observers found images of the comet in precovery images obtained before March 24, including Kin Endate from a photograph exposed on March 15, S. Otomo on March 17, and a team led by Eleanor Helin from images on March 19. An image of the comet on a Schmidt photographic plate taken on March 19 was identified on March 21 by M. Lindgren, in a project searching for comets near Jupiter. However, as his team were expecting comets to be inactive or at best exhibit a weak dust coma, and SL9 had a peculiar morphology, its true nature was not recognised until the official announcement 5 days later. No precovery images dating back to earlier than March 1993 have been found. Before the comet was captured by Jupiter, it was probably a short-period comet with an aphelion just inside Jupiter's orbit, and a perihelion interior to the asteroid belt.\n\nThe volume of space within which an object can be said to orbit Jupiter is defined by Jupiter's Hill sphere (also called the Roche sphere). When the comet passed Jupiter in the late 1960s or early 1970s, it happened to be near its aphelion, and found itself slightly within Jupiter's Hill sphere. Jupiter's gravity nudged the comet towards it. Because the comet's motion with respect to Jupiter was very small, it fell almost straight toward Jupiter, which is why it ended up on a Jove-centric orbit of very high eccentricity—that is to say, the ellipse was nearly flattened out.\n\nThe comet had apparently passed extremely close to Jupiter on July 7, 1992, just over above its cloud tops—a smaller distance than Jupiter's radius of , and well within the orbit of Jupiter's innermost moon Metis and the planet's Roche limit, inside which tidal forces are strong enough to disrupt a body held together only by gravity. Although the comet had approached Jupiter closely before, the July 7 encounter seemed to be by far the closest, and the fragmentation of the comet is thought to have occurred at this time. Each fragment of the comet was denoted by a letter of the alphabet, from \"fragment A\" through to \"fragment W\", a practice already established from previously observed broken-up comets.\n\nMore exciting for planetary astronomers was that the best orbital calculations suggested that the comet would pass within of the center of Jupiter, a distance smaller than the planet's radius, meaning that there was an extremely high probability that SL9 would collide with Jupiter in July 1994. Studies suggested that the train of nuclei would plow into Jupiter's atmosphere over a period of about five days.\n\nThe discovery that the comet was likely to collide with Jupiter caused great excitement within the astronomical community and beyond, as astronomers had never before seen two significant Solar System bodies collide. Intense studies of the comet were undertaken, and as its orbit became more accurately established, the possibility of a collision became a certainty. The collision would provide a unique opportunity for scientists to look inside Jupiter's atmosphere, as the collisions were expected to cause eruptions of material from the layers normally hidden beneath the clouds.\n\nAstronomers estimated that the visible fragments of SL9 ranged in size from a few hundred metres (around ) to across, suggesting that the original comet may have had a nucleus up to across—somewhat larger than Comet Hyakutake, which became very bright when it passed close to the Earth in 1996. One of the great debates in advance of the impact was whether the effects of the impact of such small bodies would be noticeable from Earth, apart from a flash as they disintegrated like giant meteors. The most optimistic prediction was that large, asymmetric ballistic fireballs would rise above the limb of Jupiter and into sunlight to be visible from Earth.\nOther suggested effects of the impacts were seismic waves travelling across the planet, an increase in stratospheric haze on the planet due to dust from the impacts, and an increase in the mass of the Jovian ring system. However, given that observing such a collision was completely unprecedented, astronomers were cautious with their predictions of what the event might reveal.\n\nAnticipation grew as the predicted date for the collisions approached, and astronomers trained terrestrial telescopes on Jupiter. Several space observatories did the same, including the Hubble Space Telescope, the ROSAT X-ray-observing satellite, and significantly the \"Galileo\" spacecraft, then on its way to a rendezvous with Jupiter scheduled for 1995. Although the impacts took place on the side of Jupiter hidden from Earth, \"Galileo\", then at a distance of from the planet, was able to see the impacts as they occurred. Jupiter's rapid rotation brought the impact sites into view for terrestrial observers a few minutes after the collisions.\n\nTwo other space probes made observations at the time of the impact: the \"Ulysses\" spacecraft, primarily designed for solar observations, was pointed towards Jupiter from its location away, and the distant \"Voyager 2\" probe, some from Jupiter and on its way out of the Solar System following its encounter with Neptune in 1989, was programmed to look for radio emission in the 1–390 kHz range and make observations with its ultraviolet spectrometer.\n\nThe first impact occurred at 20:13 UTC on July 16, 1994, when fragment A of the nucleus entered Jupiter's southern hemisphere at a speed of about . Instruments on \"Galileo\" detected a fireball that reached a peak temperature of about , compared to the typical Jovian cloudtop temperature of about , before expanding and cooling rapidly to about after 40 seconds. The plume from the fireball quickly reached a height of over . A few minutes after the impact fireball was detected, \"Galileo\" measured renewed heating, probably due to ejected material falling back onto the planet. Earth-based observers detected the fireball rising over the limb of the planet shortly after the initial impact.\n\nDespite published predictions, astronomers had not expected to see the fireballs from the impacts and did not have any idea in advance how visible the other atmospheric effects of the impacts would be from Earth. Observers soon saw a huge dark spot after the first impact. The spot was visible even in very small telescopes, and was about (one Earth radius) across. This and subsequent dark spots were thought to have been caused by debris from the impacts, and were markedly asymmetric, forming crescent shapes in front of the direction of impact.\n\nOver the next six days, 21 distinct impacts were observed, with the largest coming on July 18 at 07:33 UTC when fragment G struck Jupiter. This impact created a giant dark spot over across, and was estimated to have released an energy equivalent to 6,000,000 megatons of TNT (600 times the world's nuclear arsenal). Two impacts 12 hours apart on July 19 created impact marks of similar size to that caused by fragment G, and impacts continued until July 22, when fragment W struck the planet.\n\nObservers hoped that the impacts would give them a first glimpse of Jupiter beneath the cloud tops, as lower material was exposed by the comet fragments punching through the upper atmosphere. Spectroscopic studies revealed absorption lines in the Jovian spectrum due to diatomic sulfur (S) and carbon disulfide (CS), the first detection of either in Jupiter, and only the second detection of S in any astronomical object. Other molecules detected included ammonia (NH) and hydrogen sulfide (HS). The amount of sulfur implied by the quantities of these compounds was much greater than the amount that would be expected in a small cometary nucleus, showing that material from within Jupiter was being revealed. Oxygen-bearing molecules such as sulfur dioxide were not detected, to the surprise of astronomers.\n\nAs well as these molecules, emission from heavy atoms such as iron, magnesium and silicon was detected, with abundances consistent with what would be found in a cometary nucleus. Although a substantial amount of water was detected spectroscopically, it was not as much as predicted beforehand, meaning that either the water layer thought to exist below the clouds was thinner than predicted, or that the cometary fragments did not penetrate deeply enough.\n\nAs predicted beforehand, the collisions generated enormous waves that swept across Jupiter at speeds of and were observed for over two hours after the largest impacts. The waves were thought to be travelling within a stable layer acting as a waveguide, and some scientists thought the stable layer must lie within the hypothesised tropospheric water cloud. However, other evidence seemed to indicate that the cometary fragments had not reached the water layer, and the waves were instead propagating within the stratosphere.\n\nRadio observations revealed a sharp increase in continuum emission at a wavelength of after the largest impacts, which peaked at 120% of the normal emission from the planet. This was thought to be due to synchrotron radiation, caused by the injection of relativistic electrons—electrons with velocities near the speed of light—into the Jovian magnetosphere by the impacts.\n\nAbout an hour after fragment K entered Jupiter, observers recorded auroral emission near the impact region, as well as at the antipode of the impact site with respect to Jupiter's strong magnetic field. The cause of these emissions was difficult to establish due to a lack of knowledge of Jupiter's internal magnetic field and of the geometry of the impact sites. One possible explanation was that upwardly accelerating shock waves from the impact accelerated charged particles enough to cause auroral emission, a phenomenon more typically associated with fast-moving solar wind particles striking a planetary atmosphere near a magnetic pole.\n\nSome astronomers had suggested that the impacts might have a noticeable effect on the Io torus, a torus of high-energy particles connecting Jupiter with the highly volcanic moon Io. High resolution spectroscopic studies found that variations in the ion density, rotational velocity, and temperatures at the time of impact and afterwards were within the normal limits.\n\nVoyager 2 failed to detect anything with calculations showing that the fireballs were just below the craft's limit of detection. Ulysses also failed to detect anything.\n\nSeveral models were devised to compute the density and size of Shoemaker–Levy 9. Its average density was calculated to be about ; the breakup of a much less dense comet would not have resembled the observed string of objects. The size of the parent comet was calculated to be about in diameter. These predictions were among the few that were actually confirmed by subsequent observation.\n\nOne of the surprises of the impacts was the small amount of water revealed compared to prior predictions. Before the impact, models of Jupiter's atmosphere had indicated that the break-up of the largest fragments would occur at atmospheric pressures of anywhere from 30 kilopascals to a few tens of megapascals (from 0.3 to a few hundred bar), with some predictions that the comet would penetrate a layer of water and create a bluish shroud over that region of Jupiter.\n\nAstronomers did not observe large amounts of water following the collisions, and later impact studies found that fragmentation and destruction of the cometary fragments in an 'airburst' probably occurred at much higher altitudes than previously expected, with even the largest fragments being destroyed when the pressure reached , well above the expected depth of the water layer. The smaller fragments were probably destroyed before they even reached the cloud layer.\n\nThe visible scars from the impacts could be seen on Jupiter for many months. They were extremely prominent, and observers described them as even more easily visible than the Great Red Spot. A search of historical observations revealed that the spots were probably the most prominent transient features ever seen on the planet, and that although the Great Red Spot is notable for its striking color, no spots of the size and darkness of those caused by the SL9 impacts had ever been recorded before, or since.\n\nSpectroscopic observers found that ammonia and carbon disulfide persisted in the atmosphere for at least fourteen months after the collisions, with a considerable amount of ammonia being present in the stratosphere as opposed to its normal location in the troposphere.\n\nCounterintuitively, the atmospheric temperature dropped to normal levels much more quickly at the larger impact sites than at the smaller sites: at the larger impact sites, temperatures were elevated over a region wide, but dropped back to normal levels within a week of the impact. At smaller sites, temperatures higher than the surroundings persisted for almost two weeks. Global stratospheric temperatures rose immediately after the impacts, then fell to below pre-impact temperatures 2–3 weeks afterwards, before rising slowly to normal temperatures.\n\nSL9 is not unique in having orbited Jupiter for a time; five comets, (including 82P/Gehrels, 147P/Kushida–Muramatsu, and 111P/Helin–Roman–Crockett) are known to have been temporarily captured by the planet.\nCometary orbits around Jupiter are unstable, as they will be highly elliptical and likely to be strongly perturbed by the Sun's gravity at apojove (the furthest point on the orbit from the planet).\n\nBy far the most massive planet in the Solar System, Jupiter can capture objects relatively frequently, but the size of SL9 makes it a rarity: one post-impact study estimated that comets in diameter impact the planet once in approximately 500 years and those in diameter do so just once in every 6,000 years.\n\nThere is very strong evidence that comets have previously been fragmented and collided with Jupiter and its satellites. During the Voyager missions to the planet, planetary scientists identified 13 crater chains on Callisto and three on Ganymede, the origin of which was initially a mystery. Crater chains seen on the Moon often radiate from large craters, and are thought to be caused by secondary impacts of the original ejecta, but the chains on the Jovian moons did not lead back to a larger crater. The impact of SL9 strongly implied that the chains were due to trains of disrupted cometary fragments crashing into the satellites.\n\nOn July 19, 2009, exactly 15 years after the SL9 impacts, a new black spot about the size of the Pacific Ocean appeared in Jupiter's southern hemisphere. Thermal infrared measurements showed the impact site was warm and spectroscopic analysis detected the production of excess hot ammonia and silica-rich dust in the upper regions of Jupiter's atmosphere. Scientists have concluded that another impact event had occurred, but this time a more compact and strong object, probably a small undiscovered asteroid, was the cause.\n\nThe impact of SL9 highlighted Jupiter's role as a \"cosmic vacuum cleaner\" for the inner Solar System (Jupiter Barrier). The planet's strong gravitational influence leads to many small comets and asteroids colliding with the planet, and the rate of cometary impacts on Jupiter is thought to be between 2,000–8,000 times higher than the rate on Earth.\n\nThe extinction of the dinosaurs at the end of the Cretaceous period is generally thought to have been caused by the Cretaceous–Paleogene impact event (along with its antipodal Siberian traps mantle plume), which created the Chicxulub crater, demonstrating that impacts are a serious threat to life on Earth. Astronomers have speculated that without Jupiter to mop up potential impactors, extinction events might have been more frequent on Earth, and complex life might not have been able to develop. This is part of the argument used in the Rare Earth hypothesis.\n\nIn 2009, it was shown that the presence of a smaller planet at Jupiter's position in the Solar System might increase the impact rate of comets on the Earth significantly. A planet of Jupiter's mass still seems to provide increased protection against asteroids, but the total effect on all orbital bodies within the Solar System is unclear. Computer simulations in 2016 have continued to erode the theory.\n\nBULLET::::- List of Jupiter events\nBULLET::::- Atmosphere of Jupiter\nBULLET::::- 73P/Schwassmann–Wachmann, a near-Earth comet in the process of disintegrating\n\nBULLET::::- Chodas P. W., and Yeomans D. K. (1996), \"The Orbital Motion and Impact Circumstances of Comet Shoemaker–Levy 9\", in \"The Collision of Comet Shoemaker–Levy 9 and Jupiter\", edited by K. S. Noll, P. D. Feldman, and H. A. Weaver, Cambridge University Press, pp. 1–30\nBULLET::::- Chodas P. W. (2002), \"Communication of Orbital Elements to Selden E. Ball, Jr.\" Accessed February 21, 2006\n\nBULLET::::- Comet Shoemaker–Levy 9 FAQ\nBULLET::::- Comet Shoemaker–Levy 9 Photo Gallery\nBULLET::::- Downloadable gif Animation showing time course of impact and size relative to earthsize\nBULLET::::- Comet Shoemaker-Levy 9 Dan Bruton, Texas A&M University\nBULLET::::- Jupiter Swallows Comet Shoemaker Levy 9 APOD: November 5, 2000\nBULLET::::- Comet Shoemaker–Levy Collision with Jupiter\nBULLET::::- National Space Science Data Center information\nBULLET::::- Simulation of the orbit of SL-9 showing the passage that fragmented the comet and the collision 2 years later\nBULLET::::- Interactive space simulator that includes accurate 3D simulation of the Shoemaker Levy 9 collision\nBULLET::::- \"Shoemaker-Levy 9\" Jupiter Impact Observing Campaign Archive at the NASA Planetary Data System, Small Bodies Node\n"}
{"id": "6796", "url": "https://en.wikipedia.org/wiki?curid=6796", "title": "Ceres Brewery", "text": "Ceres Brewery\n\nCeres Brewery was a brewery company located in Århus, Denmark. It was part of Royal Unibrew since 1989. The factories in central Aarhus, was closed in 2008 and the grounds are now being redeveloped into a new neighbourhood of the city, known as CeresByen (The CeresCity).\nCeres made several popular beers and sodas, that still exists today, now being produced by Royal Unibrew A/S.\n\nCeres Brewery was founded by a grocer named Malthe Conrad Lottrup, with help from the chemists A. S. Aagard and Knud Redelien, as the city's seventh brewery. It was named after the Roman goddess Ceres, and its opening was announced in the local newspaper, \"Stiftstidende\", in 1856.\n\nThe brewery was successful, and Lottrup became one of the most prominent people of Aarhus. After ten years, he expanded the brewery, adding a grand new building as his own private residence, where he entertained other local figures.\n\nLottrup's son-in-law, Laurits Christian Meulengracht, took over the running of the brewery after that, and was in charge for nearly thirty years, expanding it further. He then sold it to another brewery, Østjyske Bryggerier A/S.\n\nThe brewery gained more esteem in 1914, when it was made \"Purveyor to the Royal Danish Court\".\n\nIn 2008 the factory closed because the brewery could not live up to the expectations from its owner Royal Unibrew.\n\nBULLET::::- Ceres Brewery\n"}
{"id": "6799", "url": "https://en.wikipedia.org/wiki?curid=6799", "title": "COBOL", "text": "COBOL\n\nCOBOL (; an acronym for \"common business-oriented language\") is a compiled English-like computer programming language designed for business use. It is imperative, procedural and, since 2002, object-oriented. COBOL is primarily used in business, finance, and administrative systems for companies and governments. COBOL is still widely used in legacy applications deployed on mainframe computers, such as large-scale batch and transaction processing jobs. But due to its declining popularity and the retirement of experienced COBOL programmers, programs are being migrated to new platforms, rewritten in modern languages or replaced with software packages. Most programming in COBOL is now purely to maintain existing applications.\n\nCOBOL was designed in 1959 by CODASYL and was partly based on previous programming language design work by Grace Hopper, commonly referred to as \"the (grand)mother of COBOL\". It was created as part of a US Department of Defense effort to create a portable programming language for data processing. It was originally seen as a stopgap, but the Department of Defense promptly forced computer manufacturers to provide it, resulting in its widespread adoption. It was standardized in 1968 and has since been revised four times. Expansions include support for structured and object-oriented programming. The current standard is \"ISO/IEC 1989:2014\".\n\nCOBOL statements have an English-like syntax, which was designed to be self-documenting and highly readable. However, it is verbose and uses over 300 reserved words. In contrast with modern, succinct syntax like , COBOL has a more English-like syntax (in this case, ).\nCOBOL code is split into four \"divisions\" (identification, environment, data and procedure) containing a rigid hierarchy of sections, paragraphs and sentences. Lacking a large standard library, the standard specifies 43 statements, 87 functions and just one class.\n\nAcademic computer scientists were generally uninterested in business applications when COBOL was created and were not involved in its design; it was (effectively) designed from the ground up as a computer language for business, with an emphasis on inputs and outputs, whose only data types were numbers and strings of text.\nCOBOL has been criticized throughout its life, for its verbosity, design process, and poor support for structured programming. These weaknesses result in monolithic and, though intended to be English-like, not easily comprehensible and verbose programs.\n\nIn the late 1950s, computer users and manufacturers were becoming concerned about the rising cost of programming. A 1959 survey had found that in any data processing installation, the programming cost US$800,000 on average and that translating programs to run on new hardware would cost $600,000. At a time when new programming languages were proliferating at an ever-increasing rate, the same survey suggested that if a common business-oriented language were used, conversion would be far cheaper and faster.\n\nOn 8 April 1959, Mary K. Hawes, a computer scientist at Burroughs Corporation, called a meeting of representatives from academia, computer users, and manufacturers at the University of Pennsylvania to organize a formal meeting on common business languages. Representatives included Grace Hopper, inventor of the English-like data processing language FLOW-MATIC, Jean Sammet and Saul Gorn.\n\nAt the April meeting, the group asked the Department of Defense (DoD) to sponsor an effort to create a common business language. The delegation impressed Charles A. Phillips, director of the Data System Research Staff at the DoD, who thought that they \"thoroughly understood\" the DoD's problems. The DoD operated 225 computers, had a further 175 on order and had spent over $200 million on implementing programs to run on them. Portable programs would save time, reduce costs and ease modernization.\n\nPhillips agreed to sponsor the meeting and tasked the delegation with drafting the agenda.\n\nOn 28 and 29 May 1959 (exactly one year after the Zürich ALGOL 58 meeting), a meeting was held at the Pentagon to discuss the creation of a common programming language for business. It was attended by 41 people and was chaired by Phillips. The Department of Defense was concerned about whether it could run the same data processing programs on different computers. FORTRAN, the only mainstream language at the time, lacked the features needed to write such programs.\n\nRepresentatives enthusiastically described a language that could work in a wide variety of environments, from banking and insurance to utilities and inventory control. They agreed unanimously that more people should be able to program and that the new language should not be restricted by the limitations of contemporary technology. A majority agreed that the language should make maximal use of English, be capable of change, be machine-independent and be easy to use, even at the expense of power.\n\nThe meeting resulted in the creation of a steering committee and short-, intermediate- and long-range committees. The short-range committee was given to September (three months) to produce specifications for an interim language, which would then be improved upon by the other committees. Their official mission, however, was to identify the strengths and weaknesses of existing programming languages and did not explicitly direct them to create a new language.\nThe deadline was met with disbelief by the short-range committee.\nOne member, Betty Holberton, described the three-month deadline as \"gross optimism\" and doubted that the language really would be a stopgap.\n\nThe steering committee met on 4 June and agreed to name the entire activity as the \"Committee on Data Systems Languages\", or CODASYL, and to form an executive committee.\n\nThe short-range committee was made up of members representing six computer manufacturers and three government agencies. The six computer manufacturers were Burroughs Corporation, IBM, Minneapolis-Honeywell (Honeywell Labs), RCA, Sperry Rand, and Sylvania Electric Products. The three government agencies were the US Air Force, the Navy's David Taylor Model Basin, and the National Bureau of Standards (now the National Institute of Standards and Technology). The committee was chaired by Joseph Wegstein of the US National Bureau of Standards. Work began by investigating data description, statements, existing applications and user experiences.\n\nThe committee mainly examined the FLOW-MATIC, AIMACO and COMTRAN programming languages.\nThe FLOW-MATIC language was particularly influential because it had been implemented and because AIMACO was a derivative of it with only minor changes.\nFLOW-MATIC's inventor, Grace Hopper, also served as a technical adviser to the committee. FLOW-MATIC's major contributions to COBOL were long variable names, English words for commands and the separation of data descriptions and instructions.\n\nIBM's COMTRAN language, invented by Bob Bemer, was regarded as a competitor to FLOW-MATIC by a short-range committee made up of colleagues of Grace Hopper.\nSome of its features were not incorporated into COBOL so that it would not look like IBM had dominated the design process, and Jean Sammet said in 1981 that there had been a \"strong anti-IBM bias\" from some committee members (herself included).\nIn one case, after Roy Goldfinger, author of the COMTRAN manual and intermediate-range committee member, attended a subcommittee meeting to support his language and encourage the use of algebraic expressions, Grace Hopper sent a memo to the short-range committee reiterating Sperry Rand's efforts to create a language based on English.\nIn 1980, Grace Hopper commented that \"COBOL 60 is 95% FLOW-MATIC\" and that COMTRAN had had an \"extremely small\" influence. Furthermore, she said that she would claim that work was influenced by both FLOW-MATIC and COMTRAN only to \"keep other people happy [so they] wouldn't try to knock us out\".\nFeatures from COMTRAN incorporated into COBOL included formulas, the clause, an improved codice_1 statement, which obviated the need for GO TOs, and a more robust file management system.\n\nThe usefulness of the committee's work was subject of great debate. While some members thought the language had too many compromises and was the result of design by committee, others felt it was better than the three languages examined. Some felt the language was too complex; others, too simple.\nControversial features included those some considered useless or too advanced for data processing users. Such features included boolean expressions, formulas and table \"\" (indices). Another point of controversy was whether to make keywords context-sensitive and the effect that would have on readability. Although context-sensitive keywords were rejected, the approach was later used in PL/I and partially in COBOL from 2002. Little consideration was given to interactivity, interaction with operating systems (few existed at that time) and functions (thought of as purely mathematical and of no use in data processing).\n\nThe specifications were presented to the Executive Committee on 4 September. They fell short of expectations: Joseph Wegstein noted that \"it contains rough spots and requires some additions\", and Bob Bemer later described them as a \"hodgepodge\". The subcommittee was given until December to improve it.\n\nAt a mid-September meeting, the committee discussed the new language's name. Suggestions included \"BUSY\" (Business System), \"INFOSYL\" (Information System Language) and \"COCOSYL\" (Common Computer Systems Language). The name \"COBOL\" was suggested by Bob Bemer.\n\nIn October, the intermediate-range committee received copies of the FACT language specification created by Roy Nutt. Its features impressed the committee so much that they passed a resolution to base COBOL on it.\nThis was a blow to the short-range committee, who had made good progress on the specification. Despite being technically superior, FACT had not been created with portability in mind or through manufacturer and user consensus. It also lacked a demonstrable implementation, allowing supporters of a FLOW-MATIC-based COBOL to overturn the resolution. RCA representative Howard Bromberg also blocked FACT, so that RCA's work on a COBOL implementation would not go to waste.\n\nIt soon became apparent that the committee was too large for any further progress to be made quickly. A frustrated Howard Bromberg bought a $15 tombstone with \"COBOL\" engraved on it and sent it to Charles Phillips to demonstrate his displeasure.\nA sub-committee was formed to analyze existing languages and was made up of six individuals:\n\nBULLET::::- William Selden and Gertrude Tierney of IBM,\nBULLET::::- Howard Bromberg and Howard Discount of RCA,\nBULLET::::- Vernon Reeves and Jean E. Sammet of Sylvania Electric Products.\n\nThe sub-committee did most of the work creating the specification, leaving the short-range committee to review and modify their work before producing the finished specification.\n\nThe specifications were approved by the Executive Committee on 8 January 1960, and sent to the government printing office, which printed these as \"COBOL 60\". The language's stated objectives were to allow efficient, portable programs to be easily written, to allow users to move to new systems with minimal effort and cost, and to be suitable for inexperienced programmers.\nThe CODASYL Executive Committee later created the COBOL Maintenance Committee to answer questions from users and vendors and to improve and expand the specifications.\n\nDuring 1960, the list of manufacturers planning to build COBOL compilers grew. By September, five more manufacturers had joined CODASYL (Bendix, Control Data Corporation, General Electric (GE), National Cash Register and Philco), and all represented manufacturers had announced COBOL compilers. GE and IBM planned to integrate COBOL into their own languages, GECOM and COMTRAN, respectively. In contrast, International Computers and Tabulators planned to replace their language, CODEL, with COBOL.\n\nMeanwhile, RCA and Sperry Rand worked on creating COBOL compilers. The first COBOL program ran on 17 August on an RCA 501.\nOn 6 and 7 December, the same COBOL program (albeit with minor changes) ran on an RCA computer and a Remington-Rand Univac computer, demonstrating that compatibility could be achieved.\n\nThe relative influences of which languages were used continues to this day in the recommended advisory printed in all COBOL reference manuals:\nMany logical flaws were found in \"COBOL 60\", leading GE's Charles Katz to warn that it could not be interpreted unambiguously. A reluctant short-term committee enacted a total cleanup and, by March 1963, it was reported that COBOL's syntax was as definable as ALGOL's, although semantic ambiguities remained.\n\nEarly COBOL compilers were primitive and slow. A 1962 US Navy evaluation found compilation speeds of 3–11 statements per minute. By mid-1964, they had increased to 11–1000 statements per minute. It was observed that increasing memory would drastically increase speed and that compilation costs varied wildly: costs per statement were between $0.23 and $18.91.\n\nIn late 1962, IBM announced that COBOL would be their primary development language and that development of COMTRAN would cease.\n\nThe COBOL specification was revised three times in the five years after its publication.\nCOBOL-60 was replaced in 1961 by COBOL-61. This was then replaced by the COBOL-61 Extended specifications in 1963, which introduced the sort and report writer facilities.\nThe added facilities corrected flaws identified by Honeywell in late 1959 in a letter to the short-range committee.\nCOBOL Edition 1965 brought further clarifications to the specifications and introduced facilities for handling mass storage files and tables.\n\nEfforts began to standardize COBOL to overcome incompatibilities between versions. In late 1962, both ISO and the United States of America Standards Institute (now ANSI) formed groups to create standards. ANSI produced \"USA Standard COBOL X3.23\" in August 1968, which became the cornerstone for later versions. This version was known as American National Standard (ANS) COBOL and was adopted by ISO in 1972.\n\nBy 1970, COBOL had become the most widely used programming language in the world.\n\nIndependently of the ANSI committee, the CODASYL Programming Language Committee was working on improving the language. They described new versions in 1968, 1969, 1970 and 1973, including changes such as new inter-program communication, debugging and file merging facilities as well as improved string-handling and library inclusion features.\nAlthough CODASYL was independent of the ANSI committee, the \"CODASYL Journal of Development\" was used by ANSI to identify features that were popular enough to warrant implementing.\nThe Programming Language Committee also liaised with ECMA and the Japanese COBOL Standard committee.\n\nThe Programming Language Committee was not well-known, however. The vice-president, William Rinehuls, complained that two-thirds of the COBOL community did not know of the committee's existence. It was also poor, lacking the funds to make public documents, such as minutes of meetings and change proposals, freely available.\n\nIn 1974, ANSI published a revised version of (ANS) COBOL, containing new features such as file organizations, the statement and the segmentation module.\nDeleted features included the statement, the statement (which was replaced by ) and the implementer-defined random access module (which was superseded by the new sequential and relative I/O modules). These made up 44 changes, which rendered existing statements incompatible with the new standard.\nThe report writer was slated to be removed from COBOL, but was reinstated before the standard was published. ISO later adopted the updated standard in 1978.\n\nIn June 1978, work began on revising COBOL-74. The proposed standard (commonly called COBOL-80) differed significantly from the previous one, causing concerns about incompatibility and conversion costs. In January 1981, Joseph T. Brophy, Senior Vice-President of Travelers Insurance, threatened to sue the standard committee because it was not upwards compatible with COBOL-74. Mr. Brophy described previous conversions of their 40-million-line code base as \"non-productive\" and a \"complete waste of our programmer resources\".\nLater that year, the Data Processing Management Association (DPMA) said it was \"strongly opposed\" to the new standard, citing \"prohibitive\" conversion costs and enhancements that were \"forced on the user\".\n\nDuring the first public review period, the committee received 2,200 responses, of which 1,700 were negative form letters.\nOther responses were detailed analyses of the effect COBOL-80 would have on their systems; conversion costs were predicted to be at least 50 cents per line of code. Fewer than a dozen of the responses were in favor of the proposed standard.\n\nISO TC97-SC5 installed in 1979 the international COBOL Experts Group, on initiative of Wim Ebbinkhuijsen. The group consisted of COBOL experts from many countries, including the USA. Its goal was to achieve mutual understanding and respect between ANSI and the rest of the world with regard to the need of new COBOL features. After three years, ISO changed the status of the group to a formal Working Group: WG4 COBOL. The group took primary ownership and development of the COBOL standard, where ANSI did most of the proposals.\n\nIn 1983, the DPMA withdrew its opposition to the standard, citing the responsiveness of the committee to public concerns. In the same year, a National Bureau of Standards study concluded that the proposed standard would present few problems. A year later, a COBOL-80 compiler was released to DEC VAX users, who noted that conversion of COBOL-74 programs posed few problems. The new codice_2 statement and inline codice_3 were particularly well received and improved productivity, thanks to simplified control flow and debugging.\n\nThe second public review drew another 1,000 (mainly negative) responses, while the last drew just 25, by which time many concerns had been addressed.\n\nIn 1985, the ISO Working Group 4 accepted the then-version of the ANSI proposed standard, made several changes and set it as the new ISO standard COBOL 85. It was published in late 1985.\n\nSixty features were changed or deprecated and many were added, such as:\n\nBULLET::::- Scope terminators (codice_4, codice_5, codice_6, etc.)\nBULLET::::- Nested subprograms\nBULLET::::- codice_7, a no-operation statement\nBULLET::::- codice_2, a switch statement\nBULLET::::- codice_9, a statement that can set groups of data to their default values\nBULLET::::- Inline codice_3 loop bodies – previously, loop bodies had to be specified in a separate procedure\nBULLET::::- Reference modification, which allows access to substrings\nBULLET::::- I/O status codes.\n\nThe new standard was adopted by all national standard bodies, including ANSI.\n\nTwo amendments followed in 1989 and 1993, the first introducing intrinsic functions and the other providing corrections.,\n\nIn 1997, Gartner Group estimated that there were a total of 200 billion lines of COBOL in existence, which ran 80% of all business programs.\n\nIn the early 1990s, work began on adding object-orientation in the next full revision of COBOL. Object-oriented features were taken from C++ and Smalltalk.\nThe initial estimate was to have this revision completed by 1997, and an ISO Committee Draft (CD) was available by 1997. Some vendors (including Micro Focus, Fujitsu, and IBM) introduced object-oriented syntax based on drafts of the full revision. The final approved ISO standard was approved and published in late 2002.\n\nFujitsu/GTSoftware, Micro Focus and RainCode introduced object-oriented COBOL compilers targeting the .NET Framework.\n\nThere were many other new features, many of which had been in the \"CODASYL COBOL Journal of Development\" since 1978 and had missed the opportunity to be included in COBOL-85. These other features included:\n\nBULLET::::- Free-form code\nBULLET::::- User-defined functions\nBULLET::::- Recursion\nBULLET::::- Locale-based processing\nBULLET::::- Support for extended character sets such as Unicode\nBULLET::::- Floating-point and binary data types (until then, binary items were truncated based on their declaration's base-10 specification)\nBULLET::::- Portable arithmetic results\nBULLET::::- Bit and boolean data types\nBULLET::::- Pointers and syntax for getting and freeing storage\nBULLET::::- The for text-based user interfaces\nBULLET::::- The facility\nBULLET::::- Improved interoperability with other programming languages and framework environments such as .NET and Java.\n\nThree corrigenda were published for the standard: two in 2006 and one in 2009.\n\nBetween 2003 and 2009, three technical reports were produced describing object finalization, XML processing and collection classes for COBOL.\n\nCOBOL 2002 suffered from poor support: no compilers completely supported the standard. Micro Focus found that it was due to a lack of user demand for the new features and due to the abolition of the NIST test suite, which had been used to test compiler conformance. The standardization process was also found to be slow and under-resourced.\n\nCOBOL 2014 includes the following changes:\nBULLET::::- Portable arithmetic results have been replaced by IEEE 754 data types\nBULLET::::- Major features have been made optional, such as the codice_11 facility, the report writer and the screen-handling facility.\nBULLET::::- Method overloading\nBULLET::::- Dynamic capacity tables (a feature dropped from the draft of COBOL 2002)\n\nCOBOL programs are used globally in governments and businesses and are running on diverse operating systems such as z/OS, z/VSE, VME, Unix, OpenVMS and Windows. In 1997, the Gartner Group reported that 80% of the world's business ran on COBOL with over 200 billion lines of code and 5 billion lines more being written annually.\n\nNear the end of the 20th century, the year 2000 problem (Y2K) was the focus of significant COBOL programming effort, sometimes by the same programmers who had designed the systems decades before. The particular level of effort required to correct COBOL code has been attributed to the large amount of business-oriented COBOL, as business applications use dates heavily, and to fixed-length data fields. After the clean-up effort put into these programs for Y2K, a 2003 survey found that many remained in use.\nThe authors said that the survey data suggest \"a gradual decline in the importance of Cobol in application development over the [following] 10 years unless ... integration with other languages and technologies can be adopted\".\n\nIn 2006 and 2012, \"Computerworld\" surveys found that over 60% of organizations used COBOL (more than C++ and Visual Basic .NET) and that for half of those, COBOL was used for the majority of their internal software. 36% of managers said they planned to migrate from COBOL, and 25% said they would like to if it was cheaper. Instead, some businesses have migrated their systems from expensive mainframes to cheaper, more modern systems, while maintaining their COBOL programs.\n\nTestimony before the House of Representatives in 2016 indicated that COBOL is still in use by many federal agencies.\n\nCOBOL has an English-like syntax, which is used to describe nearly everything in a program. For example, a condition can be expressed as   or more concisely as    or  . More complex conditions can be \"abbreviated\" by removing repeated conditions and variables. For example,    can be shortened to . As a consequence of this English-like syntax, COBOL has over 300 keywords. Some of the keywords are simple alternative or pluralized spellings of the same word, which provides for more English-like statements and clauses; e.g., the and keywords can be used interchangeably, as can and , and and .\n\nEach COBOL program is made up of four basic lexical items: words, literals, picture character-strings (see ) and separators. Words include reserved words and user-defined identifiers. They are up to 31 characters long and may include letters, digits, hyphens and underscores. Literals include numerals (e.g. ) and strings (e.g. ). Separators include the space character and commas and semi-colons followed by a space.\n\nA COBOL program is split into four divisions: the identification division, the environment division, the data division and the procedure division. The identification division specifies the name and type of the source element and is where classes and interfaces are specified. The environment division specifies any program features that depend on the system running it, such as files and character sets. The data division is used to declare variables and parameters. The procedure division contains the program's statements. Each division is sub-divided into sections, which are made up of paragraphs.\n\nCOBOL's syntax is usually described with a unique metalanguage using braces, brackets, bars and underlining. The metalanguage was developed for the original COBOL specifications. Although Backus–Naur form did exist at the time, the committee had not heard of it.\n\n+ Elements of COBOL's metalanguage\n! Element\n! Appearance\n! Function\n\nAs an example, consider the following description of an codice_12 statement:\n\nformula_1\n\nThis description permits the following variants:\nADD 1 TO x\nADD 1, a, b TO x ROUNDED, y, z ROUNDED\n\nADD a, b TO c\nEND-ADD\n\nADD a TO b\nCOBOL can be written in two formats: fixed (the default) or free. In fixed-format, code must be aligned to fit in certain areas (a hold-over from using punched cards). Until COBOL 2002, these were:\n\n! Name\n! Column(s)\n! Usage\nBULLET::::- codice_13 – Comment line\nBULLET::::- codice_14 – Comment line that will be printed on a new page of a source listing\nBULLET::::- codice_15 – Continuation line, where words or literals from the previous line are continued\nBULLET::::- codice_16 – Line enabled in debugging mode, which is otherwise ignored\n Area A\n\nIn COBOL 2002, Areas A and B were merged to form the program-text area, which now ends at an implementor-defined column.\n\nCOBOL 2002 also introduced free-format code. Free-format code can be placed in any column of the file, as in newer programming languages. Comments are specified using codice_19, which can be placed anywhere and can also be used in fixed-format source code. Continuation lines are not present, and the codice_20 directive replaces the codice_14 indicator.\n\nThe identification division identifies the following code entity and contains the definition of a class or interface.\n\nClasses and interfaces have been in COBOL since 2002. Classes have factory objects, containing class methods and variables, and instance objects, containing instance methods and variables. Inheritance and interfaces provide polymorphism. Support for generic programming is provided through parameterized classes, which can be instantiated to use any class or interface. Objects are stored as references which may be restricted to a certain type. There are two ways of calling a method: the statement, which acts similarly to , or through inline method invocation, which is analogous to using functions.\nBULLET::::- > These are equivalent.\nINVOKE my-class \"foo\" RETURNING var\nMOVE my-class::\"foo\" TO var *> Inline method invocation\nCOBOL does not provide a way to hide methods. Class data can be hidden, however, by declaring it without a clause, which leaves the user with no way to access it. Method overloading was added in COBOL 2014.\n\nThe environment division contains the configuration section and the input-output section. The configuration section is used to specify variable features such\nas currency signs, locales and character sets. The input-output section contains file-related information.\n\nCOBOL supports three file formats, or ': sequential, indexed and relative. In sequential files, records are contiguous and must be traversed sequentially, similarly to a linked list. Indexed files have one or more indexes which allow records to be randomly accessed and which can be sorted on them. Each record must have a unique key, but other, ', record keys need not be unique. Implementations of indexed files vary between vendors, although common implementations, such as C‑ISAM and VSAM, are based on IBM's ISAM. Relative files, like indexed files, have a unique record key, but they do not have alternate keys. A relative record's key is its ordinal position; for example, the 10th record has a key of 10. This means that creating a record with a key of 5 may require the creation of (empty) preceding records. Relative files also allow for both sequential and random access.\n\nA common non-standard extension is the \" organization, used to process text files. Records in a file are terminated by a newline and may be of varying length.\n\nThe data division is split into six sections which declare different items: the file section, for file records; the working-storage section, for static variables; the local-storage section, for automatic variables; the linkage section, for parameters and the return value; the report section and the screen section, for text-based user interfaces.\n\nData items in COBOL are declared hierarchically through the use of level-numbers which indicate if a data item is part of another. An item with a higher level-number is subordinate to an item with a lower one. Top-level data items, with a level-number of 1, are called '. Items that have subordinate aggregate data are called '; those that do not are called \". Level-numbers used to describe standard data items are between 1 and 49.\nIn the above example, elementary item and group item are subordinate to the record , while elementary items , , and are part of the group item .\n\nSubordinate items can be disambiguated with the (or ) keyword. For example, consider the example code above along with the following example:\n\nThe names , , and are ambiguous by themselves, since more than one data item is defined with those names. To specify a particular data item, for instance one of the items contained within the group, the programmer would use (or the equivalent ). (This syntax is similar to the \"dot notation\" supported by most contemporary languages.)\n\nA level-number of 66 is used to declare a re-grouping of previously defined items, irrespective of how those items are structured. This data level, also referred to by the associated , is rarely used and, circa 1988, was usually found in old programs. Its ability to ignore the hierarchical and logical structure data meant its use was not recommended and many installations forbade its use.\n\nA 77 level-number indicates the item is stand-alone, and in such situations is equivalent to the level-number 01. For example, the following code declares two 77-level data items, and , which are non-group data items that are independent of (not subordinate to) any other data items:\n\nAn 88 level-number declares a \" (a so-called 88-level) which is true when its parent data item contains one of the values specified in its clause. For example, the following code defines two 88-level condition-name items that are true or false depending on the current character data value of the data item. When the data item contains a value of , the condition-name is true, whereas when it contains a value of or , the condition-name is true. If the data item contains some other value, both of the condition-names are false.\n\nStandard COBOL provides the following data types:\n\n! Data type\n! Sample declaration\n! Notes\n\nType safety is variable in COBOL. Numeric data is converted between different representations and sizes silently and alphanumeric data can be placed in any data item that can be stored as a string, including numeric and group data. In contrast, object references and pointers may only be assigned from items of the same type and their values may be restricted to a certain type.\n\nA (or ) clause is a string of characters, each of which represents a portion of the data item and what it may contain. Some picture characters specify the type of the item and how many characters or digits it occupies in memory. For example, a indicates a decimal digit, and an indicates that the item is signed. Other picture characters (called ' and ' characters) specify how an item should be formatted. For example, a series of characters define character positions as well as how a leading sign character is to be positioned within the final character data; the rightmost non-numeric character will contain the item's sign, while other character positions corresponding to a to the left of this position will contain a space. Repeated characters can be specified more concisely by specifying a number in parentheses after a picture character; for example, is equivalent to . Picture specifications containing only digit () and sign () characters define purely ' data items, while picture specifications containing alphabetic () or alphanumeric () characters define ' data items. The presence of other formatting characters define ' or ' data items.\n\n+ Examples\n! clause\n! Value in\n! Value out\n\nThe clause declares the format data is stored in. Depending on the data type, it can either complement or be used instead of a clause. While it can be used to declare pointers and object references, it is mostly geared towards specifying numeric types. These numeric formats are:\n\nBULLET::::- Binary, where a minimum size is either specified by the codice_25 clause or by a codice_26 clause such as codice_27.\nBULLET::::- , where data may be stored in whatever format the implementation provides; often equivalent to\nBULLET::::- , the default format, where data is stored as a string\nBULLET::::- Floating-point, in either an implementation-dependent format or according to IEEE 754.\nBULLET::::- , where data is stored as a string using an extended character set\nBULLET::::- , where data is stored in the smallest possible decimal format (typically packed binary-coded decimal)\n\nThe report writer is a declarative facility for creating reports. The programmer need only specify the report layout and the data required to produce it, freeing them from having to write code to handle things like page breaks, data formatting, and headings and footings.\n\nReports are associated with report files, which are files which may only be written to through report writer statements.\nEach report is defined in the report section of the data division. A report is split into report groups which define the report's headings, footings and details. Reports work around hierarchical \". Control breaks occur when a key variable changes it value; for example, when creating a report detailing customers' orders, a control break could occur when the program reaches a different customer's orders. Here is an example report description for a report which gives a salesperson's sales and which warns of any invalid records:\n\nThe above report description describes the following layout:\nFour statements control the report writer: , which prepares the report writer for printing; , which prints a report group; , which suppresses the printing of a report group; and , which terminates report processing. For the above sales report example, the procedure division might look like this:\n\nUse of the Report Writer facility tended to vary considerably; some organizations used it extensively and some not at all. In addition, implementations of Report Writer ranged in quality, with those at the lower end sometimes using excessive amounts of memory at runtime.\n\nThe sections and paragraphs in the procedure division (collectively called procedures) can be used as labels and as simple subroutines. Unlike in other divisions, paragraphs do not need to be in sections.\nExecution goes down through the procedures of a program until it is terminated.\nTo use procedures as subroutines, the verb is used.\n\nA statement somewhat resembles a procedure call in a modern language in the sense that execution returns to the code following the statement at the end of the called code; however, it does not provide any mechanism for parameter passing or for returning a result value. If a subroutine is invoked using a simple statement like , then control returns at the end of the called procedure. However, is unusual in that it may be used to call a range spanning a sequence of several adjacent procedures. This is done with the construct:\nPROCEDURE so-and-so.\nALPHA.\nBETA.\nGAMMA.\nThe output of this program will be: \"A A B C\".\n\nThe reason is that COBOL, rather than a \"return address\", operates with what may be called a continuation address. When control flow reaches the end of any procedure, the continuation address is looked up and control is transferred to that address. Before the program runs, the continuation address for every procedure is initialised to the start address of the procedure that comes next in the program text so that, if no statements happen, control flows from top to bottom through the program. But when a statement executes, it modifies the continuation address of the called procedure (or the last procedure of the called range, if was used), so that control will return to the call site at the end. The original value is saved and is restored afterwards, but there is only one storage position. If two nested invocations operate on overlapping code, they may interfere which each other's management of the continuation address in several ways.\n\nThe following example (taken from Veerman/Verhoeven, 2006) illustrates the problem:\nLABEL1.\nLABEL2.\nLABEL3.\nLABEL4.\nOne might expect that the output of this program would be \"1 2 3 4 3\": After displaying \"2\", the second causes \"3\" and \"4\" to be displayed, and then the first invocation continues on with \"3\". In traditional COBOL implementations, this is not the case. Rather, the first statement sets the continuation address at the end of so that it will jump back to the call site inside . The second statement sets the return at the end of but does not modify the continuation address of , expecting it to be the default continuation. Thus, when the inner invocation arrives at the end of , it jumps back to the outer statement, and the program stops having printed just \"1 2 3\". On the other hand, in some COBOL implementations like the open-source TinyCOBOL compiler, the two statements do not interfere with each other and the output is indeed \"1 2 3 4 3\". Therefore, the behaviour in such cases is not only (perhaps) surprising, it is also not portable.\n\nA special consequence of this limitation is that cannot be used to write recursive code. Another simple example to illustrate this (slightly simplified from Veerman/Verhoeven, 2006):\nLABEL.\nOne might expect that the output is \"1 2 3 END END END\", and in fact that is what some COBOL compilers will produce. But some compilers, like IBM COBOL, will produce code that prints \"1 2 3 END END END END ...\" and so on, printing \"END\" over and over in an endless loop. Since there is limited space to store backup continuation addresses, the backups get overwritten in the course of recursive invocations, and all that can be restored is the jump back to .\n\nCOBOL 2014 has 47 statements (also called \"), which can be grouped into the following broad categories: control flow, I/O, data manipulation and the report writer. The report writer statements are covered in the report writer section.\n\nCOBOL's conditional statements are and . is a switch-like statement with the added capability of evaluating multiple values and conditions. This can be used to implement decision tables. For example, the following might be used to control a CNC lathe: \nEVALUATE TRUE ALSO desired-speed ALSO current-speed\nEND-EVALUATE\nThe statement is used to define loops which are executed a condition is true (not true, which is more common in other languages). It is also used to call procedures or ranges of procedures (see the procedures section for more details). and call subprograms and methods, respectively. The name of the subprogram/method is contained in a string which may be a literal or a data item. Parameters can be passed by reference, by content (where a copy is passed by reference) or by value (but only if a prototype is available).\n\nThe statement is a return statement and the statement stops the program. The statement has six different formats: it can be used as a return statement, a break statement, a continue statement, an end marker or to leave a procedure.\n\nExceptions are raised by a statement and caught with a handler, or \", defined in the portion of the procedure division. Declaratives are sections beginning with a statement which specify the errors to handle. Exceptions can be names or objects. is used in a declarative to jump to the statement after the one that raised the exception or to a procedure outside the . Unlike other languages, uncaught exceptions may not terminate the program and the program can proceed unaffected.\n\nFile I/O is handled by the self-describing , , , and statements along with a further three: , which updates a record; , which selects subsequent records to access by finding a record with a certain key; and , which releases a lock on the last record accessed.\n\nUser interaction is done using and .\n\nThe following verbs manipulate data:\nBULLET::::- , which sets data items to their default values.\nBULLET::::- , which assigns values to data items ; \"MOVE CORRESPONDING\" assigns corresponding like-named fields.\nBULLET::::- , which has 15 formats: it can modify indices, assign object references and alter table capacities, among other functions.\nBULLET::::- , , , , and , which handle arithmetic (with assigning the result of a formula to a variable).\nBULLET::::- and , which handle dynamic memory.\nBULLET::::- , which validates and distributes data as specified in an item's description in the data division.\nBULLET::::- and , which concatenate and split strings, respectively.\nBULLET::::- , which tallies or replaces instances of specified substrings within a string.\nBULLET::::- , which searches a table for the first entry satisfying a condition.\n\nFiles and tables are sorted using and the verb merges and sorts files. The verb provides records to sort and retrieves sorted records in order.\n\nSome statements, such as and , may themselves contain statements. Such statements may be terminated in two ways: by a period (\"\"), which terminates \"all\" unterminated statements contained, or by a scope terminator, which terminates the nearest matching open statement.\nBULLET::::- > Terminator period (\"implicit termination\")\nIF invalid-record\n\nBULLET::::- > Scope terminators (\"explicit termination\")\nIF invalid-record\nEND-IF\nNested statements terminated with a period are a common source of bugs. For example, examine the following code:\nIF x\nHere, the intent is to display codice_28 and codice_29 if condition codice_30 is true. However, codice_29 will be displayed whatever the value of codice_30 because the codice_1 statement is terminated by an erroneous period after .\n\nAnother bug is a result of the dangling else problem, when two codice_1 statements can associate with an codice_35.\nIF x\nELSE\nIn the above fragment, the codice_35 associates with the    statement instead of the    statement, causing a bug. Prior to the introduction of explicit scope terminators, preventing it would require    to be placed after the inner codice_1.\n\nThe original (1959) COBOL specification supported the infamous    statement, for which many compilers generated self-modifying code. codice_38 and codice_39 are procedure labels, and the single    statement in procedure codice_38 executed after such an statement means    instead. Many compilers still support it,\nbut it was deemed obsolete in the COBOL 1985 standard and deleted in 2002.\n\nThe statement was poorly regarded because it undermined \"locality of context\" and made a program's overall logic difficult to comprehend. As textbook author Daniel D. McCracken wrote in 1976, when \"someone who has never seen the program before must become familiar with it as quickly as possible, sometimes under critical time pressure because the program has failed ... the sight of a GO TO statement in a paragraph by itself, signaling as it does the existence of an unknown number of ALTER statements at unknown locations throughout the program, strikes fear in the heart of the bravest programmer.\"\n\nA \"Hello, world\" program in COBOL:\n\nWhen the – now famous – \"Hello, World!\" program example in \"The C Programming Language\" was first published in 1978 a similar mainframe COBOL program sample would have been submitted through JCL, very likely using a punch card reader, and 80 column punch cards. The listing below, \"with an empty DATA DIVISION\", was tested using GNU/Linux and the System/370 Hercules emulator running MVS 3.8J. The JCL, written in July 2015, is derived from the Hercules tutorials and samples hosted by Jay Moseley. In keeping with COBOL programming of that era, HELLO, WORLD is displayed in all capital letters.\n//COBUCLG JOB (001),'COBOL BASE TEST', 00010000\n// CLASS=A,MSGCLASS=A,MSGLEVEL=(1,1) 00020000\n//BASETEST EXEC COBUCLG 00030000\n//COB.SYSIN DD * 00040000\n//LKED.SYSLIB DD DSNAME=SYS1.COBLIB,DISP=SHR 00190000\n// DD DSNAME=SYS1.LINKLIB,DISP=SHR 00200000\n//GO.SYSPRINT DD SYSOUT=A 00210000\n// 00220000\nAfter submitting the JCL, the MVS console displayed:\n\"Line 10 of the console listing above is highlighted for effect, the highlighting is not part of the actual console output\".\n\nThe associated compiler listing generated over four pages of technical detail and job run information, for the single line of output from the 14 lines of COBOL.\n\nIn the 1970s, adoption of the structured programming paradigm was becoming increasingly widespread. Edsger Dijkstra, a preeminent computer scientist, wrote a letter to the editor of Communications of the ACM, published 1975 entitled \"How do we tell truths that might hurt?\", in which he was critical of COBOL and several other contemporary languages; remarking that \"the use of COBOL cripples the mind\".\nIn a published dissent to Dijkstra's remarks, the computer scientist Howard E. Tompkins claimed that unstructured COBOL tended to be \"written by programmers that have never had the benefit of structured COBOL taught well\", arguing that the issue was primarily one of training.\n\nOne cause of spaghetti code was the statement. Attempts to remove s from COBOL code, however, resulted in convoluted programs and reduced code quality. s were largely replaced by the statement and procedures, which promoted modular programming and gave easy access to powerful looping facilities. However, could only be used with procedures so loop bodies were not located where they were used, making programs harder to understand.\n\nCOBOL programs were infamous for being monolithic and lacking modularization.\nCOBOL code could only be modularized through procedures, which were found to be inadequate for large systems. It was impossible to restrict access to data, meaning a procedure could access and modify data item. Furthermore, there was no way to pass parameters to a procedure, an omission Jean Sammet regarded as the committee's biggest mistake.\nAnother complication stemmed from the ability to a specified sequence of procedures. This meant that control could jump to and return from any procedure, creating convoluted control flow and permitting a programmer to break the single-entry single-exit rule.\n\nThis situation improved as COBOL adopted more features. COBOL-74 added subprograms, giving programmers the ability to control the data each part of the program could access. COBOL-85 then added nested subprograms, allowing programmers to hide subprograms. Further control over data and code came in 2002 when object-oriented programming, user-defined functions and user-defined data types were included.\n\nNevertheless, much important legacy COBOL software uses unstructured code, which has become unmaintainable. It can be too risky and costly to modify even a simple section of code, since it may be used from unknown places in unknown ways.\n\nCOBOL was intended to be a highly portable, \"common\" language. However, by 2001, around 300 dialects had been created. One source of dialects was the standard itself: the 1974 standard was composed of one mandatory nucleus and eleven functional modules, each containing two or three levels of support. This permitted 104,976 official variants.\n\nCOBOL-85 was not fully compatible with earlier versions, and its development was controversial. Joseph T. Brophy, the CIO of Travelers Insurance, spearheaded an effort to inform COBOL users of the heavy reprogramming costs of implementing the new standard. As a result, the ANSI COBOL Committee received more than 2,200 letters from the public, mostly negative, requiring the committee to make changes. On the other hand, conversion to COBOL-85 was thought to increase productivity in future years, thus justifying the conversion costs.\n\nCOBOL syntax has often been criticized for its verbosity. Proponents say that this was intended to make the code self-documenting, easing program maintenance. COBOL was also intended to be easy for programmers to learn and use, while still being readable to non-technical staff such as managers.\nThe desire for readability led to the use of English-like syntax and structural elements, such as nouns, verbs, clauses, sentences, sections, and divisions. Yet by 1984, maintainers of COBOL programs were struggling to deal with \"incomprehensible\" code and the main changes in COBOL-85 were there to help ease maintenance.\n\nJean Sammet, a short-range committee member, noted that \"little attempt was made to cater to the professional programmer, in fact people whose main interest is programming tend to be very unhappy with COBOL\" which she attributed to COBOL's verbose syntax.\n\nThe COBOL community has always been isolated from the computer science community. No academic computer scientists participated in the design of COBOL: all of those on the committee came from commerce or government. Computer scientists at the time were more interested in fields like numerical analysis, physics and system programming than the commercial file-processing problems which COBOL development tackled. Jean Sammet attributed COBOL's unpopularity to an initial \"snob reaction\" due to its inelegance, the lack of influential computer scientists participating in the design process and a disdain for business data processing. The COBOL specification used a unique \"notation\", or metalanguage, to define its syntax rather than the new Backus–Naur form which the committee did not know of. This resulted in \"severe\" criticism.\n\nLater, COBOL suffered from a shortage of material covering it; it took until 1963 for introductory books to appear (with Richard D. Irwin publishing a college textbook on COBOL in 1966). By 1985, there were twice as many books on Fortran and four times as many on BASIC as on COBOL in the Library of Congress. University professors taught more modern, state-of-the-art languages and techniques instead of COBOL which was said to have a \"trade school\" nature. Donald Nelson, chair of the CODASYL COBOL committee, said in 1984 that \"academics ... hate COBOL\" and that computer science graduates \"had 'hate COBOL' drilled into them\". A 2013 poll by Micro Focus found that 20% of university academics thought COBOL was outdated or dead and that 55% believed their students thought COBOL was outdated or dead. The same poll also found that only 25% of academics had COBOL programming on their curriculum even though 60% thought they should teach it.\nIn contrast, in 2003, COBOL featured in 80% of information systems curricula in the United States, the same proportion as C++ and Java.\n\nThere was also significant condescension towards COBOL in the business community from users of other languages, for example FORTRAN or assembler, implying that COBOL could be used only for non-challenging problems.\n\nDoubts have been raised about the competence of the standards committee. Short-term committee member Howard Bromberg said that there was \"little control\" over the development process and that it was \"plagued by discontinuity of personnel and ... a lack of talent.\" Jean Sammet and Jerome Garfunkel also noted that changes introduced in one revision of the standard would be reverted in the next, due as much to changes in who was in the standard committee as to objective evidence.\n\nCOBOL standards have repeatedly suffered from delays: COBOL-85 arrived five years later than hoped,\nCOBOL 2002 was five years late,\nand COBOL 2014 was six years late.\nTo combat delays, the standard committee allowed the creation of optional addenda which would add features more quickly than by waiting for the next standard revision. However, some committee members raised concerns about incompatibilities between implementations and frequent modifications of the standard.\n\nCOBOL's data structures influenced subsequent programming languages. Its record and file structure influenced PL/I and Pascal, and the codice_41 clause was a predecessor to Pascal's variant records. Explicit file structure definitions preceded the development of database management systems and aggregated data was a significant advance over Fortran's arrays.\ncodice_25 data declarations were incorporated into PL/I, with minor changes.\n\nCOBOL's facility, although considered \"primitive\",\ninfluenced the development of include directives.\n\nThe focus on portability and standardization meant programs written in COBOL could be portable and facilitated the spread of the language to a wide variety of hardware platforms and operating systems. Additionally, the well-defined division structure restricts the definition of external references to the Environment Division, which simplifies platform changes in particular.\n\nBULLET::::- Alphabetical list of programming languages\nBULLET::::- BLIS/COBOL\nBULLET::::- COBOL ReSource\nBULLET::::- CODASYL\nBULLET::::- Comparison of programming languages\n\n\n"}
{"id": "6801", "url": "https://en.wikipedia.org/wiki?curid=6801", "title": "Crew", "text": "Crew\n\nA crew is a body or a class of people who work at a common activity, generally in a structured or hierarchical organization. A location in which a crew works is called a crewyard or a workyard. The word has nautical resonances: the tasks involved in operating a ship, particularly a sailing ship, providing numerous specialities within a ship's crew, often organised with a chain of command. Traditional nautical usage strongly distinguishes officers from crew, though the two groups combined form the ship's company. Members of a crew are often referred to by the title \"Crewman\".\n\n\"Crew\" also refers to the sport of rowing, where teams row competitively in racing shells.\n\nBULLET::::- For a specific sporting usage, see rowing crew.\nBULLET::::- For filmmaking usage, see film crew.\nBULLET::::- For live music usage, see road crew.\nBULLET::::- For analogous entities in research on human judgment and decision-making, see team and judge–advisor system.\nBULLET::::- For stagecraft usage, see stage crew.\nBULLET::::- For video productvubevjlen usage, see television crew.\nBULLET::::- For the comic strip, see Motley's Crew.\nBULLET::::- For the sports team, see Columbus Crew SC.\nBULLET::::- For the 2014 video game, see The Crew (video game).\nBULLET::::- For crews in aviation and the airline industry, see groundcrew and aircrew.\nBULLET::::- Tank crew\nBULLET::::- Boat crew\n\nBULLET::::- Yacht Job Descriptions and Salary Guide\n"}
{"id": "6803", "url": "https://en.wikipedia.org/wiki?curid=6803", "title": "CCD", "text": "CCD\n\nCCD may refer to:\n\nBULLET::::- Charge-coupled device, an electronic light sensor used in various devices including digital cameras\nBULLET::::- .ccd, the filename extension for CloneCD's CD image file\n\nBULLET::::- Carbonate compensation depth, a property of oceans\nBULLET::::- Colony collapse disorder, a phenomenon involving the abrupt disappearance of honey bees in a beehive or Western honey bee colony\nBULLET::::- centicandela (ccd), an SI unit of luminous intensity denoting one hundredth of a candela\nBULLET::::- Central composite design, an experimental design in response surface methodology for building a second order model for a response variable without a complete three-level factorial\n\nBULLET::::- Complementary cumulative distribution function\nBULLET::::- Continuous collision detection, especially in rigid-body dynamics\nBULLET::::- Countercurrent distribution, used for separating mixtures\n\nBULLET::::- Canine compulsive disorder, a behavioral condition in dogs, similar to human obsessive-compulsive disorder (OCD)\nBULLET::::- Caput-collum-diaphyseal angle, the angle between the neck and the shaft of the femur in the hip\nBULLET::::- Cleidocranial dysostosis (also called cleidocranial dysplasia), a genetic abnormality in humans\nBULLET::::- Central core disease, a rare neuromuscular disorder\nBULLET::::- Congenital chloride diarrhea, a rare disorder in babies\nBULLET::::- Continuity of Care Document, an XML-based markup standard for patient medical document exchange\nBULLET::::- Cross-reactive carbohydrate determinants, protein-linked carbohydrate structures that have a role in the phenomenon of cross-reactivity in allergic patients.\n\nBULLET::::- Census county division, a term used by the US Census Bureau\nBULLET::::- Center City District, an economic development agency for the Center City area of Philadelphia\nBULLET::::- Consular Consolidated Database, a database used for visa processing by the Bureau of Consular Affairs, US Department of State\n\nBULLET::::- Café Coffee Day, a chain of coffee shops in India\nBULLET::::- Country Club of Detroit\nBULLET::::- Centre of Cricket Development, a cricket team; see Namibia Cricket Board\nBULLET::::- Cricket Club of Dhakuria; see Gopal Bose\n\nBULLET::::- Confraternity of Christian Doctrine, religious education programs of the Catholic Church normally designed for children, originated in Italy\nBULLET::::- Community College of Denver, a community college with six campuses around Denver, US\nBULLET::::- Cincinnati Country Day School, a non-parochial, private school in Indian Hill\n\nBULLET::::- Christian Care Foundation for Children with Disabilities, in Thailand\nBULLET::::- Council for a Community of Democracies, in the US\nBULLET::::- Canadian Coalition for Democracies, a former advocacy organization in Canada\n\nBULLET::::- \"Centro Cristiano Democratico\" (Christian Democratic Centre), a defunct Italian political party\nBULLET::::- City and County of Denver, the consolidated city-county government in the capital of Colorado, US\n\nBULLET::::- Centre Counter Defence, a chess opening\nBULLET::::- Colorado Springs and Cripple Creek District Railway\nBULLET::::- Convention Centre Dublin, a convention centre in the Dublin Docklands, Ireland\n"}
{"id": "6804", "url": "https://en.wikipedia.org/wiki?curid=6804", "title": "Charge-coupled device", "text": "Charge-coupled device\n\nA charge-coupled device (CCD) is a device for the movement of electrical charge, usually from within the device to an area where the charge can be manipulated, such as conversion into a digital value. This is achieved by \"shifting\" the signals between stages within the device one at a time. CCDs move charge between capacitive \"bins\" in the device, with the shift allowing for the transfer of charge between bins.\n\nCCD is a major technology for digital imaging. In a CCD image sensor, pixels are represented by p-doped metal–oxide–semiconductor (MOS) capacitors. These MOS capacitors, the basic building blocks of a CCD, are biased above the threshold for inversion when image acquisition begins, allowing the conversion of incoming photons into electron charges at the semiconductor-oxide interface; the CCD is then used to read out these charges. Although CCDs are not the only technology to allow for light detection, CCD image sensors are widely used in professional, medical, and scientific applications where high-quality image data are required. In applications with less exacting quality demands, such as consumer and professional digital cameras, active pixel sensors, also known as CMOS sensors (complementary MOS sensors), are generally used. However, the large quality advantage CCDs enjoyed early on has narrowed over time.\n\nThe basis for the CCD is the metal–oxide–semiconductor (MOS) structure, with MOS capacitors being the basic building blocks of a CCD, and a depleted MOS structure used as the photodetector in early CCD devices. MOS technology was originally invented by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959.\n\nIn the late 1960s, Willard Boyle and George E. Smith at Bell Labs were researching MOS technology while working on semiconductor bubble memory. They realized that an electric charge was the analogy of the magnetic bubble and that it could be stored on a tiny MOS capacitor. As it was fairly straightforward to fabricate a series of MOS capacitors in a row, they connected a suitable voltage to them so that the charge could be stepped along from one to the next. This led to the invention of the charge-coupled device by Boyle and Smith in 1969. They conceived of the design of what they termed, in their notebook, \"Charge 'Bubble' Devices\".\n\nThe initial paper describing the concept in April 1970 listed possible uses as memory, a delay line, and an imaging device. The device could also be used as a shift register. The essence of the design was the ability to transfer charge along the surface of a semiconductor from one storage capacitor to the next. The concept was similar in principle to the bucket-brigade device (BBD), which was developed at Philips Research Labs during the late 1960s.\n\nThe first experimental device demonstrating the principle was a row of closely spaced metal squares on an oxidized silicon surface electrically accessed by wire bonds. It was demonstrated by Gil Amelio, Michael Francis Tompsett and George Smith in April 1970. This was the first experimental application of the CCD in image sensor technology, and used a depleted MOS structure as the photodetector. The first patent () on the application of CCDs to imaging was assigned to Tompsett, who filed the application in 1971.\n\nThe first working CCD made with integrated circuit technology was a simple 8-bit shift register, reported by Tompsett, Amelio and Smith in August 1970. This device had input and output circuits and was used to demonstrate its use as a shift register and as a crude eight pixel linear imaging device.\nDevelopment of the device progressed at a rapid rate. By 1971, Bell researchers led by Michael Tompsett were able to capture images with simple linear devices.\nSeveral companies, including Fairchild Semiconductor, RCA and Texas Instruments, picked up on the invention and began development programs. Fairchild's effort, led by ex-Bell researcher Gil Amelio, was the first with commercial devices, and by 1974 had a linear 500-element device and a 2-D 100 x 100 pixel device. Steven Sasson, an electrical engineer working for Kodak, invented the first digital still camera using a Fairchild CCD in 1975.\n\nThe interline transfer (ILT) CCD device was proposed by L. Walsh and R. Dyck at Fairchild in 1973 to reduce smear and eliminate a mechanical shutter. To further reduce smear from bright light sources, the frame-interline-transfer (FIT) CCD architecture was developed by K. Horii, T. Kuroda and T. Kunii at Matsushita (now Panasonic) in 1981.\n\nThe first KH-11 KENNEN reconnaissance satellite equipped with charge-coupled device array ( pixels) technology for imaging was launched in December 1976. Under the leadership of Kazuo Iwama, Sony started a large development effort on CCDs involving a significant investment. Eventually, Sony managed to mass-produce CCDs for their camcorders. Before this happened, Iwama died in August 1982; subsequently, a CCD chip was placed on his tombstone to acknowledge his contribution. The first mass-produced consumer CCD video camera was released by Sony in 1983, based on a prototype developed by Yoshiaki Hagiwara in 1981.\n\nEarly CCD sensors suffered from shutter lag. This was largely resolved with the invention of the pinned photodiode (PPD). It was invented by Nobukazu Teranishi, Hiromitsu Shiraki and Yasuo Ishihara at NEC in 1980. They recognized that lag can be eliminated if the signal carriers could be transferred from the photodiode to the CCD. This led to their invention of the pinned photodiode, a photodetector structure with low lag, low noise, high quantum efficiency and low dark current. It was first publicly reported by Teranishi and Ishihara with A. Kohono, E. Oda and K. Arai in 1982, with the addition of an anti-blooming structure. The new photodetector structure invented at NEC was given the name \"pinned photodiode\" (PPD) by B.C. Burkey at Kodak in 1984. In 1987, the PPD began to be incorporated into most CCD devices, becoming a fixture in consumer electronic video cameras and then digital still cameras. Since then, the PPD has been used in nearly all CCD sensors and then CMOS sensors.\n\nIn January 2006, Boyle and Smith were awarded the National Academy of Engineering Charles Stark Draper Prize, and in 2009 they were awarded the Nobel Prize for Physics, for their invention of the CCD concept. Michael Tompsett was awarded the 2010 National Medal of Technology and Innovation, for pioneering work and electronic technologies including the design and development of the first CCD imagers. He was also awarded the 2012 IEEE Edison Medal for \"pioneering contributions to imaging devices including CCD Imagers, cameras and thermal imagers\".\n\nIn a CCD for capturing images, there is a photoactive region (an epitaxial layer of silicon), and a transmission region made out of a shift register (the CCD, properly speaking).\n\nAn image is projected through a lens onto the capacitor array (the photoactive region), causing each capacitor to accumulate an electric charge proportional to the light intensity at that location. A one-dimensional array, used in line-scan cameras, captures a single slice of the image, whereas a two-dimensional array, used in video and still cameras, captures a two-dimensional picture corresponding to the scene projected onto the focal plane of the sensor. Once the array has been exposed to the image, a control circuit causes each capacitor to transfer its contents to its neighbor (operating as a shift register). The last capacitor in the array dumps its charge into a charge amplifier, which converts the charge into a voltage. By repeating this process, the controlling circuit converts the entire contents of the array in the semiconductor to a sequence of voltages. In a digital device, these voltages are then sampled, digitized, and usually stored in memory; in an analog device (such as an analog video camera), they are processed into a continuous analog signal (e.g. by feeding the output of the charge amplifier into a low-pass filter), which is then processed and fed out to other circuits for transmission, recording, or other processing.\n\nBefore the MOS capacitors are exposed to light, they are biased into the depletion region; in n-channel CCDs, the silicon under the bias gate is slightly \"p\"-doped or intrinsic. The gate is then biased at a positive potential, above the threshold for strong inversion, which will eventually result in the creation of a \"n\" channel below the gate as in a MOSFET. However, it takes time to reach this thermal equilibrium: up to hours in high-end scientific cameras cooled at low temperature. Initially after biasing, the holes are pushed far into the substrate, and no mobile electrons are at or near the surface; the CCD thus operates in a non-equilibrium state called deep depletion.\nThen, when electron–hole pairs are generated in the depletion region, they are separated by the electric field, the electrons move toward the surface, and the holes move toward the substrate. Four pair-generation processes can be identified:\nBULLET::::- photo-generation (up to 95% of quantum efficiency),\nBULLET::::- generation in the depletion region,\nBULLET::::- generation at the surface, and\nBULLET::::- generation in the neutral bulk.\n\nThe last three processes are known as dark-current generation, and add noise to the image; they can limit the total usable integration time. The accumulation of electrons at or near the surface can proceed either until image integration is over and charge begins to be transferred, or thermal equilibrium is reached. In this case, the well is said to be full. The maximum capacity of each well is known as the well depth, typically about 10 electrons per pixel.\n\nThe photoactive region of a CCD is, generally, an epitaxial layer of silicon. It is lightly \"p\" doped (usually with boron) and is grown upon a substrate material, often p++. In buried-channel devices, the type of design utilized in most modern CCDs, certain areas of the surface of the silicon are ion implanted with phosphorus, giving them an n-doped designation. This region defines the channel in which the photogenerated charge packets will travel. Simon Sze details the advantages of a buried-channel device:\nThis thin layer (= 0.2–0.3 micron) is fully depleted and the accumulated photogenerated charge is kept away from the surface. This structure has the advantages of higher transfer efficiency and lower dark current, from reduced surface recombination. The penalty is smaller charge capacity, by a factor of 2–3 compared to the surface-channel CCD. The gate oxide, i.e. the capacitor dielectric, is grown on top of the epitaxial layer and substrate.\n\nLater in the process, polysilicon gates are deposited by chemical vapor deposition, patterned with photolithography, and etched in such a way that the separately phased gates lie perpendicular to the channels. The channels are further defined by utilization of the LOCOS process to produce the channel stop region.\n\nChannel stops are thermally grown oxides that serve to isolate the charge packets in one column from those in another. These channel stops are produced before the polysilicon gates are, as the LOCOS process utilizes a high-temperature step that would destroy the gate material. The channel stops are parallel to, and exclusive of, the channel, or \"charge carrying\", regions.\n\nChannel stops often have a p+ doped region underlying them, providing a further barrier to the electrons in the charge packets (this discussion of the physics of CCD devices assumes an electron transfer device, though hole transfer is possible).\n\nThe clocking of the gates, alternately high and low, will forward and reverse bias the diode that is provided by the buried channel (n-doped) and the epitaxial layer (p-doped). This will cause the CCD to deplete, near the p–n junction and will collect and move the charge packets beneath the gates—and within the channels—of the device.\n\nCCD manufacturing and operation can be optimized for different uses. The above process describes a frame transfer CCD. While CCDs may be manufactured on a heavily doped p++ wafer it is also possible to manufacture a device inside p-wells that have been placed on an n-wafer. This second method, reportedly, reduces smear, dark current, and infrared and red response. This method of manufacture is used in the construction of interline-transfer devices.\n\nAnother version of CCD is called a peristaltic CCD. In a peristaltic charge-coupled device, the charge-packet transfer operation is analogous to the peristaltic contraction and dilation of the digestive system. The peristaltic CCD has an additional implant that keeps the charge away from the silicon/silicon dioxide interface and generates a large lateral electric field from one gate to the next. This provides an additional driving force to aid in transfer of the charge packets.\n\nThe CCD image sensors can be implemented in several different architectures. The most common are full-frame, frame-transfer, and interline. The distinguishing characteristic of each of these architectures is their approach to the problem of shuttering.\n\nIn a full-frame device, all of the image area is active, and there is no electronic shutter. A mechanical shutter must be added to this type of sensor or the image smears as the device is clocked or read out.\n\nWith a frame-transfer CCD, half of the silicon area is covered by an opaque mask (typically aluminum). The image can be quickly transferred from the image area to the opaque area or storage region with acceptable smear of a few percent. That image can then be read out slowly from the storage region while a new image is integrating or exposing in the active area. Frame-transfer devices typically do not require a mechanical shutter and were a common architecture for early solid-state broadcast cameras. The downside to the frame-transfer architecture is that it requires twice the silicon real estate of an equivalent full-frame device; hence, it costs roughly twice as much.\n\nThe interline architecture extends this concept one step further and masks every other column of the image sensor for storage. In this device, only one pixel shift has to occur to transfer from image area to storage area; thus, shutter times can be less than a microsecond and smear is essentially eliminated. The advantage is not free, however, as the imaging area is now covered by opaque strips dropping the fill factor to approximately 50 percent and the effective quantum efficiency by an equivalent amount. Modern designs have addressed this deleterious characteristic by adding microlenses on the surface of the device to direct light away from the opaque regions and on the active area. Microlenses can bring the fill factor back up to 90 percent or more depending on pixel size and the overall system's optical design.\n\nThe choice of architecture comes down to one of utility. If the application cannot tolerate an expensive, failure-prone, power-intensive mechanical shutter, an interline device is the right choice. Consumer snap-shot cameras have used interline devices. On the other hand, for those applications that require the best possible light collection and issues of money, power and time are less important, the full-frame device is the right choice. Astronomers tend to prefer full-frame devices. The frame-transfer falls in between and was a common choice before the fill-factor issue of interline devices was addressed. Today, frame-transfer is usually chosen when an interline architecture is not available, such as in a back-illuminated device.\n\nCCDs containing grids of pixels are used in digital cameras, optical scanners, and video cameras as light-sensing devices. They commonly respond to 70 percent of the incident light (meaning a quantum efficiency of about 70 percent) making them far more efficient than photographic film, which captures only about 2 percent of the incident light.\n\nMost common types of CCDs are sensitive to near-infrared light, which allows infrared photography, night-vision devices, and zero lux (or near zero lux) video-recording/photography. For normal silicon-based detectors, the sensitivity is limited to 1.1 μm. One other consequence of their sensitivity to infrared is that infrared from remote controls often appears on CCD-based digital cameras or camcorders if they do not have infrared blockers.\n\nCooling reduces the array's dark current, improving the sensitivity of the CCD to low light intensities, even for ultraviolet and visible wavelengths. Professional observatories often cool their detectors with liquid nitrogen to reduce the dark current, and therefore the thermal noise, to negligible levels.\n\nThe frame transfer CCD imager was the first imaging structure proposed for CCD Imaging by Michael Tompsett at Bell Laboratories. A frame transfer CCD is a specialized CCD, often used in astronomy and some professional video cameras, designed for high exposure efficiency and correctness.\n\nThe normal functioning of a CCD, astronomical or otherwise, can be divided into two phases: exposure and readout. During the first phase, the CCD passively collects incoming photons, storing electrons in its cells. After the exposure time is passed, the cells are read out one line at a time. During the readout phase, cells are shifted down the entire area of the CCD. While they are shifted, they continue to collect light. Thus, if the shifting is not fast enough, errors can result from light that falls on a cell holding charge during the transfer. These errors are referred to as \"vertical smear\" and cause a strong light source to create a vertical line above and below its exact location. In addition, the CCD cannot be used to collect light while it is being read out. Unfortunately, a faster shifting requires a faster readout, and a faster readout can introduce errors in the cell charge measurement, leading to a higher noise level.\n\nA frame transfer CCD solves both problems: it has a shielded, not light sensitive, area containing as many cells as the area exposed to light. Typically, this area is covered by a reflective material such as aluminium. When the exposure time is up, the cells are transferred very rapidly to the hidden area. Here, safe from any incoming light, cells can be read out at any speed one deems necessary to correctly measure the cells' charge. At the same time, the exposed part of the CCD is collecting light again, so no delay occurs between successive exposures.\n\nThe disadvantage of such a CCD is the higher cost: the cell area is basically doubled, and more complex control electronics are needed.\n\nAn intensified charge-coupled device (ICCD) is a CCD that is optically connected to an image intensifier that is mounted in front of the CCD.\n\nAn image intensifier includes three functional elements: a photocathode, a micro-channel plate (MCP) and a phosphor screen. These three elements are mounted one close behind the other in the mentioned sequence. The photons which are coming from the light source fall onto the photocathode, thereby generating photoelectrons. The photoelectrons are accelerated towards the MCP by an electrical control voltage, applied between photocathode and MCP. The electrons are multiplied inside of the MCP and thereafter accelerated towards the phosphor screen. The phosphor screen finally converts the multiplied electrons back to photons which are guided to the CCD by a fiber optic or a lens.\n\nAn image intensifier inherently includes a shutter functionality: If the control voltage between the photocathode and the MCP is reversed, the emitted photoelectrons are not accelerated towards the MCP but return to the photocathode. Thus, no electrons are multiplied and emitted by the MCP, no electrons are going to the phosphor screen and no light is emitted from the image intensifier. In this case no light falls onto the CCD, which means that the shutter is closed. The process of reversing the control voltage at the photocathode is called \"gating\" and therefore ICCDs are also called gateable CCD cameras.\n\nBesides the extremely high sensitivity of ICCD cameras, which enable single photon detection, the gateability is one of the major advantages of the ICCD over the EMCCD cameras. The highest performing ICCD cameras enable shutter times as short as 200 picoseconds.\n\nICCD cameras are in general somewhat higher in price than EMCCD cameras because they need the expensive image intensifier. On the other hand, EMCCD cameras need a cooling system to cool the EMCCD chip down to temperatures around 170 K. This cooling system adds additional costs to the EMCCD camera and often yields heavy condensation problems in the application.\n\nICCDs are used in night vision devices and in various scientific applications.\n\nAn electron-multiplying CCD (EMCCD, also known as an L3Vision CCD, a product commercialized by e2v Ltd., GB, L3CCD or Impactron CCD, a now-discontinued product offered in the past by Texas Instruments) is a charge-coupled device in which a gain register is placed between the shift register and the output amplifier. The gain register is split up into a large number of stages. In each stage, the electrons are multiplied by impact ionization in a similar way to an avalanche diode. The gain probability at every stage of the register is small (\"P\" < 2%), but as the number of elements is large (N > 500), the overall gain can be very high (formula_1), with single input electrons giving many thousands of output electrons. Reading a signal from a CCD gives a noise background, typically a few electrons. In an EMCCD, this noise is superimposed on many thousands of electrons rather than a single electron; the devices' primary advantage is thus their negligible readout noise. It is to be noted that the use of avalanche breakdown for amplification of photo charges had already been described in the in 1973 by George E. Smith/Bell Telephone Laboratories.\n\nEMCCDs show a similar sensitivity to intensified CCDs (ICCDs). However, as with ICCDs, the gain that is applied in the gain register is stochastic and the \"exact\" gain that has been applied to a pixel's charge is impossible to know. At high gains (> 30), this uncertainty has the same effect on the signal-to-noise ratio (SNR) as halving the quantum efficiency (QE) with respect to operation with a gain of unity. However, at very low light levels (where the quantum efficiency is most important), it can be assumed that a pixel either contains an electron — or not. This removes the noise associated with the stochastic multiplication at the risk of counting multiple electrons in the same pixel as a single electron. To avoid multiple counts in one pixel due to coincident photons in this mode of operation, high frame rates are essential. The dispersion in the gain is shown in the graph on the right. For multiplication registers with many elements and large gains it is well modelled by the equation:\n\nformula_2 if formula_3\n\nwhere \"P\" is the probability of getting \"n\" output electrons given \"m\" input electrons and a total mean multiplication register gain of \"g\".\n\nBecause of the lower costs and better resolution, EMCCDs are capable of replacing ICCDs in many applications. ICCDs still have the advantage that they can be gated very fast and thus are useful in applications like range-gated imaging. EMCCD cameras indispensably need a cooling system — using either thermoelectric cooling or liquid nitrogen — to cool the chip down to temperatures in the range of . This cooling system unfortunately adds additional costs to the EMCCD imaging system and may yield condensation problems in the application. However, high-end EMCCD cameras are equipped with a permanent hermetic vacuum system confining the chip to avoid condensation issues.\n\nThe low-light capabilities of EMCCDs find use in astronomy and biomedical research, among other fields. In particular, their low noise at high readout speeds makes them very useful for a variety of astronomical applications involving low light sources and transient events such as lucky imaging of faint stars, high speed photon counting photometry, Fabry-Pérot spectroscopy and high-resolution spectroscopy. More recently, these types of CCDs have broken into the field of biomedical research in low-light applications including small animal imaging, single-molecule imaging, Raman spectroscopy, super resolution microscopy as well as a wide variety of modern fluorescence microscopy techniques thanks to greater SNR in low-light conditions in comparison with traditional CCDs and ICCDs.\n\nIn terms of noise, commercial EMCCD cameras typically have clock-induced charge (CIC) and dark current (dependent on the extent of cooling) that together lead to an effective readout noise ranging from 0.01 to 1 electrons per pixel read. However, recent improvements in EMCCD technology have led to a new generation of cameras capable of producing significantly less CIC, higher charge transfer efficiency and an EM gain 5 times higher than what was previously available. These advances in low-light detection lead to an effective total background noise of 0.001 electrons per pixel read, a noise floor unmatched by any other low-light imaging device.\n\nDue to the high quantum efficiencies of charge-coupled device (CCD) (for a quantum efficiency of 100%, one count equals one photon), linearity of their outputs, ease of use compared to photographic plates, and a variety of other reasons, CCDs were very rapidly adopted by astronomers for nearly all UV-to-infrared applications.\n\nThermal noise and cosmic rays may alter the pixels in the CCD array. To counter such effects, astronomers take several exposures with the CCD shutter closed and opened. The average of images taken with the shutter closed is necessary to lower the random noise. Once developed, the dark frame average image is then subtracted from the open-shutter image to remove the dark current and other systematic defects (dead pixels, hot pixels, etc.) in the CCD.\n\nThe Hubble Space Telescope, in particular, has a highly developed series of steps (“data reduction pipeline”) to convert the raw CCD data to useful images.\n\nCCD cameras used in astrophotography often require sturdy mounts to cope with vibrations from wind and other sources, along with the tremendous weight of most imaging platforms. To take long exposures of galaxies and nebulae, many astronomers use a technique known as auto-guiding. Most autoguiders use a second CCD chip to monitor deviations during imaging. This chip can rapidly detect errors in tracking and command the mount motors to correct for them.\n\nAn unusual astronomical application of CCDs, called drift-scanning, uses a CCD to make a fixed telescope behave like a tracking telescope and follow the motion of the sky. The charges in the CCD are transferred and read in a direction parallel to the motion of the sky, and at the same speed. In this way, the telescope can image a larger region of the sky than its normal field of view. The Sloan Digital Sky Survey is the most famous example of this, using the technique to a survey of over a quarter of the sky.\n\nIn addition to imagers, CCDs are also used in an array of analytical instrumentation including spectrometers and interferometers.\n\nDigital color cameras generally use a Bayer mask over the CCD. Each square of four pixels has one filtered red, one blue, and two green (the human eye is more sensitive to green than either red or blue). The result of this is that luminance information is collected at every pixel, but the color resolution is lower than the luminance resolution.\n\nBetter color separation can be reached by three-CCD devices (3CCD) and a dichroic beam splitter prism, that splits the image into red, green and blue components. Each of the three CCDs is arranged to respond to a particular color. Many professional video camcorders, and some semi-professional camcorders, use this technique, although developments in competing CMOS technology have made CMOS sensors, both with beam-splitters and bayer filters, increasingly popular in high-end video and digital cinema cameras. Another advantage of 3CCD over a Bayer mask device is higher quantum efficiency (and therefore higher light sensitivity for a given aperture size). This is because in a 3CCD device most of the light entering the aperture is captured by a sensor, while a Bayer mask absorbs a high proportion (about 2/3) of the light falling on each CCD pixel.\n\nFor still scenes, for instance in microscopy, the resolution of a Bayer mask device can be enhanced by microscanning technology. During the process of color co-site sampling, several frames of the scene are produced. Between acquisitions, the sensor is moved in pixel dimensions, so that each point in the visual field is acquired consecutively by elements of the mask that are sensitive to the red, green and blue components of its color. Eventually every pixel in the image has been scanned at least once in each color and the resolution of the three channels become equivalent (the resolutions of red and blue channels are quadrupled while the green channel is doubled).\n\nSensors (CCD / CMOS) come in various sizes, or image sensor formats. These sizes are often referred to with an inch fraction designation such as 1/1.8″ or 2/3″ called the optical format. This measurement actually originates back in the 1950s and the time of Vidicon tubes.\n\nWhen a CCD exposure is long enough, eventually the electrons that collect in the \"bins\" in the brightest part of the image will overflow the bin, resulting in blooming. The structure of the CCD allows the electrons to flow more easily in one direction than another, resulting in vertical streaking.\n\nSome anti-blooming features that can be built into a CCD reduce its sensitivity to light by using some of the pixel area for a drain structure.\nJames M. Early developed a vertical anti-blooming drain that would not detract from the light collection area, and so did not reduce light sensitivity.\n\nBULLET::::- Photodiode\nBULLET::::- CMOS sensor\nBULLET::::- Angle-sensitive pixel\nBULLET::::- Rotating line camera\nBULLET::::- Superconducting camera\nBULLET::::- Wide dynamic range\nBULLET::::- Hole accumulation diode (HAD)\nBULLET::::- Multi-layer CCD\nBULLET::::- Andor Technology – Manufacturer of EMCCD cameras\nBULLET::::- Photometrics - Manufacturer of EMCCD cameras\nBULLET::::- QImaging - Manufacturer of EMCCD cameras\nBULLET::::- PI/Acton – Manufacturer of EMCCD cameras\nBULLET::::- Stanford Computer Optics – Manufacturer of ICCD cameras\nBULLET::::- Time delay and integration (TDI)\nBULLET::::- Glossary of video terms\n\nBULLET::::- Journal Article On Basics of CCDs\nBULLET::::- Nikon microscopy introduction to CCDs\nBULLET::::- Concepts in Digital Imaging Technology\nBULLET::::- More statistical properties\nBULLET::::- L3CCDs used in astronomy\n"}
{"id": "6806", "url": "https://en.wikipedia.org/wiki?curid=6806", "title": "Computer memory", "text": "Computer memory\n\nIn computing, memory refers to a device that is used to store information for immediate use in a computer or related computer hardware device. It typically refers to semiconductor memory, specifically metal–oxide–semiconductor (MOS) memory, where data is stored within MOS memory cells on a silicon integrated circuit chip. The term \"memory\" is often synonymous with the term \"primary storage\". Computer memory operates at a high speed, for example random-access memory (RAM), as a distinction from storage that provides slow-to-access information but offers higher capacities. If needed, contents of the computer memory can be transferred to secondary storage; a very common way of doing this is through a memory management technique called \"virtual memory\". An archaic synonym for memory is store.\n\nThe term \"memory\", meaning \"primary storage\" or \"main memory\", is often associated with addressable semiconductor memory, i.e. integrated circuits consisting of silicon-based MOS transistors, used for example as primary storage but also other purposes in computers and other digital electronic devices. There are two main kinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory are flash memory (used as secondary memory) and ROM, PROM, EPROM and EEPROM memory (used for storing firmware such as BIOS). Examples of volatile memory are primary storage, which is typically dynamic random-access memory (DRAM), and fast CPU cache memory, which is typically static random-access memory (SRAM) that is fast but energy-consuming, offering lower memory areal density than DRAM.\n\nMost semiconductor memory is organized into memory cells or bistable flip-flops, each storing one bit (0 or 1). Flash memory organization includes both one bit per memory cell and multiple bits per cell (called MLC, Multiple Level Cell). The memory cells are grouped into words of fixed word length, for example 1, 2, 4, 8, 16, 32, 64 or 128 bit. Each word can be accessed by a binary address of \"N\" bit, making it possible to store 2 raised by \"N\" words in the memory. This implies that processor registers normally are not considered as memory, since they only store one word and do not include an addressing mechanism.\n\nTypical secondary storage devices are hard disk drives and solid-state drives.\n\nIn the early 1940s, memory technology often permitted a capacity of a few bytes. The first electronic programmable digital computer, the ENIAC, using thousands of octal-base radio vacuum tubes, could perform simple calculations involving 20 numbers of ten decimal digits which were held in the vacuum tube accumulators.\n\nThe next significant advance in computer memory came with acoustic delay line memory, developed by J. Presper Eckert in the early 1940s. Through the construction of a glass tube filled with mercury and plugged at each end with a quartz crystal, delay lines could store bits of information in the form of sound waves propagating through mercury, with the quartz crystals acting as transducers to read and write bits. Delay line memory would be limited to a capacity of up to a few hundred thousand bits to remain efficient.\n\nTwo alternatives to the delay line, the Williams tube and Selectron tube, originated in 1946, both using electron beams in glass tubes as means of storage. Using cathode ray tubes, Fred Williams would invent the Williams tube, which would be the first random-access computer memory. The Williams tube would prove more capacious than the Selectron tube (the Selectron was limited to 256 bits, while the Williams tube could store thousands) and less expensive. The Williams tube would nevertheless prove to be frustratingly sensitive to environmental disturbances.\n\nEfforts began in the late 1940s to find non-volatile memory. Magnetic-core memory allowed for recall of memory after power loss. It was developed by Frederick W. Viehe and An Wang in the late 1940s, and improved by Jay Forrester and Jan A. Rajchman in the early 1950s, before being commercialised with the Whirlwind computer in 1953. Magnetic-core memory would become the dominant form of memory until the development of MOS semiconductor memory in the 1960s.\n\nSemiconductor memory began in the early 1960s with bipolar memory, which used bipolar transistors. Bipolar semiconductor memory made from discrete devices was first shipped by Texas Instruments to the United States Air Force in 1961. The same year, the concept of solid-state memory on an integrated circuit (IC) chip was proposed by applications engineer Bob Norman at Fairchild Semiconductor. The first bipolar semiconductor memory IC chip was the SP95 introduced by IBM in 1965. While bipolar memory offered improved performance over magnetic-core memory, it could not compete with the lower price of magnetic-core, which remained dominant up until the late 1960s. Bipolar memory failed to replace magnetic-core memory because bipolar flip-flop circuits were too large and expensive.\n\nThe invention of the MOSFET (metal–oxide–semiconductor field-effect transistor, or MOS transistor), by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959, enabled the practical use of metal–oxide–semiconductor (MOS) transistors as memory cell storage elements, a function previously served by magnetic cores. MOS memory was developed by John Schmidt at Fairchild Semiconductor in 1964. In addition to higher performance, MOS semiconductor memory was cheaper and consumed less power than magnetic core memory. In 1965, J. Wood and R. Ball of the Royal Radar Establishment proposed digital storage systems that use CMOS (complementary MOS) memory cells, in addition to MOSFET power devices for the power supply, switched cross-coupling, switches and delay line storage. The development of silicon-gate MOS integrated circuit (MOS IC) technology by Federico Faggin at Fairchild in 1968 enabled the production of MOS memory chips. NMOS memory was commercialized by IBM in the early 1970s. MOS memory overtook magnetic core memory as the dominant memory technology in the early 1970s.\n\nThe two main types of volatile random-access memory (RAM) are static random-access memory (SRAM) and dynamic random-access memory (DRAM). Bipolar SRAM was invented by Robert Norman at Fairchild Semiconductor in 1963, followed by the development of MOS SRAM by John Schmidt at Fairchild in 1964. SRAM became an alternative to magnetic-core memory, but required six MOS transistors for each bit of data. Commercial use of SRAM began in 1965, when IBM introduced their SP95 SRAM chip for the System/360 Model 95.\n\nToshiba introduced bipolar DRAM memory cells for its Toscal BC-1411 electronic calculator in 1965. While it offered improved performance over magnetic-core memory, bipolar DRAM could not compete with the lower price of the then dominant magnetic-core memory. MOS technology is the basis for modern DRAM. In 1966, Dr. Robert H. Dennard at the IBM Thomas J. Watson Research Center was working on MOS memory. While examining the characteristics of MOS technology, he found it was capable of building capacitors, and that storing a charge or no charge on the MOS capacitor could represent the 1 and 0 of a bit, while the MOS transistor could control writing the charge to the capacitor. This led to his development of a single-transistor DRAM memory cell. In 1967, Dennard filed a patent under IBM for a single-transistor DRAM memory cell, based on MOS technology. This led to the first commercial DRAM IC chip, the Intel 1103, in October 1970. Synchronous dynamic random-access memory (SDRAM) later debuted with the Samsung KM48SL2000 chip in 1992.\n\nThe term \"memory\" is also often used to refer to non-volatile memory, specifically flash memory. It has origins in read-only memory (ROM). Programmable read-only memory (PROM) was invented by Wen Tsing Chow in 1956, while working for the Arma Division of the American Bosch Arma Corporation. In 1967, Dawon Kahng and Simon Sze of Bell Labs proposed that the floating gate of a MOS semiconductor device could be used for the cell of a reprogrammable read-only memory (ROM), which led to Dov Frohman of Intel inventing EPROM (erasable PROM) in 1971. EEPROM (electrically erasable PROM) was developed by Yasuo Tarui, Yutaka Hayashi and Kiyoko Naga at the Electrotechnical Laboratory in 1972. Flash memory was invented by Fujio Masuoka at Toshiba in the early 1980s. Masuoka and colleagues presented the invention of NOR flash in 1984, and then NAND flash in 1987. Toshiba commercialized NAND flash memory in 1987.\n\nDevelopments in technology and economies of scale have made possible so-called Very Large Memory (VLM) computers.\n\nVolatile memory is computer memory that requires power to maintain the stored information. Most modern semiconductor volatile memory is either static RAM (SRAM) or dynamic RAM (DRAM). SRAM retains its contents as long as the power is connected and is simpler for interfacing, but uses six transistors per bit. Dynamic RAM is more complicated for interfacing and control, needing regular refresh cycles to prevent losing its contents, but uses only one transistor and one capacitor per bit, allowing it to reach much higher densities and much cheaper per-bit costs.\n\nSRAM is not worthwhile for desktop system memory, where DRAM dominates, but is used for their cache memories. SRAM is commonplace in small embedded systems, which might only need tens of kilobytes or less. Forthcoming volatile memory technologies that aim at replacing or competing with SRAM and DRAM include Z-RAM and A-RAM.\n\nNon-volatile memory is computer memory that can retain the stored information even when not powered. Examples of non-volatile memory include read-only memory (see ROM), flash memory, most types of magnetic computer storage devices (e.g. hard disk drives, floppy disks and magnetic tape), optical discs, and early computer storage methods such as paper tape and punched cards.\n\nForthcoming non-volatile memory technologies include FERAM, CBRAM, PRAM, STT-RAM, SONOS, RRAM, racetrack memory, NRAM, 3D XPoint, and millipede memory.\n\nA third category of memory is \"semi-volatile\". The term is used to describe a memory which has some limited non-volatile duration after power is removed, but then data is ultimately lost. A typical goal when using a semi-volatile memory is to provide high performance/durability/etc. associated with volatile memories, while providing some benefits of a true non-volatile memory.\n\nFor example, some non-volatile memory types can wear out, where a \"worn\" cell has increased volatility but otherwise continues to work. Data locations which are written frequently can thus be directed to use worn circuits. As long as the location is updated within some known retention time, the data stays valid. If the retention time \"expires\" without an update, then the value is copied to a less-worn circuit with longer retention. Writing first to the worn area allows a high write rate while avoiding wear on the not-worn circuits.\n\nAs a second example, an STT-RAM can be made non-volatile by building large cells, but the cost per bit and write power go up, while the write speed goes down. Using small cells improves cost, power, and speed, but leads to semi-volatile behavior. In some applications the increased volatility can be managed to provide many benefits of a non-volatile memory, for example by removing power but forcing a wake-up before data is lost; or by caching read-only data and discarding the cached data if the power-off time exceeds the non-volatile threshold.\n\nThe term semi-volatile is also used to describe semi-volatile behavior constructed from other memory types. For example, a volatile and a non-volatile memory may be combined, where an external signal copies data from the volatile memory to the non-volatile memory, but if power is removed without copying, the data is lost. Or, a battery-backed volatile memory, and if external power is lost there is some known period where the battery can continue to power the volatile memory, but if power is off for an extended time, the battery runs down and data is lost.\n\nProper management of memory is vital for a computer system to operate properly. Modern operating systems have complex systems to properly manage memory. Failure to do so can lead to bugs, slow performance, and at worst case, takeover by viruses and malicious software.\n\nNearly everything computer programmers do requires them to consider how to manage memory. Even storing a number in memory requires the programmer to specify how the memory should store it.\n\nImproper management of memory is a common cause of bugs, including the following types:\n\nBULLET::::- In an arithmetic overflow, a calculation results in a number larger than the allocated memory permits. For example, a signed 8-bit integer allows the numbers −128 to +127. If its value is 127 and it is instructed to add one, the computer can not store the number 128 in that space. Such a case will result in undesired operation, such as changing the number's value to −128 instead of +128.\nBULLET::::- A memory leak occurs when a program requests memory from the operating system and never returns the memory when it's done with it. A program with this bug will gradually require more and more memory until the program fails as it runs out.\nBULLET::::- A segmentation fault results when a program tries to access memory that it does not have permission to access. Generally a program doing so will be terminated by the operating system.\nBULLET::::- A buffer overflow means that a program writes data to the end of its allocated space and then continues to write data to memory that has been allocated for other purposes. This may result in erratic program behavior, including memory access errors, incorrect results, a crash, or a breach of system security. They are thus the basis of many software vulnerabilities and can be maliciously exploited.\n\nIn early computer systems, programs typically specified the location to write memory and what data to put there. This location was a physical location on the actual memory hardware. The slow processing of such computers did not allow for the complex memory management systems used today. Also, as most such systems were single-task, sophisticated systems were not required as much.\n\nThis approach has its pitfalls. If the location specified is incorrect, this will cause the computer to write the data to some other part of the program. The results of an error like this are unpredictable. In some cases, the incorrect data might overwrite memory used by the operating system. Computer crackers can take advantage of this to create viruses and malware.\n\nVirtual memory is a system where all physical memory is controlled by the operating system. When a program needs memory, it requests it from the operating system. The operating system then decides in what physical location to place the program's code and data.\n\nThis offers several advantages. Computer programmers no longer need to worry about where their data is physically stored or whether the user's computer will have enough memory. It also allows multiple types of memory to be used. For example, some data can be stored in physical RAM chips while other data is stored on a hard drive (e.g. in a swapfile), functioning as an extension of the cache hierarchy. This drastically increases the amount of memory available to programs. The operating system will place actively used data in physical RAM, which is much faster than hard disks. When the amount of RAM is not sufficient to run all the current programs, it can result in a situation where the computer spends more time moving data from RAM to disk and back than it does accomplishing tasks; this is known as thrashing.\n\nProtected memory is a system where each program is given an area of memory to use and is not permitted to go outside that range. Use of protected memory greatly enhances both the reliability and security of a computer system.\n\nWithout protected memory, it is possible that a bug in one program will alter the memory used by another program. This will cause that other program to run off of corrupted memory with unpredictable results. If the operating system's memory is corrupted, the entire computer system may crash and need to be rebooted. At times programs intentionally alter the memory used by other programs. This is done by viruses and malware to take over computers. It may also be used benignly by desirable programs which are intended to modify other programs; in the modern age, this is generally considered bad programming practice for application programs, but it may be used by system development tools such as debuggers, for example to insert breakpoints or hooks.\n\nProtected memory assigns programs their own areas of memory. If the operating system detects that a program has tried to alter memory that does not belong to it, the program is terminated (or otherwise restricted or redirected). This way, only the offending program crashes, and other programs are not affected by the misbehavior (whether accidental or intentional).\n\nProtected memory systems almost always include virtual memory as well.\n\nBULLET::::- Semiconductor memory\nBULLET::::- Memory geometry\nBULLET::::- Memory hierarchy\n"}
{"id": "6809", "url": "https://en.wikipedia.org/wiki?curid=6809", "title": "CDC", "text": "CDC\n\nCDC may refer to:\n\nBULLET::::- Centers for Disease Control and Prevention, a United States government public health agency\nBULLET::::- Centers for Disease Control and Prevention Korea, a South Korean government public health agency\nBULLET::::- Centers for Disease Control (Taiwan), an agency in Taiwan (the Republic of China) that combats the threat of communicable diseases\nBULLET::::- Community Development Council, Singapore government-led community programs\nBULLET::::- Community of Democratic Choice, an intergovernmental organization in Eastern Europe to promote democracy and human rights\nBULLET::::- Canadian Dairy Commission, a Canadian government Crown Corporation that oversees dairy production\n\nBULLET::::- Congress for Democratic Change, a Liberian political party\nBULLET::::- Democratic Convergence of Catalonia (), a former political party in Catalonia, Spain (1974–2016)\nBULLET::::- California Democratic Council, an unofficial umbrella organization of volunteer Democratic Clubs in the United States\n\nBULLET::::- Coalition to Diversify Computing, a joint organization of the Association for Computing Machinery (ACM) and the Computing Research Association (CRA)\n\nBULLET::::- Community development corporation, any non-profit organization that promotes and supports a community\nBULLET::::- Certified Development Company, a U.S. Small Business Administration program designed to provide financing for the purchase of fixed assets\nBULLET::::- Commission for Developing Countries, a Commission of the International Mathematical Union\n\nBULLET::::- \" (\"Deposits and Consignments Fund\"), a financial institution owned by the French government\nBULLET::::- CDC Group, formerly the Commonwealth Development Corporation and Colonial Development Corporation, a British development organisation owned by the UK Government\nBULLET::::- Cameroon Development Corporation, an agribusiness company located in Limbe, Cameroon\nBULLET::::- Control Data Corporation, former supercomputer company\nBULLET::::- CDC Software, a computer software company spun off from Control Data Corporation\nBULLET::::- ComfortDelGro Australia, a major Australian operator of buses formerly named ComfortDelGro Cabcharge\nBULLET::::- Construction Data Company, also known as CDC News and CDC Publishing, a commercial construction reporting service\nBULLET::::- Loong Air, by ICAO code\n\nBULLET::::- Cult of the Dead Cow (cDc), a computer hacker and DIY media organization\n\nBULLET::::- Center Day Camp, a summer day camp in North Windham, Maine, U.S.\nBULLET::::- \", specialized library in Quebec, Canada\nBULLET::::- Communicable Disease Centre, a hospital at Moulmein Road in Novena, Singapore\nBULLET::::- Cedar City Regional Airport, by IATA code\n\nBULLET::::- Cholesterol-dependent cytolysin, pore forming exotoxins, secreted by Gram positive bacteria\nBULLET::::- Cell-division cycle, also cell cycle in biology\nBULLET::::- Cell-division cycle protein, e.g. Cdc42\nBULLET::::- Complement-dependent cytotoxicity\nBULLET::::- Conventional dendritic cell (previously called Myeloid dendritic cell, cDC)\n\nBULLET::::- Change data capture, a methodology used in data warehousing and databases\nBULLET::::- Clock domain crossing, a signal that crosses between different clock domains of a system\nBULLET::::- Connected Device Configuration, Java framework for building Java ME applications on embedded devices\nBULLET::::- Communications daughter card, an Ethernet, Modem or Bluetooth expansion card for mobiles\nBULLET::::- Carbide-derived carbon, a family of carbon structures with tunable properties produced via etching of metal carbides\nBULLET::::- USB communications device class, a composite Universal Serial Bus device class\n\nBULLET::::- Book titles:\nBULLET::::- \"CDC?\", a children's book by William Steig (sequel to \"CDB!\")\nBULLET::::- \"CDC\", 20th-century diet book \"Calories Don't Count\"\nBULLET::::- Combat Direction Center, the tactical center of an aircraft carrier\nBULLET::::- Cul de canard, duck feathers used in fly fishing\nBULLET::::- Continuous Discharge Certificate, seafarer's identity document\n\nBULLET::::- C.DC., the Swiss botany author abbreviation of Anne Casimir de Candolle\n"}
{"id": "6811", "url": "https://en.wikipedia.org/wiki?curid=6811", "title": "Centers for Disease Control and Prevention", "text": "Centers for Disease Control and Prevention\n\nThe Centers for Disease Control and Prevention (CDC) is the leading national public health institute of the United States. The CDC is a United States federal agency under the Department of Health and Human Services and is headquartered in Atlanta, Georgia.\n\nIts main goal is to protect public health and safety through the control and prevention of disease, injury, and disability in the US and internationally. The CDC focuses national attention on developing and applying disease control and prevention. It especially focuses its attention on infectious disease, food borne pathogens, environmental health, occupational safety and health, health promotion, injury prevention and educational activities designed to improve the health of United States citizens. In addition, the CDC researches and provides information on non-infectious diseases such as obesity and diabetes and is a founding member of the International Association of National Public Health Institutes.\n\nThe Communicable Disease Center was founded July 1, 1946, as the successor to the World War II Malaria Control in War Areas program of the Office of National Defense Malaria Control Activities.\n\nPreceding its founding, organizations with global influence in malaria control were the Malaria Commission of the League of Nations and the Rockefeller Foundation. The Rockefeller Foundation greatly supported malaria control, sought to have the governments take over some of its efforts, and collaborated with the agency.\n\nThe new agency was a branch of the U.S. Public Health Service and Atlanta was chosen as the location because malaria was endemic in the Southern United States. The agency changed names (see infobox on top) before adopting the name \"Communicable Disease Center\" in 1946. Offices were located on the sixth floor of the Volunteer Building on Peachtree Street.\n\nWith a budget at the time of about $1million, 59 percent of its personnel were engaged in mosquito abatement and habitat control with the objective of control and eradication of malaria in the United States (see National Malaria Eradication Program).\n\nAmong its 369 employees, the main jobs at CDC were originally entomology and engineering. In CDC's initial years, more than six and a half million homes were sprayed, mostly with DDT. In 1946, there were only seven medical officers on duty and an early organization chart was drawn, somewhat fancifully, in the shape of a mosquito. Under Joseph Walter Mountin, the CDC continued to advocate for public health issues and pushed to extend its responsibilities to many other communicable diseases.\n\nIn 1947, the CDC made a token payment of $10 to Emory University for of land on Clifton Road in DeKalb County, still the home of CDC headquarters as of 2019. CDC employees collected the money to make the purchase. The benefactor behind the \"gift\" was Robert W. Woodruff, chairman of the board of The Coca-Cola Company. Woodruff had a long-time interest in malaria control, which had been a problem in areas where he went hunting. The same year, the PHS transferred its San Francisco based plague laboratory into the CDC as the Epidemiology Division, and a new Veterinary Diseases Division was established. An Epidemic Intelligence Service (EIS) was established in 1951, originally due to biological warfare concerns arising from the Korean War; it evolved into two-year postgraduate training program in epidemiology, and a prototype for Field Epidemiology Training Programs (FETP), now found in numerous countries, reflecting CDC's influence in promoting this model internationally.\n\nThe mission of CDC expanded beyond its original focus on malaria to include sexually transmitted diseases when the Venereal Disease Division of the U.S. Public Health Service (PHS) was transferred to the CDC in 1957. Shortly thereafter, Tuberculosis Control was transferred (in 1960) to the CDC from PHS, and then in 1963 the Immunization program was established.\n\nIt became the \"National Communicable Disease Center (NCDC)\" effective July 1, 1967. The organization was renamed the \"Center for Disease Control (CDC)\" on June 24, 1970, and \"Centers for Disease Control\" effective October 14, 1980. An act of the United States Congress appended the words \"and Prevention\" to the name effective October 27, 1992. However, Congress directed that the initialism \"CDC\" be retained because of its name recognition.\n\nCurrently, the CDC focus has broadened to include chronic diseases, disabilities, injury control, workplace hazards, environmental health threats, and terrorism preparedness. CDC combats emerging diseases and other health risks, including birth defects, West Nile virus, obesity, avian, swine, and pandemic flu, E. coli, and bioterrorism, to name a few. The organization would also prove to be an important factor in preventing the abuse of penicillin. In May 1994 the CDC admitted having sent several biological warfare agents to the Iraqi government from 1984 through 1989, including Botulinum toxin, West Nile virus, Yersinia pestis and Dengue fever virus.\n\nOn April 21, 2005, then–CDC Director Julie Gerberding formally announced the reorganization of CDC to \"confront the challenges of 21st-century health threats\". The four Coordinating Centers—established under the G. W. Bush Administration and Gerberding—\"diminished the influence of national centers under [their] umbrella\", and were ordered cut under the Obama Administration in 2009.\n\nToday, the CDC's Biosafety Level 4 laboratories are among the few that exist in the world, and serve as one of only two official repositories of smallpox in the world. The second smallpox store resides at the State Research Center of Virology and Biotechnology VECTOR in the Russian Federation. The CDC revealed in 2014 that it had discovered several misplaced smallpox samples and also that lab workers had potentially been infected with anthrax.\n\nThe CDC is organized into \"Centers, Institutes, and Offices\" (CIOs), with each organizational unit implementing the agency's activities in a particular area of expertise while also providing intra-agency support and resource-sharing for cross-cutting issues and specific health threats. Generally, CDC \"Offices\" are subdivided into Centers, which in turn are composed of Divisions and Branches. However, the Center for Global Health and the National Institute for Occupational Safety and Health are freestanding organizational units and do not belong to a parent Office.\n\nAs of August 2019, the CIOs are:\n\nBULLET::::- Director\nBULLET::::- Principal Deputy Director\nBULLET::::- Deputy Director - Public Health Service and Implementation Science\nBULLET::::- Office of Minority Health and Health Equity\nBULLET::::- Center for Global Health\nBULLET::::- Center for Preparedness and Response\nBULLET::::- Center for State, Tribal, Local, and Territory Support\nBULLET::::- Deputy Director - Public Health Science and Surveillance\nBULLET::::- Office of Science\nBULLET::::- Office of Laboratory Science and Safety\nBULLET::::- Center for Surveillance, Epidemiology, and Laboratory Services\nBULLET::::- National Center for Health Statistics\nBULLET::::- Deputy Director - Non-Infectious Diseases\nBULLET::::- National Center on Birth Defects and Developmental Disabilities\nBULLET::::- National Center for Chronic Disease Prevention and Health Promotion\nBULLET::::- National Center for Environmental Health and Agency for Toxic Substances and Disease Registry\nBULLET::::- National Center for Injury Prevention and Control\nBULLET::::- Deputy Director - Infectious Diseases\nBULLET::::- National Center for Immunization and Respiratory Diseases\nBULLET::::- National Center for Emerging and Zoonotic Infectious Diseases\nBULLET::::- National Center for HIV/AIDS, Viral Hepatitis, STD, and TB Prevention\nBULLET::::- National Institute for Occupational Safety and Health\nBULLET::::- Office of the Director\nBULLET::::- Chief of Staff\nBULLET::::- Chief Operating Officer\nBULLET::::- Human Resources Office\nBULLET::::- Office of Financial Resources\nBULLET::::- Office of Safety, Security, and Asset Management\nBULLET::::- Office of the Chief Information Officer\nBULLET::::- Chief Medical Officer\nBULLET::::- CDC Washington Office\nBULLET::::- Office of Equal Employment Opportunity\nBULLET::::- Associate Director - Communication\nBULLET::::- Associate Director - Laboratory Science and Safety\nBULLET::::- Associate Director - Policy and Strategy\n\nThe Office of Public Health Preparedness was created during the 2001 anthrax attacks shortly after the terrorist attacks of September 11, 2001. Its purpose was to coordinate among the government the response to a range of biological terrorism threats.\n\nCDC's budget for fiscal year 2018 is $11.9billion.\n\nIn addition to its Atlanta headquarters, CDC's other domestic locations are Anchorage, Fort Collins, Hyattsville, Research Triangle Park, San Juan (Puerto Rico), and Washington, D.C., while NIOSH operates its own facilities in Cincinnati, Morgantown, Pittsburgh, Spokane, Denver, and Anchorage. In addition, CDC operates quarantine facilities in 20 cities in the U.S.\n\nThe CDC offers grants that help many organizations each year advance health, safety and awareness at the community level throughout the United States. The CDC awards over 85 percent of its annual budget through these grants.\n\n, CDC staff numbered approximately 15,000 (including 6,000 contractors and 840 Commissioned Corps officers) in 170 occupations. Eighty percent held bachelor's degrees or higher; almost half had advanced degrees (a master's degree or a doctorate such as a PhD, D.O., or M.D.).\n\nCommon CDC job titles include engineer, entomologist, epidemiologist, biologist, physician, veterinarian, behavioral scientist, nurse, medical technologist, economist, public health advisor, health communicator, toxicologist, chemist, computer scientist, and statistician.\n\nThe CDC also operates a number of notable training and fellowship programs, including those indicated below.\n\nThe Epidemic Intelligence Service (EIS) is composed of \"boots-on-the-ground disease detectives\" who investigate public health problems domestically and globally. When called upon by a governmental body, EIS officers may embark on short-term epidemiological assistance assignments, or \"Epi-Aids\", to provide technical expertise in containing and investigating disease outbreaks. The EIS program is a model for the international Field Epidemiology Training Program.\n\nThe CDC also operates the Public Health Associate Program (PHAP), a two-year paid fellowship for recent college graduates to work in public health agencies all over the United States. PHAP was founded in 2007 and currently has 159 associates in 34 states.\n\nThe Director of CDC is a Senior Executive Service position that may be filled either by a career employee, or as a political appointment that does not require Senate confirmation, with the latter method typically being used. The director serves at the pleasure of the President and may be fired at any time. The CDC director concurrently serves as the Administrator of the Agency for Toxic Substances and Disease Registry.\n\nSixteen directors have served the CDC or its predecessor agencies.\n\nBULLET::::- Louis L. Williams Jr., MD (1942–1943)\nBULLET::::- Mark D. Hollis, ScD (1944–1946)\nBULLET::::- Raymond A. Vonderlehr, MD (1947–1951)\nBULLET::::- Justin M. Andrews, ScD (1952–1953)\nBULLET::::- Theodore J. Bauer, MD (1953–1956)\nBULLET::::- Robert J. Anderson, MD, MPH (1956–1960)\nBULLET::::- Clarence A. Smith, MD, MPH (1960–1962)\nBULLET::::- James L. Goddard, MD, MPH (1962–1966)\nBULLET::::- David J. Sencer, MD, MPH (1966–1977)\nBULLET::::- William H. Foege, MD, MPH (1977–1983)\nBULLET::::- James O. Mason, MD, MPH, Ph.D (1983–1989)\nBULLET::::- William L. Roper, MD, MPH (1990–1993)\nBULLET::::- David Satcher, MD, PhD (1993–1998)\nBULLET::::- Jeffrey P. Koplan, MD, MPH (1998–2002)\nBULLET::::- Julie Gerberding, MD, MPH (2002–2008)\nBULLET::::- Thomas R. Frieden, MD, MPH (2009 – Jan 2017)\nBULLET::::- Anne Schuchat, MD, RADM USPHS (Jan–July 2017)\nBULLET::::- Brenda Fitzgerald, MD (July 2017 – Jan 2018)\nBULLET::::- Anne Schuchat, MD (Jan–Mar 2018)\nBULLET::::- Robert R. Redfield, MD (March 2018–present)\n\nBULLET::::- CDC Scientific Data, Surveillance, Health Statistics, and Laboratory Information.\nBULLET::::- Behavioral Risk Factor Surveillance System (BRFSS), the world's largest, ongoing telephone health-survey system.\nBULLET::::- Mortality Medical Data System.\nBULLET::::- Abortion statistics in the United States\n\nThe CDC's programs address more than 400 diseases, health threats, and conditions that are major causes of death, disease, and disability. The CDC's website has information on various infectious (and noninfectious) diseases, including smallpox, measles, and others.\n\nThe CDC has launched campaigns targeting the transmission of influenza, including the H1N1 swine flu, and launched websites to educate people in proper hygiene.\n\nWithin the division are two programs: the Federal Select Agent Program (FSAP) and the Import Permit Program. The FSAP is run jointly with an office within the U.S. Department of Agriculture, regulating agents that can cause disease in humans, animals, and plants. The Import Permit Program regulates the importation of \"infectious biological materials.\"\n\nThe CDC runs a program that protects the public from rare and dangerous substances such as anthrax and the Ebola virus. The program, called the Federal Select Agent Program, calls for inspections of labs in the U.S. that work with dangerous pathogens.\n\nDuring the 2014 Ebola outbreak in West Africa, the CDC helped coordinate the return of two infected American aid workers for treatment at Emory University Hospital, the home of a special unit to handle highly infectious diseases.\n\nAs a response to the 2014 Ebola outbreak, Congress passed a Continuing Appropriations Resolution allocating $30,000,000 towards CDC's efforts to fight the virus.\n\nThe CDC also works on non-communicable diseases, including chronic diseases caused by obesity, physical inactivity and tobacco-use.\n\nThe CDC implemented their \"National Action Plan for Combating Antibiotic Resistant Bacteria\" as a measure against the spread of antibiotic resistance in the United States. This initiative has a budget of $161million and includes the development of the Antibiotic Resistance Lab Network.\n\nThe CDC works with other organizations around the world to address global health challenges and contain disease threats at their source. It works closely with many international organizations such as the World Health Organization (WHO) as well as ministries of health and other groups on the front lines of outbreaks. The agency maintains staff in more than 60 countries, including some from the U.S. but even more from the countries in which it operates. The agency's global divisions include the Division of Global HIV and TB (DGHT), the Division of Parasitic Diseases and Malaria (DPDM), the Division of Global Health Protection (DGHP), and the Global Immunization Division (GID).\n\nThe CDC is integral in working with the WHO to implement the \"International Health Regulations\" (IHR), a legally binding agreement between 196 countries to prevent, control, and report on the international spread of disease, through initiatives including the Global Disease Detection Program (GDD).\n\nThe CDC is also a lead implementer of key U.S. global health initiatives such as the President's Emergency Plan for AIDS Relief (PEPFAR) and the President's Malaria Initiative.\n\nThe CDC collects and publishes health information for travelers in a comprehensive book, \"CDC Health Information for International Travel\", which is commonly known as the \"yellow book.\" The book is available online and in print as a new edition every other year and includes current travel health guidelines, vaccine recommendations, and information on specific travel destinations. The CDC also issues travel health notices on its website, consisting of three levels:\n\n\"Watch\": Level 1 (practice usual precautions)\n\n\"Alert\": Level 2 (practice enhanced precautions)\n\n\"Warning\": Level 3 (avoid nonessential travel)\n\nThe CDC Foundation operates independently from CDC as a private, nonprofit 501(c)(3) organization incorporated in the State of Georgia. The creation of the Foundation was authorized by section 399F of the Public Health Service Act to support the mission of CDC in partnership with the private sector, including organizations, foundations, businesses, educational groups, and individuals.\n\nBecause of the Lead contamination in Washington, D.C. drinking water the United States House of Representatives conducted an investigation, that uncovered that the CDC had made claims in a report that had indicated there was no risk from high lead levels - although it is the opposite.\n\nFor 15 years, the CDC had direct oversight over the Tuskegee syphilis experiment. In the study, which lasted from 1932 to 1972, a group of African American men (nearly 400 of whom had syphilis) were studied to learn more about the disease. Notably, the disease was left untreated in the research subjects and they never gave their informed consent to serve as research subjects. The Tuskegee Study was initiated in 1932 by the Public Health Service. The CDC took over the study in 1957.\n\nThe CDC's response to the AIDS crisis in the 1980s has been criticized for promoting some public health policies that harmed HIV+ people and for providing ineffective public education. The agency's response to the 2001 anthrax attacks was also criticized for ineffective communication with other public health agencies and with the public.\n\nOn May 16, 2011, the Centers for Disease Control and Prevention's blog what to do to prepare for a zombie invasion. While the article did not claim that such a scenario was possible, it did use the popular culture appeal as a means of urging citizens to prepare for all potential hazards, such as earthquakes, tornadoes, and floods.\n\nAccording to David Daigle, the Associate Director for Communications, Public Health Preparedness and Response, the idea arose when his team was discussing their upcoming hurricane-information campaign and Daigle mused that \"we say pretty much the same things every year, in the same way, and I just wonder how many people are paying attention.\" A social-media employee mentioned that the subject of zombies had come up a lot on Twitter when she had been tweeting about the Fukushima Daiichi nuclear disaster and radiation. The team realized that a campaign like this would most likely reach a different audience from the one that normally pays attention to hurricane-preparedness warnings and went to work on the zombie campaign, launching it right before hurricane season began. \"The whole idea was, if you're prepared for a zombie apocalypse, you're prepared for pretty much anything,\" said Daigle.\n\nOnce the blog article became popular, the CDC announced an open contest for YouTube submissions of the most creative and effective videos covering preparedness for a zombie apocalypse (or apocalypse of any kind), to be judged by the \"CDC Zombie Task Force\". Submissions were open until October 11, 2011. They also released a zombie-themed graphic novella available on their website. Zombie-themed educational materials for teachers are available on the site.\n\nOne area of current partisan dispute related to CDC funding is studying gun violence. The 1996 Dickey Amendment states \"none of the funds made available for injury prevention and control at the Centers for Disease Control and Prevention may be used to advocate or promote gun control\". Advocates for gun control oppose the amendment and have tried to overturn it.\n\nIn 1992, Mark L. Rosenberg and five CDC colleagues founded the CDC's National Center for Injury Prevention and Control, with an annual budget of c. $260,000 that focused on \"identifying the root causes of firearm deaths and the best methods to prevent them\". Their first report which was published in the \"New England Journal of Medicine\" in 1993, entitled \"Gun Ownership as a Risk Factor for Homicide in the Home\" reported that the \"mere presence of a gun in a home increased the risk of a firearm-related death by 2.7 percent, and suicide fivefold—a \"huge\" increase.\" In response, the NRA launched a \"campaign to shut down the Injury Center.\" Doctors for Responsible Gun Ownership and Doctors for Integrity and Policy Research joined the pro-gun effort and by 1995, politicians also supported the pro-gun initiative. In 1996, Jay Dickey (R) Arkansas introduced the Dickey Amendment statement \"which stated \"none of the funds made available for injury prevention and control at the Centers for Disease Control and Prevention may be used to advocate or promote gun control\" as a rider in the 1996 appropriations bill.\" In 1997, \"Congress redirected all of the money previously earmarked for gun violence research to the study of traumatic brain injury.\" David Satcher, who was the CDC head from 1993 to 1998 advocated for gun violence research until he left in 1998. In 1999 Rosenberg was fired. Over a dozen \"public health insiders, including current and former CDC senior leaders\" told \"The Trace\" interviewers that CDC senior leaders took an overly cautious stance in their interpretation of the Dickey amendment. They could have done much more. Rosenberg told \"The Trace\" in 2016, \"Right now, there is nothing stopping them from addressing this life-and-death national problem.\"\n\nThe American Medical Association, the American Psychological Association and the American Academy of Pediatrics sent a letter to the leaders of the Senate Appropriations Committee in 2013 asking them \"to support at least $10million within the Centers for Disease Control and Prevention (CDC) in FY 2014 along with sufficient new funding at the National Institutes of Health to support research into the causes and prevention of gun violence. Furthermore, we urge Members to oppose any efforts to reduce, eliminate, or condition CDC funding related to gun violence prevention research.\" Congress maintained the ban in subsequent budgets.\n\nIn December 2017, \"The Washington Post\" reported that the Trump administration had issued a list of seven words that were forbidden in official CDC documentation. Yuval Levin, after contacting HHS officials, wrote in \"National Review\" that the \"Post\" story was not accurate.\n\nBULLET::::- CDC publications\nBULLET::::- State of CDC report\nBULLET::::- CDC Programs in Brief\nBULLET::::- \"Morbidity and Mortality Weekly Report\"\nBULLET::::- \"Emerging Infectious Diseases\" (monthly journal)\nBULLET::::- \"Preventing Chronic Disease\"\nBULLET::::- Vital statistics\n\nBULLET::::- U.S. Consumer Product Safety Commission\nBULLET::::- Gun violence in the United States\nBULLET::::- Haddon Matrix\nBULLET::::- Home Safety Council\nBULLET::::- National Highway Traffic Safety Administration\nBULLET::::- National Institute for Occupational Safety and Health\n\nBULLET::::- CDC in the Federal Register\nBULLET::::- CDC Online Newsroom\nBULLET::::- CDC Public Health Image Library\nBULLET::::- CDC Global Communications Center\nBULLET::::- CDC Emerging Infectious Diseases LaboratoryAtlanta, Georgia\n"}
{"id": "6813", "url": "https://en.wikipedia.org/wiki?curid=6813", "title": "Chandrasekhar limit", "text": "Chandrasekhar limit\n\nThe Chandrasekhar limit () is the maximum mass of a stable white dwarf star. The currently accepted value of the Chandrasekhar limit is about ().\n\nWhite dwarfs resist gravitational collapse primarily through electron degeneracy pressure (compare main sequence stars, which resist collapse through thermal pressure). The Chandrasekhar limit is the mass above which electron degeneracy pressure in the star's core is insufficient to balance the star's own gravitational self-attraction. Consequently, a white dwarf with a mass greater than the limit is subject to further gravitational collapse, evolving into a different type of stellar remnant, such as a neutron star or black hole. Those with masses under the limit remain stable as white dwarfs.\n\nThe limit was named after Subrahmanyan Chandrasekhar, an Indian astrophysicist who improved upon the accuracy of the calculation in 1930, at the age of 20, in India by calculating the limit for a polytrope model of a star in hydrostatic equilibrium, and comparing his limit to the earlier limit found by E. C. Stoner for a uniform density star. Importantly, the existence of a limit, based on the conceptual breakthrough of combining relativity with Fermi degeneracy, was indeed first established in separate papers published by Wilhelm Anderson and E. C. Stoner in 1929. The limit was initially ignored by the community of scientists because such a limit would logically require the existence of black holes, which were considered a scientific impossibility at the time. That the roles of Stoner and Anderson are often forgotten in the astronomy community has been noted.\n\nElectron degeneracy pressure is a quantum-mechanical effect arising from the Pauli exclusion principle. Since electrons are fermions, no two electrons can be in the same state, so not all electrons can be in the minimum-energy level. Rather, electrons must occupy a band of energy levels. Compression of the electron gas increases the number of electrons in a given volume and raises the maximum energy level in the occupied band. Therefore, the energy of the electrons increases on compression, so pressure must be exerted on the electron gas to compress it, producing electron degeneracy pressure. With sufficient compression, electrons are forced into nuclei in the process of electron capture, relieving the pressure.\n\nIn the nonrelativistic case, electron degeneracy pressure gives rise to an equation of state of the form , where is the pressure, is the mass density, and is a constant. Solving the hydrostatic equation leads to a model white dwarf that is a polytrope of index – and therefore has radius inversely proportional to the cube root of its mass, and volume inversely proportional to its mass.\n\nAs the mass of a model white dwarf increases, the typical energies to which degeneracy pressure forces the electrons are no longer negligible relative to their rest masses. The velocities of the electrons approach the speed of light, and special relativity must be taken into account. In the strongly relativistic limit, the equation of state takes the form . This yields a polytrope of index 3, which has a total mass, say, depending only on .\n\nFor a fully relativistic treatment, the equation of state used interpolates between the equations for small and for large . When this is done, the model radius still decreases with mass, but becomes zero at . This is the Chandrasekhar limit. The curves of radius against mass for the non-relativistic and relativistic models are shown in the graph. They are colored blue and green, respectively. has been set equal to 2. Radius is measured in standard solar radii or kilometers, and mass in standard solar masses.\n\nCalculated values for the limit vary depending on the nuclear composition of the mass. Chandrasekhar gives the following expression, based on the equation of state for an ideal Fermi gas:\nwhere:\nBULLET::::- is the reduced Planck constant\nBULLET::::- is the speed of light\nBULLET::::- is the gravitational constant\nBULLET::::- is the average molecular weight per electron, which depends upon the chemical composition of the star.\nBULLET::::- is the mass of the hydrogen atom.\nBULLET::::- is a constant connected with the solution to the Lane–Emden equation.\nAs is the Planck mass, the limit is of the order of\n\nA more accurate value of the limit than that given by this simple model requires adjusting for various factors, including electrostatic interactions between the electrons and nuclei and effects caused by nonzero temperature. Lieb and Yau have given a rigorous derivation of the limit from a relativistic many-particle Schrödinger equation.\n\nIn 1926, the British physicist Ralph H. Fowler observed that the relationship between the density, energy, and temperature of white dwarfs could be explained by viewing them as a gas of nonrelativistic, non-interacting electrons and nuclei that obey Fermi–Dirac statistics. This Fermi gas model was then used by the British physicist Edmund Clifton Stoner in 1929 to calculate the relationship among the mass, radius, and density of white dwarfs, assuming they were homogeneous spheres. Wilhelm Anderson applied a relativistic correction to this model, giving rise to a maximum possible mass of approximately . In 1930, Stoner derived the internal energy–density equation of state for a Fermi gas, and was then able to treat the mass–radius relationship in a fully relativistic manner, giving a limiting mass of approximately (for ). Stoner went on to derive the pressure–density equation of state, which he published in 1932. These equations of state were also previously published by the Soviet physicist Yakov Frenkel in 1928, together with some other remarks on the physics of degenerate matter. Frenkel's work, however, was ignored by the astronomical and astrophysical community.\n\nA series of papers published between 1931 and 1935 had its beginning on a trip from India to England in 1930, where the Indian physicist Subrahmanyan Chandrasekhar worked on the calculation of the statistics of a degenerate Fermi gas. In these papers, Chandrasekhar solved the hydrostatic equation together with the nonrelativistic Fermi gas equation of state, and also treated the case of a relativistic Fermi gas, giving rise to the value of the limit shown above. Chandrasekhar reviews this work in his Nobel Prize lecture. This value was also computed in 1932 by the Soviet physicist Lev Davidovich Landau, who, however, did not apply it to white dwarfs and concluded that quantum laws might be invalid for stars heavier than 1.5 solar mass.\n\nChandrasekhar's work on the limit aroused controversy, owing to the opposition of the British astrophysicist Arthur Eddington. Eddington was aware that the existence of black holes was theoretically possible, and also realized that the existence of the limit made their formation possible. However, he was unwilling to accept that this could happen. After a talk by Chandrasekhar on the limit in 1935, he replied:\n\nEddington's proposed solution to the perceived problem was to modify relativistic mechanics so as to make the law universally applicable, even for large . Although Niels Bohr, Fowler, Wolfgang Pauli, and other physicists agreed with Chandrasekhar's analysis, at the time, owing to Eddington's status, they were unwilling to publicly support Chandrasekhar. Through the rest of his life, Eddington held to his position in his writings, including his work on his fundamental theory. The drama associated with this disagreement is one of the main themes of \"Empire of the Stars\", Arthur I. Miller's biography of Chandrasekhar. In Miller's view:\n\nThe core of a star is kept from collapsing by the heat generated by the fusion of nuclei of lighter elements into heavier ones. At various stages of stellar evolution, the nuclei required for this process are exhausted, and the core collapses, causing it to become denser and hotter. A critical situation arises when iron accumulates in the core, since iron nuclei are incapable of generating further energy through fusion. If the core becomes sufficiently dense, electron degeneracy pressure will play a significant part in stabilizing it against gravitational collapse.\n\nIf a main-sequence star is not too massive (less than approximately 8 solar masses), it eventually sheds enough mass to form a white dwarf having mass below the Chandrasekhar limit, which consists of the former core of the star. For more-massive stars, electron degeneracy pressure does not keep the iron core from collapsing to very great density, leading to formation of a neutron star, black hole, or, speculatively, a quark star. (For very massive, low-metallicity stars, it is also possible that instabilities destroy the star completely.) During the collapse, neutrons are formed by the capture of electrons by protons in the process of electron capture, leading to the emission of neutrinos. The decrease in gravitational potential energy of the collapsing core releases a large amount of energy on the order of 10 joules (100 foes). Most of this energy is carried away by the emitted neutrinos. This process is believed responsible for supernovae of types Ib, Ic, and II.\n\nType Ia supernovae derive their energy from runaway fusion of the nuclei in the interior of a white dwarf. This fate may befall carbon–oxygen white dwarfs that accrete matter from a companion giant star, leading to a steadily increasing mass. As the white dwarf's mass approaches the Chandrasekhar limit, its central density increases, and, as a result of compressional heating, its temperature also increases. This eventually ignites nuclear fusion reactions, leading to an immediate carbon detonation, which disrupts the star and causes the supernova.\n\nA strong indication of the reliability of Chandrasekhar's formula is that the absolute magnitudes of supernovae of Type Ia are all approximately the same; at maximum luminosity, is approximately −19.3, with a standard deviation of no more than 0.3. A 1-sigma interval therefore represents a factor of less than 2 in luminosity. This seems to indicate that all type Ia supernovae convert approximately the same amount of mass to energy.\n\nIn April 2003, the Supernova Legacy Survey observed a type Ia supernova, designated SNLS-03D3bb, in a galaxy approximately 4 billion light years away. According to a group of astronomers at the University of Toronto and elsewhere, the observations of this supernova are best explained by assuming that it arose from a white dwarf that grew to twice the mass of the Sun before exploding. They believe that the star, dubbed the \"Champagne Supernova\" by University of Oklahoma astronomer David R. Branch, may have been spinning so fast that a centrifugal tendency allowed it to exceed the limit. Alternatively, the supernova may have resulted from the merger of two white dwarfs, so that the limit was only violated momentarily. Nevertheless, they point out that this observation poses a challenge to the use of type Ia supernovae as standard candles.\n\nSince the observation of the Champagne Supernova in 2003, more very bright type Ia supernovae have been observed that are thought to have originated from white dwarfs whose masses exceeded the Chandrasekhar limit. These include SN 2006gz, SN 2007if and SN 2009dc. The super-Chandrasekhar mass white dwarfs that gave rise to these supernovae are believed to have had masses up to 2.4–2.8 solar masses. One way to potentially explain the problem of the Champagne Supernova was considering it the result of an aspherical explosion of a white dwarf. However, spectropolarimetric observations of SN 2009dc showed it had a polarization smaller than 0.3, making the large asphericity theory unlikely.\n\nAfter a supernova explosion, a neutron star may be left behind. These objects are even more compact than white dwarfs and are also supported, in part, by degeneracy pressure. A neutron star, however, is so massive and compressed that electrons and protons have combined to form neutrons, and the star is thus supported by neutron degeneracy pressure (as well as short-range repulsive neutron-neutron interactions mediated by the strong force) instead of electron degeneracy pressure. The limiting value for neutron star mass, analogous to the Chandrasekhar limit, is known as the Tolman–Oppenheimer–Volkoff limit.\n\nBULLET::::- Bekenstein bound\nBULLET::::- Schönberg–Chandrasekhar limit\n\nBULLET::::- \"On Stars, Their Evolution and Their Stability\", Nobel Prize lecture, Subrahmanyan Chandrasekhar, December 8, 1983.\nBULLET::::- \"White dwarf stars and the Chandrasekhar limit\", Masters' thesis, Dave Gentile, DePaul University, 1995.\nBULLET::::- Estimating Stellar Parameters from Energy Equipartition, sciencebits.com. Discusses how to find mass-radius relations and mass limits for white dwarfs using simple energy arguments.\n"}
{"id": "6814", "url": "https://en.wikipedia.org/wiki?curid=6814", "title": "Congregationalist polity", "text": "Congregationalist polity\n\nCongregationalist polity, or congregational polity, often known as congregationalism, is a system of ecclesiastical polity in which every local church congregation is independent, ecclesiastically sovereign, or \"autonomous\". Its first articulation in writing is the Cambridge Platform of 1648 in New England. Among those major Protestant Christian traditions that employ congregationalism are those Congregational churches known by the \"Congregationalist\" name that descended from the Independent Reformed wing of the Anglo-American Puritan movement of the 17th century, Quakerism, the Baptist churches, as well as the Congregational Methodist Church. More recent generations have witnessed also a growing number of nondenominational churches, which are most often congregationalist in their governance.\n\nCongregationalism is distinguished from episcopal polity which is governance by a hierarchy of bishops, and is distinct from presbyterian polity in which higher assemblies of congregational representatives can exercise considerable authority over individual congregations.\n\nCongregationalism is not limited only to organization of Christian church congregations. The principles of congregationalism have been inherited by the Unitarian Universalist Association and the Canadian Unitarian Council. Most Jewish synagogues, many Sikh Gurdwaras and most Islamic mosques in the US operate under congregational government, with no hierarchies.\n\nThe term \"congregationalist polity\" describes a form of church governance that is based on the local congregation. Each local congregation is independent and self-supporting, governed by its own members. Some band into loose voluntary associations with other congregations that share similar beliefs (e.g., the Willow Creek Association and the American Unitarian Association). Others join \"conventions\", such as the Southern Baptist Convention, the National Baptist Convention or the American Baptist Churches USA (formerly the Northern Baptist Convention). In Quaker Congregationalism, monthly meetings, which are the most basic unit of administration, may be organized into larger Quarterly meetings or Yearly Meetings. Monthly, quarterly, or yearly meetings may also be associated with large \"umbrella\" associations such as Friends General Conference or Friends United Meeting. These conventions generally provide stronger ties between congregations, including some doctrinal direction and pooling of financial resources. Congregations that belong to associations and conventions are still independently governed. Most non-denominational churches are organized along congregationalist lines. Many do not see these voluntary associations as \"denominations\", because they \"believe that there is no church other than the local church, and denominations are in variance to Scripture.\"\n\nThe earmarks of Congregationalism can be traced back to the Pilgrim societies of the United States in the early 17th century. Congregationalism expressed the viewpoint that (1) every local church is a full realization in miniature of the entire Church of Jesus Christ; and (2) the Church, while on earth, besides the local church, can only be invisible and ideal. While other theories may insist on the truth of the former, the latter precept of congregationalism gives the entire theory a unique character among plans of church government. There is no other reference than the local congregation for the \"visible church\" in Congregationalism. And yet, the connection of all Christians is also asserted, albeit in a way that defenders of this view usually decline, often intentionally, to elaborate more clearly or consistently. This first, foundational principle by which congregationalism is guided results in confining it to operate with the consent of each gathering of believers.\n\nAlthough \"congregational rule\" may seem to suggest that pure democracy reigns in congregational churches, this is seldom the case. It is granted, with few exceptions (namely in some Anabaptist churches), that God has given the government of the Church into the hands of an ordained ministry. What makes congregationalism unique is its system of checks and balances, which constrains the authority of the clergy, the lay officers, and the members.\n\nMost importantly, the boundaries of the powers of the ministers and church officers are set by clear and constant reminders of the freedoms guaranteed by the Gospel to the laity, collectively and individually. With that freedom comes the responsibility upon each member to govern himself or herself under Christ. This requires lay people to exercise great charity and patience in debating issues with one another and to seek the glory and service of God as the foremost consideration in all of their decisions.\n\nThe authority of all of the people, including the officers, is limited in the local congregation by a definition of union, or a covenant, by which the terms of their cooperation together are spelled out and agreed to. This might be something as minimal as a charter specifying a handful of doctrines and behavioral expectations, or even a statement only guaranteeing specific freedoms. Or, it may be a constitution describing a comprehensive doctrinal system and specifying terms under which the local church is connected to other local churches, to which participating congregations give their assent. In congregationalism, rather uniquely, the church is understood to be a truly voluntary association.\n\nFinally, the congregational theory strictly forbids ministers from ruling their local churches by themselves. Not only does the minister serve by the approval of the congregation, but committees further constrain the pastor from exercising power without consent by either the particular committee, or the entire congregation. It is a contradiction of the congregational principle if a minister makes decisions concerning the congregation without the vote of these other officers.\n\nThe other officers may be called \"deacons\", \"elder\" or \"session\" (borrowing Presbyterian terminology), or even \"vestry\" (borrowing the Anglican term) – it is not their label that is important to the theory, but rather their lay status and their equal vote, together with the pastor, in deciding the issues of the church. While other forms of church government are more likely to define \"tyranny\" as \"the imposition of unjust rule\", a congregationally governed church would more likely define \"tyranny\" as \"transgression of liberty\" or equivalently, \"rule by one man\". To a congregationalist, no abuse of authority is worse than the concentration of all decisive power in the hands of one ruling body, or one person.\n\nFollowing this sentiment, congregationalism has evolved over time to include even more participation of the congregation, more kinds of lay committees to whom various tasks are apportioned, and more decisions subject to the vote of the entire membership.\n\nOne of the most notable characteristics of New England (or British)-heritage Congregationalism has been its consistent leadership role in the formation of \"unions\" with other churches. Such sentiments especially grew strong in the late 19th and early 20th centuries, when ecumenism evolved out of a liberal, non-sectarian perspective on relations to other Christian groups that accompanied the relaxation of Calvinist stringencies held by earlier generations. The congregationalist theory of independence within a union has been a cornerstone of most ecumenical movements since the 18th century.\n\nMost Baptists hold that no church or ecclesiastical organization has inherent authority over a Baptist church. Churches can properly relate to each other under this polity only through voluntary cooperation, never by any sort of coercion. Furthermore, this Baptist polity calls for freedom from governmental control. Exceptions to this local form of local governance include a few churches that submit to the leadership of a body of elders, as well as the Episcopal Baptists that have an episcopal system.\n\nIndependent Baptist churches have no formal organizational structure above the level of the local congregation. More generally among Baptists, a variety of parachurch agencies and evangelical educational institutions may be supported generously or not at all, depending entirely upon the local congregation's customs and predilections. Usually doctrinal conformity is held as a first consideration when a church makes a decision to grant or decline financial contributions to such agencies, which are legally external and separate from the congregations they serve. These practices also find currency among non-denominational fundamentalist or charismatic fellowships, many of which derive from Baptist origins, culturally if not theologically.\n\nMost Southern Baptist and National Baptist congregations, by contrast, generally relate more closely to external groups such as mission agencies and educational institutions than do those of independent persuasion. However, they adhere to a very similar ecclesiology, refusing to permit outside control or oversight of the affairs of the local church.\n\nEcclesiastical government is congregational rather than denominational. Churches of Christ purposefully have no central headquarters, councils, or other organizational structure above the local church level. Rather, the independent congregations are a network with each congregation participating at its own discretion in various means of service and fellowship with other congregations. Churches of Christ are linked by their shared commitment to restoration principles.\n\nCongregations are generally overseen by a plurality of elders (also known in some congregations as shepherds, bishops, or pastors) who are sometimes assisted in the administration of various works by deacons. Elders are generally seen as responsible for the spiritual welfare of the congregation, while deacons are seen as responsible for the non-spiritual needs of the church. Deacons serve under the supervision of the elders, and are often assigned to direct specific ministries. Successful service as a deacon is often seen as preparation for the eldership. Elders and deacons are chosen by the congregation based on the qualifications found in Timothy 3 and Titus 1. Congregations look for elders who have a mature enough understanding of scripture to enable them to supervise the minister and to teach, as well as to perform governance functions. In lieu of willing men who meet these qualifications, congregations are sometimes overseen by an unelected committee of the congregation's men.\n\nWhile the early Restoration Movement had a tradition of itinerant preachers rather than \"located Preachers\", during the 20th century a long-term, formally trained congregational minister became the norm among Churches of Christ. Ministers are understood to serve under the oversight of the elders. While the presence of a long-term professional minister has sometimes created \"significant \"de facto\" ministerial authority\" and led to conflict between the minister and the elders, the eldership has remained the \"ultimate locus of authority in the congregation\".\n\nChurches of Christ hold to the priesthood of all believers. No special titles are used for preachers or ministers that would identify them as clergy. Churches of Christ emphasize that there is no distinction between \"clergy\" and \"laity\" and that every member has a gift and a role to play in accomplishing the work of the church.\n\nMethodists who disagreed with the episcopal polity of the Methodist Episcopal Church, South (MECS) left their mother church to form the Congregational Methodist Church, which retains Wesleyan-Arminian theology but adopts congregationalist polity as a distinctive.\n\nBULLET::::- United and uniting churches\n\n"}
