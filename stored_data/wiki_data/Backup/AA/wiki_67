{"id": "6991", "url": "https://en.wikipedia.org/wiki?curid=6991", "title": "Cimabue", "text": "Cimabue\n\nCimabue (, , – 1302), also known as Cenni di Pepo or Cenni di Pepi, was an Italian painter and designer of mosaics from Florence.\n\nAlthough heavily influenced by Byzantine models, Cimabue is generally regarded as one of the first great Italian painters to break from the Italo-Byzantine style. While medieval art then was scenes and forms that appeared relatively flat and highly stylized, Cimabue's figures were depicted with more advanced lifelike proportions and shading than other artists of his time. According to Italian painter and historian Giorgio Vasari, Cimabue was the teacher of Giotto, the first great artist of the Italian Proto-Renaissance. However, many scholars today tend to discount Vasari's claim by citing earlier sources that suggest otherwise.\n\nLittle is known about Cimabue's early life. One source that recounts his career is Vasari's \"Lives of the Most Excellent Painters, Sculptors, and Architects\", but its accuracy is uncertain. \nHe was born in Florence and died in Pisa. Hayden Maginnis speculates that he could have trained in Florence under masters who were culturally connected to Byzantine art.\n\nItalian art historian Pietro Toesca attributed the \"Crucifixion\" in the church of San Domenico in Arezzo to Cimabue, dating around 1270, making it the earliest known attributed work that departs from the Byzantine style. Cimabue's Christ is bent, and the clothes have the golden striations that were introduced by Coppo di Marcovaldo.\n\nAround 1272, Cimabue is documented as being present in Rome, and a little later he made another \"Crucifix\" for the Florentine church of Santa Croce. Now restored, having been damaged by the 1966 Arno River flood, the work was larger and more advanced than the one in Arezzo, with traces of naturalism perhaps inspired by the works of Nicola Pisano.\n\nAccording to Vasari, Cimabue, while travelling from Florence to Vespignano, came upon the 10-year-old Giotto (c. 1277) drawing his sheep with a rough rock upon a smooth stone. He asked if Giotto would like to come and stay with him, which the child accepted with his father's permission. Vasari ellaborates that during Giotto's apprenticeship, he allegedly painted a fly on the nose of a portrait Cimabue was working on; the teacher attempted to sweep the fly away several times before he understood his pupil's prank. Many scholars now discount Vasari's claim that he took Giotto as his pupil, citing earlier sources that suggest otherwise.\n\nAround 1280, Cimabue painted the \"Maestà\", originally displayed in the church of San Francesco at Pisa, but now at the Louvre. This work established a style that was followed subsequently by numerous artists, including Duccio di Buoninsegna in his \"Rucellai Madonna\" (in the past, wrongly attributed to Cimabue) as well as Giotto. Other works from the period, which were said to have heavily influenced Giotto, include a \"Flagellation\" (Frick Collection), mosaics for the Baptistery of Florence (now largely restored), the \"Maestà\" at the Santa Maria dei Servi in Bologna and the \"Madonna\" in the Pinacoteca of Castelfiorentino. A workshop painting, perhaps assignable to a slightly later period, is the \"Maestà with Saints Francis and Dominic\" currently housed in the Uffizi.\n\nDuring the pontificate of Pope Nicholas IV, the first Franciscan pope, Cimabue worked in Assisi. At Assisi, in the transept of the Lower Basilica of San Francesco, he created a fresco named \"Madonna with Child Enthroned, Four Angels and St Francis\". The left portion of this fresco is lost, but it may have shown St Anthony of Padua (the authorship of the painting has been recently disputed for technical and stylistic reasons). Cimabue was subsequently commissioned to decorate the apse and the transept of the Upper Basilica of Assisi, in the same period of time that Roman artists were decorating the nave. The cycle he created there comprises scenes from the Gospels, the lives of the Virgin Mary, St Peter and St Paul. The paintings are now in poor condition because of oxidation of the brighter colours that were used by the artist.\n\nThe \"Maestà of Santa Trinita\", dated to c. 1290–1300, which was originally painted for the church of Santa Trinita in Florence, is now in the Uffizi Gallery. The softer expression of the characters suggests that it was influenced by Giotto, who was by then already active as a painter.\n\nCimabue spent the last period of his life, 1301 to 1302, in Pisa. There, he was commissioned to finish a mosaic of Christ Enthroned, originally begun by Maestro Francesco, in the apse of the city's cathedral. Cimabue was to create the part of the mosaic depicting St John the Evangelist, which remains the sole surviving work documented as being by the artist. Cimabue died around 1302.\n\nAccording to Vasari, quoting a contemporary of Cimabue, \"Cimabue of Florence was a painter who lived during the author's own time, a nobler man than anyone knew but he was as a result so haughty and proud that if someone pointed out to him any mistake or defect in his work, or if he had noted any himself... he would immediately destroy the work, no matter how precious it might be.\" \n\nThe nickname Cimabue translates as \"bull-head\" but also possibly as \"one who crushes the views of others\", from the Latin word \"cimare\", meaning \"top\", \"shear\", and \"blunt\". The conclusion for the second meaning is drawn from similar commentaries on Dante, who was also known \"for being contemptuous of criticism\".\n\nHistory has long regarded Cimabue as the last of an era that was overshadowed by the Italian Renaissance. As early as 1543, Vasari wrote of Cimabue, \"Cimabue was, in one sense, the principal cause of the renewal of painting,\" with the qualification that, \"Giotto truly eclipsed Cimabue's fame just as a great light eclipses a much smaller one.\"\n\nIn Canto XI of his \"Purgatorio\", Dante laments Cimabue's quick loss of public interest in the face of Giotto's revolution in art:\n\n<poem>\nO vanity of human powers,\nhow briefly lasts the crowning green of glory,\nunless an age of darkness follows!\nIn painting Cimabue thought he held the field\nbut now it's Giotto has the cry,\nso that the other's fame is dimmed.\n</poem>\n\nOn 27 October 2019 \"Christ Mocked\", discovered the previous month in northern France, in the kitchen of an elderly French woman, sold for €24m (£20m; $26.6m) at auction, setting a new record. The sale price was four times the estimate. Acteon Auction House said the sum, paid by an anonymous buyer from northern France, was a new world record for a medieval painting sold at auction.\n\n\nBULLET::::- Cimabue. Pictures and Biography\nBULLET::::- Cimabue \"Santa Trinita Madonna\" (1280–1290). A video discussion about the painting from smarthistory.khanacademy.org\n"}
{"id": "6997", "url": "https://en.wikipedia.org/wiki?curid=6997", "title": "Corporatocracy", "text": "Corporatocracy\n\nCorporatocracy (, from corporate and ), short form corpocracy, is a recent term used to refer to an economic and political system controlled by corporations or corporate interests. It is most often used as a term to describe the economic situation in the United States. This is different from corporatism, which is the organisation of society into groups with common interests. Corporatocracy as a term is often used by observers across the political spectrum.\n\nThis collective is what author C Wright Mills in 1956 called the \"power elite\", wealthy individuals who hold prominent positions in corporatocracies. They control the process of determining a society's economic and political policies.\n\nThe concept has been used in explanations of bank bailouts, excessive pay for CEOs as well as complaints such as the exploitation of national treasuries, people and natural resources. It has been used by critics of globalization, sometimes in conjunction with criticism of the World Bank or unfair lending practices as well as criticism of \"free trade agreements\".\n\nUnregulated capitalism will naturally result in the development of corporate monopolies, such as in the Gilded Age in the United States. During this period, corruption was rampant as business leaders spent significant amounts of money ensuring that government did not regulate their activities. Historian Howard Zinn argues that the U.S. government was acting exactly as Karl Marx described capitalist states: \"pretending neutrality to maintain order, but serving the interests of the rich\". Government regulations, such as the Sherman Antitrust Act of 1890, were passed in order to ensure market competition. During the neoliberal period, the weakening of such regulations, along with globalization and increased financialization, saw the entrenchment of corporate power on a global scale and the rise of what economist Joseph Stiglitz describes as \"global behemoths\" (primarily from the United States) such as Apple, Microsoft, Google, Cisco, and Oracle. This development coincided with a sharp increase in economic inequality and a global race to the bottom, which Ramaa Vasudevan, associate professor of economics at Colorado State University, describes as a \"relentless quest for cheap labor.\"\nEdmund Phelps published an analysis in 2010 theorizing that the cause of income inequality is not free market capitalism, but instead is the result of the rise of corporatization. In this view, corporatization is the antithesis of free market capitalism. It is characterized by semi-monopolistic organizations and banks, big employer confederations, often acting with complicit state institutions, in ways that discourage (or block) the natural workings of a free economy. The primary effects of corporatization are the consolidation of economic power and wealth, with the end results being the attrition of entrepreneurial and free market dynamism.\n\nHis follow-up book, \"Mass Flourishing\", further defines corporatization by the following attributes: power-sharing between government and large corporations (exemplified in the U.S. by widening government power in areas such as financial services, healthcare, energy, law enforcement/prison systems, and the military through regulation and outsourcing), an expansion of corporate lobbying and campaign support in exchange for government reciprocity, escalation in the growth and influence of financial and banking sectors, increased consolidation of the corporate landscape through merger and acquisition (with ensuing increases in corporate executive compensation), increased potential for corporate/government corruption and malfeasance, and a lack of entrepreneurial and small business development leading to lethargic and stagnant economic conditions.\n\nWith regard to income inequality, the 2014 income analysis of University of California, Berkeley economist Emmanuel Saez confirms that relative growth of income and wealth is not occurring among small and mid-sized entrepreneurs and business owners (who generally populate the lower half of top one per-centers in income), but instead only among the top .1 percent of income distribution, whom economics Nobel Prize winner, Paul Krugman describes as \"super-elites – corporate bigwigs and financial wheeler-dealers\" who earn $2,000,000 or more every year.\n\nEconomist Jeffrey Sachs described the United States as a corporatocracy in \"The Price of Civilization\" (2011). He suggested that it arose from four trends: weak national parties and strong political representation of individual districts, the large U.S. military establishment after World War II, large corporations using money to finance election campaigns, and globalization tilting the balance of power away from workers.\n\nCorporate power can also increase income inequality. Nobel Prize winner of economics Joseph Stiglitz wrote in May 2011: \"Much of today’s inequality is due to manipulation of the financial system, enabled by changes in the rules that have been bought and paid for by the financial industry itself—one of its best investments ever. The government lent money to financial institutions at close to zero percent interest and provided generous bailouts on favorable terms when all else failed. Regulators turned a blind eye to a lack of transparency and to conflicts of interest.\" Stiglitz explained that the top 1% got nearly \"one-quarter\" of the income and own approximately 40% of the wealth.\n\nMeasured relative to GDP, total compensation and its component wages and salaries have been declining since 1970. This indicates a shift in income from labor (persons who derive income from hourly wages and salaries) to capital (persons who derive income via ownership of businesses, land and assets).\n\nSome five percent of U.S. GDP was approximately $850 billion in 2013. This represents an additional $7,000 in compensation for each of the 120 million U.S. households. Larry Summers estimated in 2007 that the lower 80% of families were receiving $664 billion less income than they would be with a 1979 income distribution (a period of much greater equality), or approximately $7,000 per family.\n\nNot receiving this income may have led many families to increase their debt burden, a significant factor in the 2007–2009 subprime mortgage crisis, as highly leveraged homeowners suffered a much larger reduction in their net worth during the crisis. Further, since lower income families tend to spend relatively more of their income than higher income families, shifting more of the income to wealthier families may slow economic growth.\n\nSome large U.S. corporations have used a strategy called tax inversion to change their headquarters to a non-U.S. country to reduce their tax liability. About 46 companies have reincorporated in low-tax countries since 1982, including 15 since 2012. Six more also planned to do so in 2015.\n\nOne indication of increasing corporate power was the removal of restrictions on their ability to buy back stock, contributing to increased income inequality. Writing in the \"Harvard Business Review\" in September 2014, William Lazonick blamed record corporate stock buybacks for reduced investment in the economy and a corresponding impact on prosperity and income inequality. Between 2003 and 2012, the 449 companies in the S&P 500 used 54% of their earnings ($2.4 trillion) to buy back their own stock. An additional 37% was paid to stockholders as dividends. Together, these were 91% of profits. This left little for investment in productive capabilities or higher income for employees, shifting more income to capital rather than labor. He blamed executive compensation arrangements, which are heavily based on stock options, stock awards and bonuses for meeting earnings per share (EPS) targets. EPS increases as the number of outstanding shares decreases. Legal restrictions on buybacks were greatly eased in the early 1980s. He advocates changing these incentives to limit buybacks.\n\nIn the 12 months to March 31, 2014, S&P 500 companies increased their stock buyback payouts by 29% year on year, to $534.9 billion. U.S. companies are projected to increase buybacks to $701 billion in 2015 according to Goldman Sachs, an 18% increase over 2014. For scale, annual non-residential fixed investment (a proxy for business investment and a major GDP component) was estimated to be about $2.1 trillion for 2014.\n\nBrid Brennan of the Transnational Institute explained how concentration of corporations increases their influence over government: ”It’s not just their size, their enormous wealth and assets that make the TNCs [transnational corporations] dangerous to democracy. It’s also their concentration, their capacity to influence, and often infiltrate, governments and their ability to act as a genuine international social class in order to defend their commercial interests against the common good. It is such decision making power as well as the power to impose deregulation over the past 30 years, resulting in changes to national constitutions, and to national and international legislation which has created the environment for corporate crime and impunity.\"\n\nAn example of such industry concentration is in banking. The top 5 U.S. banks had approximately 30% of the U.S. banking assets in 1998; this rose to 45% by 2008 and to 48% by 2010, before falling to 47% in 2011.\n\nThe Economist also explained how an increasingly profitable corporate financial and banking sector caused Gini coefficients to rise in the U.S. since 1980: \"Financial services' share of GDP in America doubled to 8% between 1980 and 2000; over the same period their profits rose from about 10% to 35% of total corporate profits, before collapsing in 2007–09. Bankers are being paid more, too. In America the compensation of workers in financial services was similar to average compensation until 1980. Now it is twice that average.\"\n\nThe summary argument, considering these findings, is that if corporatization is the consolidation and sharing of economic and political power between large corporations and the state ... then a corresponding concentration of income and wealth (with resulting income inequality) is an expected by-product of such a consolidation.\n\nCorporations have significant influence on the regulations and regulators that monitor them. For example, Senator Elizabeth Warren explained in December 2014 how an omnibus spending bill required to fund the government was modified late in the process to weaken banking regulations. The modification made it easier to allow taxpayer-funded bailouts of banking \"swaps entities\", which the Dodd-Frank banking regulations prohibited. She singled out Citigroup, one of the largest banks, which had a role in modifying the legislation. She also explained how both Wall Street bankers and members of the government that formerly had worked on Wall Street stopped bi-partisan legislation that would have broken up the largest banks. She repeated President Theodore Roosevelt's warnings regarding powerful corporate entities that threatened the \"very foundations of Democracy.\"\n\nSeveral companies that typify corporatocracy power structures are listed below by incorporation date:\n\nBULLET::::- 1600: Company rule in India by the British East India Company\nBULLET::::- 1602: Dutch East India Company\nBULLET::::- 1616: Danish East India Company\nBULLET::::- 1664: French East India Company\n\nBULLET::::- 1621: Dutch West India Company\nBULLET::::- 1671: Danish West India Company\nBULLET::::- 1674: French West India Company\n\nBULLET::::- 1670: The Hudson's Bay Company which operated as not only a monopoly, but the de facto government, in parts of North America which would later become Canada and the United States\n\nBULLET::::- 1672: Compagnie du Sénégal\nBULLET::::- 1879: International Association of the Congo\nBULLET::::- 1889: Company rule in Rhodesia by the British South Africa Company\n\nBULLET::::- 1899: United Fruit Company (which later became Chiquita Brands International), operating as a banana republic in Guatemala, Costa Rica, and Honduras\nBULLET::::- 1924: Standard Fruit Company (which later became Dole Food Company), operating as a banana republic in Honduras and other countries\n\nCorporations have held the right to vote in some jurisdictions. For example, livery companies currently appoint most of the voters for the City of London Corporation, which is the municipal government for the area centered on the financial district.\n\nBULLET::::- Weyland-Yutani of the \"Alien\" movie franchise\nBULLET::::- Nea So Copros in the novel \"Cloud Atlas\"\nBULLET::::- The TV series \"Continuum\"\nBULLET::::- The TV series \"Dark Matter\"\nBULLET::::- The Caldari State in the video game \"EVE Online\"\nBULLET::::- The novel \"Jennifer Government\"\nBULLET::::- The \"MaddAddam\" trilogy of novels\nBULLET::::- Omni Consumer Products in the \"Robocop\" movie franchise\nBULLET::::- TriOptimum in the video game \"System Shock 2\"\nBULLET::::- The New Conglomerate in the video game \"PlanetSide 2\"\nBULLET::::- The movie \"Rollerball\"\nBULLET::::- The nation of Cascadia, ruled by the Conglomerate, in the video game \"Mirror's Edge Catalyst\"\nBULLET::::- The United States Government in the comic book series \"Marvel 2099\"\nBULLET::::- The novel \"Snow Crash\"\nBULLET::::- The Trade Federation and the Confederacy of Independent Systems in the \"Star Wars\" movie franchise\nBULLET::::- Spiga Biotech from the TV series \"Incorporated\"\nBULLET::::- The Umbrella Corporation from the \"Resident Evil\" video game and movie franchise\nBULLET::::- The Kel-Morian Combine from the Starcraft video game\nBULLET::::- The Usean Continent under Neucom and General Resource, from the video game \"\"\nBULLET::::- TF Industries, from the video game \"Team Fortress 2\"\nBULLET::::- Vault-Tec Corporation, from the \"Fallout\" video game franchise\nBULLET::::- Buy n Large in the Pixar movie \"WALL-E\"\nBULLET::::- The movie \"War, Inc.\"\nBULLET::::- The American Trade Organisation also known as The Corporation from State of Emergency (video game)\nBULLET::::- The different manufacturers from the Borderlands (video game) franchise\nBULLET::::- The Shinra electric power Company ruling the city of Midgar Final Fantasy VII\nBULLET::::- Pepsi Presents New Zanzibar in \"Simpsons Safari\"\nBULLET::::- The many corporations from The Outer Worlds video game franchise.\n\nBULLET::::- Works\nBULLET::::- \"The Corporation\" (film)\nBULLET::::- \"The Power Elite\" (book)\n\nBULLET::::- lecture on Corporatocracy John Perkins lecture on Corporatocracy\nBULLET::::- Teaching for Democracy in an Age of Corporatocracy by Christine E. Sleeter, Teachers College, Columbia University.\nBULLET::::- Crimes of Globalization: The Impact of U.S. Corporatocracy in Third World Countries by John Flores-Hidones\n"}
{"id": "6999", "url": "https://en.wikipedia.org/wiki?curid=6999", "title": "Culture of Canada", "text": "Culture of Canada\n\nThe culture of Canada embodies the artistic, culinary, literary, humour, musical, political and social elements that are representative of Canada and Canadians. Throughout Canada's history, its culture has been influenced by European culture and traditions, especially British and French, and by its own indigenous cultures. Over time, elements of the cultures of Canada's immigrant populations have become incorporated to form a Canadian cultural mosaic. The population has also been influenced by American culture because of a shared language, proximity, television and migration between the two countries.\n\nCanada is often characterized as being \"very progressive, diverse, and multicultural\". Canada's federal government has often been described as the instigator of multicultural ideology because of its public emphasis on the social importance of immigration. Canada's culture draws from its broad range of constituent nationalities, and policies that promote a just society are constitutionally protected. Canadian Government policies—such as publicly funded health care; higher and more progressive taxation; outlawing capital punishment; strong efforts to eliminate poverty; an emphasis on cultural diversity; strict gun control; the legalization of same-sex marriage, pregnancy terminations, euthanasia and cannabis — are social indicators of the country's political and cultural values. Canadians identify with the country's institutions of health care, military peacekeeping, the national park system and the \"Canadian Charter of Rights and Freedoms\".\n\nThe Canadian government has influenced culture with programs, laws and institutions. It has created crown corporations to promote Canadian culture through media, such as the Canadian Broadcasting Corporation (CBC) and the National Film Board of Canada (NFB), and promotes many events which it considers to promote Canadian traditions. It has also tried to protect Canadian culture by setting legal minimums on Canadian content in many media using bodies like the Canadian Radio-television and Telecommunications Commission (CRTC).\n\nFor thousands of years Canada has been inhabited by indigenous peoples from a variety of different cultures and of several major linguistic groupings. Although not without conflict and bloodshed, early European interactions with First Nations and Inuit populations in what is now Canada were arguably peaceful. First Nations and Métis peoples played a critical part in the development of European colonies in Canada, particularly for their role in assisting European coureur des bois and voyageurs in the exploration of the continent during the North American fur trade. Combined with late economic development in many regions, this comparably nonbelligerent early history allowed indigenous Canadians to have a lasting influence on the national culture (see: The Canadian Crown and Aboriginal peoples). Over the course of three centuries, countless North American Indigenous words, inventions, concepts, and games have become an everyday part of Canadian language and use. Many places in Canada, both natural features and human habitations, use indigenous names. The name \"Canada\" itself derives from the St. Lawrence Iroquoian word meaning \"village\" or \"settlement\". The name of Canada's capital city Ottawa comes from the Algonquin language term \"adawe\" meaning \"to trade\".\n\nThe French originally settled New France along the shores of the Atlantic Ocean and Saint Lawrence River during the early part of the 17th century. The British conquest of New France during the mid-18th century brought 70,000 Francophones under British rule, creating a need for compromise and accommodation. The migration of 40,000 to 50,000 United Empire Loyalists from the Thirteen Colonies during the American Revolution (1775–1783) brought American colonial influences. Following the War of 1812, a large wave of Irish, Scottish and English settlers arrived in Upper Canada and Lower Canada.\n\nThe Canadian Forces and overall civilian participation in the First World War and Second World War helped to foster Canadian nationalism; however, in 1917 and 1944, conscription crises highlighted the considerable rift along ethnic lines between Anglophones and Francophones. As a result of the First and Second World Wars, the Government of Canada became more assertive and less deferential to British authority. Canada until the 1940s saw itself in terms of English and French cultural, linguistic and political identities, and to some extent aboriginal.\n\nLegislative restrictions on immigration (such as the Continuous journey regulation and \"Chinese Immigration Act\") that had favoured British, American and other European immigrants (such as Dutch, German, Italian, Polish, Swedish and Ukrainian) were amended during the 1960s, resulting in an influx of diverse people from Asia, Africa, and the Caribbean. By the end of the 20th century, immigrants were increasingly Chinese, Indian, Vietnamese, Jamaican, Filipino, Lebanese and Haitian. As of 2006, Canada has grown to have thirty four ethnic groups with at least one hundred thousand members each, of which eleven have over 1,000,000 people and numerous others are represented in smaller numbers. 16.2% of the population self identify as a visible minority. The Canadian public as-well as the major political parties support immigration.\n\nThemes and symbols of pioneers, trappers, and traders played an important part in the early development of Canadian culture. Modern Canadian culture as it is understood today can be traced to its time period of westward expansion and nation building. Contributing factors include Canada's unique geography, climate, and cultural makeup. Being a cold country with long winter nights for most of the year, certain unique leisure activities developed in Canada during this period including hockey and embracement of the summer indigenous game of lacrosse.\n\nBy the 19th century Canadians came to believe themselves possessed of a unique \"northern character,\" due to the long, harsh winters that only those of hardy body and mind could survive. This hardiness was claimed as a Canadian trait, and such sports as snowshoeing and cross-country skiing that reflected this were asserted as characteristically Canadian. During this period the churches tried to steer leisure activities, by preaching against drinking and scheduling annual revivals and weekly club activities. In a society in which most middle-class families now owned a harmonium or piano, and standard education included at least the rudiments of music, the result was often an original song. Such stirrings frequently occurred in response to noteworthy events, and few local or national excitements were allowed to pass without some musical comment.\n\nBy the 1930s radio played a major role in uniting Canadians behind their local or regional teams. Rural areas were especially influenced by sports coverage and the propagation of national myths. Outside the sports and music arena Canadians express the national characteristics of being hard working, peaceful, orderly and polite.\n\nFrench Canada's early development was relatively cohesive during the 17th and 18th centuries, and this was preserved by the \"Quebec Act\" of 1774, which allowed Roman Catholics to hold offices and practice their faith. In 1867, the \"Constitution Act\" was thought to meet the growing calls for Canadian autonomy while avoiding the overly strong decentralization that contributed to the Civil War in the United States. The compromises reached during this time between the English- and French-speaking Fathers of Confederation set Canada on a path to bilingualism which in turn contributed to an acceptance of diversity. The English and French languages have had limited constitutional protection since 1867 and full official status since 1969. Section 133 of the Constitution Act of 1867 (BNA Act) guarantees that both languages may be used in the Parliament of Canada. Canada adopted its \"first Official Languages Act\" in 1969, giving English and French equal status in the government of Canada. Doing so makes them \"official\" languages, having preferred status in law over all other languages used in Canada.\n\nPrior to the advent of the \"Canadian Bill of Rights\" in 1960 and its successor the \"Canadian Charter of Rights and Freedoms\" in 1982, the laws of Canada did not provide much in the way of civil rights and this issue was typically of limited concern to the courts. Canada since the 1960s has placed emphasis on equality and inclusiveness for all people. Multiculturalism in Canada was adopted as the official policy of the Canadian government and is enshrined in Section 27 of the Canadian Charter of Rights and Freedoms. In 1995, the Supreme Court of Canada ruled in \"Egan v. Canada\" that sexual orientation should be \"read in\" to Section Fifteen of the Canadian Charter of Rights and Freedoms, a part of the Constitution of Canada guaranteeing equal rights to all Canadians. Following a series of decisions by provincial courts and the Supreme Court of Canada, on July 20, 2005, the \"Civil Marriage Act\" (Bill C-38) received Royal Assent, legalizing same-sex marriage in Canada. Furthermore, sexual orientation was included as a protected status in the human-rights laws of the federal government and of all provinces and territories.\n\nCanadian governments at the federal level have a tradition of liberalism, and govern with a moderate, centrist political ideology. Canada's egalitarian approach to governance emphasizing social justice and multiculturalism, is based on selective immigration, social integration, and suppression of far-right politics that has wide public and political support. Peace, order, and good government are constitutional goals of the Canadian government.\n\nCanada has a multi-party system in which many of its legislative customs derive from the unwritten conventions of and precedents set by the Westminster parliament of the United Kingdom. The country has been dominated by two parties, the centre-left Liberal Party of Canada and the centre-right Conservative Party of Canada. The historically predominant Liberals position themselves at the centre of the political scale with the Conservatives sitting on the right and the New Democratic Party occupying the left. Smaller parties like the Quebec nationalist Bloc Québécois and the Green Party of Canada have also been able to exert their influence over the political process by representation at the federal level.\n\nIn general, Canadian nationalists are highly concerned about the protection of Canadian sovereignty and loyalty to the Canadian State, placing them in the civic nationalist category. It has likewise often been suggested that anti-Americanism plays a prominent role in Canadian nationalist ideologies. A unified, bi-cultural, tolerant and sovereign Canada remains an ideological inspiration to many Canadian nationalists. Alternatively French Canadian nationalism and support for maintaining French Canadian culture would inspire Quebec nationalists, many of whom were supporters of the Quebec sovereignty movement during the late-20th century.\n\nCultural protectionism in Canada has, since the mid-20th century, taken the form of conscious, interventionist attempts on the part of various Canadian governments to promote Canadian cultural production. Sharing a large border and (for the majority) a common language with the United States, Canada faces a difficult position in regard to American culture, be it direct attempts at the Canadian market or the general diffusion of American culture in the globalized media arena. While Canada tries to maintain its cultural differences, it also must balance this with responsibility in trade arrangements such as the General Agreement on Tariffs and Trade (GATT) and the North American Free Trade Agreement (NAFTA).\n\nCanadian values are the perceived commonly shared ethical and human values of Canadians. The major political parties have claimed explicitly that they uphold Canadian values, but use generalities to specify them. Historian Ian MacKay argues that, thanks to the long-term political impact of \"Rebels, Reds, and Radicals\", and allied leftist political elements, \"egalitarianism, social equality, and peace... are now often simply referred to...as 'Canadian values.'\"\n\nA 2013 Statistics Canada survey found that an \"overwhelming majority\" of Canadians shared the values of human rights (with 92% of respondents agreeing that they are a shared Canadian value), respect for the law (92%) and gender equality (91%). Universal access to publicly funded health services \"is often considered by Canadians as a fundamental value that ensures national health care insurance for everyone wherever they live in the country.\" \n\nThe Canadian Charter of Rights and Freedoms, was intended to be a source for Canadian values and national unity. The 15th Prime Minister Pierre Trudeau wrote in his \"Memoirs\" that:\n\nNumerous scholar, beginning in the 1940s with American sociologist Seymour Martin Lipset; have tried to identify, measure and compare them with other countries, especially the United States. However, there are critics who say that such a task is practically impossible.\n\nDenis Stairs a professor of political Science at Dalhousie University; links the concept of Canadian values with nationalism. [Canadians typically]...believe, in particular, that they subscribe to a distinctive set of values - \"Canadian\" values - and that those values are special in the sense of being unusually virtuous.\n\nCanada's large geographic size, the presence of a significant number of indigenous peoples, the conquest of one European linguistic population by another and relatively open immigration policy have led to an extremely diverse society. As a result, the issue of Canadian identity remains under scrutiny.\n\nCanada has constitutional protection for policies that promote multiculturalism rather than cultural assimilation or a single national myth. In Quebec, cultural identity is strong, and many commentators speak of a French Canadian culture as distinguished from English Canadian culture. However, as a whole, Canada is in theory, a cultural mosaic—a collection of several regional, and ethnic subcultures. Political philosopher Charles Blattberg suggests that Canada is a \"multinational country\"; as all Canadians are members of Canada as a civic or political community, a community of citizens, and this is a community that contains many other kinds within it. These include not only communities of ethnic, regional, religious, and civic (the provincial and municipal governments) sorts, but also national communities, which often include or overlap with many of the other kinds.\n\nJournalist and author Richard Gwyn has suggested that \"tolerance\" has replaced \"loyalty\" as the touchstone of Canadian identity. Journalist and professor Andrew Cohen wrote in 2007:\nCanada's 15th prime minister Pierre Trudeau in regards to uniformity stated:\n\nThe question of Canadian identity was traditionally dominated by three fundamental themes: first, the often conflicted relations between English Canadians and French Canadians stemming from the French Canadian imperative for cultural and linguistic survival; secondly, the generally close ties between English Canadians and the British Empire, resulting in a gradual political process towards complete independence from the imperial power; and finally, the close proximity of English-speaking Canadians to the United States. Much of the debate over contemporary Canadian identity is argued in political terms, and defines Canada as a country defined by its government policies, which are thought to reflect deeper cultural values.\n\nIn 2013, more than 90% of Canadians believed that the \"Canadian Charter of Rights and Freedoms\" and the national flag were the top symbols of Canadian identity. Next highest were the national anthem, the Royal Canadian Mounted Police and hockey.\n\nWestern alienation is the notion that the western provinces have historically been alienated, and in extreme cases excluded, from mainstream Canadian political affairs in favour of Eastern Canada or more specifically the central provinces. Western alienation claims that these latter two are politically represented, and economically favoured, more significantly than the former, which has given rise to the sentiment of alienation among many western Canadians. Likewise; the Quebec sovereignty movement of the late 20th century that lead to the Québécois nation being recognized as a \"distinct society\" within Canada, highlights the sharp divisions between the Anglo and Francophone population.\n\nThough more than half of Canadians live in just two provinces: Ontario and Quebec, each province is largely self-contained due to provincial economic self-sufficiency. Only 15 percent of Canadians live in a different province from where they were born, and only 10 percent go to another province for university. Canada has always been like that, and stands in sharp contrast to the United States' internal mobility which is much higher. For example 30 percent live in a different state from where they were born, and 30 percent go away for university. Scott Gilmore in \"Maclean's\" argues that \"Canada is a nation of strangers\", in the sense that for most individuals, the rest of Canada outside their province is little-known. Another factor is the cost of internal travel. Intra-Canadian airfares are high—it is cheaper and more common to visit the United States than to visit another province. Gilmore argues that the mutual isolation makes it difficult to muster national responses to major national issues.\n\nCanadian humour is an integral part of the Canadian Identity. There are several traditions in Canadian humour in both English and French. While these traditions are distinct and at times very different, there are common themes that relate to Canadians' shared history and geopolitical situation in the Western Hemisphere and the world. Various trends can be noted in Canadian comedy. One trend is the portrayal of a \"typical\" Canadian family in an ongoing radio or television series. Other trends include outright absurdity, and political and cultural satire. Irony, parody, satire, and self-deprecation are arguably the primary characteristics of Canadian humour.\n\nThe beginnings of Canadian national radio comedy date to the late 1930s with the debut of \"The Happy Gang\", a long-running weekly variety show that was regularly sprinkled with corny jokes in between tunes. Canadian television comedy begins with Wayne and Shuster, a sketch comedy duo who performed as a comedy team during the Second World War, and moved their act to radio in 1946 before moving on to television. \"Second City Television\", otherwise known as \"SCTV\", \"Royal Canadian Air Farce\", \"This Hour Has 22 Minutes\", \"The Kids in the Hall\" and more recently \"Trailer Park Boys\" are regarded as television shows which were very influential on the development of Canadian humour. Canadian comedians have had great success in the film industry and are amongst the most recognized in the world.\n\nHumber College in Toronto and the École nationale de l'humour in Montreal offer post-secondary programmes in comedy writing and performance. Montreal is also home to the bilingual (English and French) Just for Laughs festival and to the Just for Laughs Museum, a bilingual, international museum of comedy. Canada has a national television channel, The Comedy Network, devoted to comedy. Many Canadian cities feature comedy clubs and showcases, most notable, The Second City branch in Toronto (originally housed at The Old Fire Hall) and the Yuk Yuk's national chain. The Canadian Comedy Awards were founded in 1999 by the Canadian Comedy Foundation for Excellence, a not-for-profit organization.\n\nPredominant symbols of Canada include the maple leaf, beaver, and the Canadian horse. Many official symbols of the country such as the Flag of Canada have been changed or modified over the past few decades to Canadianize them and de-emphasise or remove references to the United Kingdom. Other prominent symbols include the sports of hockey and lacrosse, the Canadian Goose, the Royal Canadian Mounted Police, the Canadian Rockies, and more recently the totem pole and Inuksuk. With material items such as Canadian beer, maple syrup, tuques, canoes, nanaimo bars, butter tarts and the Quebec dish of poutine being defined as uniquely Canadian. Symbols of the Canadian monarchy continue to be featured in, for example, the Arms of Canada, the armed forces, and the prefix Her Majesty's Canadian Ship. The designation \"Royal\" remains for institutions as varied as the Royal Canadian Armed Forces, Royal Canadian Mounted Police and the Royal Winnipeg Ballet.\n\nIndigenous artists were producing art in the territory that is now called Canada for thousands of years prior to the arrival of European settler colonists and the eventual establishment of Canada as a nation state. Like the peoples that produced them, indigenous art traditions spanned territories that extended across the current national boundaries between Canada and the United States. The majority of indigenous artworks preserved in museum collections date from the period after European contact and show evidence of the creative adoption and adaptation of European trade goods such as metal and glass beads. Canadian sculpture has been enriched by the walrus ivory, muskox horn and caribou antler and soapstone carvings by the Inuit artists. These carvings show objects and activities from the daily life, myths and legends of the Inuit. Inuit art since the 1950s has been the traditional gift given to foreign dignitaries by the Canadian government.\n\nThe works of most early Canadian painters followed European trends. During the mid-19th century, Cornelius Krieghoff, a Dutch-born artist in Quebec, painted scenes of the life of the \"habitants\" (French-Canadian farmers). At about the same time, the Canadian artist Paul Kane painted pictures of indigenous life in western Canada. A group of landscape painters called the Group of Seven developed the first distinctly Canadian style of painting. All these artists painted large, brilliantly coloured scenes of the Canadian wilderness.\n\nSince the 1930s, Canadian painters have developed a wide range of highly individual styles. Emily Carr became famous for her paintings of totem poles in British Columbia. Other noted painters have included the landscape artist David Milne, the painters Jean-Paul Riopelle, Harold Town and Charles Carson and multi-media artist Michael Snow. The abstract art group Painters Eleven, particularly the artists William Ronald and Jack Bush, also had an important impact on modern art in Canada. Government support has played a vital role in their development enabling visual exposure through publications and periodicals featuring Canadian art, as has the establishment of numerous art schools and colleges across the country.\n\nCanadian literature is often divided into French- and English-language literatures, which are rooted in the literary traditions of France and Britain, respectively. Canada's early literature, whether written in English or French, often reflects the Canadian perspective on nature, frontier life, and Canada's position in the world, for example the poetry of Bliss Carman or the memoirs of Susanna Moodie and Catherine Parr Traill. These themes, and Canada's literary history, inform the writing of successive generations of Canadian authors, from Leonard Cohen to Margaret Atwood.\n\nBy the mid-20th century, Canadian writers were exploring national themes for Canadian readers. Authors were trying to find a distinctly Canadian voice, rather than merely emulating British or American writers. Canadian identity is closely tied to its literature. The question of national identity recurs as a theme in much of Canada's literature, from Hugh MacLennan's \"Two Solitudes\" (1945) to Alistair MacLeod's \"No Great Mischief\" (1999). Canadian literature is often categorized by region or province; by the socio-cultural origins of the author (for example, Acadians, indigenous peoples, LGBT, and Irish Canadians); and by literary period, such as \"Canadian postmoderns\" or \"Canadian Poets Between the Wars\".\n\nCanadian authors have accumulated numerous international awards. In 1992, Michael Ondaatje became the first Canadian to win the Man Booker Prize for \"The English Patient\". Margaret Atwood won the Booker in 2000 for \"The Blind Assassin\" and Yann Martel won it in 2002 for the \"Life of Pi\". Carol Shields's \"The Stone Diaries\" won the Governor General's Awards in Canada in 1993, the 1995 Pulitzer Prize for Fiction, and the 1994 National Book Critics Circle Award. In 2013, Alice Munro was the first Canadian to be awarded the Nobel Prize in Literature for her work as \"master of the modern short story\". Munro is also a recipient of the Man Booker International Prize for her lifetime body of work, and three-time winner of Canada's Governor General's Award for fiction.\n\nCanada has had a thriving stage theatre scene since the late 1800s. Theatre festivals draw many tourists in the summer months, especially the Stratford Shakespeare Festival in Stratford, Ontario, and the Shaw Festival in Niagara-on-the-Lake, Ontario. The Famous People Players are only one of many touring companies that have also developed an international reputation. Canada also hosts one of the largest fringe festivals, the Edmonton International Fringe Festival.\nCanada's largest cities host a variety of modern and historical venues. The Toronto Theatre District is Canada's largest, as well as being the third largest English-speaking theatre district in the world. In addition to original Canadian works, shows from the West End and Broadway frequently tour in Toronto. Toronto's Theatre District includes the venerable Roy Thomson Hall; the Princess of Wales Theatre; the Tim Sims Playhouse; The Second City; the Canon Theatre; the Panasonic Theatre; the Royal Alexandra Theatre; historic Massey Hall; and the city's new opera house, the Sony Centre for the Performing Arts. Toronto's Theatre District also includes the Theatre Museum Canada.\n\nMontreal's theatre district (\"Quartier des Spectacles\") is the scene of performances that are mainly French-language, although the city also boasts a lively anglophone theatre scene, such as the Centaur Theatre. Large French theatres in the city include Théâtre Saint-Denis and Théâtre du Nouveau Monde.\n\nVancouver is host to, among others, the Vancouver Fringe Festival, the Arts Club Theatre Company, Carousel Theatre, Bard on the Beach, Theatre Under the Stars and Studio 58.\n\nCalgary is home to Theatre Calgary, a mainstream regional theatre; Alberta Theatre Projects, a major centre for new play development in Canada; the Calgary Animated Objects Society; and One Yellow Rabbit, a touring company.\n\nThere are three major theatre venues in Ottawa; the Ottawa Little Theatre, originally called the Ottawa Drama League at its inception in 1913, is the longest-running community theatre company in Ottawa. Since 1969, Ottawa has been the home of the National Arts Centre, a major performing-arts venue that houses four stages and is home to the National Arts Centre Orchestra, the Ottawa Symphony Orchestra and Opera Lyra Ottawa. Established in 1975, the Great Canadian Theatre Company specializes in the production of Canadian plays at a local level.\n\nCanadian television, especially supported by the Canadian Broadcasting Corporation, is the home of a variety of locally produced shows. French-language television, like French Canadian film, is buffered from excessive American influence by the fact of language, and likewise supports a host of home-grown productions. The success of French-language domestic television in Canada often exceeds that of its English-language counterpart. In recent years nationalism has been used to prompt products on television. The \"I Am Canadian\" campaign by Molson beer, most notably the commercial featuring Joe Canadian, infused domestically brewed beer and nationalism.\n\nCanada's television industry is in full expansion as a site for Hollywood productions. Since the 1980s, Canada, and Vancouver in particular, has become known as Hollywood North. The American TV series \"Queer as Folk\" was filmed in Toronto. Canadian producers have been very successful in the field of science fiction since the mid-1990s, with such shows as \"The X-Files\", \"Stargate SG-1\", \"\", the new \"Battlestar Galactica\", \"My Babysitter's A Vampire\", \"Smallville\", and \"The Outer Limits\", all filmed in Vancouver.\n\nThe CRTC's Canadian content regulations dictate that a certain percentage of a domestic broadcaster's transmission time must include content that is produced by Canadians, or covers Canadian subjects. These regulations also apply to US cable television channels such as MTV and the Discovery Channel, which have local versions of their channels available on Canadian cable networks. Similarly, BBC Canada, while showing primarily BBC shows from the United Kingdom, also carries Canadian output.\n\nA number of Canadian pioneers in early Hollywood significantly contributed to the creation of the motion picture industry in the early days of the 20th century. Over the years, many Canadians have made enormous contributions to the American entertainment industry, although they are frequently not recognized as Canadians.\n\nCanada has developed a vigorous film industry that has produced a variety of well-known films, actors and actresses. In fact, this eclipsing may sometimes be creditable for the bizarre and innovative directions of some works, such as auteurs Atom Egoyan (\"The Sweet Hereafter\", 1997) and David Cronenberg (\"The Fly\", \"Naked Lunch\", \"A History of Violence\") and the \"avant-garde\" work of Michael Snow and Jack Chambers. Also, the distinct French-Canadian society permits the work of directors such as Denys Arcand and Denis Villeneuve, while First Nations cinema includes the likes of \"\". At the 76th Academy Awards, Arcand's \"The Barbarian Invasions\" became Canada's first film to win the Academy Award for Best Foreign Language Film. \n\nThe National Film Board of Canada is 'a public agency that produces and distributes films and other audiovisual works which reflect Canada to Canadians and the rest of the world'. Canada has produced many popular documentaries such as \"The Corporation\", \"Nanook of the North\", \"Final Offer\", and \"\". The Toronto International Film Festival (TIFF) is considered by many to be one of the most prevalent film festivals for Western cinema. It is the première film festival in North America from which the Oscars race begins.\n\nThe music of Canada has reflected the multi-cultural influences that have shaped the country. Indigenous, the French, and the British have all made historical contributions to the musical heritage of Canada. The country has produced its own composers, musicians and ensembles since the mid-1600s. From the 17th century onward, Canada has developed a music infrastructure that includes church halls; chamber halls; conservatories; academies; performing arts centres; record companys; radio stations, and television music-video channels. The music has subsequently been heavily influenced by American culture because of its proximity and migration between the two countries. Canadian rock has had a considerable impact on the development of modern popular music and the development of the most popular subgenres. \n\nPatriotic music in Canada dates back over 200 years as a distinct category from British patriotism, preceding the first legal steps to independence by over 50 years. The earliest known song, \"The Bold Canadian\", was written in 1812. The national anthem of Canada, \"O Canada\" adopted in 1980, was originally commissioned by the Lieutenant Governor of Quebec, the Honourable Théodore Robitaille, for the 1880 Saint-Jean-Baptiste Day ceremony. Calixa Lavallée wrote the music, which was a setting of a patriotic poem composed by the poet and judge Sir Adolphe-Basile Routhier. The text was originally only in French, before English lyrics were written in 1906.\n\nMusic broadcasting in the country is regulated by the Canadian Radio-television and Telecommunications Commission (CRTC). The Canadian Academy of Recording Arts and Sciences presents Canada's music industry awards, the Juno Awards, which were first awarded in a ceremony during the summer of 1970.\n\nCanada has a well-developed media sector, but its cultural output—particularly in English films, television shows, and magazines—is often overshadowed by imports from the United States. Television, magazines, and newspapers are primarily for-profit corporations based on advertising, subscription, and other sales-related revenues. Nevertheless, both the television broadcasting and publications sectors require a number of government interventions to remain profitable, ranging from regulation that bars foreign companies in the broadcasting industry to tax laws that limit foreign competition in magazine advertising.\n\nThe promotion of multicultural media in Canada began in the late 1980s as the multicultural policy was legislated in 1988. In the \"Multiculturalism Act\", the federal government proclaimed the recognition of the diversity of Canadian culture. Thus, multicultural media became an integral part of Canadian media overall. Upon numerous government reports showing lack of minority representation or minority misrepresentation, the Canadian government stressed separate provision be made to allow minorities and ethnicities of Canada to have their own voice in the media.\n\nSports in Canada consists of a variety of games. Although there are many contests that Canadians value, the most common are ice hockey, box lacrosse, Canadian football, basketball, soccer, curling, baseball and ringette. All but curling and soccer are considered domestic sports as they were either invented by Canadians or trace their roots to Canada.\n\nIce hockey, referred to as simply \"hockey\", is Canada's most prevalent winter sport, its most popular spectator sport, and its most successful sport in international competition. It is Canada's official national winter sport. Lacrosse, a sport with indigenous origins, is Canada's oldest and official summer sport. Canadian football is Canada's second most popular spectator sport, and the Canadian Football League's annual championship, the Grey Cup, is the country's largest annual sports event.\n\nWhile other sports have a larger spectator base, association football, known in Canada as \"soccer\" in both English and French, has the most registered players of any team sport in Canada, and is the most played sport with all demographics, including ethnic origin, ages and genders. Professional teams exist in many cities in Canada – with a trio of teams in North America's top pro league, Major League Soccer – and international soccer competitions such as the FIFA World Cup, UEFA Euro and the UEFA Champions League attract some of the biggest audiences in Canada. Other popular team sports include curling, street hockey, cricket, rugby league, rugby union, softball and Ultimate frisbee. Popular individual sports include auto racing, boxing, karate, kickboxing, hunting, sport shooting, fishing, cycling, golf, hiking, horse racing, ice skating, skiing, snowboarding, swimming, triathlon, disc golf, water sports, and several forms of wrestling.\n\nAs a country with a generally cool climate, Canada has enjoyed greater success at the Winter Olympics than at the Summer Olympics, although significant regional variations in climate allow for a wide variety of both team and individual sports. Great achievements in Canadian sports are recognized by Canada's Sports Hall of Fame, while the Lou Marsh Trophy is awarded annually to Canada's top athlete by a panel of journalists. There are numerous other Sports Halls of Fame in Canada.\n\nCanadian cuisine varies widely depending on the region. The former Canadian prime minister Joe Clark has been paraphrased to have noted: \"Canada has a cuisine of cuisines. Not a stew pot, but a smorgasbord.\" There are considerable overlaps between Canadian food and the rest of the cuisine in North America, many unique dishes (or versions of certain dishes) are found and available only in the country. Common contenders for the Canadian national food include poutine and butter tarts. Other popular Canadian made foods include indigenous fried bread bannock, French tourtière, Kraft Dinner, ketchup chips, date squares, nanaimo bars, back bacon, and the caesar cocktail. Canada is the birthplace and world's largest producer of maple syrup.\n\nThe three earliest cuisines of Canada have First Nations, English, and French roots, with the traditional cuisine of English Canada closely related to British and American cuisine, while the traditional cuisine of French Canada has evolved from French cuisine and the winter provisions of fur traders. With subsequent waves of immigration in the 18th and 19th century from Central, Southern, and Eastern Europe, and then from Asia, Africa and Caribbean, the regional cuisines were subsequently augmented. The Jewish immigrants to Canada during the late 1800s also play a significant role to foods in Canada. The Montreal-style bagel and Montreal-style smoked meat are both food items originally developed by Jewish communities living in Montreal.\n\nIn a 2002 interview with the \"Globe and Mail\", Aga Khan, the 49th Imam of the Ismaili Muslims, described Canada as \"the most successful pluralist society on the face of our globe\", citing it as \"a model for the world\". A 2007 poll ranked Canada as the country with the most positive influence in the world. 28,000 people in 27 countries were asked to rate 12 countries as either having a positive or negative worldwide influence. Canada's overall influence rating topped the list with 54 per cent of respondents rating it mostly positive and only 14 per cent mostly negative. A global opinion poll for the BBC saw Canada ranked the second most positively viewed nation in the world (behind Germany) in 2013 and 2014.\n\nThe United States is home to a number of perceptions about Canadian culture, due to the countries' partially shared heritage and the relatively large number of cultural features common to both the US and Canada. For example, the average Canadian may be perceived as more reserved than his or her American counterpart. Canada and the United States are often inevitably compared as sibling countries, and the perceptions that arise from this oft-held contrast have gone to shape the advertised worldwide identities of both nations: the United States is seen as the rebellious child of the British Crown, forged in the fires of violent revolution; Canada is the calmer offspring of the United Kingdom, known for a more relaxed national demeanour.\n\nBULLET::::- Canadiana\nBULLET::::- Canadian folklore\nBULLET::::- Culture of Alberta\nBULLET::::- Culture of Manitoba\nBULLET::::- Culture of Saskatchewan\nBULLET::::- Culture of Quebec\nBULLET::::- History of free speech in Canada\nBULLET::::- Public holidays in Canada\nBULLET::::- Canadian French\nBULLET::::- List of Canadians\n\n\nBULLET::::- Canadian Heritage\nBULLET::::- Culture.CA - Canadian cultural portal online\nBULLET::::- Cultural Information - Canada - Global Affairs Canada\n"}
{"id": "7000", "url": "https://en.wikipedia.org/wiki?curid=7000", "title": "List of companies of Canada", "text": "List of companies of Canada\n\nCanada is a country in the northern part of North America.\nCanada is the world's tenth-largest economy , with a nominal GDP of approximately US$1.52 trillion. It is a member of the Organisation for Economic Co-operation and Development (OECD) and the Group of Eight (G8), and is one of the world's top ten trading nations, with a highly globalized economy. Canada is a mixed economy, ranking above the US and most western European nations on The Heritage Foundation's index of economic freedom, and experiencing a relatively low level of income disparity. The country's average household disposable income per capita is over US$23,900, higher than the OECD average. Furthermore, the Toronto Stock Exchange is the seventh-largest stock exchange in the world by market capitalization, listing over 1,500 companies with a combined market capitalization of over US$2 trillion .\n\nFor further information on the types of business entities in this country and their abbreviations, see \"Business entities in Canada\".\n\nThis list shows firms in the Fortune Global 500, which ranks firms by total revenues reported before March 31, 2017. Only the top five firms (if available) are included as a sample.\n\n!Rank\n!Image\n!Name\n!2016 revenues (USD $M)\n!Employees\n!Notes\nalign=\"right\" $40,238\nalign=\"right\" 34,500\nalign=\"right\" $38,286\nalign=\"right\" 30,259\nalign=\"right\" $36,445\nalign=\"right\" 155,450\nalign=\"right\" $36,211\nalign=\"right\" 195,000\nalign=\"right\" $34,904\nalign=\"right\" 75,510\n\nThis list includes notable companies with primary headquarters located in the country. The industry and sector follow the Industry Classification Benchmark taxonomy. Organizations which have ceased operations are included and noted as defunct.\n\nBULLET::::- List of largest companies in Canada\nBULLET::::- List of largest public companies in Canada by profit\nBULLET::::- List of Canadian mobile phone companies\nBULLET::::- List of mutual fund companies in Canada\nBULLET::::- List of Canadian telephone companies\nBULLET::::- List of defunct Canadian companies\nBULLET::::- List of government-owned companies\n"}
{"id": "7003", "url": "https://en.wikipedia.org/wiki?curid=7003", "title": "Cauchy distribution", "text": "Cauchy distribution\n\nThe Cauchy distribution, named after Augustin Cauchy, is a continuous probability distribution. It is also known, especially among physicists, as the Lorentz distribution (after Hendrik Lorentz), Cauchy–Lorentz distribution, Lorentz(ian) function, or Breit–Wigner distribution. The Cauchy distribution formula_1 is the distribution of the -intercept of a ray issuing from formula_2 with a uniformly distributed angle. It is also the distribution of the ratio of two independent normally distributed random variables with mean zero.\n\nThe Cauchy distribution is often used in statistics as the canonical example of a \"pathological\" distribution since both its expected value and its variance are undefined. (But see the section \"Explanation of undefined moments\" below.) The Cauchy distribution does not have finite moments of order greater than or equal to one; only fractional absolute moments exist. The Cauchy distribution has no moment generating function.\n\nIn mathematics, it is closely related to the Poisson kernel, which is the fundamental solution for the Laplace equation in the upper half-plane. \n\nIt is one of the few distributions that is stable and has a probability density function that can be expressed analytically, the others being the normal distribution and the Lévy distribution.\n\nFunctions with the form of the density function of the Cauchy distribution were studied by mathematicians in the 17th century, but in a different context and under the title of the witch of Agnesi. Despite its name, the first explicit analysis of the properties of the Cauchy distribution was published by the French mathematician Poisson in 1824, with Cauchy only becoming associated with it during an academic controversy in 1853. As such, the name of the distribution is a case of Stigler's Law of Eponymy. Poisson noted that if the mean of observations following such a distribution were taken, the mean error did not converge to any finite number. As such, Laplace's use of the Central Limit Theorem with such a distribution was inappropriate, as it assumed a finite mean and variance. Despite this, Poisson did not regard the issue as important, in contrast to Bienaymé, who was to engage Cauchy in a long dispute over the matter.\n\nThe Cauchy distribution has the probability density function (PDF)\n\nwhere formula_4 is the location parameter, specifying the location of the peak of the distribution, and formula_5 is the scale parameter which specifies the half-width at half-maximum (HWHM), alternatively formula_6 is full width at half maximum (FWHM). formula_5 is also equal to half the interquartile range and is sometimes called the probable error. Augustin-Louis Cauchy exploited such a density function in 1827 with an infinitesimal scale parameter, defining what would now be called a Dirac delta function.\n\nThe maximum value or amplitude of the Cauchy PDF is formula_8, located at formula_9.\n\nIt is sometimes convenient to express the PDF in terms of the complex parameter formula_10\n\nThe special case when formula_12 and formula_13 is called the standard Cauchy distribution with the probability density function\n\nIn physics, a three-parameter Lorentzian function is often used:\nwhere formula_16 is the height of the peak. The three-parameter Lorentzian function indicated is not, in general, a probability density function, since it does not integrate to 1, except in the special case where formula_17\n\nThe cumulative distribution function of the Cauchy distribution is:\n\nand the quantile function (inverse cdf) of the Cauchy distribution is\nIt follows that the first and third quartiles are formula_20, and hence the interquartile range is formula_6.\n\nFor the standard distribution, the cumulative distribution function simplifies to arctangent function formula_22:\n\nThe entropy of the Cauchy distribution is given by:\n\nThe derivative of the quantile function, the quantile density function, for the Cauchy distribution is:\n\nThe differential entropy of a distribution can be defined in terms of its quantile density, specifically:\n\nThe Cauchy distribution is the maximum entropy probability distribution for a random variate formula_27 for which \n\nor, alternatively, for a random variate formula_27 for which \n\nIn its standard form, it is the maximum entropy probability distribution for a random variate formula_27 for which \n\nThe Cauchy distribution is an example of a distribution which has no mean, variance or higher moments defined. Its mode and median are well defined and are both equal to formula_4.\n\nWhen formula_34 and formula_35 are two independent normally distributed random variables with expected value 0 and variance 1, then the ratio formula_36 has the standard Cauchy distribution.\n\nIf formula_37 is a formula_38 positive-semidefinite covariance matrix with strictly positive diagonal entries, then for independent and identically distributed formula_39 and any random formula_40-vector formula_41 independent of formula_27 and formula_43 such that formula_44 and formula_45 (defining a categorical distribution) it holds that\n\nIf formula_47 are independent and identically distributed random variables, each with a standard Cauchy distribution, then the sample mean formula_48 has the same standard Cauchy distribution. To see that this is true, compute the characteristic function of the sample mean:\n\nwhere formula_50 is the sample mean. This example serves to show that the hypothesis of finite variance in the central limit theorem cannot be dropped. It is also an example of a more generalized version of the central limit theorem that is characteristic of all stable distributions, of which the Cauchy distribution is a special case.\n\nThe Cauchy distribution is an infinitely divisible probability distribution. It is also a strictly stable distribution.\n\nThe standard Cauchy distribution coincides with the Student's \"t\"-distribution with one degree of freedom.\n\nLike all stable distributions, the location-scale family to which the Cauchy distribution belongs is closed under linear transformations with real coefficients. In addition, the Cauchy distribution is closed under linear fractional transformations with real coefficients. In this connection, see also McCullagh's parametrization of the Cauchy distributions.\n\nLet formula_27 denote a Cauchy distributed random variable. The characteristic function of the Cauchy distribution is given by\n\nwhich is just the Fourier transform of the probability density. The original probability density may be expressed in terms of the characteristic function, essentially by using the inverse Fourier transform:\n\nThe \"n\"th moment of a distribution is the \"n\"th derivative of the characteristic function evaluated at formula_54. Observe that the characteristic function is not differentiable at the origin: this corresponds to the fact that the Cauchy distribution does not have well-defined moments higher than the zeroth moment.\n\nIf a probability distribution has a density function formula_55, then the mean, if it exists, is given by\n\nWe may evaluate this two-sided improper integral by computing the sum of two one-sided improper integrals. That is,\nfor an arbitrary real number formula_58.\n\nFor the integral to exist (even as an infinite value), at least one of the terms in this sum should be finite, or both should be infinite and have the same sign. But in the case of the Cauchy distribution, both the terms in this sum (2) are infinite and have opposite sign. Hence (1) is undefined, and thus so is the mean.\n\nNote that the Cauchy principal value of the mean of the Cauchy distribution is\n\nwhich is zero. On the other hand, the related integral\n\nis \"not\" zero, as can be seen easily by computing the integral. This again shows that the mean (1) cannot exist.\n\nVarious results in probability theory about expected values, such as the strong law of large numbers, fail to hold for the Cauchy distribution.\n\nThe Cauchy distribution does not have finite moments of any order. Some of the higher raw moments do exist and have a value of infinity, for example the raw second moment:\n\nBy re-arranging the formula, one can see that the second moment is essentially the infinite integral of a constant (here 1). Higher even-powered raw moments will also evaluate to infinity. Odd-powered raw moments, however, are undefined, which is distinctly different from existing with the value of infinity. The odd-powered raw moments are undefined because their values are essentially equivalent to formula_62 since the two halves of the integral both diverge and have opposite signs. The first raw moment is the mean, which, being odd, does not exist. (See also the discussion above about this.) This in turn means that all of the central moments and standardized moments are undefined, since they are all based on the mean. The variance—which is the second central moment—is likewise non-existent (despite the fact that the raw second moment exists with the value infinity).\n\nThe results for higher moments follow from Hölder's inequality, which implies that higher moments (or halves of moments) diverge if lower ones do.\n\nConsider the truncated distribution defined by restricting the standard Cauchy distribution to the interval . Such a truncated distribution has all moments (and the central limit theorem applies for i.i.d. observations from it); yet for almost all practical purposes it behaves like a Cauchy distribution.\n\nBecause the parameters of the Cauchy distribution do not correspond to a mean and variance, attempting to estimate the parameters of the Cauchy distribution by using a sample mean and a sample variance will not succeed. For example, if an i.i.d. sample of size \"n\" is taken from a Cauchy distribution, one may calculate the sample mean as:\n\nAlthough the sample values formula_64 will be concentrated about the central value formula_4, the sample mean will become increasingly variable as more observations are taken, because of the increased probability of encountering sample points with a large absolute value. In fact, the distribution of the sample mean will be equal to the distribution of the observations themselves; i.e., the sample mean of a large sample is no better (or worse) an estimator of formula_4 than any single observation from the sample. Similarly, calculating the sample variance will result in values that grow larger as more observations are taken.\n\nTherefore, more robust means of estimating the central value formula_4 and the scaling parameter formula_5 are needed. One simple method is to take the median value of the sample as an estimator of formula_4 and half the sample interquartile range as an estimator of formula_5. Other, more precise and robust methods have been developed For example, the truncated mean of the middle 24% of the sample order statistics produces an estimate for formula_4 that is more efficient than using either the sample median or the full sample mean. However, because of the fat tails of the Cauchy distribution, the efficiency of the estimator decreases if more than 24% of the sample is used.\n\nMaximum likelihood can also be used to estimate the parameters formula_4 and formula_5. However, this tends to be complicated by the fact that this requires finding the roots of a high degree polynomial, and there can be multiple roots that represent local maxima. Also, while the maximum likelihood estimator is asymptotically efficient, it is relatively inefficient for small samples. The log-likelihood function for the Cauchy distribution for sample size formula_74 is:\n\nMaximizing the log likelihood function with respect to formula_4 and formula_5 produces the following system of equations:\n\nNote that\n\nis a monotone function in formula_5 and that the solution formula_5 must satisfy\n\nSolving just for formula_4 requires solving a polynomial of degree formula_85, and solving just for formula_5 requires solving a polynomial of degree formula_74 (first for formula_88, then formula_4). Therefore, whether solving for one parameter or for both parameters simultaneously, a numerical solution on a computer is typically required. The benefit of maximum likelihood estimation is asymptotic efficiency; estimating formula_4 using the sample median is only about 81% as asymptotically efficient as estimating formula_4 by maximum likelihood. The truncated sample mean using the middle 24% order statistics is about 88% as asymptotically efficient an estimator of formula_4 as the maximum likelihood estimate. When Newton's method is used to find the solution for the maximum likelihood estimate, the middle 24% order statistics can be used as an initial solution for formula_4.\n\nA random vector formula_94 is said to have the multivariate Cauchy distribution if every linear combination of its components formula_95 has a Cauchy distribution. That is, for any constant vector formula_96, the random variable formula_97 should have a univariate Cauchy distribution. The characteristic function of a multivariate Cauchy distribution is given by:\n\nwhere formula_99 and formula_100 are real functions with formula_99 a homogeneous function of degree one and formula_100 a positive homogeneous function of degree one. More formally:\n\nfor all formula_105.\n\nAn example of a bivariate Cauchy distribution can be given by:\nNote that in this example, even though there is no analogue to a covariance matrix, formula_107 and formula_108 are not statistically independent.\n\nWe also can write this formula for complex variable. Then the probability density function of complex cauchy is :\n\nAnalogous to the univariate density, the multidimensional Cauchy density also relates to the multivariate Student distribution. They are equivalent when the degrees of freedom parameter is equal to one. The density of a formula_110 dimension Student distribution with one degree of freedom becomes:\n\nProperties and details for this density can be obtained by taking it as a particular case of the multivariate Student density.\n\nBULLET::::- If formula_112 then formula_113\nBULLET::::- If formula_114 and formula_115 are independent, then formula_116 and formula_117\nBULLET::::- If formula_118 then formula_119\nBULLET::::- McCullagh's parametrization of the Cauchy distributions: Expressing a Cauchy distribution in terms of one complex parameter formula_120, define formula_121 to mean formula_122. If formula_121 then:\nwhere formula_58, formula_126, formula_127 and formula_128 are real numbers.\nBULLET::::- Using the same convention as above, if formula_121 then:\n\nThe Cauchy distribution is the stable distribution of index 1. The Lévy–Khintchine representation of such a stable distribution of parameter formula_132 is given, for formula_133 by:\n\nwhere\n\nand formula_136 can be expressed explicitly. In the case formula_137 of the Cauchy distribution, one has formula_138.\n\nThis last representation is a consequence of the formula\n\nBULLET::::- formula_140 Student's \"t\" distribution\nBULLET::::- formula_141 non-standardized Student's \"t\" distribution\nBULLET::::- If formula_142 independent, then formula_143\nBULLET::::- If formula_144 then formula_145\nBULLET::::- If formula_146 then formula_147\nBULLET::::- The Cauchy distribution is a limiting case of a Pearson distribution of type 4\nBULLET::::- The Cauchy distribution is a special case of a Pearson distribution of type 7.\nBULLET::::- The Cauchy distribution is a stable distribution: if formula_148, then formula_149.\nBULLET::::- The Cauchy distribution is a singular limit of a hyperbolic distribution\nBULLET::::- The wrapped Cauchy distribution, taking values on a circle, is derived from the Cauchy distribution by wrapping it around the circle.\n\nIn nuclear and particle physics, the energy profile of a resonance is described by the relativistic Breit–Wigner distribution, while the Cauchy distribution is the (non-relativistic) Breit–Wigner distribution.\n\nBULLET::::- In spectroscopy, the Cauchy distribution describes the shape of spectral lines which are subject to homogeneous broadening in which all atoms interact in the same way with the frequency range contained in the line shape. Many mechanisms cause homogeneous broadening, most notably collision broadening. Lifetime or natural broadening also gives rise to a line shape described by the Cauchy distribution.\n\nBULLET::::- Applications of the Cauchy distribution or its transformation can be found in fields working with exponential growth. A 1958 paper by White derived the test statistic for estimators of formula_150 for the equation formula_151 and where the maximum likelihood estimator is found using ordinary least squares showed the sampling distribution of the statistic is the Cauchy distribution.\nBULLET::::- The Cauchy distribution is often the distribution of observations for objects that are spinning. The classic reference for this is called the Gull's lighthouse problem and as in the above section as the Breit–Wigner distribution in particle physics.\n\nBULLET::::- In hydrology the Cauchy distribution is applied to extreme events such as annual maximum one-day rainfalls and river discharges. The blue picture illustrates an example of fitting the Cauchy distribution to ranked monthly maximum one-day rainfalls showing also the 90% confidence belt based on the binomial distribution. The rainfall data are represented by plotting positions as part of the cumulative frequency analysis.\nBULLET::::- The expression for imaginary part of complex electrical permittivity according to Lorentz model is a Cauchy distribution.\n\nBULLET::::- Lévy flight and Lévy process\nBULLET::::- Cauchy process\nBULLET::::- Stable process\nBULLET::::- Slash distribution\n\nBULLET::::- Earliest Uses: The entry on Cauchy distribution has some historical information.\nBULLET::::- GNU Scientific Library – Reference Manual\nBULLET::::- Ratios of Normal Variables by George Marsaglia\n"}
{"id": "7011", "url": "https://en.wikipedia.org/wiki?curid=7011", "title": "Control engineering", "text": "Control engineering\n\nControl engineering or control systems engineering is an engineering discipline that applies automatic control theory to design systems with desired behaviors in control environments. The discipline of controls overlaps and is usually taught along with electrical engineering at many institutions around the world.\n\nThe practice uses sensors and detectors to measure the output performance of the process being controlled; these measurements are used to provide corrective feedback helping to achieve the desired performance. Systems designed to perform without requiring human input are called automatic control systems (such as cruise control for regulating the speed of a car). Multi-disciplinary in nature, control systems engineering activities focus on implementation of control systems mainly derived by mathematical modeling of a diverse range of systems.\n\nModern day control engineering is a relatively new field of study that gained significant attention during the 20th century with the advancement of technology. It can be broadly defined or classified as practical application of control theory. Control engineering plays an essential role in a wide range of control systems, from simple household washing machines to high-performance F-16 fighter aircraft. It seeks to understand physical systems, using mathematical modelling, in terms of inputs, outputs and various components with different behaviors; to use control system design tools to develop controllers for those systems; and to implement controllers in physical systems employing available technology. A system can be mechanical, electrical, fluid, chemical, financial or biological, and its mathematical modelling, analysis and controller design uses control theory in one or many of the time, frequency and complex-s domains, depending on the nature of the design problem.\n\nAutomatic control systems were first developed over two thousand years ago. The first feedback control device on record is thought to be the ancient Ktesibios's water clock in Alexandria, Egypt around the third century B.C.E. It kept time by regulating the water level in a vessel and, therefore, the water flow from that vessel. This certainly was a successful device as water clocks of similar design were still being made in Baghdad when the Mongols captured the city in 1258 A.D. A variety of automatic devices have been used over the centuries to accomplish useful tasks or simply just to entertain. The latter includes the automata, popular in Europe in the 17th and 18th centuries, featuring dancing figures that would repeat the same task over and over again; these automata are examples of open-loop control. Milestones among feedback, or \"closed-loop\" automatic control devices, include the temperature regulator of a furnace attributed to Drebbel, circa 1620, and the centrifugal flyball governor used for regulating the speed of steam engines by James Watt in 1788.\n\nIn his 1868 paper \"On Governors\", James Clerk Maxwell was able to explain instabilities exhibited by the flyball governor using differential equations to describe the control system. This demonstrated the importance and usefulness of mathematical models and methods in understanding complex phenomena, and it signaled the beginning of mathematical control and systems theory. Elements of control theory had appeared earlier but not as dramatically and convincingly as in Maxwell's analysis.\n\nControl theory made significant strides over the next century. New mathematical techniques, as well as advancements in electronic and computer technologies, made it possible to control significantly more complex dynamical systems than the original flyball governor could stabilize. New mathematical techniques included developments in optimal control in the 1950s and 1960s followed by progress in stochastic, robust, adaptive, nonlinear control methods in the 1970s and 1980s. Applications of control methodology have helped to make possible space travel and communication satellites, safer and more efficient aircraft, cleaner automobile engines, and cleaner and more efficient chemical processes.\n\nBefore it emerged as a unique discipline, control engineering was practiced as a part of mechanical engineering and control theory was studied as a part of electrical engineering since electrical circuits can often be easily described using control theory techniques. In the very first control relationships, a current output was represented by a voltage control input. However, not having adequate technology to implement electrical control systems, designers were left with the option of less efficient and slow responding mechanical systems. A very effective mechanical controller that is still widely used in some hydro plants is the governor. Later on, previous to modern power electronics, process control systems for industrial applications were devised by mechanical engineers using pneumatic and hydraulic control devices, many of which are still in use today.\n\nThere are two major divisions in control theory, namely, classical and modern, which have direct implications for the control engineering applications. \nThe scope of classical control theory is limited to single-input and single-output (SISO) system design, except when analyzing for disturbance rejection using a second input. The system analysis is carried out in the time domain using differential equations, in the complex-s domain with the Laplace transform, or in the frequency domain by transforming from the complex-s domain. Many systems may be assumed to have a second order and single variable system response in the time domain. A controller designed using classical theory often requires on-site tuning due to incorrect design approximations. Yet, due to the easier physical implementation of classical controller designs as compared to systems designed using modern control theory, these controllers are preferred in most industrial applications. The most common controllers designed using classical control theory are PID controllers. A less common implementation may include either or both a Lead or Lag filter. The ultimate end goal is to meet requirements typically provided in the time-domain called the step response, or at times in the frequency domain called the open-loop response. The step response characteristics applied in a specification are typically percent overshoot, settling time, etc. The open-loop response characteristics applied in a specification are typically Gain and Phase margin and bandwidth. These characteristics may be evaluated through simulation including a dynamic model of the system under control coupled with the compensation model.\n\nModern control theory is carried out in the state space, and can deal with multiple-input and multiple-output (MIMO) systems. This overcomes the limitations of classical control theory in more sophisticated design problems, such as fighter aircraft control, with the limitation that no frequency domain analysis is possible. In modern design, a system is represented to the greatest advantage as a set of decoupled first order differential equations defined using state variables. Nonlinear, multivariable, adaptive and robust control theories come under this division. Matrix methods are significantly limited for MIMO systems where linear independence cannot be assured in the relationship between inputs and outputs . Being fairly new, modern control theory has many areas yet to be explored. Scholars like Rudolf E. Kalman and Aleksandr Lyapunov are well-known among the people who have shaped modern control theory.\n\nControl engineering is the engineering discipline that focuses on the modeling of a diverse range of dynamic systems (e.g. mechanical systems) and the design of controllers that will cause these systems to behave in the desired manner. Although such controllers need not be electrical, many are and hence control engineering is often viewed as a subfield of electrical engineering. However, the falling price of microprocessors is making the actual implementation of a control system essentially trivial. As a result, focus is shifting back to the mechanical and process engineering discipline, as intimate knowledge of the physical system being controlled is often desired.\n\nElectrical circuits, digital signal processors and microcontrollers can all be used to implement control systems. Control engineering has a wide range of applications from the flight and propulsion systems of commercial airliners to the cruise control present in many modern automobiles. \n\nIn most cases, control engineers utilize feedback when designing control systems. This is often accomplished using a PID controller system. For example, in an automobile with cruise control the vehicle's speed is continuously monitored and fed back to the system, which adjusts the motor's torque accordingly. Where there is regular feedback, control theory can be used to determine how the system responds to such feedback. In practically all such systems stability is important and control theory can help ensure stability is achieved.\n\nAlthough feedback is an important aspect of control engineering, control engineers may also work on the control of systems without feedback. This is known as open loop control. A classic example of open loop control is a washing machine that runs through a pre-determined cycle without the use of sensors.\n\nAt many universities around the world, control engineering courses are taught primarily in electrical engineering and mechanical engineering, but some courses can be instructed in mechatronics engineering, and aerospace engineering. In others, control engineering is connected to computer science, as most control techniques today are implemented through computers, often as embedded systems (as in the automotive field). The field of control within chemical engineering is often known as process control. It deals primarily with the control of variables in a chemical process in a plant. It is taught as part of the undergraduate curriculum of any chemical engineering program and employs many of the same principles in control engineering. Other engineering disciplines also overlap with control engineering as it can be applied to any system for which a suitable model can be derived. However, specialised control engineering departments do exist, for example, the Department of Automatic Control and Systems Engineering at the University of Sheffield and the Department of Robotics and Control Engineering at the United States Naval Academy.\n\nControl engineering has diversified applications that include science, finance management, and even human behavior. Students of control engineering may start with a linear control system course dealing with the time and complex-s domain, which requires a thorough background in elementary mathematics and Laplace transform, called classical control theory. In linear control, the student does frequency and time domain analysis. Digital control and nonlinear control courses require Z transformation and algebra respectively, and could be said to complete a basic control education.\n\nA control engineer's career starts with a bachelor's degree and can continue through the college process. Control engineer degrees are well paired with an electrical or mechanical engineering degree. Control engineers usually get jobs in technical managing where they typically lead interdisciplinary projects. There are many job opportunities in aerospace companies, manufacturing companies, automobile companies, power companies, and government agencies. Some place that hire Control Engineers include companies such as Rockwell Automation, NASA, Ford, and Goodrich. Control Engineers can possibly earn $66k annually from Lockheed Martin Corp. They can also earn up to $96k annually from General Motors Corporation.\n\nAccording to a \"Control Engineering\" survey, most of the people who answered were control engineers in various forms of their own career. There are not very many careers that are classified as \"control engineer,\" most of them are specific careers that have a small semblance to the overarching career of control engineering. A majority of the control engineers that took the survey in 2019 are system or product designers, or even control or instrument engineers. Most of the jobs involve process engineering or production or even maintenance, they are some variation of control engineering.\n\nOriginally, control engineering was all about continuous systems. Development of computer control tools posed a requirement of discrete control system engineering because the communications between the computer-based digital controller and the physical system are governed by a computer clock. The equivalent to Laplace transform in the discrete domain is the Z-transform. Today, many of the control systems are computer controlled and they consist of both digital and analog components.\n\nTherefore, at the design stage either digital components are mapped into the continuous domain and the design is carried out in the continuous domain, or analog components are mapped into discrete domain and design is carried out there. The first of these two methods is more commonly encountered in practice because many industrial systems have many continuous systems components, including mechanical, fluid, biological and analog electrical components, with a few digital controllers.\n\nSimilarly, the design technique has progressed from paper-and-ruler based manual design to computer-aided design and now to computer-automated design or CAutoD which has been made possible by evolutionary computation. CAutoD can be applied not just to tuning a predefined control scheme, but also to controller structure optimisation, system identification and invention of novel control systems, based purely upon a performance requirement, independent of any specific control scheme.\n\nResilient control systems extend the traditional focus of addressing only planned disturbances to frameworks and attempt to address multiple types of unexpected disturbance; in particular, adapting and transforming behaviors of the control system in response to malicious actors, abnormal failure modes, undesirable human action, etc.\n\nBULLET::::- Electrical engineering\nBULLET::::- Communications engineering\nBULLET::::- Satellite navigation\nBULLET::::- Outline of control engineering\nBULLET::::- Advanced process control\nBULLET::::- Building automation\nBULLET::::- Computer-automated design (CAutoD, CAutoCSD)\nBULLET::::- Control reconfiguration\nBULLET::::- Feedback\nBULLET::::- H-infinity\nBULLET::::- Lead–lag compensator\nBULLET::::- List of control engineering topics\nBULLET::::- Quantitative feedback theory\nBULLET::::- Robotic unicycle\nBULLET::::- State space\nBULLET::::- Sliding mode control\nBULLET::::- Systems engineering\nBULLET::::- Testing controller\nBULLET::::- VisSim\nBULLET::::- Control Engineering (magazine)\nBULLET::::- EICASLAB\nBULLET::::- Time series\nBULLET::::- Process control system\nBULLET::::- Robotic control\nBULLET::::- Mechatronics\n\n\nBULLET::::- Control Labs Worldwide\nBULLET::::- The Michigan Chemical Engineering Process Dynamics and Controls Open Textbook\nBULLET::::- Control System Integrators Association\nBULLET::::- List of control systems integrators\nBULLET::::- Institution of Mechanical Engineers - Mechatronics, Informatics and Control Group (MICG)\nBULLET::::- Systems Science & Control Engineering: An Open Access Journal\n"}
{"id": "7012", "url": "https://en.wikipedia.org/wiki?curid=7012", "title": "Chagas disease", "text": "Chagas disease\n\nChagas disease, also known as American trypanosomiasis, is a tropical parasitic disease caused by the protist \"Trypanosoma cruzi\". It is spread mostly by insects known as Triatominae, or \"kissing bugs\". The symptoms change over the course of the infection. In the early stage, symptoms are typically either not present or mild, and may include fever, swollen lymph nodes, headaches, or local swelling at the site of the bite. After 8–12 weeks, individuals enter the chronic phase of disease and in 60–70% it never produces further symptoms. The other 30–40% of people develop further symptoms 10–30 years after the initial infection, including enlargement of the ventricles of the heart in 20–30%, leading to heart failure. An enlarged esophagus or an enlarged colon may also occur in 10% of people.\n\"T. cruzi\" is commonly spread to humans and other mammals by the blood-sucking \"kissing bugs\" of the subfamily Triatominae. These insects are known by a number of local names, including: \"vinchuca\" in Argentina, Bolivia, Chile and Paraguay, \"barbeiro\" (the barber) in Brazil, \"pito\" in Colombia, \"chinche\" in Central America, and \"chipo\" in Venezuela. The disease may also be spread through blood transfusion, organ transplantation, eating food contaminated with the parasites, and by vertical transmission (from a mother to her fetus). Diagnosis of early disease is by finding the parasite in the blood using a microscope. Chronic disease is diagnosed by finding antibodies for \"T. cruzi\" in the blood.\nPrevention mostly involves eliminating kissing bugs and avoiding their bites. This may involve the use of insecticides or bed-nets. Other preventive efforts include screening blood used for transfusions. A vaccine has not been developed as of 2017. Early infections are treatable with the medication benznidazole or nifurtimox. Medication nearly always results in a cure if given early, but becomes less effective the longer a person has had Chagas disease. When used in chronic disease, medication may delay or prevent the development of end–stage symptoms. Benznidazole and nifurtimox cause temporary side effects in up to 40% of people including skin disorders, brain toxicity, and digestive system irritation.\nIt is estimated that 6.6 million people, mostly in Mexico, Central America and South America, have Chagas disease as of 2015. In 2015, Chagas was estimated to result in 8,000 deaths. Most people with the disease are poor, and most do not realize they are infected. Large-scale population movements have increased the areas where Chagas disease is found and these include many European countries and the United States. These areas have also seen an increase in the years up to 2014. The disease was first described in 1909 by the Brazilian physician Carlos Chagas, after whom it is named. Chagas disease is classified as a neglected tropical disease. It affects more than 150 other animals.\n\nThe human disease occurs in two stages: an acute stage, which occurs shortly after an initial infection, and a chronic stage that develops over many years.\n\nThe acute phase lasts for the first few weeks or months of infection. It usually occurs unnoticed because it is symptom-free or exhibits only mild symptoms that are not unique to Chagas disease. These can include fever, fatigue, body aches, muscle pain, headache, rash, loss of appetite, diarrhea, nausea, swollen eyelids, and vomiting. The signs on physical examination can include mild enlargement of the liver or spleen, swollen glands, and local swelling (a chagoma) where the parasite entered the body.\n\nThe most recognized marker of acute Chagas disease is called Romaña's sign, which includes swelling of the eyelids on the side of the face near the bite wound or where the bug feces were deposited or accidentally rubbed into the eye. Rarely, people may die from the acute disease due to severe inflammation/infection of the heart muscle (myocarditis) or brain (meningoencephalitis). The acute phase also can be severe in people with weakened immune systems.\n\nIf symptoms develop during the acute phase, they usually resolve spontaneously within three to eight weeks in approximately 90% of individuals. Although the symptoms resolve, even with treatment the infection persists and enters a chronic phase. Of individuals with chronic Chagas disease, 60–80% will never develop symptoms (called \"indeterminate\" chronic Chagas disease), while the remaining 20–40% will develop life-threatening heart and/or digestive disorders during their lifetime (called \"determinate\" chronic Chagas disease). In 10% of individuals, the disease progresses directly from the acute form to a symptomatic clinical form of chronic Chagas disease.\n\nThe symptomatic (determinate) chronic stage affects the nervous system, digestive system and heart. About two-thirds of people with chronic symptoms have cardiac damage, including dilated cardiomyopathy, which causes heart rhythm abnormalities and may result in sudden death. About one-third of patients go on to develop digestive system damage, resulting in dilation of the digestive tract (megacolon and megaesophagus), accompanied by severe weight loss. Swallowing difficulties (secondary achalasia) may be the first symptom of digestive disturbances and may lead to malnutrition.\n\n20–50% of individuals with intestinal involvement also exhibit cardiac involvement. Up to 10% of chronically infected individuals develop neuritis that results in altered tendon reflexes and sensory impairment. Isolated cases exhibit central nervous system involvement, including dementia, confusion, chronic encephalopathy and sensory and motor deficits.\n\nThe clinical manifestations of Chagas disease are due to cell death in the target tissues that occurs during the infective cycle, by sequentially inducing an inflammatory response, cellular lesions, and fibrosis. For example, intracellular amastigotes destroy the intramural neurons of the autonomic nervous system in the intestine and heart, leading to megaintestine and heart aneurysms, respectively. If left untreated, Chagas disease can be fatal, in most cases due to heart muscle damage.\n\nIn Chagas-endemic areas, the main mode of transmission is through an insect vector called a triatomine bug. A triatomine becomes infected with \"T. cruzi\" by feeding on the blood of an infected person or animal. During the day, triatomines hide in crevices in the walls and roofs.\n\nThe bugs emerge at night, when the inhabitants are sleeping. Because they tend to feed on people's faces, triatomine bugs are also known as \"kissing bugs\". After they bite and ingest blood, they defecate on the person. Triatomines pass \"T. cruzi\" parasites (called trypomastigotes) in feces left near the site of the bite wound.\n\nScratching the site of the bite causes the trypomastigotes to enter the host through the wound, or through intact mucous membranes, such as the conjunctiva. Once inside the host, the trypomastigotes invade cells, where they differentiate into intracellular amastigotes. The amastigotes multiply by binary fission and differentiate into trypomastigotes, which are then released into the bloodstream. This cycle is repeated in each newly infected cell. Replication resumes only when the parasites enter another cell or are ingested by another vector. (See also: )\n\nDense vegetation (such as that of tropical rainforests) and urban habitats are not ideal for the establishment of the human transmission cycle. However, in regions where the sylvatic habitat and its fauna are thinned by economic exploitation and human habitation, such as in newly deforested areas, piassava palm culture areas, and some parts of the Amazon region, a human transmission cycle may develop as the insects search for new food sources.\n\n\"T. cruzi\" can also be transmitted through blood transfusions. With the exception of blood derivatives (such as fractionated antibodies), all blood components are infective. The parasite remains viable at 4 °C for at least 18 days or up to 250 days when kept at room temperature. It is unclear whether \"T. cruzi\" can be transmitted through frozen-thawed blood components.\n\nOther modes of transmission include organ transplantation, through breast milk, and by accidental laboratory exposure. Chagas disease can also be spread congenitally (from a pregnant woman to her baby) through the placenta, and accounts for approximately 13% of stillborn deaths in parts of Brazil.\n\nTransmission from eating contaminated food has been described. In 1991, farm workers in the state of Paraíba, Brazil, were infected by eating contaminated food; transmission has also occurred via contaminated açaí palm fruit juice and garapa. A 2007 outbreak in 103 Venezuelan school children was attributed to contaminated guava juice.\n\nChagas disease is a growing problem in Europe, because the majority of cases with chronic infection are asymptomatic and because of migration from Latin America.\n\nThe presence of \"T. cruzi\" is diagnostic of Chagas disease. It can be detected by microscopic examination of fresh anticoagulated blood, or its buffy coat, for motile parasites; or by preparation of thin and thick blood smears stained with Giemsa, for direct visualization of parasites. Microscopically, \"T. cruzi\" can be confused with \"Trypanosoma rangeli\", which is not known to be pathogenic in humans. Isolation of \"T. cruzi\" can occur by inoculation into mice, by culture in specialized media (for example, NNN, LIT); and by xenodiagnosis, where uninfected Reduviidae bugs are fed on the patient's blood, and their gut contents examined for parasites.\n\nVarious immunoassays for \"T. cruzi\" are available and can be used to distinguish among strains (zymodemes of \"T.cruzi\" with divergent pathogenicities). These tests include: detecting complement fixation, indirect hemagglutination, indirect fluorescence assays, radioimmunoassays, and ELISA. Alternatively, diagnosis and strain identification can be made using polymerase chain reaction (PCR).\nThere is currently no vaccine against Chagas disease. Prevention is generally focused on decreasing the numbers of the insect that spreads it (\"Triatoma\") and decreasing their contact with humans. This is done by using insecticides (usually cypermethrin or permethrin), and improving housing and sanitary conditions in rural areas. For urban dwellers, spending vacations and camping out in the wilderness or sleeping at hostels or mud houses in endemic areas can be dangerous; a mosquito net is recommended. Some measures of vector control include:\nBULLET::::- A yeast trap can be used for monitoring infestations of certain species of triatomine bugs (\"Triatoma sordida\", \"Triatoma brasiliensis\", \"Triatoma pseudomaculata\", and \"Panstrongylus megistus\").\nBULLET::::- Promising results were gained with the treatment of vector habitats with the fungus \"Beauveria bassiana\".\nBULLET::::- Targeting the symbionts of Triatominae through paratransgenesis can be done.\n\nA number of potential vaccines are currently being tested. Vaccination with \"Trypanosoma rangeli\" has produced positive results in animal models. More recently, the potential of DNA vaccines for immunotherapy of acute and chronic Chagas disease is being tested by several research groups.\n\nBlood transfusion was formerly the second-most common mode of transmission for Chagas disease, but the development and implementation of blood bank screening tests has dramatically reduced this risk in the 21st century. Blood donations in all endemic Latin American countries undergo Chagas screening, and testing is expanding in countries, such as France, Spain and the United States, that have significant or growing populations of immigrants from endemic areas. In Spain, donors are evaluated with a questionnaire to identify individuals at risk of Chagas exposure for screening tests.\n\nThe US FDA has approved two Chagas tests, including one approved in April 2010, and has published guidelines that recommend testing of all donated blood and tissue products. While these tests are not required in US, an estimated 75–90% of the blood supply is currently tested for Chagas, including all units collected by the American Red Cross, which accounts for 40% of the U.S. blood supply. The Chagas Biovigilance Network reports current incidents of Chagas-positive blood products in the United States, as reported by labs using the screening test approved by the FDA in 2007.\n\nThere are two approaches to treating Chagas disease: antiparasitic treatment, to kill the parasite; and symptomatic treatment, to manage the symptoms and signs of the infection. Management uniquely involves addressing selective incremental failure of the parasympathetic nervous system. Autonomic disease imparted by Chagas may eventually result in megaesophagus, megacolon and accelerated dilated cardiomyopathy. The mechanisms that explain why Chagas targets the parasympathetic autonomic nervous system and spares the sympathetic autonomic nervous system remain poorly understood.\n\nAntiparasitic treatment is most effective early in the course of infection, but is not limited to cases in the acute phase. Drugs of choice include azole or nitro derivatives, such as benznidazole or nifurtimox. Both agents are limited in their capacity to completely eliminate \"T. cruzi\" from the body (parasitologic cure), especially in chronically infected patients, and resistance to these drugs has been reported.\n\nStudies suggest antiparasitic treatment leads to parasitological cure in more than 90% of infants but only about 60–85% of adults treated in the first year of acute phase Chagas disease. Children aged six to 12 years with chronic disease have a cure rate of about 60% with benznidazole. While the rate of cure declines the longer an adult has been infected with Chagas, treatment with benznidazole has been shown to slow the onset of heart disease in adults with chronic Chagas infections.\n\nTreatment of chronic infection in women prior to or during pregnancy does not appear to reduce the probability the disease will be passed on to the infant. Likewise, it is unclear whether prophylactic treatment of chronic infection is beneficial in persons who will undergo immunosuppression (for example, organ transplant recipients) or in persons who are already immunosuppressed (for example, those with HIV infection).\n\nIn the chronic stage, treatment involves managing the clinical manifestations of the disease. For example, pacemakers and medications for irregular heartbeats, such as the anti-arrhythmia drug amiodarone, may be life saving for some patients with chronic cardiac disease, while surgery may be required for megaintestine. The disease cannot be cured in this phase, however. Chronic heart disease caused by Chagas disease is now a common reason for heart transplantation surgery. Until recently, however, Chagas disease was considered a contraindication for the procedure, since the heart damage could recur as the parasite was expected to seize the opportunity provided by the immunosuppression that follows surgery.\n\nChagas disease affects 8 to 10 million people living in endemic Latin American countries, with an additional 300,000–400,000 living in nonendemic countries, including Spain and the United States. An estimated 41,200 new cases occur annually in endemic countries, and 14,400 infants are born with congenital Chagas disease annually. in 2010 it resulted in approximately 10,300 deaths up from 9,300 in 1990.\n\nThe disease is present in 18 countries on the American continents, ranging from the southern United States to northern Argentina. Chagas exists in two different ecological zones. In the Southern Cone region, the main vector lives in and around human homes. In Central America and Mexico, the main vector species lives both inside dwellings and in uninhabited areas. In both zones, Chagas occurs almost exclusively in rural areas, where triatomines breed and feed on the more than 150 species from 24 families of domestic and wild mammals, as well as humans, that are the natural reservoirs of \"T. cruzi\".\n\nAlthough Triatominae bugs feed on them, birds appear to be immune to infection and therefore are not considered to be a \"T. cruzi\" reservoir. Even when colonies of insects are eradicated from a house and surrounding domestic animal shelters, they can re-emerge from plants or animals that are part of the ancient, sylvatic (referring to wild animals) infection cycle. This is especially likely in zones with mixed open savannah, with clumps of trees interspersed by human habitation.\n\nThe primary wildlife reservoirs for \"Trypanosoma cruzi\" in the United States include opossums, raccoons, armadillos, squirrels, woodrats, and mice. Opossums are particularly important as reservoirs, because the parasite can complete its life cycle in the anal glands of this animal without having to re-enter the insect vector. Recorded prevalence of the disease in opossums in the U.S. ranges from 8.3% to 37.5%.\n\nStudies on raccoons in the Southeast have yielded infection rates ranging from 47% to as low as 15.5%. Armadillo prevalence studies have been described in Louisiana, and range from a low of 1.1% to 28.8%. Additionally, small rodents, including squirrels, mice, and rats, are important in the sylvatic transmission cycle because of their importance as bloodmeal sources for the insect vectors. A Texas study revealed 17.3% percent \"T. cruzi\" prevalence in 75 specimens representing four separate small rodent species.\n\nChronic Chagas disease remains a major health problem in many Latin American countries, despite the effectiveness of hygienic and preventive measures, such as eliminating the transmitting insects. However, several landmarks have been achieved in the fight against it in Latin America, including a reduction by 72% of the incidence of human infection in children and young adults in the countries of the Southern Cone Initiative, and at least three countries (Uruguay, in 1997, and Chile, in 1999, and Brazil in 2006) have been certified free of vectorial and transfusional transmission. In Argentina, vectorial transmission has been interrupted in 13 of the 19 endemic provinces, and major progress toward this goal has also been made in both Paraguay and Bolivia.\n\nScreening of donated blood, blood components, and solid organ donors, as well as donors of cells, tissues, and cell and tissue products for \"T. cruzi\" is mandated in all Chagas-endemic countries and has been implemented. Approximately 300,000 infected people live in the United States, which is likely the result of immigration from Latin American countries, and there have been 23 cases acquired from kissing bugs in the United States reported between 1955 and 2014. With increased population movements, the possibility of transmission by blood transfusion became more substantial in the United States. Transfusion blood and tissue products are now actively screened in the U.S., thus addressing and minimizing this risk.\n\nThe earliest detection of a \"T. cruzi\" infection comes from a 9000-year-old Chinchorro mummy. Other exhumed mummies in the Andean region as well as pre-Columbian Peruvian ceramics shed light on the existence of Chagas Disease and have provided anthropologists reasons for how and why the illness spread. In 1707, the Portuguese physician, Miguel Dial Pimenta, was the first individual to provide a clinical report relating to the possible intestinal symptoms of Chagas Disease.\n\nThe disease was named after the Brazilian physician and epidemiologist Carlos Chagas, who first described it in 1909. The disease was not seen as a major public health problem in humans until the 1960s (the outbreak of Chagas disease in Brazil in the 1920s went widely ignored). Dr Chagas discovered that the intestines of Triatomidae (now Reduviidae: Triatominae) harbored a flagellate protozoan, a new species of the genus \"Trypanosoma\", and was able to demonstrate experimentally that it could be transmitted to marmoset monkeys that were bitten by the infected bug. Later studies showed squirrel monkeys were also vulnerable to infection.\n\nChagas named the pathogenic parasite as \"Trypanosoma cruzi\" and later that year as \"Schizotrypanum cruzi\", both honoring Oswaldo Cruz, the noted Brazilian physician and epidemiologist who successfully fought epidemics of yellow fever, smallpox, and bubonic plague in Rio de Janeiro and other cities in the beginning of the 20th century. Chagas was also the first to unknowingly discover and illustrate the parasitic fungal genus \"Pneumocystis\", later infamously linked to PCP (\"Pneumocystis\" pneumonia in AIDS victims). Confusion between the two pathogens' life-cycles led him to briefly recognize his genus \"Schizotrypanum\", but following the description of \"Pneumocystis\" by others as an independent genus, Chagas returned to the use of the name \"Trypanosoma cruzi\".\n\nIn Argentina, the disease is known as \"mal de Chagas-Mazza\", in honor of Salvador Mazza, the Argentine physician who in 1926 began investigating the disease and over the years became the principal researcher of this disease in the country. Mazza produced the first scientific confirmation of the existence of \"Trypanosoma cruzi\" in Argentina in 1927, eventually leading to support from local and European medical schools and Argentine government policy makers.\n\nIt has been hypothesized that Charles Darwin might have suffered from Chagas disease as a result of a bite of the so-called great black bug of the Pampas (\"vinchuca\") (see Charles Darwin's illness). The episode was reported by Darwin in his diaries of the Voyage of the Beagle as occurring in March 1835 to the east of the Andes near Mendoza. Darwin was young and generally in good health, though six months previously he had been ill for a month near Valparaiso, but in 1837, almost a year after he returned to England, he began to suffer intermittently from a strange group of symptoms, becoming incapacitated for much of the rest of his life. Attempts to test Darwin's remains at Westminster Abbey by using modern PCR techniques were met with a refusal by the Abbey's curator.\n\nSeveral experimental treatments have shown promise in animal models. These include inhibitors of oxidosqualene cyclase and squalene synthase, cysteine protease inhibitors, dermaseptins collected from frogs in the genus \"Phyllomedusa\" (\"P. oreades\" and \"P. distincta\"), the sesquiterpene lactone dehydroleucodine (DhL), which affects the growth of cultured epimastigote-phase \"Trypanosoma cruzi\", inhibitors of purine uptake, and inhibitors of enzymes involved in trypanothione metabolism. Hopefully, new drug targets may be revealed following the sequencing of the \"T. cruzi\" genome.\n\nChagas disease has a serious economic impact on the United States and the world. The cost of treatment in the United States alone, where the disease is not indigenous, is estimated to be $900 million annually, which includes hospitalization and medical devices such as pacemakers. The global cost is estimated at $7 billion.\n\nMegazol in a study seems more active against Chagas than benznidazole but has not been studied in humans. A Chagas vaccine (TcVac3) has been found to be effective in mice with plans for studies in dogs. It is hoped that it will be commercially available by 2018.\n\nBULLET::::- Drugs for Neglected Diseases Initiative\nBULLET::::- Association for the Promotion of Independent Disease Control in Developing Countries\n\nBULLET::::- An editorial.\nBULLET::::- A special issue of the \"Memórias do Instituto Oswaldo Cruz\", covering all aspects of Chagas Disease\n\nBULLET::::- Chagas information at the U.S. Centers for Disease Control\nBULLET::::- Chagas information from the Drugs for Neglected Diseases initiative\nBULLET::::- UNHCO site on Chagas Disease\nBULLET::::- Chagas Disease information for travellers from IAMAT (International Association for Medical Assistance to Travellers)\n"}
{"id": "7015", "url": "https://en.wikipedia.org/wiki?curid=7015", "title": "Christiaan Barnard", "text": "Christiaan Barnard\n\nChristiaan Neethling Barnard (8 November 1922 – 2 September 2001) was a South African cardiac surgeon who performed the world's first highly publicized heart transplant and the first one in which the patient regained consciousness. On 3 December 1967, Barnard transplanted the heart of accident-victim Denise Darvall into the chest of 54-year-old Louis Washkansky, with Washkansky regaining full consciousness and being able to easily talk with his wife, before dying 18 days later of pneumonia. The anti-rejection drugs that suppressed his immune system were a major contributing factor. Barnard had told Mr. and Mrs. Washkansky that the operation had an 80% chance of success, a claim which has been criticised as misleading. Barnard's second transplant patient Philip Blaiberg, whose operation was performed at the beginning of 1968, lived for a year and a half and was able to go home from the hospital.\n\nBorn in Beaufort West, Cape Province, Barnard studied medicine and practised for several years in his native South Africa. As a young doctor experimenting on dogs, Barnard developed a remedy for the infant defect of intestinal atresia. His technique saved the lives of ten babies in Cape Town and was adopted by surgeons in Britain and the United States. In 1955, he travelled to the United States and was initially assigned further gastrointestinal work by Owen Harding Wangensteen. He was introduced to the heart-lung machine, and Barnard was allowed to transfer to the service run by open heart surgery pioneer Walt Lillehei. Upon returning to South Africa in 1958, Barnard was appointed head of the Department of Experimental Surgery at the Groote Schuur Hospital, Cape Town.\n\nHe retired as Head of the Department of Cardiothoracic Surgery in Cape Town in 1983 after developing rheumatoid arthritis in his hands which ended his surgical career. He became interested in anti-aging research, and in 1986 his reputation suffered when he promoted \"Glycel\", an expensive \"anti-aging\" skin cream, whose approval was withdrawn by the United States Food and Drug Administration soon thereafter. During his remaining years, he established the Christiaan Barnard Foundation, dedicated to helping underprivileged children throughout the world. He died in 2001 at the age of 78 after an asthma attack.\n\nBarnard grew up in Beaufort West, Cape Province, Union of South Africa. His father, Adam Barnard, was a minister in the Dutch Reformed Church. One of his four brothers, Abraham, was a \"blue baby\" who died of a heart problem at the age of three (Barnard would later guess that it was tetralogy of Fallot). The family also experienced the loss of a daughter who was stillborn and who had been the fraternal twin of Barnard's older brother Johannes, who was twelve years older than Chris. Barnard matriculated from the Beaufort West High School in 1940, and went to study medicine at the University of Cape Town Medical School, where he obtained his MB ChB in 1945.\n\nHis father served as a missionary to mixed-race peoples. His mother, the former Maria Elisabeth de Swart, instilled in the surviving brothers the belief that they could do anything they set their minds to.\n\nBarnard did his internship and residency at the Groote Schuur Hospital in Cape Town, after which he worked as a general practitioner in Ceres, a rural town in the Cape Province. In 1951, he returned to Cape Town where he worked at the City Hospital as a Senior Resident Medical Officer, and in the Department of Medicine at the Groote Schuur Hospital as a registrar. He completed his master's degree, receiving Master of Medicine in 1953 from the University of Cape Town. In the same year he obtained a doctorate in medicine (MD) from the same university for a dissertation titled \"The treatment of tuberculous meningitis\".\n\nSoon after qualifying as a doctor, Barnard performed experiments on dogs investigating intestinal atresia, a birth defect which allows life-threatening gaps to develop in the intestines. He followed a medical hunch that this was caused by inadequate blood flow to the fetus. After nine months and forty-three attempts, Barnard was able to reproduce this condition in a fetus puppy by tying off some of the blood supply to a puppy's intestines and then placing the animal back in the womb, after which it was born some two weeks later, with the condition of intestinal atresia. He was also able to cure the condition by removing the piece of intestine with inadequate blood supply. The mistake of previous surgeons had been attempting to reconnect ends of intestine which themselves still had inadequate blood supply. To be successful, it was typically necessary to remove between 15 and 20 centimeters of intestine (6 to 8 inches). Jannie Louw used this innovation in a clinical setting, and Barnard's method saved the lives of ten babies in Cape Town. This technique was also adapted by surgeons in Britain and the US. In addition, Barnard analyzed 259 cases of tubercular meningitis.\n\nOwen Wangensteen in Minnesota had been impressed by the work of Alan Thal, a young South African doctor working in Minnesota. He asked Groote Schuur Head of Medicine John Brock if he might recommend any similarly talented South Africans and Brock recommended Barnard. In December 1955, Barnard travelled to the University of Minnesota, Minneapolis, United States, to begin a two-year scholarship under Chief of Surgery Wangensteen, who assigned Barnard more work on the intestines, which Barnard accepted even though he wanted to move onto something new. Simply by luck, whenever Barnard needed a break from this work, he could wander across the hall and talk with Vince Gott who ran the lab for open-heart surgery pioneer Walt Lillehei. Gott had begun to develop a technique of running blood backwards through the veins of the heart so Lillehei could more easily operate on the aortic valve (McRae writes, \"It was the type of inspired thinking that entranced Barnard\"). In March 1956, Gott asked Barnard to help him run the heart-lung machine for an operation. Shortly thereafter, Wangensteen agreed to let Barnard switch to Lillehei's service. It was during this time that Barnard first became acquainted with fellow future heart transplantation surgeon Norman Shumway. Barnard also became friendly with Gil Campbell who had demonstrated that a dog's lung could be used to oxygenate blood during open-heart surgery. (The year before Barnard arrived, Lillehei and Campbell had used this procedure for twenty minutes during surgery on a 13-year-old boy with ventricular septal defect, and the boy had made a full recovery.) Barnard and Campbell met regularly for early breakfast. In 1958, Barnard received a Master of Science in Surgery for a thesis titled \"The aortic valve – problems in the fabrication and testing of a prosthetic valve\". The same year he was awarded a Ph.D. for his dissertation titled \"The aetiology of congenital intestinal atresia\". Barnard described the two years he spent in the United States as \"the most fascinating time in my life.\"\n\nUpon returning to South Africa in 1958, Barnard was appointed head of the Department of Experimental Surgery at Groote Schuur hospital, as well as holding a joint post at the University of Cape Town. He was promoted to full-time lecturer and Director of Surgical Research at the University of Cape Town. In 1960, he flew to Moscow in order to meet Vladimir Demikhov, a top expert on organ transplants (later he credited Demikhov's accomplishment saying that \"if there is a father of heart and lung transplantation then Demikhov certainly deserves this title.\") In 1961 he was appointed Head of the Division of Cardiothoracic Surgery at the teaching hospitals of the University of Cape Town. He rose to the position of Associate Professor in the Department of Surgery at the University of Cape Town in 1962. Barnard's younger brother Marius, who also studied medicine, eventually became Barnard's right-hand man at the department of Cardiac Surgery. Over time, Barnard became known as a brilliant surgeon with many contributions to the treatment of cardiac diseases, such as the Tetralogy of Fallot and Ebstein's anomaly. He was promoted to Professor of Surgical Science in the Department of Surgery at the University of Cape Town in 1972. In 1981, Barnard became a founding member of the World Cultural Council. Among the many awards he received over the years, he was named Professor Emeritus in 1984.\n\nFollowing the first successful kidney transplant in 1953, in the United States, Barnard performed the second kidney transplant in South Africa in October 1967, the first being done in Johannesburg the previous year.\n\nOn 23 January 1964, James Hardy at the University of Mississippi Medical Center in Jackson, Mississippi, performed the world's first heart transplant and world's first cardiac xenotransplant by transplanting the heart of a chimpanzee into a desperately ill and dying man. This heart did beat in the patient's chest for approximately 60 to 90 minutes. The patient, Boyd Rush, died without ever regaining consciousness.\n\nBarnard had experimentally transplanted forty-eight hearts into dogs, which was about a fifth the number that Adrian Kantrowitz had performed at Maimonides Medical Center in New York and about a sixth the number Norman Shumway had performed at Stanford University in California. Barnard had no dogs which had survived longer than ten days, unlike Kantrowitz and Shumway who had had dogs survive for more than a year.\nWith the availability of new breakthroughs introduced by several pioneers, also including Richard Lower at the Medical College of Virginia, several surgical teams were in a position to prepare for a human heart transplant. Barnard had a patient willing to undergo the procedure, but as with other surgeons, he needed a suitable donor.\n\nDuring the Apartheid era in South Africa, non-white persons and citizens were not given equal opportunities in the medical professions. At Groote Schuur Hospital, Hamilton Naki was an informally taught surgeon. He started out as a gardener and cleaner. One day he was asked to help out with an experiment on a giraffe. From this modest beginning, Naki became principal lab technician and taught hundreds of surgeons, and assisted with Barnard's organ transplant program. Barnard said, \"Hamilton Naki had better technical skills than I did. He was a better craftsman than me, especially when it came to stitching, and had very good hands in the theatre\". A popular myth, propagated principally by a widely discredited documentary film called \"Hidden Heart\" and an erroneous newspaper article, maintains incorrectly that Naki was present during the Washkansky transplant.\n\nBarnard performed the world's first human-to-human heart transplant operation in the early morning hours of Sunday 3 December 1967. Louis Washkansky, a 54-year-old grocer who was suffering from diabetes and incurable heart disease, was the patient. Barnard was assisted by his brother Marius Barnard, as well as a team of thirty staff members. The operation lasted approximately five hours.\n\nBarnard stated to Washkansky and his wife Ann Washkansky that the transplant had an 80% chance of success. This has been criticised by the ethicists Peter Singer and Helga Kuhse as making claims for chances of success to the patient and family which were \"unfounded\" and \"misleading\".\nBarnard later wrote, \"For a dying man it is not a difficult decision because he knows he is at the end. If a lion chases you to the bank of a river filled with crocodiles, you will leap into the water, convinced you have a chance to swim to the other side.\" The donor heart came from a young woman, Denise Darvall, who had been rendered brain dead in an accident on 2 December 1967, while crossing a street in Cape Town. On examination at Groote Schuur hospital, Darvall had two serious fractures in her skull, with no electrical activity in her brain detected, and no sign of pain when ice water was poured into her ear. Coert Venter and Bertie Bosman requested permission from Darvall's father for Denise's heart to be used in the transplant attempt. The afternoon before his first transplant, Barnard dozed at his home while listening to music. When he awoke, he decided to modify Shumway and Lower's technique. Instead of cutting straight across the back of the atrial chambers of the donor heart, he would avoid damage to the septum and instead cut two small holes for the venae cavae and pulmonary veins. Prior to the transplant, rather than wait for Darvall's heart to stop beating, at his brother Marius Barnard's urging, Christiaan had injected potassium into her heart to paralyse it and render her technically dead by the whole-body standard. Twenty years later, Marius Barnard recounted, \"Chris stood there for a few moments, watching, then stood back and said, 'It works.'\"\n\nWashkansky survived the operation and lived for 18 days, having succumbed to pneumonia as he was taking immunosuppressive drugs.\n\nBarnard and his patient received worldwide publicity.\n\nWorldwide, approximately 100 transplants were performed by various doctors during 1968. However, only a third of these patients lived longer than three months. Many medical centers stopped performing transplants. In fact, a U.S. National Institutes of Health publication states, \"Within several years, only Shumway's team at Stanford was attempting transplants.\"\n\nBarnard's second transplant operation was conducted on 2 January 1968, and the patient, Philip Blaiberg, survived for 19 months. Blaiberg's heart was donated by Clive Haupt, a 24-year-old black man who suffered a stroke, inciting controversy (especially in the African-American press) during the time of South African apartheid. Dirk van Zyl, who received a new heart in 1971, was the longest-lived recipient, surviving over 23 years.\n\nBetween December 1967 and November 1974 at Groote Schuur Hospital in Cape Town, South Africa, ten heart transplants were performed, as well as a heart and lung transplant in 1971. Of these ten patients, four lived longer than 18 months, with two of these four becoming long-term survivors. One patient lived for over thirteen years and another for over twenty-four years.\n\nFull recovery of donor heart function often takes place over hours or days, during which time considerable damage can occur. Other deaths to patients can occur from preexisting conditions. For example, in pulmonary hypertension the patient's right ventricle has often adapted to the higher pressure over time and, although diseased and hypertrophied, is often capable of maintaining circulation to the lungs. Barnard designed the idea of the heterotopic (or \"piggy back\" transplant) in which the patient's diseased heart is left in place while the donor heart is added, essentially forming a \"double heart\". Barnard performed the first such heterotopic heart transplant in 1974.\n\nFrom November 1974 through December 1983, 49 consecutive heterotopic heart transplants on 43 patients were performed at Groote Schuur. The survival rate for patients at one year was over 60%, as compared to less than 40% with standard transplants, and the survival rate at five years was over 36% as compared to less than 20% with standard transplants.\n\nMany surgeons gave up cardiac transplantation due to poor results, often due to rejection of the transplanted heart by the patient's immune system. Barnard persisted until the advent of cyclosporine, an effective immunosuppressive drug, which helped revive the operation throughout the world. He also attempted xenotransplantation in a human patient, while attempting to save the life of a girl who was unable to leave artificial life support after her second aortic valve replacement.\n\nBarnard was an outspoken opponent of South Africa's laws of apartheid, and was not afraid to criticise his nation's government, although he had to temper his remarks to some extent to travel abroad. Rather than leaving his homeland, he used his fame to campaign for a change in the law. Christiaan's brother, Marius Barnard, went into politics, and was elected to the legislature from Progressive Federal Party. Barnard later stated that the reason he never won the Nobel Prize in Physiology or Medicine was probably because he was a \"white South African\".\n\nShortly before his visit to Kenya in 1978, the following was written about his views regarding race relations in South Africa;\n\"While he believes in the participation of Africans in the political process of South Africa, he is opposed to a one-man-one-vote system in South Africa\".\n\nIn answering a hypothetical question on how he would solve the race problem were he a \"benevolent dictator in South Africa\", Barnard stated the following in a long interview at the Weekly Review:\nBULLET::::- While \"I would abolish Social discrimination\", political discrimination would continue.\nBULLET::::- He favoured the total division of the country along racial lines. His words were; \"I somehow feel ... but we may have to divide South Africa into two equal divisions\". In a follow-up question about where the coloured people would end up in that scenario, he replied that 'I would include them in the white South Africa\".\nBULLET::::- That coloured people have \"always been accepted\" among whites.\nBULLET::::- That \"the black man will not accept this view\" of universal suffrage.\nBULLET::::- That \"we are still out of the Olympic games\" despite the fact that \"in the field of sports where we have virtually integrated completely.\"\nBULLET::::- Regarding the Soweto uprising, he claimed \"there was ... a lot of external stirring up of turbulence\". Regarding the anger from the black population when Steve Biko was murdered, he said that \"I think that something like $50,000 came in from outside to work up feelings at that funeral.\"\nBULLET::::- He stated that the National Party members were as upset about Biko's murder as were blacks; \"The white community was thoroughly upset, let me tell you. The nationalists themselves were very upset.\"\n\nThe interview ended with the following summary from he himself;\n\"I often say that, like King Lear, South Africa is a country more sinned against than sinning.\"\n\nBarnard's first marriage was to Aletta Gertruida Louw, a nurse, whom he married in 1948 while practising medicine in Ceres. The couple had two children: Deirdre (born 1950) and Andre (1951–1984). International fame took a toll on his personal life, and in 1969, Barnard and his wife divorced. In 1970, he married heiress Barbara Zoellner when she was 19, the same age as his son, and they had two children: Frederick (born 1972) and Christiaan Jr. (born 1974). He divorced Zoellner in 1982. Barnard married for a third time in 1988 to Karin Setzkorn, a young model. They also had two children, Armin (born 1989) and Lara (born 1997), but this last marriage also ended in divorce in 2000.\n\nBarnard described in his autobiography \"The Second Life\" a one-night extramarital affair with Italian film star Gina Lollobrigida, that occurred in January 1968. During that visit to Rome he received an audience from Pope Paul VI.\n\nIn October 2016, U.S. Congresswoman Ann McLane Kuster (D-NH) stated that Barnard sexually assaulted her when she was 23 years old. According to Kuster, he attempted to grope her under her skirt, while seated at a business luncheon with Rep. Pete McCloskey (R-CA), whom she worked for at the time.\n\nBarnard retired as Head of the Department of Cardiothoracic Surgery in Cape Town in 1983 after developing rheumatoid arthritis in his hands which ended his surgical career. He had struggled with arthritis since 1956, when it was diagnosed during his postgraduate work in the United States. After retirement, he spent two years as the Scientist-In-Residence at the Oklahoma Transplantation Institute in the United States and as an acting consultant for various institutions.\n\nHe had by this time become very interested in anti-aging research, and his reputation suffered in 1986 when he promoted \"Glycel\", an expensive \"anti-aging\" skin cream, whose approval was withdrawn by the United States Food and Drug Administration soon thereafter. He also spent time as a research advisor to the Clinique la Prairie, in Switzerland, where the controversial \"rejuvenation therapy\" was practised.\n\nBarnard divided the remainder of his years between Austria, where he established the Christiaan Barnard Foundation, dedicated to helping underprivileged children throughout the world, and his game farm in Beaufort West, South Africa.\n\nChristiaan Barnard died on 2 September 2001, while on holiday in Paphos, Cyprus. Early reports stated that he had died of a heart attack, but an autopsy showed his death was caused by a severe asthma attack.\n\nChristiaan Barnard wrote two autobiographies. His first book, \"One Life\", was published in 1969 () and sold copies worldwide. Some of the proceeds were used to set up the Chris Barnard Fund for research into heart disease and heart transplants in Cape Town. His second autobiography, \"The Second Life\", was published in 1993, eight years before his death ().\n\nApart from his autobiographies, Dr Barnard also wrote several other books including:\nBULLET::::- \"The Donor\"\nBULLET::::- \"Your Healthy Heart\"\nBULLET::::- \"In The Night Season\"\nBULLET::::- \"The Best Medicine\"\nBULLET::::- \"Arthritis Handbook: How to Live With Arthritis\"\nBULLET::::- \"Good Life Good Death: A Doctor's Case for Euthanasia and Suicide\"\nBULLET::::- \"South Africa: Sharp Dissection\"\nBULLET::::- \"50 Ways to a Healthy Heart\"\nBULLET::::- \"Body Machine\"\n\nBULLET::::- René Favaloro\nBULLET::::- Pierre Grondin\nBULLET::::- Hamilton Naki\nBULLET::::- Geoffrey Tovey\n\nBULLET::::- Christiaan Barnard: his first transplants and their impact on concepts of death\nBULLET::::- To Transplant and Beyond : First Human Heart Transplant\nBULLET::::- In Memoriam : Christiaan Neethling Barnard\nBULLET::::- 40th anniversary of first human heart transplant\nBULLET::::- Official Heart Transplant Museum – Heart Of Cape Town\n"}
{"id": "7016", "url": "https://en.wikipedia.org/wiki?curid=7016", "title": "Concubinage", "text": "Concubinage\n\nConcubinage () is an interpersonal and sexual relationship in which the couple are not or cannot be married. The inability to marry may be due to multiple factors such as differences in social rank status, an existing marriage, religious or professional prohibitions (for example Roman soldiers), or a lack of recognition by appropriate authorities. The woman or man in such a relationship is referred to as a concubine (). In Judaism, a concubine is a marital companion of inferior status to a wife. A concubine among polygamous peoples is a secondary wife, usually of inferior rank.\n\nThe prevalence of concubinage and the status of rights and expectations of a concubine have varied among cultures, as have the rights of children of a concubine. Whatever the status and rights of the concubine, they were always inferior to those of the wife, and typically neither she nor her children had rights of inheritance. Historically, concubinage was usually voluntary (by the woman or her family), as it provided a measure of economic security for the woman. Involuntary or servile concubinage sometimes involved sexual slavery of one member of the relationship, usually the woman. Nevertheless, sexual relations outside marriage were not uncommon, especially among royalty and nobility, and the woman in such relationships was commonly described as a mistress. The children of such relationships were counted as illegitimate and in some societies were barred from inheriting the father's title or estates, even in the absence of legitimate heirs.\n\nWhile forms of long-term sexual relationships and cohabitation short of marriage have become increasingly common in the Western world, these are generally not described as concubinage. The terms concubinage and concubine are used today primarily when referring to non-marital partnerships of earlier eras. In modern usage, a non-marital domestic relationship is commonly referred to as cohabitation (or similar terms), and the woman in such a relationship is generally referred to as a girlfriend, mistress, fiancée, lover or life partner.\n\nConcubinage was highly popular before the early 20th century all over East Asia. The main function of concubinage was producing additional heirs, as well as bringing males pleasure. Children of concubines had lower rights in account to inheritance, which was regulated by the Dishu system.\n\nIn China, successful men often had concubines until the practice was outlawed when the Communist Party of China came to power in 1949. The standard Chinese term translated as \"concubine\" was \"qiè\" , a term that has been used since ancient times, which means \"concubine; I, your servant (deprecating self reference)\". Concubinage resembled marriage in that concubines were recognized sexual partners of a man and were expected to bear children for him. Unofficial concubines () were of lower status, and their children were considered illegitimate. The English term concubine is also used for what the Chinese refer to as \"pínfēi\" (), or \"consorts of emperors\", an official position often carrying a very high rank.\n\nIn premodern China it was illegal and socially disreputable for a man to have more than one wife at a time, but it was acceptable to have concubines. In the earliest records a man could have as many concubines as he could afford. From the Eastern Han period (AD 25–220) onward, the number of concubines a man could have was limited by law. The higher rank and the more noble identity a man possessed, the more concubines he was permitted to have.\n\nA concubine's treatment and situation was variable and was influenced by the social status of the male to whom she was attached, as well as the attitude of his wife. In the \"Book of Rites\" chapter on \"The Pattern of the Family\" () it says, “If there were betrothal rites, she became a wife; and if she went without these, a concubine.” Wives brought a dowry to a relationship, but concubines did not. A concubinage relationship could be entered into without the ceremonies used in marriages, and neither remarriage nor a return to her natal home in widowhood were allowed to a concubine.\n\nThe position of the concubine was generally inferior to that of the wife. Although a concubine could produce heirs, her children would be inferior in social status to a wife's children, although they were of higher status than illegitimate children. The child of a concubine had to show filial duty to two women, their biological mother and their legal mother—the wife of their father. After the death of a concubine, her sons would make an offering to her, but these offerings were not continued by the concubine's grandsons, who only made offerings to their grandfather's wife.\n\nThere are early records of concubines allegedly being buried alive with their masters to \"keep them company in the afterlife\". Until the Song dynasty (960–1276), it was considered a serious breach of social ethics to promote a concubine to a wife.\n\nDuring the Qing dynasty (1644–1911), the status of concubines improved. It became permissible to promote a concubine to wife, if the original wife had died and the concubine was the mother of the only surviving sons. Moreover, the prohibition against forcing a widow to remarry was extended to widowed concubines. During this period tablets for concubine-mothers seem to have been more commonly placed in family ancestral altars, and genealogies of some lineages listed concubine-mothers.\n\nImperial concubines, kept by emperors in the Forbidden City, had different ranks and were traditionally guarded by eunuchs to ensure that they could not be impregnated by anyone but the emperor. In Ming China (1368–1644) there was an official system to select concubines for the emperor. The age of the candidates ranged mainly from 14 to 16. Virtues, behavior, character, appearance and body condition were the selection criteria.\n\nDespite the limitations imposed on Chinese concubines, there are several examples in history and literature of concubines who achieved great power and influence. Lady Yehenara, otherwise known as Empress Dowager Cixi, was arguably one of the most successful concubines in Chinese history. Cixi first entered the court as a concubine to Xianfeng Emperor and gave birth to his only surviving son, who later became Tongzhi Emperor. She eventually became the \"de facto\" ruler of Qing China for 47 years after her husband's death.\n\nAn examination of concubinage features in one of the Four Great Classical Novels, \"Dream of the Red Chamber\" (believed to be a semi-autobiographical account of author Cao Xueqin's family life). Three generations of the Jia family are supported by one notable concubine of the emperor, Jia Yuanchun, the full elder sister of the male protagonist Jia Baoyu. In contrast, their younger half-siblings by concubine Zhao, Jia Tanchun and Jia Huan, develop distorted personalities because they are the children of a concubine.\n\nEmperors' concubines and harems are emphasized in 21st-century romantic novels written for female readers and set in ancient times. As a plot element, the children of concubines are depicted with a status much inferior to that in actual history. The zhai dou (，residential intrigue) and gong dou(，harem intrigue) genres show concubines and wives, as well as their children, scheming secretly to gain power. Empresses in the Palace, a \"gong dou\" type novel and TV drama, has had great success in 21st-century China.\nHong Kong officially abolished the Great Qing Legal Code in 1971, thereby making concubinage illegal. Casino magnate Stanley Ho of Macau took his \"second wife\" as his official concubine in 1957, while his \"third and fourth wives\" retain no official status.\n\nBefore monogamy was legally imposed in the Meiji period, concubinage was common among the nobility. Its purpose was to ensure male heirs. For example, the son of an Imperial concubine often had a chance of becoming emperor. Yanagihara Naruko, a high-ranking concubine of Emperor Meiji, gave birth to Emperor Taishō, who was later legally adopted by Empress Haruko, Emperor Meiji's formal wife. Even among merchant families, concubinage was occasionally used to ensure heirs. Asako Hirooka, an entrepreneur who was the daughter of a concubine, worked hard to help her husband's family survive after the Meiji Restoration. She lost her fertility giving birth to her only daughter, Kameko; so her husband—with whom she got along well—took Asako's maid-servant as a concubine and fathered three daughters and a son with her. Kameko, as the child of the formal wife, married a noble man and matrilineally carried on the family name.\n\nJoseon monarchs had a harem which contained concubines of different ranks. Empress Myeongseong managed to have sons, preventing sons of concubines getting power.\n\nChildren of concubines often had lower value in account of marriage. A daughter of concubine could not marry a wife-born son of the same class. For example, Jang Nok-su was a concubine-born daughter of a mayor, who was initially married to a slave-servant, and later became a high-ranking concubine of Yeonsangun.\n\nIn Ancient Greece, the practice of keeping a slave concubine ( \"pallakís\") was little recorded but appeared throughout Athenian history. The law prescribed that a man could kill another man caught attempting a relationship with his concubine for the production of free children, which suggests that a concubine's children were not granted citizenship. While references to the sexual exploitation of maidservants appear in literature, it was considered disgraceful for a man to keep such women under the same roof as his wife. Some interpretations of \"hetaera\" have held they were concubines when they had a permanent relationship with a single man.\n\nConcubinage was an institution practiced in ancient Rome that allowed a man to enter into an informal but recognized relationship with a woman (\"concubina\", plural \"concubinae\") who was not his wife, most often a woman whose lower social status was an obstacle to marriage. Concubinage was \"tolerated to the degree that it did not threaten the religious and legal integrity of the family\". It was not considered derogatory to be called a \"concubina\", as the title was often inscribed on tombstones.\n\nA \"concubinus\" was a young male slave sexually exploited by his master as a sexual partner (see homosexuality in ancient Rome). These relations, however, were expected to play a secondary role to marriage, within which institution an adult male demonstrated his masculine authority as head of the household (\"pater familias\"). In one of his epithalamiums, Catullus ( mid-1st century BC) assumes that the young bridegroom has a \"concubinus\" who considers himself elevated above the other slaves, but who will be set aside as his master turns his attention to marriage and family life.\n\nAmong the Israelites, men commonly acknowledged their concubines, and such women enjoyed the same rights in the house as legitimate wives.\n\nThe term concubine did not necessarily refer to women after the first wife. A man could have many wives and concubines. Legally, any children born to a concubine was considered to be the child of the wife to whom she was under. Sarah had to get Ishmael out of her house because legally Ishmael would always be the first born son even though Isaac was her natural child.\nThe concubine may not have commanded the exact amount of respect as the wife. In the Levitical rules on sexual relations, the Hebrew word that is commonly translated as \"wife\" is distinct from the Hebrew word that means \"concubine\". However, on at least one other occasion the term is used to refer to a woman who is not a wife specifically, the handmaiden of Jacob's wife. In the Levitical code, sexual intercourse between a man and a wife of a different man was forbidden and punishable by death for both persons involved. Since it was regarded as the highest blessing to have many children, wives often gave their maids to their husbands if they were barren, as in the cases of Sarah and Hagar, and Rachel and Bilhah. The children of the concubine often had equal rights with those of the wife; for example, King Abimelech was the son of Gideon and his concubine. Later biblical figures such as Gideon, and Solomon had concubines in addition to many childbearing wives. For example, the Books of Kings say that Solomon had 700 wives and 300 concubines.\nThe account of the unnamed Levite in Judges 19–20 shows that the taking of concubines was not the exclusive preserve of kings or patriarchs in Israel during the time of the Judges, and that the rape of a concubine was completely unacceptable to the Israelite nation and led to a civil war. In the story, the Levite appears to be an ordinary member of the tribe, whose concubine was a woman from Bethlehem in Judah. This woman was unfaithful, and eventually abandoned him to return to her paternal household. However, after four months, the Levite, referred to as her husband, decided to travel to her father's house to persuade his concubine to return. She is amenable to returning with him, and the father-in-law is very welcoming. The father-in-law convinces the Levite to remain several additional days, until the party leaves behind schedule in the late evening. The group pass up a nearby non-Israelite town to arrive very late in the city of Gibeah, which is in the land of the Benjaminites. The group sit around the town square, waiting for a local to invite them in for the evening, as was the custom for travelers. A local old man invites them to stay in his home, offering them guest right by washing their feet and offering them food. A band of wicked townsmen attack the house and demand the host send out the Levite man so they can have sex with him. The host offers to send out his virgin daughter as well as the Levite's concubine for them to rape, to avoid breaking guest right towards the Levite. Eventually, to ensure his own safety and that of his host, the Levite gives the men his concubine, who is raped and abused through the night, until she is left collapsed against the front door at dawn. In the morning, the Levite finds her when he tries to leave. When she fails to respond to her husband's order to get up, most likely because she is dead, the Levite places her on his donkey and continues home. Once home, he dismembers her body and distributes the 12 parts throughout the nation of Israel. The Israelites gather to learn why they were sent such grisly gifts, and are told of the sadistic rape of his concubine by the Levite. The crime is considered outrageous by the Israelite tribesmen, who then wreak total retribution on the men of Gibeah, as well as the surrounding tribe of Benjamin when they support the Gibeans, killing them without mercy and burning all their towns. The inhabitants of (the town of) Jabesh Gilead are then slaughtered as a punishment for not joining the eleven tribes in their war against the Benjaminites, and their four hundred unmarried daughters given in forced marriage to the six hundred Benjamite survivors. Finally, the two hundred Benjaminite survivors who still have no wives are granted a mass marriage by abduction by the other tribes.\n\nIn Judaism, concubines are referred to by the Hebrew term pilegesh (). The term is a loanword from Ancient Greek , meaning \"a mistress staying in house\".\n\nAccording to the Babylonian Talmud, the difference between a concubine and a legitimate wife was that the latter received a ketubah and her marriage (\"nissu'in\") was preceded by an erusin (\"formal betrothal\"), which was not the case for a concubine. One opinion in the Jerusalem Talmud argues that the concubine should also receive a \"marriage contract\", but without a clause specifying a divorce settlement. According to Rashi, \"wives with kiddushin and ketubbah, concubines with kiddushin but without ketubbah\"; this reading is from the Jerusalem Talmud,\n\nCertain Jewish thinkers, such as Maimonides, believed that concubines were strictly reserved for Royal leadership, kings and queens, and thus that a commoner may not have a concubine. Indeed, such thinkers argued that commoners may not engage in any type of sexual relations outside of a marriage.\n\nMaimonides was not the first Jewish thinker to criticise concubinage. For example, Leviticus Rabbah severely condemns the custom. Other Jewish thinkers, such as Nahmanides, Samuel ben Uri Shraga Phoebus, and Jacob Emden, strongly objected to the idea that concubines should be forbidden.\n\nIn the Hebrew of the contemporary State of Israel, \"pilegesh\" is often used as the equivalent of the English word \"mistress\"—i.e., the female partner in extramarital relations—regardless of legal recognition. Attempts have been initiated to popularise \"pilegesh\" as a form of premarital, non-marital or extramarital relationship (which, according to the perspective of the enacting person(s), is permitted by Jewish law).\n\nConcubinage in Islamic sexual jurisprudence is permitted in Islam, though some muslim scholars vehemently object to this and reject Concubinage all along, which was not considered prostitution, and was very common during the Arab slave trade throughout the Middle Ages and early modern period, when women and girls from the Caucasus, Africa, Central Asia and Europe were captured and served as concubines in the harems of the Arab World. Ibn Battuta tells us several times that he was given or purchased female slaves.\n\nConcubinage is permitted and regulated in Islam. Al-Muminun 6 and Al-Maarij 30 both, in identical wording, draw a distinction between spouses and \"those whom one's right hands possess\" (concubines), saying \" أَزْوَاجِهِمْ أَوْ مَا مَلَكَتْ أَيْمَانُهُمْ\" (literally, \"their spouses or what their right hands possess\"), while clarifying that sexual intercourse with either is permissible. However both these surahs literal wording do not specifically use the term wife but instead the more general & both-gender including term \"spouse\" in the grammatically masculine plural (azwajihim), thus Mohammad Asad in his commentary to both these Surahs rules out concubinage due to the fact that \"\"since the term azwaj (\"spouses\"), too, denotes both the male and the female partners in marriage, there is no reason for attributing to the phrase aw ma malakat aymanuhum the meaning of \"their female slaves\"; and since, on the other hand, it is out of the question that female and male slaves could have been referred to here, it is obvious that this phrase does not relate to slaves at all, but has the same meaning as in 4:24 namely, \"those whom they rightfully possess through wedlock\" with the significant difference that in the present context this expression relates to both husbands and wives, who \"rightfully possess\" one another by virtue of marriage\".\" Following this approach, Mohammad Asads translation of the mentioned verses denotes a different picture, which is as follows: \"with any but their spouses - that is, those whom they rightfully possess [through wedlock]\". Sayyid Abul Ala Maududi explains that \"two categories of women have been excluded from the general command of guarding the private parts: (a) wives, (b) women who are legally in one's possession\". \"Concubine\" (\"surriyya\") refers to the female slave (\"jāriya\"), whether Muslim or non-Muslim, with whom her master engages in sexual intercourse. The word \"\"surriyya\"\" is not mentioned in the Qur'an. However, the expression \"Ma malakat aymanukum\" (that which your right hands own), which occurs fifteen times in the sacred book, refers to slaves and therefore, though not necessarily, to concubines. Concubinage was a pre-Islamic custom that was allowed to be practiced under Islam with Jews and non-Muslim people to marry concubine after teaching her and instructing her well and then giving them freedom. Rationale given for recognition of concubinage in Islam is that \"it satisfied the sexual desire of the female slaves and thereby prevented the spread of immorality in the Muslim community.\" Most schools restrict concubinage to a relationship where the female slave is required to be monogamous to her master (though the master's monogamy to her is not required), but according to Sikainga, \"in reality, however, female slaves in many Muslim societies were prey for [male] members of their owners' household, their [owner's male] neighbors, and their [owner's male] guests.\" \nConcubines were common in pre-Islamic Arabia and when Islam arrived, it had a society with concubines. Islam introduced legal restrictions to the concubinage and encouraged manumission. Islam furthermore endorsed educating, freeing and marrying female slaves. In verse 23:6 in the Quran it is allowed to have sexual intercourse with concubines after marrying them, as Islam forbids sexual intercourse outside of marriage.\nChildren of former concubines were generally declared as legitimate with or without wedlock, and the mother of a free child was considered free upon the death of the male partner.\n\nAccording to Shia Muslims, Muhammad sanctioned Nikah mut‘ah (fixed-term marriage, called muta'a in Iraq and sigheh in Iran) which has instead been used as a legitimizing cover for sex workers, in a culture where prostitution is otherwise forbidden. Some Western writers have argued that mut'ah approximates prostitution. Julie Parshall writes that mut'ah is legalised prostitution which has been sanctioned by the Twelver Shia authorities. She quotes the Oxford encyclopedia of modern Islamic world to differentiate between marriage (nikah) and Mut'ah, and states that while nikah is for procreation, mut'ah is just for sexual gratification. According to Zeyno Baran, this kind of temporary marriage provides Shi'ite men with a religiously sanctioned equivalent to prostitution. According to Elena Andreeva's observation published in 2007, Russian travellers to Iran consider mut'ah to be \"legalized profligacy\" which is indistinguishable from prostitution. Religious supporters of mut'ah argue that temporary marriage is different from prostitution for a couple of reasons, including the necessity of iddah in case the couple have sexual intercourse. It means that if a woman marries a man in this way and has sex, she has to wait for a number of months before marrying again and therefore, a woman cannot marry more than 3 or 4 times in a year.\n\nIn ancient times, two sources for concubines were permitted under an Islamic regime. Primarily, non-Muslim women taken as prisoners of war were made concubines as happened after the Battle of the Trench, or in numerous later Caliphates.\nIt was encouraged to manumit slave women who rejected their initial faith and embraced Islam, or to bring them into formal marriage.\n\nAccording to the rules of Islamic Fiqh, what is \"halal\" (permitted) by Allah in the Quran cannot be altered by any authority or individual. Therefore, although the \"concept\" of concubinage is \"halal\", concubines are mostly no longer available in this modern era nor allowed to be sold or purchased in accordance with the latest human rights standards.\n\nIt is further clarified that all domestic and organizational female employees are not concubines in this era and hence sex is forbidden with them unless Nikah, Nikah mut‘ah or Nikah Misyar is committed through the proper channels.\n\nWhen slavery became institutionalized in the North American colonies, white men, whether or not they were married, sometimes took enslaved women and men as concubines. Marriage between the races was prohibited by law in the colonies and the later United States. Many colonies and states also had laws against miscegenation, or any interracial relations. From 1662 the Colony of Virginia, followed by others, incorporated into law the principle that children took their mother's status, i.e., the principle of \"partus sequitur ventrem\". All children born to enslaved mothers were born into slavery, regardless of their father's status or ancestry. This led to generations of multiracial slaves, some of whom were otherwise considered legally white (one-eighth or less African, equivalent to a great-grandparent) before the American Civil War.\n\nIn some cases, men had long-term \"relationships\" with enslaved women, giving them and their mixed-race children freedom and providing their children with apprenticeships, education and transfer of capital. A purported relationship between Thomas Jefferson and Sally Hemings is an example of this. Such arrangements were more prevalent in the Southern states during the antebellum years.\n\nIn Louisiana and former French territories, a formalized system of concubinage called \"plaçage\" developed. European men took enslaved or free women of color as mistresses after making arrangements to give them a dowry, house or other transfer of property, and sometimes, if they were enslaved, offering freedom and education for their children. A third class of free people of color developed, especially in New Orleans. Many became educated, artisans and property owners. French-speaking and practicing Catholicism, these women combined French and African-American culture and created an elite between those of European descent and the slaves. Today, descendants of the free people of color are generally called Louisiana Creole people.\n\nBULLET::::- Concubinage in Canada\nBULLET::::- Cicisbeo\nBULLET::::- Cohabitation\nBULLET::::- Common-law marriage\nBULLET::::- Courtesan\nBULLET::::- Cullagium\nBULLET::::- Cuckquean\nBULLET::::- Free union\nBULLET::::- Harem\nBULLET::::- Monogamy in Christianity\nBULLET::::- Morganatic marriage\nBULLET::::- Paramour\nBULLET::::- Polygamy\nBULLET::::- Polyamory\nBULLET::::- Polygyny\nBULLET::::- Slavery in the United States\nBULLET::::- Servile Concubinage\n"}
{"id": "7017", "url": "https://en.wikipedia.org/wiki?curid=7017", "title": "Central Plaza (Hong Kong)", "text": "Central Plaza (Hong Kong)\n\nCentral Plaza is a 78-storey, skyscraper completed in August 1992 at 18 Harbour Road, in Wan Chai on Hong Kong Island in Hong Kong. It is the third tallest tower in the city after 2 International Finance Centre in Central and the ICC in West Kowloon. It was the tallest building in Asia from 1992 to 1996, until the Shun Hing Square was built in Shenzhen, a neighbouring city. Central Plaza surpassed the Bank of China Tower as the tallest building in Hong Kong until the completion of 2 IFC.\n\nCentral Plaza was also the tallest reinforced concrete building in the world, until it was surpassed by CITIC Plaza, Guangzhou. The building uses a triangular floor plan. On the top of the tower is a four-bar neon clock that indicates the time by displaying different colours for 15-minute periods, blinking at the change of the quarter.\n\nAn anemometer is installed on the tip of the building's mast, at above sea level. The mast has a height of . It also houses the world's highest church inside a skyscraper, Sky City Church.\n\nThe land upon which Central Plaza sits was reclaimed from Victoria Harbour in the 1970s. The site was auctioned off by the Hong Kong Government at City Hall Theatre on 25 January 1989. It was sold for a record HK$3.35 billion to a joint venture called \"Cheer City Properties\", owned 50 per cent by Sun Hung Kai Properties and 50 per cent by fellow real estate conglomerate Sino Land and their shareholders the Ng Teng Fong family. A third developer, Ryoden Development, joined the consortium afterward.\n\nThe first major tenant to sign a lease was the Provisional Airport Authority, who on 2 August 1991 agreed to lease the 24th to 26th floors. A topping-out ceremony, presided over by Sir David Ford, was held on 9 April 1992.\n\nCentral Plaza is made up of two principal components: a free standing office tower and a podium block attached to it. The tower is made up of three sections: a tower base forming the main entrance and public circulation spaces; a tower body containing 57 office floors, a sky lobby and five mechanical plant floors; and the tower top consist of six mechanical plant floors and a tower mast.\n\nThe ground level public area along with the public sitting out area form an landscaped garden with fountain, trees and artificial stone paving. No commercial element is included in the podium. The first level is a public thoroughfare for three pedestrian bridges linking the Mass Transit Railway, the Convention and Exhibition Centre and the China Resource Building. By turning these space to public use, the building got 20% plot ratio more as bonus. The shape of the tower is not truly triangular but with its three corners cut off to provide better internal office spaces.\n\nCentral Plaza was designed by the Hong Kong architectural firm Ng Chun Man and Associates and engineered by Arup. The main contractor was a joint venture, comprising the contracting firms Sanfield and Tat Lee, called Manloze Ltd.\n\nThe building was designed to be triangular in shape because it would allow 20% more of the office area to enjoy the harbour view as compared with a square or rectangular shaped buildings. From an architectural point of view, this arrangement provides better floor area utilisation, offering an internal column-free office area with a clear depth of and an overall usable floor area efficiency of 81%.\nNonetheless, the triangular building plan causes the air handling unit (AHU) room in the internal core to also assume a triangular configuration. With only limited space, this makes the adoption of a standard AHU not feasible. Furthermore, all air-conditioning ducting, electrical trunking and piping gathered inside the core area has to be squeezed into a very narrow and congested corridor ceiling void.\n\nAs the building is situated opposite to the Hong Kong Convention and Exhibition Centre, the only way to get more sea view for the building and not be obstructed by the neighbouring high-rise buildings is to build it tall enough. However, a tall building brings a lot of difficulties to structural and building services design, for example, excessive system static pressure for water systems, high line voltage drop and long distance of vertical transportation. All these problems can increase the capital cost of the building systems and impair the safety operation of the building.\n\nAs a general practice, for achieving a clear height of , a floor-to-floor height of would be required. However, because of high windload in Hong Kong for such a super high-rise building, every increase in building height by a metre would increase the structural cost by more than HK$1 million (HK$304,800 per ft). Therefore, a comprehensive study was conducted and finally a floor height of was adopted. With this issue alone, an estimated construction cost saving for a total of 58 office floors, would be around HK$30 million. Yet at the same time, a maximum ceiling height of in office area could still be achieved with careful coordination and dedicated integration.\n\nBULLET::::- The site is a newly reclaimed area with a maximum water table rises to about below ground level. In the original brief, a 6-storey basement is required, therefore a diaphragm wall design came out.\nBULLET::::- The keyword to this project is time. With a briefing in a limited detail, the structural engineer needed to start work The diaphragm wall design allowed for the basement to be constructed by the top-down method. It allows the superstructure to be constructed at the same time as the basement, thereby removing time consuming basement construction period from the critical path.\nBULLET::::- Wind loading is another major design criterion in Hong Kong as it is situated in an area influenced by typhoons. Not only must the structure be able to resist the loads generally and the cladding system and its fixings resist higher local loads, but the building must also perform dynamically in an acceptable manner such that predicted movements lie within acceptable standards of occupant comfort criteria. To ensure that all aspects of the building's performance in strong winds will be acceptable, a detailed wind tunnel study was carried out by Professor Alan Davenport at the Boundary Layer Wind Tunnel Laboratory at the University of Western Ontario.\n\nSteel structure is more commonly adopted in high-rise building. In the original scheme, an externally cross-braced framed tube was applied with primary/secondary beams carrying metal decking with reinforced concrete slab. The core was also of steelwork, designed to carry vertical load only. Later after a financial review by the developer, they decided to reduce the height of the superstructure by increasing the size of the floor plate so as to reduce the complex architectural requirements of the tower base which means a highstrength concrete solution became possible.\n\nIn the final scheme, columns at centres and floor edge beams were used to replace the large steel corner columns. As climbing form and table form construction method and efficient construction management are used in this project which make this reinforced concrete structure take no longer construction time than the steel structure. And the most attractive point is that the reinforced concrete scheme can save HK$230 million compared to that of steel structure. Hence the reinforced concrete structure was adopted and Central Plaza is now one of the tallest reinforced concrete buildings in the world.\n\nIn the reinforced concrete structure scheme, the core has a similar arrangement to the steel scheme and the wind shear is taken out from the core at the lowest basement level and transferred to the perimeter diaphragm walls. In order to reduce large shear reversals in the core walls in the basement, and at the top of the tower base level, the ground floor, basement levels 1 and 2 and the 5th and 6th floors, the floor slabs and beams are separated horizontally from the core walls.\n\nAnother advantage of using reinforced concrete structure is that it is more flexible to cope with changes in structural layout, sizes and height according to the site conditions by using table form system.\n\nThis skyscraper was visited in the seventh leg of the reality TV show \"The Amazing Race 2\", which described Central Plaza as \"the tallest building in Hong Kong\". Although contestants were told to reach the top floor, the actual task was performed on the 46th floor.\n\nBULLET::::- List of tallest buildings in Hong Kong\nBULLET::::- List of buildings and structures in Hong Kong\nBULLET::::- List of tallest freestanding structures in the world\nBULLET::::- Architectural study of the building\nBULLET::::- Hong Kong's skyscrapers in comparison\nBULLET::::- Central Plaza Elevator Layout\n"}
{"id": "7018", "url": "https://en.wikipedia.org/wiki?curid=7018", "title": "Caravaggio", "text": "Caravaggio\n\nMichelangelo Merisi (Michele Angelo Merigi or Amerighi) da Caravaggio (, , ; 29 September 1571 – 18 July 1610) was an Italian painter active in Rome, Naples, Malta, and Sicily from the early 1590s to 1610. His paintings combine a realistic observation of the human state, both physical and emotional, with a dramatic use of lighting, which had a formative influence on Baroque painting.\n\nCaravaggio employed close physical observation with a dramatic use of chiaroscuro that came to be known as tenebrism. He made the technique a dominant stylistic element, darkening shadows and transfixing subjects in bright shafts of light. Caravaggio vividly expressed crucial moments and scenes, often featuring violent struggles, torture and death. He worked rapidly, with live models, preferring to forgo drawings and work directly onto the canvas. His influence on the new Baroque style that emerged from Mannerism was profound. It can be seen directly or indirectly in the work of Peter Paul Rubens, Jusepe de Ribera, Gian Lorenzo Bernini, and Rembrandt, and artists in the following generation heavily under his influence were called the \"Caravaggisti\" or \"Caravagesques\", as well as tenebrists or \"tenebrosi\" (\"shadowists\").\n\nCaravaggio trained as a painter in Milan before moving in his twenties to Rome. He developed a considerable name as an artist, and as a violent, touchy and provocative man. A brawl led to a death sentence for murder and forced him to flee to Naples. There he again established himself as one of the most prominent Italian painters of his generation. He traveled in 1607 to Malta and on to Sicily, and pursued a papal pardon for his sentence. In 1609 he returned to Naples, where he was involved in a violent clash; his face was disfigured and rumours of his death circulated. Questions about his mental state arose from his erratic and bizarre behavior. He died in 1610 under uncertain circumstances while on his way from Naples to Rome. Reports stated that he died of a fever, but suggestions have been made that he was murdered or that he died of lead poisoning.\n\nCaravaggio's innovations inspired Baroque painting, but the Baroque incorporated the drama of his chiaroscuro without the psychological realism. The style evolved and fashions changed, and Caravaggio fell out of favor. In the 20th century interest in his work revived, and his importance to the development of Western art was reevaluated. The 20th-century art historian André Berne-Joffroy stated, \"What begins in the work of Caravaggio is, quite simply, modern painting.\"\n\nCaravaggio (Michelangelo Merisi or Amerighi) was born in Milan, where his father, Fermo (Fermo Merixio), was a household administrator and architect-decorator to the Marchese of Caravaggio, a town not far from the city of Bergamo. In 1576 the family moved to Caravaggio (Caravaggius) to escape a plague that ravaged Milan, and Caravaggio's father and grandfather both died there on the same day in 1577. It is assumed that the artist grew up in Caravaggio, but his family kept up connections with the Sforzas and with the powerful Colonna family, who were allied by marriage with the Sforzas and destined to play a major role later in Caravaggio's life.\n\nCaravaggio's mother died in 1584, the same year he began his four-year apprenticeship to the Milanese painter Simone Peterzano, described in the contract of apprenticeship as a pupil of Titian. Caravaggio appears to have stayed in the Milan-Caravaggio area after his apprenticeship ended, but it is possible that he visited Venice and saw the works of Giorgione, whom Federico Zuccari later accused him of imitating, and Titian. He would also have become familiar with the art treasures of Milan, including Leonardo da Vinci's \"Last Supper\", and with the regional Lombard art, a style that valued simplicity and attention to naturalistic detail and was closer to the naturalism of Germany than to the stylised formality and grandeur of Roman Mannerism.\n\nFollowing his initial training under Simone Peterzano, in 1592 Caravaggio left Milan for Rome, in flight after \"certain quarrels\" and the wounding of a police officer. The young artist arrived in Rome \"naked and extremely needy ... without fixed address and without provision ... short of money.\" During this period he stayed with the miserly Pandolfo Pucci, known as \"monnsignor Insalata\". A few months later he was performing hack-work for the highly successful Giuseppe Cesari, Pope Clement VIII's favourite artist, \"painting flowers and fruit\" in his factory-like workshop.\n\nIn Rome there was demand for paintings to fill the many huge new churches and palazzi being built at the time. It was also a period when the Church was searching for a stylistic alternative to Mannerism in religious art that was tasked to counter the threat of Protestantism. Caravaggio's innovation was a radical naturalism that combined close physical observation with a dramatic, even theatrical, use of chiaroscuro that came to be known as tenebrism (the shift from light to dark with little intermediate value).\n\nKnown works from this period include a small \"Boy Peeling a Fruit\" (his earliest known painting), a \"Boy with a Basket of Fruit\", and the \"Young Sick Bacchus\", supposedly a self-portrait done during convalescence from a serious illness that ended his employment with Cesari. All three demonstrate the physical particularity for which Caravaggio was to become renowned: the fruit-basket-boy's produce has been analysed by a professor of horticulture, who was able to identify individual cultivars right down to \"... a large fig leaf with a prominent fungal scorch lesion resembling anthracnose (\"Glomerella cingulata\").\"\n\nCaravaggio left Cesari, determined to make his own way after a heated argument. At this point he forged some extremely important friendships, with the painter Prospero Orsi, the architect Onorio Longhi, and the sixteen-year-old Sicilian artist Mario Minniti. Orsi, established in the profession, introduced him to influential collectors; Longhi, more balefully, introduced him to the world of Roman street-brawls. Minniti served Caravaggio as a model and, years later, would be instrumental in helping him to obtain important commissions in Sicily. Ostensibly, the first archival reference to Caravaggio in a contemporary document from Rome is the listing of his name, with that of Prospero Orsi as his partner, as an 'assistante' in a procession in October 1594 in honour of St. Luke. The earliest informative account of his life in the city is a court transcript dated 11 July 1597, when Caravaggio and Prospero Orsi were witnesses to a crime near San Luigi de' Francesi.\n\nAn early published notice on Caravaggio, dating from 1604 and describing his lifestyle three years previously, recounts that \"after a fortnight's work he will swagger about for a month or two with a sword at his side and a servant following him, from one ball-court to the next, ever ready to engage in a fight or an argument, so that it is most awkward to get along with him.\" In 1606 he killed a young man in a brawl, possibly unintentionally, and fled from Rome with a death sentence hanging over him.\n\n\"The Fortune Teller\", his first composition with more than one figure, shows a boy, likely Minniti, having his palm read by a gypsy girl, who is stealthily removing his ring as she strokes his hand. The theme was quite new for Rome, and proved immensely influential over the next century and beyond. This, however, was in the future: at the time, Caravaggio sold it for practically nothing. \"The Cardsharps\"—showing another naïve youth of privilege falling the victim of card cheats—is even more psychologically complex, and perhaps Caravaggio's first true masterpiece. Like \"The Fortune Teller\", it was immensely popular, and over 50 copies survive. More importantly, it attracted the patronage of Cardinal Francesco Maria del Monte, one of the leading connoisseurs in Rome. For Del Monte and his wealthy art-loving circle, Caravaggio executed a number of intimate chamber-pieces—\"The Musicians\", \"The Lute Player\", a tipsy \"Bacchus\", an allegorical but realistic \"Boy Bitten by a Lizard\"—featuring Minniti and other adolescent models.\n\nCaravaggio's first paintings on religious themes returned to realism, and the emergence of remarkable spirituality. The first of these was the \"Penitent Magdalene\", showing Mary Magdalene at the moment when she has turned from her life as a courtesan and sits weeping on the floor, her jewels scattered around her. \"It seemed not a religious painting at all ... a girl sitting on a low wooden stool drying her hair ... Where was the repentance ... suffering ... promise of salvation?\" It was understated, in the Lombard manner, not histrionic in the Roman manner of the time. It was followed by others in the same style: \"Saint Catherine\"; \"Martha and Mary Magdalene\"; \"Judith Beheading Holofernes\"; a \"Sacrifice of Isaac\"; a \"Saint Francis of Assisi in Ecstasy\"; and a \"Rest on the Flight into Egypt\". These works, while viewed by a comparatively limited circle, increased Caravaggio's fame with both connoisseurs and his fellow artists. But a true reputation would depend on public commissions, and for these it was necessary to look to the Church.\n\nAlready evident was the intense realism or naturalism for which Caravaggio is now famous. He preferred to paint his subjects as the eye sees them, with all their natural flaws and defects instead of as idealised creations. This allowed a full display of his virtuosic talents. This shift from accepted standard practice and the classical idealism of Michelangelo was very controversial at the time. Caravaggio also dispensed with the lengthy preparations traditional in central Italy at the time. Instead, he preferred the Venetian practice of working in oils directly from the subject—half-length figures and still life. \"Supper at Emmaus\", from c. 1600–1601, is a characteristic work of this period demonstrating his virtuoso talent.\n\nIn 1599, presumably through the influence of Del Monte, Caravaggio was contracted to decorate the Contarelli Chapel in the church of San Luigi dei Francesi. The two works making up the commission, \"The Martyrdom of Saint Matthew\" and \"The Calling of Saint Matthew\", delivered in 1600, were an immediate sensation. Thereafter he never lacked commissions or patrons.\n\nCaravaggio's tenebrism (a heightened chiaroscuro) brought high drama to his subjects, while his acutely observed realism brought a new level of emotional intensity. Opinion among his artist peers was polarised. Some denounced him for various perceived failings, notably his insistence on painting from life, without drawings, but for the most part he was hailed as a great artistic visionary: \"The painters then in Rome were greatly taken by this novelty, and the young ones particularly gathered around him, praised him as the unique imitator of nature, and looked on his work as miracles.\"\n\nCaravaggio went on to secure a string of prestigious commissions for religious works featuring violent struggles, grotesque decapitations, torture and death, most notable and most technically masterful among them \"The Taking of Christ\" of circa 1602 for the Mattei Family, recently rediscovered in Ireland after two centuries. For the most part each new painting increased his fame, but a few were rejected by the various bodies for whom they were intended, at least in their original forms, and had to be re-painted or find new buyers. The essence of the problem was that while Caravaggio's dramatic intensity was appreciated, his realism was seen by some as unacceptably vulgar.\n\nHis first version of \"Saint Matthew and the Angel\", featuring the saint as a bald peasant with dirty legs attended by a lightly clad over-familiar boy-angel, was rejected and a second version had to be painted as \"The Inspiration of Saint Matthew\". Similarly, \"The Conversion of Saint Paul\" was rejected, and while another version of the same subject, the \"Conversion on the Way to Damascus\", was accepted, it featured the saint's horse's haunches far more prominently than the saint himself, prompting this exchange between the artist and an exasperated official of Santa Maria del Popolo: \"Why have you put a horse in the middle, and Saint Paul on the ground?\" \"Because!\" \"Is the horse God?\" \"No, but he stands in God's light!\"\n\nOther works included \"Entombment\", the \"Madonna di Loreto\" (\"Madonna of the Pilgrims\"), the \"Grooms' Madonna\", and the \"Death of the Virgin\". The history of these last two paintings illustrates the reception given to some of Caravaggio's art, and the times in which he lived. The \"Grooms' Madonna\", also known as \"Madonna dei palafrenieri\", painted for a small altar in Saint Peter's Basilica in Rome, remained there for just two days, and was then taken off. A cardinal's secretary wrote: \"In this painting there are but vulgarity, sacrilege, impiousness and disgust...One would say it is a work made by a painter that can paint well, but of a dark spirit, and who has been for a lot of time far from God, from His adoration, and from any good thought...\"\n\nThe \"Death of the Virgin\", commissioned in 1601 by a wealthy jurist for his private chapel in the new Carmelite church of Santa Maria della Scala, was rejected by the Carmelites in 1606. Caravaggio's contemporary Giulio Mancini records that it was rejected because Caravaggio had used a well-known prostitute as his model for the Virgin. Giovanni Baglione, another contemporary, tells us it was due to Mary's bare legs—a matter of decorum in either case. Caravaggio scholar John Gash suggests that the problem for the Carmelites may have been theological rather than aesthetic, in that Caravaggio's version fails to assert the doctrine of the Assumption of Mary, the idea that the Mother of God did not die in any ordinary sense but was assumed into Heaven. The replacement altarpiece commissioned (from one of Caravaggio's most able followers, Carlo Saraceni), showed the Virgin not dead, as Caravaggio had painted her, but seated and dying; and even this was rejected, and replaced with a work showing the Virgin not dying, but ascending into Heaven with choirs of angels. In any case, the rejection did not mean that Caravaggio or his paintings were out of favour. The \"Death of the Virgin\" was no sooner taken out of the church than it was purchased by the Duke of Mantua, on the advice of Rubens, and later acquired by Charles I of England before entering the French royal collection in 1671.\n\nOne secular piece from these years is \"Amor Vincit Omnia\", in English also called \"Amor Victorious\", painted in 1602 for Vincenzo Giustiniani, a member of Del Monte's circle. The model was named in a memoir of the early 17th century as \"Cecco\", the diminutive for Francesco. He is possibly Francesco Boneri, identified with an artist active in the period 1610–1625 and known as Cecco del Caravaggio ('Caravaggio's Cecco'), carrying a bow and arrows and trampling symbols of the warlike and peaceful arts and sciences underfoot. He is unclothed, and it is difficult to accept this grinning urchin as the Roman god Cupid—as difficult as it was to accept Caravaggio's other semi-clad adolescents as the various angels he painted in his canvases, wearing much the same stage-prop wings. The point, however, is the intense yet ambiguous reality of the work: it is simultaneously Cupid and Cecco, as Caravaggio's Virgins were simultaneously the Mother of Christ and the Roman courtesans who modeled for them.\n\nCaravaggio led a tumultuous life. He was notorious for brawling, even in a time and place when such behavior was commonplace, and the transcripts of his police records and trial proceedings fill several pages. On 29 May 1606, he killed, possibly unintentionally, a young man named Ranuccio Tomassoni from Terni (Umbria). The circumstances of the brawl and the death of Ranuccio Tomassoni remain mysterious. Several contemporary \"avvisi\" referred to a quarrel over a gambling debt and a tennis game, and this explanation has become established in the popular imagination. But recent scholarship has made it clear that more was involved. Good modern accounts are to be found in Peter Robb's \"M\" and Helen Langdon's \"Caravaggio: A Life\". A theory relating the death to Renaissance notions of honour and symbolic wounding has been advanced by art historian Andrew Graham-Dixon. Whatever the details, it was a serious matter. Previously, his high-placed patrons had protected him from the consequences of his escapades, but this time they could do nothing. Caravaggio, outlawed, fled to Naples.\n\nThe twenty-sixth verse of the first chapter\nFollowing the death of Tomassoni, Caravaggio fled first to the estates of the Colonna family south of Rome, then on to Naples, where Costanza Colonna Sforza, widow of Francesco Sforza, in whose husband's household Caravaggio's father had held a position, maintained a palace. In Naples, outside the jurisdiction of the Roman authorities and protected by the Colonna family, the most famous painter in Rome became the most famous in Naples.\n\nHis connections with the Colonnas led to a stream of important church commissions, including the \"Madonna of the Rosary\", and \"The Seven Works of Mercy\". \"The Seven Works of Mercy\" depicts the seven corporal works of mercy as a set of compassionate acts concerning the material needs of others. The painting was made for, and is still housed in, the church of Pio Monte della Misericordia in Naples. Caravaggio combined all seven works of mercy in one composition, which became the church's altarpiece. Alessandro Giardino has also established the connection between the iconography of \"The Seven Works of Mercy\" and the cultural, scientific and philosophical circles of the painting's commissioners.\n\nDespite his success in Naples, after only a few months in the city Caravaggio left for Malta, the headquarters of the Knights of Malta. Fabrizio Sforza Colonna, Costanza's son, was a Knight of Malta and general of the Order's galleys. He appears to have facilitated Caravaggio's arrival in the island in 1607 (and his escape the next year). Caravaggio presumably hoped that the patronage of Alof de Wignacourt, Grand Master of the Knights of Saint John, could help him secure a pardon for Tomassoni's death. De Wignacourt was so impressed at having the famous artist as official painter to the Order that he inducted him as a Knight, and the early biographer Bellori records that the artist was well pleased with his success.\n\nMajor works from his Malta period include the \"Beheading of Saint John the Baptist\", his largest ever work, and the only painting to which he put his signature, Saint Jerome Writing (both housed in Saint John's Co-Cathedral, Valletta, Malta) and a \"Portrait of Alof de Wignacourt and his Page\", as well as portraits of other leading Knights. According to Andrea Pomella, \"The Beheading of Saint John the Baptist\" is widely considered \"one of the most important works in Western painting.\" Completed in 1608, the painting had been commissioned by the Knights of Malta as an altarpiece and was the largest altarpiece Caravaggio painted. It still hangs in St. John's Co-Cathedral, for which it was commissioned and where Caravaggio himself was inducted and briefly served as a knight.\n\nYet, by late August 1608, he was arrested and imprisoned, likely the result of yet another brawl, this time with an aristocratic knight, during which the door of a house was battered down and the knight seriously wounded. Caravaggio was imprisoned by the Knights at Valletta, but he managed to escape. By December, he had been expelled from the Order \"as a foul and rotten member\", a formal phrase used in all such cases.\n\nCaravaggio made his way to Sicily where he met his old friend Mario Minniti, who was now married and living in Syracuse. Together they set off on what amounted to a triumphal tour from Syracuse to Messina and, maybe, on to the island capital, Palermo. In Syracuse and Messina Caravaggio continued to win prestigious and well-paid commissions. Among other works from this period are \"Burial of St. Lucy\", \"The Raising of Lazarus\", and \"Adoration of the Shepherds\". His style continued to evolve, showing now friezes of figures isolated against vast empty backgrounds. \"His great Sicilian altarpieces isolate their shadowy, pitifully poor figures in vast areas of darkness; they suggest the desperate fears and frailty of man, and at the same time convey, with a new yet desolate tenderness, the beauty of humility and of the meek, who shall inherit the earth.\" Contemporary reports depict a man whose behaviour was becoming increasingly bizarre, which included sleeping fully armed and in his clothes, ripping up a painting at a slight word of criticism, and mocking local painters.\n\nCaravaggio displayed bizarre behaviour from very early in his career. Mancini describes him as \"extremely crazy\", a letter of Del Monte notes his strangeness, and Minniti's 1724 biographer says that Mario left Caravaggio because of his behaviour. The strangeness seems to have increased after Malta. Susinno's early-18th-century \"Le vite de' pittori Messinesi\" (\"Lives of the Painters of Messina\") provides several colourful anecdotes of Caravaggio's erratic behaviour in Sicily, and these are reproduced in modern full-length biographies such as Langdon and Robb. Bellori writes of Caravaggio's \"fear\" driving him from city to city across the island and finally, \"feeling that it was no longer safe to remain\", back to Naples. Baglione says Caravaggio was being \"chased by his enemy\", but like Bellori does not say who this enemy was.\n\nAfter only nine months in Sicily, Caravaggio returned to Naples in the late summer of 1609. According to his earliest biographer he was being pursued by enemies while in Sicily and felt it safest to place himself under the protection of the Colonnas until he could secure his pardon from the pope (now Paul V) and return to Rome. In Naples he painted \"The Denial of Saint Peter\", a final \"John the Baptist (Borghese)\", and his last picture, \"The Martyrdom of Saint Ursula\". His style continued to evolve—Saint Ursula is caught in a moment of highest action and drama, as the arrow fired by the king of the Huns strikes her in the breast, unlike earlier paintings that had all the immobility of the posed models. The brushwork was also much freer and more impressionistic.\nIn October 1609 he was involved in a violent clash, an attempt on his life, perhaps ambushed by men in the pay of the knight he had wounded in Malta or some other faction of the Order. His face was seriously disfigured and rumours circulated in Rome that he was dead. He painted a \"Salome with the Head of John the Baptist (Madrid)\", showing his own head on a platter, and sent it to de Wignacourt as a plea for forgiveness. Perhaps at this time, he painted also a \"David with the Head of Goliath\", showing the young David with a strangely sorrowful expression gazing on the severed head of the giant, which is again Caravaggio. This painting he may have sent to his patron, the unscrupulous art-loving Cardinal Scipione Borghese, nephew of the pope, who had the power to grant or withhold pardons. Caravaggio hoped Borghese could mediate a pardon, in exchange for works by the artist.\n\nNews from Rome encouraged Caravaggio, and in the summer of 1610 he took a boat northwards to receive the pardon, which seemed imminent thanks to his powerful Roman friends. With him were three last paintings, the gifts for Cardinal Scipione. What happened next is the subject of much confusion and conjecture, shrouded in much mystery.\n\nThe bare facts seem to be that on 28 July an anonymous \"avviso\" (private newsletter) from Rome to the ducal court of Urbino reported that Caravaggio was dead. Three days later another \"avviso\" said that he had died of fever on his way from Naples to Rome. A poet friend of the artist later gave 18 July as the date of death, and a recent researcher claims to have discovered a death notice showing that the artist died on that day of a fever in Porto Ercole, near Grosseto in Tuscany.\n\nCaravaggio had a fever at the time of his death, and what killed him has been a matter of historical debate and study. Traditionally historians have long thought he died of syphilis. Some have said he had malaria, or possibly brucellosis from unpasteurised dairy. Some scholars have argued that Caravaggio was actually attacked and killed by the same \"enemies\" that had been pursuing him since he fled Malta, possibly Wignacourt and/or factions of the Knights.\n\nHuman remains found in a church in Porto Ercole in 2010 are believed to almost certainly belong to Caravaggio. The findings come after a year-long investigation using DNA, carbon dating and other analyses. Initial tests suggested Caravaggio might have died of lead poisoning—paints used at the time contained high amounts of lead salts, and Caravaggio is known to have indulged in violent behavior, as caused by lead poisoning. Later tests suggested he died as the result of a wound sustained in a brawl in Naples, specifically from sepsis. Recently released Vatican documents (2002) also indicate that fatal wounds may have been sustained as a result of a vendetta, perpetrated after Caravaggio had murdered a love rival in a botched attempt at castration.\n\nCaravaggio never married and had no known children, and Howard Hibbard notes the absence of erotic female figures from the artist's oeuvre: \"In his entire career he did not paint a single female nude.\" On the other hand, the cabinet-pieces from the Del Monte period are replete with \"full-lipped, languorous boys ... who seem to solicit the onlooker with their offers of fruit, wine, flowers—and themselves\" suggesting an erotic interest in the male form. At the same time, however, a connection with a certain Lena is mentioned in a 1605 court deposition by Pasqualone, where she is described as \"Michelangelo's girl\". According to G.B. Passeri, this 'Lena' was Caravaggio's model for the \"Madonna di Loreto\"; and according to Catherine Puglisi, 'Lena' may have been the same person as the courtesan Maddalena di Paolo Antognetti, who named Caravaggio as an \"intimate friend\" by her own testimony in 1604. Caravaggio also probably enjoyed close relationships with other \"whores and courtesans\" such as Fillide Melandroni, of whom he painted a portrait.\n\nNevertheless, since the 1970s both art scholars and historians have debated the inferences of homoeroticism in Caravaggio's works as a way to better understand the man. The model of \"Amor vincit omnia\", for example, is known to have been Cecco di Caravaggio. Cecco stayed with Caravaggio even after he was obliged to leave Rome in 1606, and the two may have been lovers.\n\nCaravaggio's sexuality also received early speculation due to claims about the artist by Honoré Gabriel Riqueti, comte de Mirabeau. Writing in 1783, Mirabeau contrasted the personal life of Caravaggio directly with the writings of St Paul in the Book of Romans, arguing that \"Romans\" excessively practice sodomy or homosexuality. The Holy Mother Catholic Church teachings on morality (and so on; short book title) contains the Latin phrase \"\"Et fœminæ eorum immutaverunt naturalem usum in eum usum qui est contra naturam.\"\" The phrase, according to Mirabeau, entered Caravaggio's thoughts, and he claimed that such an \"abomination\" could be witnessed through a particular painting housed at the Museum of the Grand Duke of Tuscany—featuring a rosary of a blasphemous nature, in which a circle of thirty men (\"turpiter ligati\") are intertwined in embrace and presented in unbridled composition. Mirabeau notes the affectionate nature of Caravaggio's depiction reflects the voluptuous glow of the artist's sexuality. By the late nineteenth century, Sir Richard Francis Burton identified the painting as Caravaggio's painting of St. Rosario. Burton also identifies both St. Rosario and this painting with the practices of Tiberius mentioned by Seneca the Younger. The survival status and location of Caravaggio's painting is unknown. No such painting appears in his or his school's catalogues.\n\nAside from the paintings, evidence also comes from the libel trial brought against Caravaggio by Giovanni Baglione in 1603. Baglione accused Caravaggio and his friends of writing and distributing scurrilous doggerel attacking him; the pamphlets, according to Baglione's friend and witness Mao Salini, had been distributed by a certain Giovanni Battista, a \"bardassa,\" or boy prostitute, shared by Caravaggio and his friend Onorio Longhi. Caravaggio denied knowing any young boy of that name, and the allegation was not followed up.\n\nBaglione's painting of \"Divine Love\" has also been seen as a visual accusation of sodomy against Caravaggio. Such accusations were damaging and dangerous as sodomy was a capital crime at the time. Even though the authorities were unlikely to investigate such a well-connected person as Caravaggio, \"Once an artist had been smeared as a pederast, his work was smeared too.\" Francesco Susino in his later biography additionally relates the story of how the artist was chased by a school-master in Sicily for spending too long gazing at the boys in his care. Susino presents it as a misunderstanding, but Caravaggio may indeed have been seeking sexual solace; and the incident could explain one of his most homoerotic paintings: his last depiction of St John the Baptist.\n\nThe art historian, Andrew Graham-Dixon has summarised the debate:\nA lot has been made of Caravaggio's presumed homosexuality, which has in more than one previous account of his life been presented as the single key that explains everything, both the power of his art and the misfortunes of his life. There is no absolute proof of it, only strong circumstantial evidence and much rumour. The balance of probability suggests that Caravaggio did indeed have sexual relations with men. But he certainly had female lovers. Throughout the years that he spent in Rome he kept close company with a number of prostitutes. The truth is that Caravaggio was as uneasy in his relationships as he was in most other aspects of life. He likely slept with men. He did sleep with women. He settled with no one... [but] the idea that he was an early martyr to the drives of an unconventional sexuality is an anachronistic fiction.\n\nCaravaggio \"put the oscuro (shadows) into chiaroscuro.\" Chiaroscuro was practiced long before he came on the scene, but it was Caravaggio who made the technique a dominant stylistic element, darkening the shadows and transfixing the subject in a blinding shaft of light. With this came the acute observation of physical and psychological reality that formed the ground both for his immense popularity and for his frequent problems with his religious commissions.\n\nHe worked at great speed, from live models, scoring basic guides directly onto the canvas with the end of the brush handle; very few of Caravaggio's drawings appear to have survived, and it is likely that he preferred to work directly on the canvas. The approach was anathema to the skilled artists of his day, who decried his refusal to work from drawings and to idealise his figures. Yet the models were basic to his realism. Some have been identified, including Mario Minniti and Francesco Boneri, both fellow artists, Minniti appearing as various figures in the early secular works, the young Boneri as a succession of angels, Baptists and Davids in the later canvasses. His female models include Fillide Melandroni, Anna Bianchini, and Maddalena Antognetti (the \"Lena\" mentioned in court documents of the \"artichoke\" case as Caravaggio's concubine), all well-known prostitutes, who appear as female religious figures including the Virgin and various saints. Caravaggio himself appears in several paintings, his final self-portrait being as the witness on the far right to the \"Martyrdom of Saint Ursula\".\n\nCaravaggio had a noteworthy ability to express in one scene of unsurpassed vividness the passing of a crucial moment. \"The Supper at Emmaus\" depicts the recognition of Christ by his disciples: a moment before he is a fellow traveler, mourning the passing of the Messiah, as he never ceases to be to the inn-keeper's eyes; the second after, he is the Saviour. In \"The Calling of St Matthew\", the hand of the Saint points to himself as if he were saying \"who, me?\", while his eyes, fixed upon the figure of Christ, have already said, \"Yes, I will follow you\". With \"The Resurrection of Lazarus\", he goes a step further, giving us a glimpse of the actual physical process of resurrection. The body of Lazarus is still in the throes of rigor mortis, but his hand, facing and recognising that of Christ, is alive. Other major Baroque artists would travel the same path, for example Bernini, fascinated with themes from Ovid's \"Metamorphoses\".\n\nThe installation of the St. Matthew paintings in the Contarelli Chapel had an immediate impact among the younger artists in Rome, and Caravaggism became the cutting edge for every ambitious young painter. The first Caravaggisti included Orazio Gentileschi and Giovanni Baglione. Baglione's Caravaggio phase was short-lived; Caravaggio later accused him of plagiarism and the two were involved in a long feud. Baglione went on to write the first biography of Caravaggio. In the next generation of Caravaggisti there were Carlo Saraceni, Bartolomeo Manfredi and Orazio Borgianni. Gentileschi, despite being considerably older, was the only one of these artists to live much beyond 1620, and ended up as court painter to Charles I of England. His daughter Artemisia Gentileschi was also stylistically close to Caravaggio, and one of the most gifted of the movement. Yet in Rome and in Italy it was not Caravaggio, but the influence of his rival Annibale Carracci, blending elements from the High Renaissance and Lombard realism, which ultimately triumphed.\n\nCaravaggio's brief stay in Naples produced a notable school of Neapolitan Caravaggisti, including Battistello Caracciolo and Carlo Sellitto. The Caravaggisti movement there ended with a terrible outbreak of plague in 1656, but the Spanish connection—Naples was a possession of Spain—was instrumental in forming the important Spanish branch of his influence.\n\nA group of Catholic artists from Utrecht, the \"Utrecht Caravaggisti\", travelled to Rome as students in the first years of the 17th century and were profoundly influenced by the work of Caravaggio, as Bellori describes. On their return to the north this trend had a short-lived but influential flowering in the 1620s among painters like Hendrick ter Brugghen, Gerrit van Honthorst, Andries Both and Dirck van Baburen. In the following generation the effects of Caravaggio, although attenuated, are to be seen in the work of Rubens (who purchased one of his paintings for the Gonzaga of Mantua and painted a copy of the \"Entombment of Christ\"), Vermeer, Rembrandt and Velázquez, the last of whom presumably saw his work during his various sojourns in Italy.\n\nCaravaggio's innovations inspired the Baroque, but the Baroque took the drama of his chiaroscuro without the psychological realism. While he directly influenced the style of the artists mentioned above, and, at a distance, the Frenchmen Georges de La Tour and Simon Vouet, and the Spaniard Giuseppe Ribera, within a few decades his works were being ascribed to less scandalous artists, or simply overlooked. The Baroque, to which he contributed so much, had evolved, and fashions had changed, but perhaps more pertinently Caravaggio never established a workshop as the Carracci did, and thus had no school to spread his techniques. Nor did he ever set out his underlying philosophical approach to art, the psychological realism that may only be deduced from his surviving work.\n\nThus his reputation was doubly vulnerable to the critical demolition-jobs done by two of his earliest biographers, Giovanni Baglione, a rival painter with a personal vendetta, and the influential 17th-century critic Gian Pietro Bellori, who had not known him but was under the influence of the earlier Giovanni Battista Agucchi and Bellori's friend Poussin, in preferring the \"classical-idealistic\" tradition of the Bolognese school led by the Carracci. Baglione, his first biographer, played a considerable part in creating the legend of Caravaggio's unstable and violent character, as well as his inability to draw.\n\nIn the 1920s, art critic Roberto Longhi brought Caravaggio's name once more to the foreground, and placed him in the European tradition: \"Ribera, Vermeer, La Tour and Rembrandt could never have existed without him. And the art of Delacroix, Courbet and Manet would have been utterly different\". The influential Bernard Berenson agreed: \"With the exception of Michelangelo, no other Italian painter exercised so great an influence.\"\n\nCaravaggio's epitaph was composed by his friend Marzio Milesi. It reads:\n\nHe was commemorated on the front of the Banca d'Italia 100,000-lire banknote in the 1980s and 90s (before Italy switched to the Euro) with the back showing his \"Basket of Fruit\".\n\nThere is disagreement as to the size of Caravaggio's oeuvre, with counts as low as 40 and as high as 80. In his biography, Caravaggio scholar Alfred Moir writes \"The forty-eight colorplates in this book include almost all of the surviving works accepted by every Caravaggio expert as autograph, and even the least demanding would add fewer than a dozen more\". One, \"The Calling of Saints Peter and Andrew\", was recently authenticated and restored; it had been in storage in Hampton Court, mislabeled as a copy. Richard Francis Burton writes of a \"picture of St. Rosario (in the museum of the Grand Duke of Tuscany), showing a circle of thirty men \"turpiter ligati\"\" (\"lewdly banded\"), which is not known to have survived. The rejected version of \"The Inspiration of Saint Matthew\", intended for the Contarelli Chapel in San Luigi dei Francesi in Rome, was destroyed during the bombing of Dresden, though black and white photographs of the work exist. In June 2011 it was announced that a previously unknown Caravaggio painting of Saint Augustine dating to about 1600 had been discovered in a private collection in Britain. Called a \"significant discovery\", the painting had never been published and is thought to have been commissioned by Vincenzo Giustiniani, a patron of the painter in Rome.\n\nA painting believed by some experts to be Caravaggio's second version of \"Judith Beheading Holofernes\", tentatively dated between 1600 and 1610, was discovered in an attic in Toulouse in 2014. An export ban was placed on the painting by the French government while tests were carried out to establish its provenance. In February 2019 it was announced that the painting would be sold at auction after the Louvre had turned down the opportunity to purchase it for €100 million.\n\nIn October 1969, two thieves entered the Oratory of Saint Lawrence in Palermo, Sicily and stole Caravaggio's \"Nativity with St. Francis and St. Lawrence\" from its frame. Experts estimated its value at $20 million.\n\nFollowing the theft, Italian police set up an art theft task force with the specific aim of re-acquiring lost and stolen art works. Since the creation of this task force, many leads have been followed regarding the \"Nativity\". Former Italian mafia members have stated that \"Nativity with St. Francis and St. Lawrence\" was stolen by the Sicilian Mafia and displayed at important mafia gatherings. Former mafia members have said that the \"Nativity\" was damaged and has since been destroyed.\n\nThe whereabouts of the artwork are still unknown. A reproduction currently hangs in its place in the Oratory of San Lorenzo.\n\nCaravaggio's work has been widely influential in late-20th-century American gay culture, with frequent references to male sexual imagery in paintings such as \"The Musicians\" and \"Amor Victorious\". British filmmaker Derek Jarman made a critically applauded biopic entitled \"Caravaggio\" in 1986. Several poems written by Thom Gunn were responses to specific Caravaggio paintings.\n\nBULLET::::- Italian neorealism, a 1944–52 movement characterised by stories set amongst the disadvantaged\nBULLET::::- Utrecht Caravaggism\n\nBULLET::::- Citations\nThe main primary sources for Caravaggio's life are:\nBULLET::::- Giulio Mancini's comments on Caravaggio in \"Considerazioni sulla pittura\", c. 1617–1621\nBULLET::::- Giovanni Baglione's \"Le vite de' pittori\", 1642\nBULLET::::- Giovanni Pietro Bellori's \"Le Vite de' pittori, scultori et architetti moderni\", 1672\nAll have been reprinted in Howard Hibbard's \"Caravaggio\" and in the appendices to Catherine Puglisi's \"Caravaggio\".\n\nBULLET::::- Erin Benay (2017) \"Exporting Caravaggio: the Crucifixion of St. Andrew\" Giles Press Ltd.\nBULLET::::- Ralf van Bühren, \"Caravaggio's 'Seven Works of Mercy' in Naples. The relevance of art history to cultural journalism\", in \"Church, Communication and Culture\" 2 (2017), pp. 63–87\nBULLET::::- Claudio Strinati, \"Caravaggio Vero\", Scripta Maneant, 2014, .\nBULLET::::- Maurizio Calvesi, \"Caravaggio\", Art Dossier 1986, Giunti Editori (1986) (ISBN not available)\nBULLET::::- John Denison Champlin and Charles Callahan Perkins, Ed., \"Cyclopedia of Painters and Paintings\", Charles Scribner's Sons, New York (1885), p. 241 (available at the Harvard's Fogg Museum Library and scanned on Google Books)\nBULLET::::- Andrea Dusio, \"Caravaggio White Album\", Cooper Arte, Roma 2009,\nBULLET::::- Michael Fried, \"The Moment of Caravaggio\", Yale University Press, 2010, ISB: 9780691147017, Review\nBULLET::::- Walter Friedlaender, Caravaggio Studies, Princeton: Princeton University Press 1955\nBULLET::::- John Gash, \"Caravaggio\", Chaucer Press, (2004) )\nBULLET::::- Rosa Giorgi, \"Caravaggio: Master of light and dark – his life in paintings\", Dorling Kindersley (1999)\nBULLET::::- Andrew Graham-Dixon, \"Caravaggio: A Life Sacred and Profane\", London, Allen Lane, 2009.\nBULLET::::- Jonathan Harr (2005). \"The Lost Painting: The Quest for a Caravaggio Masterpiece\". New York: Random House. [\"The Taking of Christ\"]\nBULLET::::- Howard Hibbard, \"Caravaggio\" (1983)\nBULLET::::- Harris, Ann Sutherland. \"Seventeenth-century Art & Architecture\", Laurence King Publishing (2004), .\nBULLET::::- Michael Kitson, \"The Complete Paintings of Caravaggio\" London, Abrams, 1967. New edition: Weidenfeld & Nicolson, 1969 and 1986,\nBULLET::::- Pietro Koch, \"Caravaggio – The Painter of Blood and Darkness\", Gunther Edition, (Rome – 2004)\nBULLET::::- Gilles Lambert, \"Caravaggio\", Taschen, (2000)\nBULLET::::- Helen Langdon, \"Caravaggio: A Life\", Farrar, Straus and Giroux, 1999 (original UK edition 1998)\nBULLET::::- Denis Mahon (1947). \"Studies in Seicento Art\". London: Warburg Institute.\nBULLET::::- Alfred Moir, \"The Italian Followers of Caravaggio\", Harvard University Press (1967)\nBULLET::::- Ostrow, Steven F., review of \"Giovanni Baglione: Artistic Reputation in Baroque Rome\" by Maryvelma Smith O'Neil, \"The Art Bulletin\", Vol. 85, No. 3 (Sep., 2003), pp. 608–611, online text\nBULLET::::- Catherine Puglisi, \"Caravaggio\", Phaidon (1998)\nBULLET::::- Peter Robb, \"M\", Duffy & Snellgrove, 2003 amended edition (original edition 1998)\nBULLET::::- John Spike, with assistance from Michèle Kahn Spike, \"Caravaggio\" with Catalogue of Paintings on CD-ROM, Abbeville Press, New York (2001)\nBULLET::::- John L. Varriano, \"Caravaggio: The Art of Realism\", Pennsylvania State University Press (University Park, PA – 2006)\nBULLET::::- Rudolf Wittkower, \"Art and Architecture in Italy, 1600–1750\", Penguin/Yale History of Art, 3rd edition, 1973,\nBULLET::::- Alberto Macchi, \"L'uomo Caravaggio\" – Atto unico (pref. Stefania Macioce), AETAS, Roma 1995,\nBiography\nBULLET::::- Caravaggio, The Prince of the Night\nArticles and essays\nBULLET::::- Christiansen, Keith. \"Caravaggio (Michelangelo Merisi) (1571–1610) and his Followers.\" In \"Heilbrunn Timeline of Art History\". New York: The Metropolitan Museum of Art, 2000–. (October 2003)\nBULLET::::- FBI Art Theft Notice for Caravaggio's \"Nativity\"\nBULLET::::- \"The Passion of Caravaggio\"\nBULLET::::- \"Deconstructing Caravaggio and Velázquez\"\nBULLET::::- Interview with Peter Robb, author of \"M\"\nBULLET::::- Compare Rembrandt with Caravaggio.\nBULLET::::- Caravaggio and the Camera Obscura\nBULLET::::- Caravaggio's incisions by Ramon van de Werken\nBULLET::::- Caravaggio's use of the Camera Obscura: Lapucci\nBULLET::::- Some notes on Caravaggio – Patrick Swift\nBULLET::::- Roberta Lapucci's website and most of her publications on Caravaggio as freely downloadable PDF\n\nArt works\nBULLET::::- caravaggio-foundation.org 175 works by Caravaggio\nBULLET::::- caravaggio.org Analysis of 100 important Caravaggio works\nBULLET::::- Caravaggio, Michelangelo Merisi da Caravaggio WebMuseum, Paris webpage\nBULLET::::- Caravaggio's EyeGate Gallery\n\nMusic\nBULLET::::- Lachrimae Caravaggio, by Jordi Savall, performed by Le Concert des Nations & Hesperion XXI (Article at Answers.com)\n\nVideo\nBULLET::::- \"Caravaggio's Calling of Saint Matthew\" at Smarthistory, accessed February 13, 2013\nBULLET::::- \"Caravaggio's Crucifixion of Saint Peter\", accessed February 13, 2013\nBULLET::::- \"Caravaggio's Death of the Virgin\", accessed February 13, 2013\nBULLET::::- \"Caravaggio's Narcissus at the Source\", accessed February 13, 2013\nBULLET::::- \"Caravaggio's paintings in the Contarelli Chapel, San Luigi dei Francesi\", accessed February 13, 2013\nBULLET::::- \"Caravaggio's Supper at Emmaus\", accessed February 13, 2013\n"}
{"id": "7019", "url": "https://en.wikipedia.org/wiki?curid=7019", "title": "Jean-Baptiste-Siméon Chardin", "text": "Jean-Baptiste-Siméon Chardin\n\nJean-Baptiste-Siméon Chardin (; November 2, 1699 – December 6, 1779) was an 18th-century French painter. He is considered a master of still life, and is also noted for his genre paintings which depict kitchen maids, children, and domestic activities. Carefully balanced composition, soft diffusion of light, and granular impasto characterize his work.\n\nChardin was born in Paris, the son of a cabinetmaker, and rarely left the city. He lived on the Left Bank near Saint-Sulpice until 1757, when Louis XV granted him a studio and living quarters in the Louvre.\n\nChardin entered into a marriage contract with Marguerite Saintard in 1723, whom he did not marry until 1731. He served apprenticeships with the history painters Pierre-Jacques Cazes and Noël-Nicolas Coypel, and in 1724 became a master in the Académie de Saint-Luc.\n\nAccording to one nineteenth-century writer, at a time when it was hard for unknown painters to come to the attention of the Royal Academy, he first found notice by displaying a painting at the \"small Corpus Christi\" (held eight days after the regular one) on the Place Dauphine (by the Pont Neuf). Van Loo, passing by in 1720, bought it and later assisted the young painter.\n\nUpon presentation of \"The Ray\" in 1728, he was admitted to the Académie Royale de Peinture et de Sculpture. The following year he ceded his position in the Académie de Saint-Luc. He made a modest living by \"produc[ing] paintings in the various genres at whatever price his customers chose to pay him\", and by such work as the restoration of the frescoes at the Galerie François I at Fontainebleau in 1731.\nIn November 1731 his son Jean-Pierre was baptized, and a daughter, Marguerite-Agnès, was baptized in 1733. In 1735 his wife Marguerite died, and within two years Marguerite-Agnès had died as well.\n\nBeginning in 1737 Chardin exhibited regularly at the Salon. He would prove to be a \"dedicated academician\", regularly attending meetings for fifty years, and functioning successively as counsellor, treasurer, and secretary, overseeing in 1761 the installation of Salon exhibitions.\n\nChardin's work gained popularity through reproductive engravings of his genre paintings (made by artists such as François-Bernard Lépicié and P.-L. Sugurue), which brought Chardin income in the form of \"what would now be called royalties\". In 1744 he entered his second marriage, this time to Françoise-Marguerite Pouget. The union brought a substantial improvement in Chardin's financial circumstances. In 1745 a daughter, Angélique-Françoise, was born, but she died in 1746.\n\nIn 1752 Chardin was granted a pension of 500 livres by Louis XV. At the Salon of 1759 he exhibited nine paintings; it was the first Salon to be commented upon by Denis Diderot, who would prove to be a great admirer and public champion of Chardin's work. Beginning in 1761, his responsibilities on behalf of the Salon, simultaneously arranging the exhibitions and acting as treasurer, resulted in a diminution of productivity in painting, and the showing of 'replicas' of previous works. In 1763 his services to the Académie were acknowledged with an extra 200 livres in pension. In 1765 he was unanimously elected associate member of the Académie des Sciences, Belles-Lettres et Arts of Rouen, but there is no evidence that he left Paris to accept the honor. By 1770 Chardin was the 'Premier peintre du roi', and his pension of 1,400 livres was the highest in the Academy.\n\nIn 1772 Chardin's son, also a painter, drowned in Venice, a probable suicide. The artist's last known oil painting was dated 1776; his final Salon participation was in 1779, and featured several pastel studies. Gravely ill by November of that year, he died in Paris on December 6, at the age of 80.\n\nChardin worked very slowly and painted only slightly more than 200 pictures (about four a year) in total.\n\nChardin's work had little in common with the Rococo painting that dominated French art in the 18th century. At a time when history painting was considered the supreme classification for public art, Chardin's subjects of choice were viewed as minor categories. He favored simple yet beautifully textured still lifes, and sensitively handled domestic interiors and genre paintings. Simple, even stark, paintings of common household items (\"Still Life with a Smoker's Box\") and an uncanny ability to portray children's innocence in an unsentimental manner (\"Boy with a Top\" [right]) nevertheless found an appreciative audience in his time, and account for his timeless appeal.\n\nLargely self-taught, Chardin was greatly influenced by the realism and subject matter of the 17th-century Low Country masters. Despite his unconventional portrayal of the ascendant bourgeoisie, early support came from patrons in the French aristocracy, including Louis XV. Though his popularity rested initially on paintings of animals and fruit, by the 1730s he introduced kitchen utensils into his work (\"The Copper Cistern\", ca. 1735, Louvre). Soon figures populated his scenes as well, supposedly in response to a portrait painter who challenged him to take up the genre. \"Woman Sealing a Letter\" (ca. 1733), which may have been his first attempt, was followed by half-length compositions of children saying grace, as in \"Le Bénédicité\", and kitchen maids in moments of reflection. These humble scenes deal with simple, everyday activities, yet they also have functioned as a source of documentary information about a level of French society not hitherto considered a worthy subject for painting. The pictures are noteworthy for their formal structure and pictorial harmony. Chardin said about painting, \"Who said one paints with colors? One \"employs\" colors, but one paints with \"feeling\".\"\n\nA child playing was a favourite subject of Chardin. He depicted an adolescent building a house of cards on at least four occasions. The version at Waddesdon Manor is the most elaborate. Scenes such as these derived from 17th-century Netherlandish vanitas works, which bore messages about the transitory nature of human life and the worthlessness of material ambitions, but Chardin's also display a delight in the ephemeral phases of childhood for their own sake.\n\nChardin frequently painted replicas of his compositions—especially his genre paintings, nearly all of which exist in multiple versions which in many cases are virtually indistinguishable. Beginning with \"The Governess\" (1739, in the National Gallery of Canada, Ottawa), Chardin shifted his attention from working-class subjects to slightly more spacious scenes of bourgeois life.\n\nIn 1756 Chardin returned to the subject of the still life. In the 1770s his eyesight weakened and he took to painting in pastels, a medium in which he executed portraits of his wife and himself (see \"Self-portrait\" at top right). His works in pastels are now highly valued. Chardin's extant paintings, which number about 200, are in many major museums, including the Louvre.\n\nChardin's influence on the art of the modern era was wide-ranging, and has been well-documented. Édouard Manet's half-length \"Boy Blowing Bubbles\" and the still lifes of Paul Cézanne are equally indebted to their predecessor. He was one of Henri Matisse's most admired painters; as an art student Matisse made copies of four Chardin paintings in the Louvre. Chaim Soutine's still lifes looked to Chardin for inspiration, as did the paintings of Georges Braque, and later, Giorgio Morandi. In 1999 Lucian Freud painted and etched several copies after \"The Young Schoolmistress\" (National Gallery, London).\n\nMarcel Proust, in the chapter \"How to open your eyes?\" from \"In Search of Lost Time\" (\"À la recherche du temps perdu\"), describes a melancholic young man sitting at his simple breakfast table. The only comfort he finds is in the imaginary ideas of beauty depicted in the great masterpieces of the Louvre, materializing fancy palaces, rich princes, and the like. The author tells the young man to follow him to another section of the Louvre where the pictures of Jean-Baptiste Chardin are. There he would see the beauty in still life at home and in everyday activities like peeling turnips.\nBULLET::::- \"The Attributes of Civilian and Military Music\"\n\nBULLET::::- ArtCyclopedia: Jean-Baptiste Siméon Chardin.\nBULLET::::- Rosenberg, Pierre (2000), \"Chardin\". Munich: Prestel. .\nBULLET::::- Rosenberg, Pierre, and Florence Bruyant (2000), \"Chardin\". London: Royal Academy of Arts. .\n\nBULLET::::- Chardin exhibition at the Metropolitan Museum of Art\nBULLET::::- Getty Museum: Chardin.\nBULLET::::- WebMuseum: Jean-Baptiste-Siméon Chardin.\nBULLET::::- Jean-Baptiste-Simeon-Chardin.org 124 works by Jean-Baptiste-Siméon Chardin.\nBULLET::::- Artcylopedia: Jean-Baptiste Siméon Chardin - identifies where Chardin's work is in galleries and museums around the world.\nBULLET::::- Web Gallery of Art: Chardin.\nBULLET::::- Neil Jeffares, \"Dictionary of pastellists before 1800\", online edition\nBULLET::::- Chardin, \"Boy Building a House of Cards\" at Waddesdon Manor\n"}
{"id": "7021", "url": "https://en.wikipedia.org/wiki?curid=7021", "title": "Crookes radiometer", "text": "Crookes radiometer\n\nThe Crookes radiometer (also known as a light mill) consists of an airtight glass bulb containing a partial vacuum, with a set of vanes which are mounted on a spindle inside. The vanes rotate when exposed to light, with faster rotation for more intense light, providing a quantitative measurement of electromagnetic radiation intensity. \n\nThe reason for the rotation was a cause of much scientific debate in the ten years following the invention of the device, but in 1879 the currently accepted explanation for the rotation was published. Today the device is mainly used in physics education as a demonstration of a heat engine run by light energy.\n\nIt was invented in 1873 by the chemist Sir William Crookes as the by-product of some chemical research. In the course of very accurate quantitative chemical work, he was weighing samples in a partially evacuated chamber to reduce the effect of air currents, and noticed the weighings were disturbed when sunlight shone on the balance. Investigating this effect, he created the device named after him.\n\nIt is still manufactured and sold as an educational aid or curiosity.\n\nThe radiometer is made from a glass bulb from which much of the air has been removed to form a partial vacuum. Inside the bulb, on a low friction spindle, is a rotor with several (usually four) vertical lightweight vanes spaced equally around the axis. The vanes are polished or white on one side and black on the other.\n\nWhen exposed to sunlight, artificial light, or infrared radiation (even the heat of a hand nearby can be enough), the vanes turn with no apparent motive power, the dark sides retreating from the radiation source and the light sides advancing.\n\nCooling the radiometer causes rotation in the opposite direction.\n\nThe effect begins to be observed at partial vacuum pressures of a few torr (several hundred pascals), reaches a peak at around 7.5 x 10 torr (1 pascal) and has disappeared by the time the vacuum reaches 7.5 x 10 torr (10 pascal) (see explanations note 1). At these very high vacuums the effect of photon radiation pressure on the vanes can be observed in very sensitive apparatus (see Nichols radiometer) but this is insufficient to cause rotation.\n\nThe prefix \"\" in the title originates from the combining form of Latin \"radius\", a ray: here it refers to electromagnetic radiation. A Crookes radiometer, consistent with the suffix \"\" in its title, can provide a quantitative measurement of electromagnetic radiation intensity. This can be done, for example, by visual means (e.g., a spinning slotted disk, which functions as a simple stroboscope) without interfering with the measurement itself.\n\nRadiometers are now commonly sold worldwide as a novelty ornament; needing no batteries, but only light to get the vanes to turn. They come in various forms, such as the one pictured, and are often used in science museums to illustrate \"radiation pressure\" – a scientific principle that they do not in fact demonstrate.\n\nWhen a radiant energy source is directed at a Crookes radiometer, the radiometer becomes a heat engine. The operation of a heat engine is based on a difference in temperature that is converted to a mechanical output. In this case, the black side of the vane becomes hotter than the other side, as radiant energy from a light source warms the black side by black-body absorption faster than the silver or white side. The internal air molecules are heated up when they touch the black side of the vane. The details of exactly how this moves the warmer side of the vane forward are given in the section below.\n\nThe internal temperature rises as the black vanes impart heat to the air molecules, but the molecules are cooled again when they touch the bulb's glass surface, which is at ambient temperature. This heat loss through the glass keeps the internal bulb temperature steady with the result that the two sides of the vanes develop a temperature difference. The white or silver side of the vanes are slightly warmer than the internal air temperature but cooler than the black side, as some heat conducts through the vane from the black side. The two sides of each vane must be thermally insulated to some degree so that the polished or white side does not immediately reach the temperature of the black side. If the vanes are made of metal, then the black or white paint can be the insulation. The glass stays much closer to ambient temperature than the temperature reached by the black side of the vanes. The external air helps conduct heat away from the glass.\n\nThe air pressure inside the bulb needs to strike a balance between too low and too high. A strong vacuum inside the bulb does not permit motion, because there are not enough air molecules to cause the air currents that propel the vanes and transfer heat to the outside before both sides of each vane reach thermal equilibrium by heat conduction through the vane material. High inside pressure inhibits motion because the temperature differences are not enough to push the vanes through the higher concentration of air: there is too much air resistance for \"eddy currents\" to occur, and any slight air movement caused by the temperature difference is damped by the higher pressure before the currents can \"wrap around\" to the other side.\n\nWhen the radiometer is heated in the absence of a light source, it turns in the forward direction (i.e. black sides trailing). If a person's hands are placed around the glass without touching it, the vanes will turn slowly or not at all, but if the glass is touched to warm it quickly, they will turn more noticeably. Directly heated glass gives off enough infrared radiation to turn the vanes, but glass blocks much of the far-infrared radiation from a source of warmth not in contact with it. However, near-infrared and visible light more easily penetrate the glass.\n\nIf the glass is cooled quickly in the absence of a strong light source by putting ice on the glass or placing it in the freezer with the door almost closed, it turns backwards (i.e. the silver sides trail). This demonstrates black-body radiation from the black sides of the vanes rather than black-body absorption. The wheel turns backwards because the net exchange of heat between the black sides and the environment initially cools the black sides faster than the white sides. Upon reaching equilibrium, typically after a minute or two, reverse rotation ceases. This contrasts with sunlight, with which forward rotation can be maintained all day.\n\nOver the years, there have been many attempts to explain how a Crookes radiometer works:\n\nBULLET::::1. Crookes incorrectly suggested that the force was due to the pressure of light. This theory was originally supported by James Clerk Maxwell, who had predicted this force. This explanation is still often seen in leaflets packaged with the device. The first experiment to test this theory was done by Arthur Schuster in 1876, who observed that there was a force on the glass bulb of the Crookes radiometer that was in the opposite direction to the rotation of the vanes. This showed that the force turning the vanes was generated inside the radiometer. If light pressure were the cause of the rotation, then the better the vacuum in the bulb, the less air resistance to movement, and the faster the vanes should spin. In 1901, with a better vacuum pump, Pyotr Lebedev showed that in fact, the radiometer only works when there is low-pressure gas in the bulb, and the vanes stay motionless in a hard vacuum. Finally, if light pressure were the motive force, the radiometer would spin in the opposite direction, as the photons on the shiny side being reflected would deposit more momentum than on the black side where the photons are absorbed. This results from conservation of momentum - the momentum of the reflected photon exiting on the light side must be matched by a reaction on the vane that reflected it. The actual pressure exerted by light is far too small to move these vanes but can be measured with devices such as the Nichols radiometer.\nBULLET::::2. Another incorrect theory was that the heat on the dark side was causing the material to outgas, which pushed the radiometer around. This was later effectively disproved by both Schuster's (1876) and Lebedev's (1901) experiments.\nBULLET::::3. A partial explanation is that gas molecules hitting the warmer side of the vane will pick up some of the heat, bouncing off the vane with increased speed. Giving the molecule this extra boost effectively means that a minute pressure is exerted on the vane. The imbalance of this effect between the warmer black side and the cooler silver side means the net pressure on the vane is equivalent to a push on the black side and as a result the vanes spin round with the black side trailing. The problem with this idea is that while the faster moving molecules produce more force, they also do a better job of stopping other molecules from reaching the vane, so the net force on the vane should be the same. The greater temperature causes a decrease in local density which results in the same force on both sides. Years after this explanation was dismissed, Albert Einstein showed that the two pressures do not cancel out exactly at the edges of the vanes because of the temperature difference there. The force predicted by Einstein would be enough to move the vanes, but not fast enough.\nBULLET::::4. The final piece of the puzzle, thermal transpiration, was theorized by Osborne Reynolds in an unpublished paper of 1879 that was refereed by Maxwell, who then published his paper which contained a critique of the mathematics in Reynolds's unpublished paper. Maxwell died that year and the Royal Society refused to publish Reynolds's critique of Maxwell's rebuttal to Reynolds's unpublished paper, as it was felt that this would be an inappropriate argument when one of the people involved had already died. Reynolds found that if a porous plate is kept hotter on one side than the other, the interactions between gas molecules and the plates are such that gas will flow through from the cooler to the hotter side. The vanes of a typical Crookes radiometer are not porous, but the space past their edges behaves like the pores in Reynolds's plate. On average, the gas molecules move from the cold side toward the hot side whenever the pressure ratio is less than the square root of the (absolute) temperature ratio. The pressure difference causes the vane to move, cold (white) side forward due to the tangential force of the movement of the rarefied gas moving from the colder edge to the hotter edge.\n\nTo rotate, a light mill does not have to be coated with different colors across each vane. In 2009, researchers at the University of Texas, Austin created a monocolored light mill which has four curved vanes; each vane forms a convex and a concave surface. The light mill is uniformly coated by gold nanocrystals, which are a strong light absorber. Upon exposure, due to geometric effect, the convex side of the vane receives more photon energy than the concave side does, and subsequently the gas molecules receive more heat from the convex side than from the concave side. At rough vacuum, this asymmetric heating effect generates a net gas movement across each vane, from the concave side to the convex side, as shown by the researchers' Direct Simulation Monte Carlo (DSMC) modeling. The gas movement causes the light mill to rotate with the concave side moving forward, due to Newton's Third Law.\nThis monocolored design promotes the fabrication of micrometer- or nanometer- scaled light mills, as it is difficult to pattern materials of distinct optical properties within a very narrow, three-dimensional space.\n\nThe thermal creep from the hot side of a vane to the cold side has been demonstrated in a mill with horizontal vanes that have a two tone surface with a black half and a white half. This design is called a Hettner radiometer .This radiometer's angular speed was found to be limited by the behavior of the drag force due to the gas in the vessel more than by the behavior of the thermal creep force. This design does not experience the Einstein effect because the faces are parallel to the temperature gradient.\n\nIn 2010 researchers at the University of California, Berkeley succeeded in building a nanoscale light mill that works on an entirely different principle to the Crookes radiometer. A gold light mill, only 100 nanometers in diameter, was built and illuminated by laser light that had been tuned. The possibility of doing this had been suggested by the Princeton physicist Richard Beth in 1936. The torque was greatly enhanced by the resonant coupling of the incident light to plasmonic waves in the gold structure.\n\nBULLET::::- Crookes tube\nBULLET::::- Marangoni effect\nBULLET::::- Nichols radiometer\nBULLET::::- Photophoresis\nBULLET::::- Solar energy\nBULLET::::- Solar wind\nBULLET::::- Thermophoresis\n\nBULLET::::- General information\nBULLET::::- Loeb, Leonard B. (1934) \"The Kinetic Theory Of Gases (2nd Edition)\";McGraw-Hill Book Company; pp 353–386\nBULLET::::- Kennard, Earle H. (1938) \"Kinetic Theory of Gases\"; McGraw-Hill Book Company; pp 327–337\nBULLET::::- Patents\n\nBULLET::::- Crooke's Radiometer applet\nBULLET::::- How does a light-mill work?-Physics FAQ\nBULLET::::- The Cathode Ray Tube site\nBULLET::::- . 1933 Bell and Green experiment describing the effect of different gas pressures on the vanes.\nBULLET::::- The Properties of the Force Exerted in a Radiometer archived\n"}
{"id": "7022", "url": "https://en.wikipedia.org/wiki?curid=7022", "title": "Cold Chisel", "text": "Cold Chisel\n\nCold Chisel are an Australian pub rock band, which formed in Adelaide in 1973 by mainstay members Ian Moss on guitar and vocals, Steve Prestwich on drums and Don Walker on piano and keyboards. They were soon joined by Jimmy Barnes on lead vocals and, in 1975, Phil Small became their bass guitarist. The group disbanded in late 1983 but subsequently reformed several times. Musicologist Ian McFarlane wrote that they became \"one of Australia's best-loved groups\" as well as \"one of the best live bands\", fusing \"a combination of rockabilly, hard rock and rough-house soul'n'blues that was defiantly Australian in outlook.\"\n\nSeven of their studio albums have reached the Australian top five, \"Breakfast at Sweethearts\" (February 1979), \"East\" (June 1980), \"Circus Animals\" (March 1982, No. 1), \"Twentieth Century\" (April 1984, No. 1), \"The Last Wave of Summer\" (October 1998, No. 1), \"No Plans\" (April 2012) and \"The Perfect Crime\" (October 2015). Their top 10 singles are \"Forever Now\" (1982), \"Hands Out of My Pocket\" (1994) and \"The Things I Love in You\" (1998).\n\nAt the ARIA Music Awards of 1993 they were inducted into the Hall of Fame. In 2001 Australasian Performing Right Association (APRA), listed their single, \"Khe Sanh\" (May 1978), at No. 8 of the all-time best Australian songs. \"Circus Animals\" was listed at No. 4 in the book, \"100 Best Australian Albums\" (October 2010), while \"East\" appeared at No. 53. They won The Ted Albert Award for Outstanding Services to Australian Music at the APRA Music Awards of 2016. Cold Chisel's popularity is largely restricted to Australia and New Zealand, with their songs and musicianship highlighting working class life. Their early bass guitarist (1973–75), Les Kaczmarek, died in December 2008; Steve Prestwich died of a brain tumour in January 2011.\n\nCold Chisel originally formed as Orange in Adelaide in 1973 as a heavy metal band by Ted Broniecki on keyboards, Les Kaczmarek on bass guitar, Ian Moss on guitar and vocals, Steve Prestwich on drums and Don Walker on piano. Their early material included cover versions of Free and Deep Purple material. Broniecki left by September 1973 and seventeen-year-old singer, Jimmy Barnes — called Jim Barnes during their initial career — joined in December.\n\nThe group changed its name several times before settling on Cold Chisel in 1974 after Walker's song of that title. Barnes' relationship with the others was volatile: he often came to blows with Prestwich and left the band several times. During these periods Moss would handle vocals until Barnes returned. Walker emerged as the group's primary songwriter and spent 1974 in Armidale, completing his studies in quantum mechanics. Barnes' older brother, John Swan, was a member of Cold Chisel around this time, providing backing vocals and percussion. After several violent incidents, including beating up a roadie, he was fired. In mid-1975 Barnes left to join Fraternity as Bon Scott's replacement on lead vocals, alongside Swan on drums and vocals. Kaczmarek left Cold Chisel during 1975 and was replaced by Phil Small on bass guitar. In November of that year, without Barnes, they recorded their early demos.\n\nIn May 1976 Cold Chisel relocated to Melbourne, but \"frustrated by their lack of progress,\" they moved on to Sydney in November. In May of the following year Barnes told his fellow members that he would leave again. From July he joined Feather for a few weeks, on co-lead vocals with Swan – they were a Sydney-based hard rock group, which had evolved from Blackfeather. A farewell performance for Cold Chisel, with Barnes aboard, went so well that the singer changed his mind and returned. In the following month the Warner Music Group signed the group.\n\nIn the early months of 1978 Cold Chisel recorded their self-titled debut album with their manager and producer, Peter Walker (ex-Bakery). All tracks were written by Walker, except \"Juliet\", where Barnes composed its melody and Walker the lyrics. \"Cold Chisel\" was released in April and included guest studio musicians: Dave Blight on harmonica (who became a regular on-stage guest) and saxophonists Joe Camilleri and Wilbur Wilde (from Jo Jo Zep & The Falcons). Australian musicologist, Ian McFarlane, described how, \"[it] failed to capture the band's renowned live firepower, despite the presence of such crowd favourites as 'Khe Sanh', 'Home and Broken Hearted' and 'One Long Day'.\" It reached the top 40 on the Kent Music Report and was certified as a gold record for shipment of 35000 units.\n\nIn May 1978, \"Khe Sanh\", was released as their debut single but it was declared too offensive for commercial radio due to the sexual implication of the lyrics, \"Their legs were often open/But their minds were always closed.\" However, it was played regularly on Sydney youth radio station, Double J, which was not subject to the restrictions as it was part of the Australian Broadcasting Corporation (ABC). Another ABC program, \"Countdown\"s producers asked them to change the lyric but they refused. Despite such setbacks, \"Khe Sanh\" reached No. 41 on the Kent Music Report singles chart. It became Cold Chisel's signature tune and was popular among their fans. They later remixed the track, with re-recorded vocals, for inclusion on the international version of their third album, \"East\" (June 1980).\n\nThe band's next release was a live five-track extended play, \"You're Thirteen, You're Beautiful, and You're Mine\", in November 1978. McFarlane observed, \"It captured the band in its favoured element, fired by raucous versions of Walker's 'Merry-Go-Round' and Chip Taylor's 'Wild Thing'.\" It was recorded at Sydney's Regent Theatre in 1977, when they had Midnight Oil as one of the support acts. Australian writer, Ed Nimmervoll, described a typical performance by Cold Chisel, \"Everybody was talking about them anyway, drawn by the songs, and Jim Barnes' presence on stage, crouched, sweating, as he roared his vocals into the microphone at the top of his lungs.\" The EP peaked at No. 35 on the Kent Music Report Singles Chart.\n\n\"Merry Go Round\" was re-recorded for their second studio album, \"Breakfast at Sweethearts\" (February 1979). This was recorded between July 1978 and January 1979 with producer, Richard Batchens, who had previously worked with Richard Clapton, Sherbet and Blackfeather. Batchens smoothed out the band's rough edges and attempted to give their songs a sophisticated sound. With regards to this approach, the band were unsatisfied with the finished product. It peaked at No. 4 and was the top selling album in Australia by a locally based artist for that year; it was certified platinum for shipment of 70000 copies. The majority of its tracks were written by Walker, with Barnes and Walker on the lead single, \"Goodbye (Astrid, Goodbye)\" (September 1978), and Moss contributed to \"Dresden\". \"Goodbye (Astrid, Goodbye)\" became a live favourite, and was covered by U2 during Australian tours in the 1980s.\n\nCold Chisel had gained national chart success and increased popularity of their fans without significant commercial radio airplay or support from \"Countdown\". The members developed reputations for wild behaviour, particularly Barnes who claimed to have had sex with over 1000 women and who consumed more than a bottle of vodka each night while performing. In late 1979, severing their relationship with Batchens, Cold Chisel chose Mark Opitz to produce the next single, \"Choirgirl\" (November). It is a Walker composition dealing with a young woman's experience with abortion. Despite the subject matter it reached No. 14.\n\n\"Choirgirl\" paved the way for the group's third studio album, \"East\" (June 1980), with Opitz producing. Recorded over two months in early 1980, \"East\", reached No. 2 and is the second highest selling album by an Australian artist for that year. \"The Australian Women's Weekly\"s Gregg Flynn noticed, \"[they are] one of the few Australian bands in which each member is capable of writing hit songs.\" Despite the continued dominance of Walker, the other members contributed more tracks to their play list, and this was their first album to have songs written by each one. McFarlane described it as, \"a confident, fully realised work of tremendous scope.\" Nimmervoll explained how, \"This time everything fell into place, the sound, the songs, the playing... \"East\" was a triumph. [The group] were now the undisputed No. 1 rock band in Australia.\"\n\nThe album varied from straight ahead rock tracks, \"Standing on the Outside\" and \"My Turn to Cry\", to rockabilly-flavoured work-outs (\"Rising Sun\", written about Barnes' relationship with his then-girlfriend Jane Mahoney) and pop-laced love songs (\"My Baby\", featuring Joe Camilleri on saxophone) to a poignant piano ballad about prison life, \"Four Walls\". The cover art showed Barnes reclined in a bathtub wearing a kamikaze bandanna in a room littered with junk and was inspired by Jacques-Louis David's 1793 painting, \"The Death of Marat\". The Ian Moss-penned \"Never Before\" was chosen as the first song to air on the ABC's youth radio station, Triple J, when it switched to the FM band that year. Supporting the release of \"East\", Cold Chisel embarked on the Youth in Asia Tour from May 1980, which took its name from a lyric in \"Star Hotel\".\n\nThe Youth in Asia Tour performances were used for Cold Chisel's double live album, \"Swingshift\" (March 1981). Nimmervoll declared, \"[the group] rammed what they were all about with [this album].\" In March 1981 the band won seven categories: Best Australian Album, Most Outstanding Achievement, Best Recorded Song Writer, Best Australian Producer, Best Australian Record Cover Design, Most Popular Group and Most Popular Record, at the \"Countdown\"/\"TV Week\" pop music awards for 1980. They attended the ceremony at the Sydney Entertainment Centre and were due to perform: however, as a protest against a TV magazine's involvement, they refused to accept any trophy and finished the night with, \"My Turn to Cry\". After one verse and chorus, they smashed up the set and left the stage.\n\n\"Swingshift\" debuted at No 1, which demonstrated their status as the highest selling local act. With a slightly different track-listing, \"East\", was issued in the United States and they undertook their first US tour in mid-1981. Ahead of the tour they had issued, \"My Baby\", for the North America market and it reached the top 40 on \"Billboard\"s chart, Mainstream Rock. They were generally popular as a live act there, but the US branch of their label did little to promote the album. According to Barnes' biographer, Toby Creswell, at one point they were ushered into an office to listen to the US master tape to find it had substantial hiss and other ambient noise, which made it almost unable to be released. Notwithstanding, the album reached the lower region of the \"Billboard\" 200 in July. The group were booed off stage after a lacklustre performance in Dayton, Ohio in May 1981 opening for Ted Nugent. Other support slots they took were for Cheap Trick, Joe Walsh, Heart and the Marshall Tucker Band. European audiences were more accepting of the Australian band and they developed a fan base in Germany.\n\nIn August 1981 Cold Chisel began work on a fourth studio album, \"Circus Animals\" (March 1982), again with Opitz producing. To launch the album, the band performed under a circus tent at Wentworth Park in Sydney and toured heavily once more, including a show in Darwin that attracted more than 10 percent of the city's population. It peaked at No. 1 in both Australia and on the Official New Zealand Music Chart. In October 2010 it was listed at No. 4 in the book, \"100 Best Australian Albums\", by music journalists, Creswell, Craig Mathieson and John O'Donnell.\n\nIts lead single, \"You Got Nothing I Want\" (November 1981), is an aggressive Barnes-penned hard rock track, which attacked the US industry for its handling of the band on their recent tour. The song caused problems for Barnes when he later attempted to break into the US market as a solo performer; senior music executives there continued to hold it against him. Like its predecessor, \"Circus Animals\", contained songs of contrasting styles, with harder-edged tracks like \"Bow River\" and \"Hound Dog\" in place beside more expansive ballads such as the next two singles, \"Forever Now\" (March 1982) and \"When the War Is Over\" (August), both are written by Prestwich. \"Forever Now\" is their highest charting single in two Australasian markets: No. 4 on the Kent Music Report Singles Chart and No. 2 on the Official New Zealand Music Chart.\n\n\"When the War Is Over\" is the most covered Cold Chisel track – Uriah Heep included a version on their 1989 album, \"Raging Silence\"; John Farnham recorded it while he and Prestwich were members of Little River Band in the mid-1980s and again for his 1990 solo album, \"Age of Reason\". The song was also a No. 1 hit for former \"Australian Idol\" contestant, Cosima De Vito, in 2004 and was performed by Bobby Flynn during that show's 2006 season. \"Forever Now\" was covered, as a country waltz, by Australian band, the Reels.\n\nSuccess outside Australasia continued to elude Cold Chisel and friction occurred between the members. According to McFarlane, \"[the] failed attempts to break into the American market represented a major blow... [their] earthy, high-energy rock was overlooked.\" In early 1983 they toured Germany but the shows went so badly that in the middle of the tour Walker up-ended his keyboard and stormed off stage during one show. After returning to Australia, Preswtich was fired and replaced by Ray Arnott, formerly of the 1970s progressive rockers, Spectrum, and country rockers, the Dingoes.\n\nAfter this, Barnes requested a large advance from management. Now married with a young child, exorbitant spending had left him almost broke. His request was refused as there was a standing arrangement that any advance to one band member had to be paid to all the others. After a meeting on 17 August during which Barnes quit the band it was decided that the group would split up. A farewell concert series, The Last Stand, was planned and a final studio album, \"Twentieth Century\" (February 1984), was recorded. Prestwich returned for that tour, which began in October. Before the last four scheduled shows in Sydney, Barnes lost his voice and those dates were postponed to mid-December.\n\nThe band's final performances were at the Sydney Entertainment Centre from 12 to 15 December 1983 – ten years since their first live appearance as Cold Chisel in Adelaide – the group then disbanded. The Sydney shows formed the basis of a concert film, \"The Last Stand\" (July 1984), which became the biggest-selling cinema-released concert documentary by an Australian band to that time. Other recordings from the tour were used on a live album, \"\" (1984), the title is a reference to the pseudonym the group occasionally used when playing warm-up shows before tours. Some were also used as b-sides for a three-CD singles package, \"Three Big XXX Hits\", issued ahead of the release of their 1994 compilation album, \"Teenage Love\".\n\nDuring breaks in the tour, \"Twentieth Century\", was recorded. It was a fragmentary process, spread across various studios and sessions as the individual members often refused to work together – both Arnott (on ten tracks) and Prestwich (on three tracks) are recorded as drummers. The album reached No. 1 and provided the singles, \"Saturday Night\" (March 1984) and \"Flame Trees\" (August), both of which remain radio staples. \"Flame Trees\", co-written by Prestwich and Walker, took its title from the BBC series, \"The Flame Trees of Thika\", although it was lyrically inspired by the organist's hometown of Grafton. Barnes later recorded an acoustic version for his 1993 solo album, \"Flesh and Wood\", and it was also covered by Sarah Blasko in 2006.\n\nBarnes launched his solo career in January 1984, which has provided nine Australian number-one studio albums and an array of hit singles, including, \"Too Much Ain't Enough Love\", which peaked at No. 1. He has recorded with INXS, Tina Turner, Joe Cocker and John Farnham to become one of the country's most popular male rock singers. Prestwich joined Little River Band in 1984 and appeared on the albums, \"Playing to Win\" and \"No Reins\", before departing in 1986 to join Farnham's touring band. Moss, Small and Walker took extended breaks from music.\n\nSmall maintained a low profile as a member in a variety of minor groups, Pound, the Earls of Duke and the Outsiders. Walker formed Catfish in 1988, ostensibly a solo band with a variable membership, which included Moss, Charlie Owen and Dave Blight at times. Catfish played a modern jazz aspect and the recordings during this phase attracted little commercial success. During 1988 and 1989 Walker wrote several tracks for Moss including the singles, \"Tucker's Daughter\" (November 1988) and \"Telephone Booth\" (June 1989), which appeared on Moss' debut solo album, \"Matchbook\" (August 1989). Both the album and \"Tucker's Daughter\" peaked at No. 1. Moss won five trophies at the ARIA Music Awards of 1990. His other solo albums met with less chart or award success.\n\nThroughout the 1980s and most of the 1990s, Cold Chisel were courted to re-form but refused, at one point reportedly turning down a $5 million offer to play a sole show in each of the major Australian state capitals. Moss and Walker often collaborated on projects, neither worked with Barnes until Walker wrote, \"Stone Cold\", for the singer's sixth studio album, \"Heat\" (October 1993). The pair recorded an acoustic version for \"Flesh and Wood\" (December). Thanks primarily to continued radio airplay and Barnes' solo success, Cold Chisel's legacy remained solidly intact. By the early 1990s the group had surpassed 3 million album sales, most sold since 1983. The 1991 compilation album, \"Chisel\", was re-issued and re-packaged several times, once with the long-deleted 1978 EP as a bonus disc and a second time in 2001 as a double album. The \"Last Stand\" soundtrack album was finally released in 1992. In 1994 a complete album of previously unreleased demo and rare live recordings, \"Teenage Love\", was released, which provided three singles.\n\nCold Chisel reunited in October 1997, with the line-up of Barnes, Moss, Prestwich, Small and Walker. They recorded their sixth studio album, \"The Last Wave of Summer\" (October 1998), from February to July with the band members co-producing. They supported it with a national tour. The album debuted at number one on the ARIA Albums Chart. In 2003 they re-grouped for the Ringside Tour and in 2005 again to perform at a benefit for the victims of the Boxing Day tsunami at the Myer Music Bowl in Melbourne. Founding bass guitarist, Les Kaczmarek, died of liver failure on 5 December 2008, aged 53. Walker described him as, \"a wonderful and beguiling man in every respect.\"\n\nOn 10 September 2009 Cold Chisel announced they would reform for a one-off performance at the Sydney 500 V8 Supercars event on 5 December. The band performed at ANZ Stadium to the largest crowd of its career, with more than 45,000 fans in attendance. They played a single live show in 2010: at the Deniliquin ute muster in October. In December Moss confirmed that Cold Chisel were working on new material for an album.\n\nIn January 2011 Steve Prestwich was diagnosed with a brain tumour; he underwent surgery on 14 January but never regained consciousness and died two days later, aged 56. All six of Cold Chisel's studio albums were re-released in digital and CD formats in mid-2011. Three digital-only albums were released, \"Never Before\", \"Besides\" and \"Covered\", as well as a new compilation album, \"The Best of Cold Chisel: All for You\", which peaked at number 2 on the ARIA Charts. The thirty-date Light the Nitro Tour was announced in July along with the news that former Divinyls and Catfish drummer, Charley Drayton, had replaced Prestwich. Most shows on the tour sold out within days and new dates were later announced for early 2012.\n\n\"No Plans\", their seventh studio album, was released in April 2012, with Kevin Shirley producing, which peaked at No. 2. \"The Australian\"s Stephen Fitzpatrick rated it as four-and-a-half out-of-five and found its lead track, \"All for You\", \"speaks of redemption; of a man's ability to make something of himself through love.\" The track, \"I Got Things to Do\", was written and sung by Prestwich, which Fitzpatrick described as, \"the bittersweet finale\" and had \"a vocal track the other band members did not know existed until after his death.\" Midway through 2012 they had a short UK tour and played with Soundgarden and Mars Volta at Hard Rock Calling at London's Hyde Park.\n\nThe group's eighth studio album, \"The Perfect Crime\", appeared in October 2015, again with Shirley producing, which peaked at No. 2. Martin Boulton of \"The Sydney Morning Herald\" rated it at four-out-of-five stars and explained, \"[they] work incredibly hard, not take any shortcuts and play the hell out of the songs\" where the album, \"delves further back to their rock'n'roll roots with chief songwriter [Walker] carving up the keys, guitarist [Moss] both gritty and sublime and the [Small/Drayton] engine room firing on every cylinder. Barnes' voice sounds worn, wonderful and better than ever.\"\n\nMcFarlane described Cold Chisel's early career in his \"Encyclopedia of Australian Rock and Pop\" (1999), \"after ten years on the road, [they] called it a day. Not that the band split up for want of success; by that stage [they] had built up a reputation previously uncharted in Australian rock history. By virtue of the profound effect the band's music had on the many thousands of fans who witnessed its awesome power, Cold Chisel remains one of Australia's best-loved groups. As one of the best live bands of its day, [they] fused a combination of rockabilly, hard rock and rough-house soul'n'blues that was defiantly Australian in outlook.\" \"The Canberra Times\" Luis Feliu, in July 1978, observed how, \"This is not just another Australian rock band, no mediocrity here, and their honest, hard-working approach looks like paying off.\" While \"the range of styles tackled and done convincingly, from hard rock to blues, boogie, rhythm and blues, is where the appeal lies.\"\n\nInfluences from blues and early rock n' roll was broadly apparent, fostered by the love of those styles by Moss, Barnes and Walker. Small and Prestwich contributed strong pop sensibilities. This allowed volatile rock songs like \"You Got Nothing I Want\" and \"Merry-Go-Round\" to stand beside thoughtful ballads like \"Choirgirl\", pop-flavoured love songs like \"My Baby\" and caustic political statements like \"Star Hotel\", an attack on the late 1970s government of Malcolm Fraser, inspired by the Star Hotel riot in Newcastle.\n\nThe songs were not overtly political but rather observations of everyday life within Australian society and culture, in which the members with their various backgrounds (Moss was from Alice Springs, Walker grew up in rural New South Wales, Barnes and Prestwich were working-class immigrants from the UK) were quite well able to provide.\n\nCold Chisel's songs were about distinctly Australian experiences, a factor often cited as a major reason for the band's lack of international appeal. \"Saturday Night\" and \"Breakfast at Sweethearts\" were observations of the urban experience of Sydney's Kings Cross district where Walker lived for many years. \"Misfits\", which featured on the b-side to \"My Baby\", was about homeless kids in the suburbs surrounding Sydney. Songs like \"Shipping Steel\" and \"Standing on The Outside\" were working class anthems and many others featured characters trapped in mundane, everyday existences, yearning for the good times of the past (\"Flame Trees\") or for something better from life (\"Bow River\").\n\nAlongside contemporaries like The Angels and Midnight Oil, Cold Chisel was renowned as one of the most dynamic live acts of their day and from early in their career concerts routinely became sell-out events. But the band was also famous for its wild lifestyle, particularly the hard-drinking Barnes, who played his role as one of the wild men of Australian rock to the hilt, never seen on stage without at least one bottle of vodka and often so drunk he could barely stand upright. Despite this, by 1982 he was a devoted family man who refused to tour without his wife and daughter. All the other band members were also settled or married; Ian Moss had a long-term relationship with the actress, Megan Williams, (she even sang on \"Twentieth Century\") whose own public persona could have hardly been more different.\n\nIt was the band's public image that often had them compared less favourably with other important acts like Midnight Oil, whose music and politics (while rather more overt) were often similar but whose image and reputation was more clean-cut. Cold Chisel remained hugely popular however and by the mid-1990s they continued to sell records at such a consistent rate they became the first Australian band to achieve higher sales after their split than during their active years.\n\nAt the ARIA Music Awards of 1993 they were inducted into the Hall of Fame. While repackages and compilations accounted for much of these sales, 1994's \"Teenage Love\" provided two of its singles, which were top ten hits. When the group finally reformed in 1998 the resultant album was also a major hit and the follow-up tour sold out almost immediately. In 2001 Australasian Performing Right Association (APRA), listed their single, \"Khe Sanh\" (May 1978), at No. 8 of the all-time best Australian songs.\n\nCold Chisel were one of the first Australian acts to have become the subject of a major tribute album. In 2007, \"Standing on the Outside: The Songs of Cold Chisel\" was released, featuring a collection of the band's songs as performed by artists including The Living End, Evermore, Something for Kate, Pete Murray, Katie Noonan, You Am I, Paul Kelly, Alex Lloyd, Thirsty Merc and Ben Lee, many of whom were children when Cold Chisel first disbanded and some, like the members of Evermore, had not even been born. \"Circus Animals\" was listed at No. 4 in the book, \"100 Best Australian Albums\" (October 2010), while \"East\" appeared at No. 53. They won The Ted Albert Award for Outstanding Services to Australian Music at the APRA Music Awards of 2016.\n\nCurrent members\nBULLET::::- Ian Moss – lead guitar, vocals\nBULLET::::- Don Walker – keyboards, backing vocals\nBULLET::::- Jimmy Barnes – vocals, guitar\nBULLET::::- Phil Small – bass guitar, backing vocals\nBULLET::::- Charley Drayton – drums, backing vocals\n\nFormer members\nBULLET::::- Steve Prestwich – drums, backing vocals\nBULLET::::- Les Kaczmarek – bass guitar\nBULLET::::- John Swan – percussion, backing vocals\nBULLET::::- Ray Arnott – drums\n\nAdditional musicians\nBULLET::::- Dave Blight – harmonica\nBULLET::::- Billy Rodgers – saxophone\nBULLET::::- Jimmy Sloggett – saxophone\nBULLET::::- Andy Bickers – saxophone\nBULLET::::- Renée Geyer – backing vocals\nBULLET::::- Venetta Fields – backing vocals\nBULLET::::- Megan Williams – backing vocals\nBULLET::::- Peter Walker – acoustic guitar\nBULLET::::- Joe Camilleri – saxophone\nBULLET::::- Wilbur Wilde – saxophone\n\nBULLET::::- \"Cold Chisel\" (1978)\nBULLET::::- \"Breakfast at Sweethearts\" (1979)\nBULLET::::- \"East\" (1980)\nBULLET::::- \"Circus Animals\" (1982)\nBULLET::::- \"Twentieth Century\" (1984)\nBULLET::::- \"The Last Wave of Summer\" (1998)\nBULLET::::- \"No Plans\" (2012)\nBULLET::::- \"The Perfect Crime\" (2015)\nBULLET::::- \"Blood Moon\" (2019)\n\nBULLET::::- Timeline of trends in Australian music\nBULLET::::- ARIA Hall of Fame\n\nBULLET::::- General\nBULLET::::- Note: Archived [on-line] copy has limited functionality.\nBULLET::::- Specific\n"}
{"id": "7023", "url": "https://en.wikipedia.org/wiki?curid=7023", "title": "Confederate States of America", "text": "Confederate States of America\n\nThe Confederate States of America (CSA or C.S.)—commonly referred to as the Confederacy—was an unrecognized republic in North America that existed from 1861 to 1865. The Confederacy was originally formed by seven secessionist slave-holding states—South Carolina, Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas—in the Lower South region of the United States, whose economy was heavily dependent upon agriculture, particularly cotton, and a plantation system that relied upon the labor of African-American slaves. Convinced that the institution of slavery was threatened by the November 1860 election of Republican candidate Abraham Lincoln to the U.S. presidency on a platform which opposed the expansion of slavery into the western territories, the Confederacy declared its secession in rebellion to the United States, with the loyal states becoming known as the Union during the ensuing American Civil War. Confederate Vice President Alexander H. Stephens described its ideology as being centrally based \"upon the great truth that the negro is not equal to the white man; that slavery, subordination to the superior race, is his natural and normal condition\".\n\nBefore Lincoln took office in March, a new Confederate government was established in February 1861 which was considered illegal by the government of the United States. States volunteered militia units, and the new government hastened to form its own Confederate States Army from nothing practically overnight. After the American Civil War began in April, four slave states of the Upper South—Virginia, Arkansas, Tennessee, and North Carolina—also seceded and joined the Confederacy. The Confederacy later accepted Missouri and Kentucky as members, although neither officially declared secession nor were they ever largely controlled by Confederate forces; Confederate shadow governments attempted to control the two states but were later exiled from them. The government of the United States (the Union) rejected the claims of secession, considering it illegitimate.\n\nThe war began April 12, 1861, when the Confederates attacked Fort Sumter, a Union fort in the harbor of Charleston, South Carolina. No foreign government ever officially recognized the Confederacy as an independent country, although Great Britain and France granted it belligerent status, which allowed Confederate agents to contract with private concerns for arms and other supplies. In early 1865, after four years of heavy fighting which led to 620,000–850,000 military deaths, all Confederate forces surrendered. The war lacked a formal end; nearly all Confederate forces had been forced into surrender or deliberately disbanded by the end of 1865, by which point the dwindling manpower and resources of the Confederacy faced overwhelming odds. Jefferson Davis, the President of the Confederate States of America for the duration of the civil war, lamented that the Confederacy had \"disappeared\".\n\nAfter the war, Confederate states were readmitted to the Union during the Reconstruction era, after each ratified the 13th Amendment to the U.S. Constitution, which outlawed slavery. \"Lost Cause\" ideology—a view that the Confederate cause was a just one—emerged in the decades after the war among former Confederate generals and politicians, as well as organizations such as the Sons of Confederate Veterans and the United Daughters of the Confederacy. Particularly intense periods of Lost Cause activity came around the time of World War I, as the last Confederate veterans began to die and a push was made to preserve their memories, and during the Civil Rights Movement of the 1950s and 1960s, in reaction to growing public support for racial equality. Through activities such as building prominent Confederate monuments and writing school history textbooks, they sought to ensure future generations of Southern whites would continue to support white supremacist policies such as Jim Crow. The modern display of flags used by and associated with the Confederate States of America primarily started in the mid-20th century and has continued into the present day; their revival in the 1950s and 1960s began with Senator Strom Thurmond's Dixiecrats to show opposition to the Civil Rights Movement, among other things, in 1948.\n\nOn February 22, 1862, the Confederate Constitution of seven state signatories – Mississippi, South Carolina, Florida, Alabama, Georgia, Louisiana, and Texas – replaced the Provisional Constitution of February 8, 1861, with one stating in its preamble a desire for a \"permanent federal government\". Four additional slave-holding states – Virginia, Arkansas, Tennessee, and North Carolina – declared their secession and joined the Confederacy following a call by U.S. President Abraham Lincoln for troops from each state to recapture Sumter and other seized federal properties in the South.\n\nMissouri and Kentucky were represented by partisan factions adopting the forms of state governments without control of substantial territory or population in either case. The antebellum state governments in both maintained their representation in the Union. Also fighting for the Confederacy were two of the \"Five Civilized Tribes\" – the Choctaw and the Chickasaw – in Indian Territory and a new, but uncontrolled, Confederate Territory of Arizona. Efforts by certain factions in Maryland to secede were halted by federal imposition of martial law; Delaware, though of divided loyalty, did not attempt it. A Unionist government was formed in opposition to the secessionist state government in Richmond and administered the western parts of Virginia that had been occupied by Federal troops. The Restored Government of Virginia later recognized the new state of West Virginia, which was admitted to the Union during the war on June 20, 1863, and relocated to Alexandria for the rest of the war.\n\nConfederate control over its claimed territory and population in congressional districts steadily shrank from three-quarters to a third during the course of the American Civil War due to the Union's successful overland campaigns, its control of inland waterways into the South, and its blockade of the southern coast. With the Emancipation Proclamation on January 1, 1863, the Union made abolition of slavery a war goal (in addition to reunion). As Union forces moved southward, large numbers of plantation slaves were freed. Many joined the Union lines, enrolling in service as soldiers, teamsters and laborers. The most notable advance was Sherman's \"March to the Sea\" in late 1864. Much of the Confederacy's infrastructure was destroyed, including telegraphs, railroads and bridges. Plantations in the path of Sherman's forces were severely damaged. Internal movement within the Confederacy became increasingly difficult, weakening its economy and limiting army mobility.\n\nThese losses created an insurmountable disadvantage in men, materiel, and finance. Public support for Confederate President Jefferson Davis's administration eroded over time due to repeated military reverses, economic hardships, and allegations of autocratic government. After four years of campaigning, Richmond was captured by Union forces in April 1865. A few days later General Robert E. Lee surrendered to Union General Ulysses S. Grant, effectively signalling the collapse of the Confederacy. President Davis was captured on May 10, 1865, and jailed for treason, but no trial was ever held.\n\nThe initial Confederacy was established in the Montgomery Convention in February 1861 by seven states (South Carolina, Mississippi, Alabama, Florida, Georgia, Louisiana, adding Texas in March before Lincoln's inauguration), expanded in May–July 1861 (with Virginia, Arkansas, Tennessee, North Carolina), and was disintegrated in April–May 1865. It was formed by delegations from seven slave states of the Lower South that had proclaimed their secession from the Union. After the fighting began in April, four additional slave states seceded and were admitted. Later, two slave states (Missouri and Kentucky) and two territories were given seats in the Confederate Congress. Southern California, although having some pro-Confederate sentiment, was never organized as a territory.\n\nMany Southern whites had considered themselves more Southern than American and were prepared to fight for their state and their region to be independent of the larger nation. That regionalism became Southern nationalism, or \"the Cause\". For the duration of its existence, the Confederacy underwent trial by war. The \"Southern Cause\" transcended the ideology of states' rights, tariff policy, and internal improvements. This \"Cause\" supported, or derived from, cultural and financial dependence on the South's slavery-based economy. The convergence of race and slavery, politics, and economics raised almost all South-related policy questions to the status of moral questions over way of life, commingling love of things Southern and hatred of things Northern. Not only did national political parties split, but national churches and interstate families as well divided along sectional lines as the war approached. According to historian John M. Coski,\nSouthern Democrats had chosen John Breckinridge as their candidate during the U.S. presidential election of 1860, but in no Southern state (other than South Carolina, where the legislature chose the electors) was support for him unanimous; all the other states recorded at least some popular votes for one or more of the other three candidates (Abraham Lincoln, Stephen A. Douglas and John Bell). Support for these candidates, collectively, ranged from significant to an outright majority, with extremes running from 25% in Texas to 81% in Missouri. There were minority views everywhere, especially in the upland and plateau areas of the South, being particularly concentrated in western Virginia and eastern Tennessee.\n\nFollowing South Carolina's unanimous 1860 secession vote, no other Southern states considered the question until 1861, and when they did none had a unanimous vote. All had residents who cast significant numbers of Unionist votes in either the legislature, conventions, popular referendums, or in all three. Voting to remain in the Union did not necessarily mean that individuals were sympathizers with the North. Once hostilities began, many of these who voted to remain in the Union, particularly in the Deep South, accepted the majority decision, and supported the Confederacy.\n\nMany writers have evaluated the Civil War as an American tragedy—a \"Brothers' War\", pitting \"brother against brother, father against son, kin against kin of every degree\".\n\nAccording to historian Avery O. Craven in 1950, the Confederate States of America nation, as a state power, was created by secessionists in Southern slave states, who believed that the federal government was making them second-class citizens and refused to honor their belief – that slavery was beneficial to the Negro. They judged the agents of change to be abolitionists and anti-slavery elements in the Republican Party, whom they believed used repeated insult and injury to subject them to intolerable \"humiliation and degradation\". The \"Black Republicans\" (as the Southerners called them) and their allies soon dominated the U.S. House, Senate, and Presidency. On the U.S. Supreme Court, Chief Justice Roger B. Taney (a presumed supporter of slavery) was 83 years old and ailing.\n\nDuring the campaign for president in 1860, some secessionists threatened disunion should Lincoln (who opposed the expansion of slavery into the territories) be elected, including William L. Yancey. Yancey toured the North calling for secession as Stephen A. Douglas toured the South calling for union in the event of Lincoln's election. To the Secessionists the Republican intent was clear: to contain slavery within its present bounds and, eventually, to eliminate it entirely. A Lincoln victory presented them with a momentous choice (as they saw it), even before his inauguration – \"the Union without slavery, or slavery without the Union\".\n\nThe immediate catalyst for secession was the victory of the Republican Party and the election of Abraham Lincoln as president in the 1860 elections. American Civil War historian James M. McPherson suggested that, for Southerners, the most ominous feature of the Republican victories in the congressional and presidential elections of 1860 was the magnitude of those victories: Republicans captured over 60 percent of the Northern vote and three-fourths of its Congressional delegations. The Southern press said that such Republicans represented the anti-slavery portion of the North, \"a party founded on the single sentiment ... of hatred of African slavery\", and now the controlling power in national affairs. The \"Black Republican party\" could overwhelm conservative Yankees. \"The New Orleans Delta\" said of the Republicans, \"It is in fact, essentially, a revolutionary party\" to overthrow slavery.\n\nBy 1860, sectional disagreements between North and South concerned primarily the maintenance or expansion of slavery in the United States. Historian Drew Gilpin Faust observed that \"leaders of the secession movement across the South cited slavery as the most compelling reason for southern independence\". Although most white Southerners did not own slaves, the majority supported the institution of slavery and benefited indirectly from the slave society. For struggling yeomen and subsistence farmers, the slave society provided a large class of people ranked lower in the social scale than themselves. Secondary differences related to issues of free speech, runaway slaves, expansion into Cuba, and states' rights.\n\nHistorian Emory Thomas assessed the Confederacy's self-image by studying correspondence sent by the Confederate government in 1861–62 to foreign governments. He found that Confederate diplomacy projected multiple contradictory self-images:\n\nIn what later became known as the Cornerstone Speech, Confederate Vice President Alexander H. Stephens declared that the \"cornerstone\" of the new government \"rest[ed] upon the great truth that the negro is not equal to the white man; that slavery – subordination to the superior race – is his natural and normal condition. This, our new government, is the first, in the history of the world, based upon this great physical, philosophical, and moral truth\". After the war Stephens tried to qualify his remarks, claiming they were extemporaneous, metaphorical, and intended to refer to public sentiment rather than \"the principles of the new Government on this subject\".\n\nFour of the seceding states, the Deep South states of South Carolina,\nMississippi, Georgia, and Texas, issued formal declarations of the causes of their decision, each of which identified the threat to slaveholders' rights as the cause of, or a major cause of, secession. Georgia also claimed a general Federal policy of favoring Northern over Southern economic interests. Texas mentioned slavery 21 times, but also listed the failure of the federal government to live up to its obligations, in the original annexation agreement, to protect settlers along the exposed western frontier. Texas resolutions further stated that governments of the states and the nation were established \"exclusively by the white race, for themselves and their posterity\". They also stated that although equal civil and political rights applied to all white men, they did not apply to those of the \"African race\", further opining that the end of racial enslavement would \"bring inevitable calamities upon both [races] and desolation upon the fifteen slave-holding states\".\n\nAlabama did not provide a separate declaration of causes. Instead, the Alabama ordinance stated \"the election of Abraham Lincoln ... by a sectional party, avowedly hostile to the domestic institutions and to the peace and security of the people of the State of Alabama, preceded by many and dangerous infractions of the Constitution of the United States by many of the States and people of the northern section, is a political wrong of so insulting and menacing a character as to justify the people of the State of Alabama in the adoption of prompt and decided measures for their future peace and security\". The ordinance invited \"the slaveholding States of the South, who may approve such purpose, in order to frame a provisional as well as a permanent Government upon the principles of the Constitution of the United States\" to participate in a February 4, 1861 convention in Montgomery, Alabama.\n\nThe secession ordinances of the remaining two states, Florida and Louisiana, simply declared their severing ties with the federal Union, without stating any causes. Afterward, the Florida secession convention formed a committee to draft a declaration of causes, but the committee was discharged before completion of the task. Only an undated, untitled draft remains.\n\nFour of the Upper South states (Virginia, Arkansas, Tennessee, and North Carolina) rejected secession until after the clash at Ft. Sumter. Virginia's ordinance stated a kinship with the slave-holding states of the Lower South, but did not name the institution itself as a primary reason for its course.\n\nArkansas's secession ordinance encompassed a strong objection to the use of military force to preserve the Union as its motivating reason. Prior to the outbreak of war, the Arkansas Convention had on March 20 given as their first resolution: \"The people of the Northern States have organized a political party, purely sectional in its character, the central and controlling idea of which is hostility to the institution of African slavery, as it exists in the Southern States; and that party has elected a President ... pledged to administer the Government upon principles inconsistent with the rights and subversive of the interests of the Southern States.\"\n\nNorth Carolina and Tennessee limited their ordinances to simply withdrawing, although Tennessee went so far as to make clear they wished to make no comment at all on the \"abstract doctrine of secession\".\n\nIn a message to the Confederate Congress on April 29, 1861 Jefferson Davis cited both the tariff and slavery for the South's secession.\n\nThe pro-slavery \"Fire-Eaters\" group of Southern Democrats, calling for immediate secession, were opposed by two factions. \"Cooperationists\" in the Deep South would delay secession until several states left the union, perhaps in a Southern Convention. Under the influence of men such as Texas Governor Sam Houston, delay would have the effect of sustaining the Union. \"Unionists\", especially in the Border South, often former Whigs, appealed to sentimental attachment to the United States. Southern Unionists' favorite presidential candidate was John Bell of Tennessee, sometimes running under an \"Opposition Party\" banner.\n\nMany secessionists were active politically. Governor William Henry Gist of South Carolina corresponded secretly with other Deep South governors, and most southern governors exchanged clandestine commissioners. Charleston's secessionist \"1860 Association\" published over 200,000 pamphlets to persuade the youth of the South. The most influential were: \"The Doom of Slavery\" and \"The South Alone Should Govern the South\", both by John Townsend of South Carolina; and James D. B. De Bow's \"The Interest of Slavery of the Southern Non-slaveholder\".\n\nDevelopments in South Carolina started a chain of events. The foreman of a jury refused the legitimacy of federal courts, so Federal Judge Andrew Magrath ruled that U.S. judicial authority in South Carolina was vacated. A mass meeting in Charleston celebrating the Charleston and Savannah railroad and state cooperation led to the South Carolina legislature to call for a Secession Convention. U.S. Senator James Chesnut, Jr. resigned, as did Senator James Henry Hammond.\n\nElections for Secessionist conventions were heated to \"an almost raving pitch, no one dared dissent\", according to historian William W. Freehling. Even once–respected voices, including the Chief Justice of South Carolina, John Belton O'Neall, lost election to the Secession Convention on a Cooperationist ticket. Across the South mobs expelled Yankees and (in Texas) executed German-Americans suspected of loyalty to the United States. Generally, seceding conventions which followed did not call for a referendum to ratify, although Texas, Arkansas, and Tennessee did, as well as Virginia's second convention. Kentucky declared neutrality, while Missouri had its own civil war until the Unionists took power and drove the Confederate legislators out of the state.\n\nIn the antebellum months, the Corwin Amendment was an unsuccessful attempt by the Congress to bring the seceding states back to the Union and to convince the border slave states to remain. It was a proposed amendment to the United States Constitution by Ohio Congressman Thomas Corwin that would shield \"domestic institutions\" of the states (which in 1861 included slavery) from the constitutional amendment process and from abolition or interference by Congress.\n\nIt was passed by the 36th Congress on March 2, 1861. The House approved it by a vote of 133 to 65 and the United States Senate adopted it, with no changes, on a vote of 24 to 12. It was then submitted to the state legislatures for ratification. In his inaugural address Lincoln endorsed the proposed amendment.\n\nThe text was as follows:\n\nHad it been ratified by the required number of states prior to 1865, it would have made institutionalized slavery immune to the constitutional amendment procedures and to interference by Congress.\n\nThe first secession state conventions from the Deep South sent representatives to meet at the Montgomery Convention in Montgomery, Alabama, on February 4, 1861. There the fundamental documents of government were promulgated, a provisional government was established, and a representative Congress met for the Confederate States of America.\n\nThe new 'provisional' Confederate President Jefferson Davis issued a call for 100,000 men from the various states' militias to defend the newly formed Confederacy. All Federal property was seized, along with gold bullion and coining dies at the U.S. mints in Charlotte, North Carolina; Dahlonega, Georgia; and New Orleans. The Confederate capital was moved from Montgomery to Richmond, Virginia, in May 1861. On February 22, 1862, Davis was inaugurated as president with a term of six years.\n\nThe newly inaugurated Confederate administration pursued a policy of national territorial integrity, continuing earlier state efforts in 1860 and early 1861 to remove U.S. government presence from within their boundaries. These efforts included taking possession of U.S. courts, custom houses, post offices, and most notably, arsenals and forts. But after the Confederate attack and capture of Fort Sumter in April 1861, Lincoln called up 75,000 of the states' militia to muster under his command. The stated purpose was to re-occupy U.S. properties throughout the South, as the U.S. Congress had not authorized their abandonment. The resistance at Fort Sumter signaled his change of policy from that of the Buchanan Administration. Lincoln's response ignited a firestorm of emotion. The people of both North and South demanded war, and young men rushed to their colors in the hundreds of thousands. Four more states (Virginia, North Carolina, Tennessee, and Arkansas) refused Lincoln's call for troops and declared secession, while Kentucky maintained an uneasy \"neutrality\".\n\nSecessionists argued that the United States Constitution was a contract among sovereign states that could be abandoned at any time without consultation and that each state had a right to secede. After intense debates and statewide votes, seven Deep South cotton states passed secession ordinances by February 1861 (before Abraham Lincoln took office as president), while secession efforts failed in the other eight slave states. Delegates from those seven formed the CSA in February 1861, selecting Jefferson Davis as the provisional president. Unionist talk of reunion failed and Davis began raising a 100,000 man army.\n\nInitially, some secessionists may have hoped for a peaceful departure. Moderates in the Confederate Constitutional Convention included a provision against importation of slaves from Africa to appeal to the Upper South. Non-slave states might join, but the radicals secured a two-thirds requirement in both houses of Congress to accept them.\n\nSeven states declared their secession from the United States before Lincoln took office on March 4, 1861. After the Confederate attack on Fort Sumter April 12, 1861, and Lincoln's subsequent call for troops on April 15, four more states declared their secession:\n\nKentucky declared neutrality but after Confederate troops moved in, the state government asked for Union troops to drive them out. The splinter Confederate state government relocated to accompany western Confederate armies and never controlled the state population. By the end of the war, 90,000 Kentuckians had fought on the side of the Union, compared to 35,000 for the Confederate States.\n\nIn Missouri, a constitutional convention was approved and delegates elected by voters. The convention rejected secession 89–1 on March 19, 1861. The governor maneuvered to take control of the St. Louis Arsenal and restrict Federal movements. This led to confrontation, and in June Federal forces drove him and the General Assembly from Jefferson City. The executive committee of the constitutional convention called the members together in July. The convention declared the state offices vacant, and appointed a Unionist interim state government. The exiled governor called a rump session of the former General Assembly together in Neosho and, on October 31, 1861, passed an ordinance of secession. It is still a matter of debate as to whether a quorum existed for this vote. The Confederate state government was unable to control very much the Missouri territory. It had its capital first at Neosho, then at Cassville, before being driven out of the state. For the remainder of the war, it operated as a government in exile at Marshall, Texas.\n\nNeither Kentucky nor Missouri was declared in rebellion in Lincoln's Emancipation Proclamation. The Confederacy recognized the pro-Confederate claimants in both Kentucky (December 10, 1861) and Missouri (November 28, 1861) and laid claim to those states, granting them Congressional representation and adding two stars to the Confederate flag. Voting for the representatives was mostly done by Confederate soldiers from Kentucky and Missouri.\n\nThe order of secession resolutions and dates are:\n\nIn Virginia, the populous counties along the Ohio and Pennsylvania borders rejected the Confederacy. Unionists held a Convention in Wheeling in June 1861, establishing a \"restored government\" with a rump legislature, but sentiment in the region remained deeply divided. In the 50 counties that would make up the state of West Virginia, voters from 24 counties had voted for disunion in Virginia's May 23 referendum on the ordinance of secession. In the 1860 Presidential election \"Constitutional Democrat\" Breckenridge had outpolled \"Constitutional Unionist\" Bell in the 50 counties by 1,900 votes, 44% to 42%. Regardless of scholarly disputes over election procedures and results county by county, altogether they simultaneously supplied over 20,000 soldiers to each side of the conflict. Representatives for most of the counties were seated in both state legislatures at Wheeling and at Richmond for the duration of the war.\n\nAttempts to secede from the Confederacy by some counties in East Tennessee were checked by martial law. Although slave-holding Delaware and Maryland did not secede, citizens from those states exhibited divided loyalties. Regiments of Marylanders fought in Lee's Army of Northern Virginia. But overall, 24,000 men from Maryland joined the Confederate armed forces, compared to 63,000 who joined Union forces.\n\nDelaware never produced a full regiment for the Confederacy, but neither did it emancipate slaves as did Missouri and West Virginia. District of Columbia citizens made no attempts to secede and through the war years, referendums sponsored by President Lincoln approved systems of compensated emancipation and slave confiscation from \"disloyal citizens\".\n\nCitizens at Mesilla and Tucson in the southern part of New Mexico Territory formed a secession convention, which voted to join the Confederacy on March 16, 1861, and appointed Dr. Lewis S. Owings as the new territorial governor. They won the Battle of Mesilla and established a territorial government with Mesilla serving as its capital. The Confederacy proclaimed the Confederate Arizona Territory on February 14, 1862, north to the 34th parallel. Marcus H. MacWillie served in both Confederate Congresses as Arizona's delegate. In 1862 the Confederate New Mexico Campaign to take the northern half of the U.S. territory failed and the Confederate territorial government in exile relocated to San Antonio, Texas.\n\nConfederate supporters in the trans-Mississippi west also claimed portions of United States Indian Territory after the United States evacuated the federal forts and installations. Over half of the American Indian troops participating in the Civil War from the Indian Territory supported the Confederacy; troops and one general were enlisted from each tribe. On July 12, 1861, the Confederate government signed a treaty with both the Choctaw and Chickasaw Indian nations. After several battles Union armies took control of the territory.\n\nThe Indian Territory never formally joined the Confederacy, but it did receive representation in the Confederate Congress. Many Indians from the Territory were integrated into regular Confederate Army units. After 1863 the tribal governments sent representatives to the Confederate Congress: Elias Cornelius Boudinot representing the Cherokee and Samuel Benton Callahan representing the Seminole and Creek people. The Cherokee Nation, aligned with the Confederacy. They practiced and supported slavery, opposed abolition, and feared their lands would be seized by the Union. After the war, the Indian territory was disestablished, their black slaves were freed, and the tribes lost some of their lands.\n\nMontgomery, Alabama served as the capital of the Confederate States of America from February 4 until May 29, 1861, in the Alabama State Capitol. Six states created the Confederate States of America there on February 8, 1861. The Texas delegation was seated at the time, so it is counted in the \"original seven\" states of the Confederacy; it had no roll call vote until after its referendum made secession \"operative\". Two sessions of the Provisional Congress were held in Montgomery, adjourning May 21. The Permanent Constitution was adopted there on March 12, 1861.\n\nThe permanent capital provided for in the Confederate Constitution called for a state cession of a ten-miles square (100 square mile) district to the central government. Atlanta, which had not yet supplanted Milledgeville, Georgia as its state capital, put in a bid noting its central location and rail connections, as did Opelika, Alabama, noting its strategically interior situation, rail connections and nearby deposits of coal and iron.\n\nRichmond, Virginia was chosen for the interim capital at the Virginia State Capitol. The move was used by Vice President Stephens and others to encourage other border states to follow Virginia into the Confederacy. In the political moment it was a show of \"defiance and strength\". The war for southern independence was surely to be fought in Virginia, but it also had the largest Southern military-aged white population, with infrastructure, resources and supplies required to sustain a war. The Davis Administration's policy was that, \"It must be held at all hazards.\"\n\nThe naming of Richmond as the new capital took place on May 30, 1861, and the last two sessions of the Provisional Congress were held in the new capital. The Permanent Confederate Congress and President were elected in the states and army camps on November 6, 1861. The First Congress met in four sessions in Richmond from February 18, 1862, to February 17, 1864. The Second Congress met there in two sessions, from May 2, 1864, to March 18, 1865.\n\nAs war dragged on, Richmond became crowded with training and transfers, logistics and hospitals. Prices rose dramatically despite government efforts at price regulation. A movement in Congress led by Henry S. Foote of Tennessee argued for moving the capital from Richmond. At the approach of Federal armies in mid-1862, the government's archives were readied for removal. As the Wilderness Campaign progressed, Congress authorized Davis to remove the executive department and call Congress to session elsewhere in 1864 and again in 1865. Shortly before the end of the war, the Confederate government evacuated Richmond, planning to relocate farther south. Little came of these plans before Lee's surrender at Appomattox Court House, Virginia on April 9, 1865. Davis and most of his cabinet fled to Danville, Virginia, which served as their headquarters for about a week.\n\nUnionism was widespread in the Confederacy, especially in the mountain regions of Appalachia and the Ozarks. Unionists, led by Parson Brownlow and Senator Andrew Johnson, took control of eastern Tennessee in 1863. Unionists also attempted control over western Virginia but never effectively held more than half the counties that formed the new state of West Virginia. Union forces captured parts of coastal North Carolina, and at first were welcomed by local unionists. That changed as the occupiers became perceived as oppressive, callous, radical and favorable to the Freedmen. Occupiers engaged in pillaging, freeing of slaves, and eviction of those refusing to take or reneging on the loyalty oaths, as ex-Unionists began to support the Confederate cause.\n\nSupport for the Confederacy was perhaps weakest in Texas; Claude Elliott estimates that only a third of the population actively supported the Confederacy. Many unionists supported the Confederacy after the war began, but many others clung to their unionism throughout the war, especially in the northern counties, the German districts, and the Mexican areas. According to Ernest Wallace: \"This account of a dissatisfied Unionist minority, although historically essential, must be kept in its proper perspective, for throughout the war the overwhelming majority of the people zealously supported the Confederacy ...\" Randolph B. Campbell states, \"In spite of terrible losses and hardships, most Texans continued throughout the war to support the Confederacy as they had supported secession\". Dale Baum in his analysis of Texas politics in the era counters: \"This idea of a Confederate Texas united politically against northern adversaries was shaped more by nostalgic fantasies than by wartime realities.\" He characterizes Texas Civil War history as \"a morose story of intragovernmental rivalries coupled with wide-ranging disaffection that prevented effective implementation of state wartime policies\".\n\nIn Texas local officials harassed unionists and engaged in large-scale massacres against unionists and Germans. In Cooke County 150 suspected unionists were arrested; 25 were lynched without trial and 40 more were hanged after a summary trial. Draft resistance was widespread especially among Texans of German or Mexican descent; many of the latter went to Mexico. Potential draftees went into hiding, Confederate officials hunted them down, and many were shot.\n\nCivil liberties were of small concern in North and South. Lincoln and Davis both took a hard line against dissent. Neely explores how the Confederacy became a virtual police state with guards and patrols all about, and a domestic passport system whereby everyone needed official permission each time they wanted to travel. Over 4,000 suspected unionists were imprisoned without trial.\n\nDuring the four years of its existence under trial by war, the Confederate States of America asserted its independence and appointed dozens of diplomatic agents abroad. None were ever officially recognized by a foreign government. The United States government regarded the Southern states as being in rebellion or insurrection and so refused any formal recognition of their status.\n\nEven before Fort Sumter, U.S. Secretary of State William H. Seward issued formal instructions to the American minister to Britain, Charles Francis Adams:\nSeward instructed Adams that if the British government seemed inclined to recognize the Confederacy, or even waver in that regard, it was to receive a sharp warning, with a strong hint of war:\nThe United States government never declared war on those \"kindred and countrymen\" in the Confederacy, but conducted its military efforts beginning with a presidential proclamation issued April 15, 1861. It called for troops to recapture forts and suppress what Lincoln later called an \"insurrection and rebellion\".\n\nMid-war parleys between the two sides occurred without formal political recognition, though the laws of war predominantly governed military relationships on both sides of uniformed conflict.\n\nOn the part of the Confederacy, immediately following Fort Sumter the Confederate Congress proclaimed that \"war exists between the Confederate States and the Government of the United States, and the States and Territories thereof\". A state of war was not to formally exist between the Confederacy and those states and territories in the United States allowing slavery, although Confederate Rangers were compensated for destruction they could effect there throughout the war.\nConcerning the international status and nationhood of the Confederate States of America, in 1869 the United States Supreme Court in ruled Texas' declaration of secession was legally null and void. Jefferson Davis, former President of the Confederacy, and Alexander H. Stephens, its former Vice-President, both wrote postwar arguments in favor of secession's legality and the international legitimacy of the Government of the Confederate States of America, most notably Davis' \"The Rise and Fall of the Confederate Government\".\n\nOnce war with the United States began, the Confederacy pinned its hopes for survival on military intervention by Great Britain and France. The Confederates who had believed that \"cotton is king\" – that is, that Britain had to support the Confederacy to obtain cotton – proved mistaken. The British had stocks to last over a year and had been developing alternative sources of cotton, most notably India and Egypt. They were not about to go to war with the U.S. to acquire more cotton at the risk of losing the large quantities of food imported from the North. The Confederate government repeatedly sent delegations to Europe, but historians give them low marks for their poor diplomacy. James M. Mason went to London and John Slidell traveled to Paris. They were unofficially interviewed, but neither secured official recognition for the Confederacy.\n\nIn late 1861, the seizure of two senior Confederate diplomats aboard a British ship by the U.S. navy outraged Britain and led to a war scare in the Trent Affair. Queen Victoria insisted on giving the Americans an exit route and Lincoln took it, releasing the two diplomats. Tensions cooled, and the Confederacy gained no advantage. In recent years most historians argue that the risk of actual war over the Trent Affair was small, because it would have hurt both sides.\n\nThroughout the early years of the war, British foreign secretary Lord John Russell, Emperor Napoleon III of France, and, to a lesser extent, British Prime Minister Lord Palmerston, showed interest in recognition of the Confederacy or at least mediation of the war. William Ewart Gladstone, the British Chancellor of the Exchequer (finance minister, in office 1859–1866), whose family wealth was based on slavery, was the key Minister calling for intervention to help the Confederacy achieve independence. He failed to convince prime minister Palmerston. By September 1862 the Union victory at the Battle of Antietam, Lincoln's preliminary Emancipation Proclamation and abolitionist opposition in Britain put an end to these possibilities. The cost to Britain of a war with the U.S. would have been high: the immediate loss of American grain-shipments, the end of British exports to the U.S., and the seizure of billions of pounds invested in American securities. War would have meant higher taxes in Britain, another invasion of Canada, and full-scale worldwide attacks on the British merchant fleet. Outright recognition would have meant certain war with the United States; in mid-1862 fears of race war (as had transpired in the Haitian Revolution of 1791–1804) led to the British considering intervention for humanitarian reasons. Lincoln's Emancipation Proclamation did not lead to interracial violence, let alone a bloodbath, but it did give the friends of the Union strong talking points in the arguments that raged across Britain.\n\nJohn Slidell, the Confederate States emissary to France, did succeed in negotiating a loan of $15,000,000 from Erlanger and other French capitalists. The money went to buy ironclad warships, as well as military supplies that came in with blockade runners. The British government did allow the construction of blockade runners in Britain; they were owned and operated by British financiers and sailors; a few were owned and operated by the Confederacy. The British investors' goal was to get highly profitable cotton.\n\nSeveral European nations maintained diplomats in place who had been appointed to the U.S., but no country appointed any diplomat to the Confederacy. Those nations recognized the Union and Confederate sides as belligerents. In 1863 the Confederacy expelled European diplomatic missions for advising their resident subjects to refuse to serve in the Confederate army. Both Confederate and Union agents were allowed to work openly in British territories. Some state governments in northern Mexico negotiated local agreements to cover trade on the Texas border. Pope Pius IX wrote a letter to Jefferson Davis in which he addressed Davis as the \"Honorable President of the Confederate States of America\". The Confederacy appointed Ambrose Dudley Mann as special agent to the Holy See on September 24, 1863. But the Holy See never released a formal statement supporting or recognizing the Confederacy. In November 1863, Mann met Pope Pius IX in person and received a letter supposedly addressed \"to the Illustrious and Honorable Jefferson Davis, President of the Confederate States of America\"; Mann had mistranslated the address. In his report to Richmond, Mann claimed a great diplomatic achievement for himself, asserting the letter was \"a positive recognition of our Government\". The letter was indeed used in propaganda, but Confederate Secretary of State Judah P. Benjamin told Mann it was \"a mere inferential recognition, unconnected with political action or the regular establishment of diplomatic relations\" and thus did not assign it the weight of formal recognition.\n\nNevertheless, the Confederacy was seen internationally as a serious attempt at nationhood, and European governments sent military observers, both official and unofficial, to assess whether there had been a \"de facto\" establishment of independence. These observers included Arthur Lyon Fremantle of the British Coldstream Guards, who entered the Confederacy via Mexico, Fitzgerald Ross of the Austrian Hussars, and Justus Scheibert of the Prussian Army. European travelers visited and wrote accounts for publication. Importantly in 1862, the Frenchman Charles Girard's \"Seven months in the rebel states during the North American War\" testified \"this government ... is no longer a trial government ... but really a normal government, the expression of popular will\".\nFremantle went on to write in his book \"Three Months in the Southern States\" that he had\nFrench Emperor Napoleon III assured Confederate diplomat John Slidell that he would make \"direct proposition\" to Britain for joint recognition. The Emperor made the same assurance to British Members of Parliament John A. Roebuck and John A. Lindsay. Roebuck in turn publicly prepared a bill to submit to Parliament June 30 supporting joint Anglo-French recognition of the Confederacy. \"Southerners had a right to be optimistic, or at least hopeful, that their revolution would prevail, or at least endure.\" Following the dual reverses at Vicksburg and Gettysburg in July 1863, the Confederates \"suffered a severe loss of confidence in themselves\", and withdrew into an interior defensive position. There would be no help from the Europeans.\n\nBy December 1864, Davis considered sacrificing slavery in order to enlist recognition and aid from Paris and London; he secretly sent Duncan F. Kenner to Europe with a message that the war was fought solely for \"the vindication of our rights to self-government and independence\" and that \"no sacrifice is too great, save that of honor\". The message stated that if the French or British governments made their recognition conditional on anything at all, the Confederacy would consent to such terms. Davis's message could not explicitly acknowledge that slavery was on the bargaining table due to still-strong domestic support for slavery among the wealthy and politically influential. European leaders all saw that the Confederacy was on the verge of total defeat.\n\nThe great majority of young white men voluntarily joined Confederate national or state military units. Perman (2010) says historians are of two minds on why millions of men seemed so eager to fight, suffer and die over four years:\nCivil War historian E. Merton Coulter wrote that for those who would secure its independence, \"The Confederacy was unfortunate in its failure to work out a general strategy for the whole war\". Aggressive strategy called for offensive force concentration. Defensive strategy sought dispersal to meet demands of locally minded governors. The controlling philosophy evolved into a combination \"dispersal with a defensive concentration around Richmond\". The Davis administration considered the war purely defensive, a \"simple demand that the people of the United States would cease to war upon us\". Historian James M. McPherson is a critic of Lee's offensive strategy: \"Lee pursued a faulty military strategy that ensured Confederate defeat\".\n\nAs the Confederate government lost control of territory in campaign after campaign, it was said that \"the vast size of the Confederacy would make its conquest impossible\". The enemy would be struck down by the same elements which so often debilitated or destroyed visitors and transplants in the South. Heat exhaustion, sunstroke, endemic diseases such as malaria and typhoid would match the destructive effectiveness of the Moscow winter on the invading armies of Napoleon.\n\nEarly in the war both sides believed that one great battle would decide the conflict; the Confederates won a great victory at the First Battle of Bull Run, also known as First Manassas (the name used by Confederate forces). It drove the Confederate people \"insane with joy\"; the public demanded a forward movement to capture Washington, relocate the Confederate capital there, and admit Maryland to the Confederacy. A council of war by the victorious Confederate generals decided not to advance against larger numbers of fresh Federal troops in defensive positions. Davis did not countermand it. Following the Confederate incursion halted at the Battle of Antietam in October 1862, generals proposed concentrating forces from state commands to re-invade the north. Nothing came of it. Again in mid-1863 at his incursion into Pennsylvania, Lee requested of Davis that Beauregard simultaneously attack Washington with troops taken from the Carolinas. But the troops there remained in place during the Gettysburg Campaign.\n\nThe eleven states of the Confederacy were outnumbered by the North about four to one in white men of military age. It was overmatched far more in military equipment, industrial facilities, railroads for transport, and wagons supplying the front.\n\nConfederate military policy innovated to slow the invaders, but at heavy cost to the Southern infrastructure. The Confederates burned bridges, laid land mines in the roads, and made harbors inlets and inland waterways unusable with sunken mines (called \"torpedoes\" at the time). Coulter reports:\nThe Confederacy relied on external sources for war materials. The first came from trade with the enemy. \"Vast amounts of war supplies\" came through Kentucky, and thereafter, western armies were \"to a very considerable extent\" provisioned with illicit trade via Federal agents and northern private traders. But that trade was interrupted in the first year of war by Admiral Porter's river gunboats as they gained dominance along navigable rivers north–south and east–west. Overseas blockade running then came to be of \"outstanding importance\". On April 17, President Davis called on privateer raiders, the \"militia of the sea\", to make war on U.S. seaborne commerce. Despite noteworthy effort, over the course of the war the Confederacy was found unable to match the Union in ships and seamanship, materials and marine construction.\n\nPerhaps the greatest obstacle to success in the 19th-century warfare of mass armies was the Confederacy's lack of manpower, and sufficient numbers of disciplined, equipped troops in the field at the point of contact with the enemy. During the winter of 1862–63, Lee observed that none of his famous victories had resulted in the destruction of the opposing army. He lacked reserve troops to exploit an advantage on the battlefield as Napoleon had done. Lee explained, \"More than once have most promising opportunities been lost for want of men to take advantage of them, and victory itself had been made to put on the appearance of defeat, because our diminished and exhausted troops have been unable to renew a successful struggle against fresh numbers of the enemy.\"\n\nThe military armed forces of the Confederacy comprised three branches: Army, Navy and Marine Corps.\n\nThe Confederate military leadership included many veterans from the United States Army and United States Navy who had resigned their Federal commissions and had won appointment to senior positions in the Confederate armed forces. Many had served in the Mexican–American War (including Robert E. Lee and Jefferson Davis), but some such as Leonidas Polk (who graduated from West Point but did not serve in the Army) had little or no experience.\n\nThe Confederate officer corps consisted of men from both slave-owning and non-slave-owning families. The Confederacy appointed junior and field grade officers by election from the enlisted ranks. Although no Army service academy was established for the Confederacy, some colleges (such as The Citadel and Virginia Military Institute) maintained cadet corps that trained Confederate military leadership. A naval academy was established at Drewry's Bluff, Virginia in 1863, but no midshipmen graduated before the Confederacy's end.\n\nThe soldiers of the Confederate armed forces consisted mainly of white males aged between 16 and 28. The median year of birth was 1838, so half the soldiers were 23 or older by 1861. In early 1862, the Confederate Army was allowed to disintegrate for two months following expiration of short-term enlistments. A majority of those in uniform would not re-enlist following their one-year commitment, so on April 16, 1862, the Confederate Congress enacted the first mass conscription on the North American continent. (The U.S. Congress followed a year later on March 3, 1863, with the Enrollment Act.) Rather than a universal draft, the initial program was a selective service with physical, religious, professional and industrial exemptions. These were narrowed as the war progressed. Initially substitutes were permitted, but by December 1863 these were disallowed. In September 1862 the age limit was increased from 35 to 45 and by February 1864, all men under 18 and over 45 were conscripted to form a reserve for state defense inside state borders. By March 1864, the Superintendent of Conscription reported that all across the Confederacy, every officer in constituted authority, man and woman, \"engaged in opposing the enrolling officer in the execution of his duties\". Although challenged in the state courts, the Confederate State Supreme Courts routinely rejected legal challenges to conscription.\n\nMany thousands of slaves served as personal servants to their owner, or were hired as laborers, cooks, and pioneers. Some freed blacks and men of color served in local state militia units of the Confederacy, primarily in Louisiana and South Carolina, but their officers deployed them for \"local defense, not combat\". Depleted by casualties and desertions, the military suffered chronic manpower shortages. In early 1865, the Confederate Congress, influenced by the public support by General Lee, approved the recruitment of black infantry units. Contrary to Lee's and Davis's recommendations, the Congress refused \"to guarantee the freedom of black volunteers\". No more than two hundred black combat troops were ever raised.\n\nThe immediate onset of war meant that it was fought by the \"Provisional\" or \"Volunteer Army\". State governors resisted concentrating a national effort. Several wanted a strong state army for self-defense. Others feared large \"Provisional\" armies answering only to Davis. When filling the Confederate government's call for 100,000 men, another 200,000 were turned away by accepting only those enlisted \"for the duration\" or twelve-month volunteers who brought their own arms or horses.\n\nIt was important to raise troops; it was just as important to provide capable officers to command them. With few exceptions the Confederacy secured excellent general officers. Efficiency in the lower officers was \"greater than could have been reasonably expected\". As with the Federals, political appointees could be indifferent. Otherwise, the officer corps was governor-appointed or elected by unit enlisted. Promotion to fill vacancies was made internally regardless of merit, even if better officers were immediately available.\n\nAnticipating the need for more \"duration\" men, in January 1862 Congress provided for company level recruiters to return home for two months, but their efforts met little success on the heels of Confederate battlefield defeats in February. Congress allowed for Davis to require numbers of recruits from each governor to supply the volunteer shortfall. States responded by passing their own draft laws.\n\nThe veteran Confederate army of early 1862 was mostly twelve-month volunteers with terms about to expire. Enlisted reorganization elections disintegrated the army for two months. Officers pleaded with the ranks to re-enlist, but a majority did not. Those remaining elected majors and colonels whose performance led to officer review boards in October. The boards caused a \"rapid and widespread\" thinning out of 1,700 incompetent officers. Troops thereafter would elect only second lieutenants.\n\nIn early 1862, the popular press suggested the Confederacy required a million men under arms. But veteran soldiers were not re-enlisting, and earlier secessionist volunteers did not reappear to serve in war. One Macon, Georgia, newspaper asked how two million brave fighting men of the South were about to be overcome by four million northerners who were said to be cowards.\n\nThe Confederacy passed the first American law of national conscription on April 16, 1862. The white males of the Confederate States from 18 to 35 were declared members of the Confederate army for three years, and all men then enlisted were extended to a three-year term. They would serve only in units and under officers of their state. Those under 18 and over 35 could substitute for conscripts, in September those from 35 to 45 became conscripts. The cry of \"rich man's war and a poor man's fight\" led Congress to abolish the substitute system altogether in December 1863. All principals benefiting earlier were made eligible for service. By February 1864, the age bracket was made 17 to 50, those under eighteen and over forty-five to be limited to in-state duty.\n\nConfederate conscription was not universal; it was a selective service. The First Conscription Act of April 1862 exempted occupations related to transportation, communication, industry, ministers, teaching and physical fitness. The Second Conscription Act of October 1862 expanded exemptions in industry, agriculture and conscientious objection. Exemption fraud proliferated in medical examinations, army furloughs, churches, schools, apothecaries and newspapers.\n\nRich men's sons were appointed to the socially outcast \"overseer\" occupation, but the measure was received in the country with \"universal odium\". The legislative vehicle was the controversial Twenty Negro Law that specifically exempted one white overseer or owner for every plantation with at least 20 slaves. Backpedalling six months later, Congress provided overseers under 45 could be exempted only if they held the occupation before the first Conscription Act. The number of officials under state exemptions appointed by state Governor patronage expanded significantly. By law, substitutes could not be subject to conscription, but instead of adding to Confederate manpower, unit officers in the field reported that over-50 and under-17-year-old substitutes made up to 90% of the desertions.\nThe Conscription Act of February 1864 \"radically changed the whole system\" of selection. It abolished industrial exemptions, placing detail authority in President Davis. As the shame of conscription was greater than a felony conviction, the system brought in \"about as many volunteers as it did conscripts.\" Many men in otherwise \"bombproof\" positions were enlisted in one way or another, nearly 160,000 additional volunteers and conscripts in uniform. Still there was shirking. To administer the draft, a Bureau of Conscription was set up to use state officers, as state Governors would allow. It had a checkered career of \"contention, opposition and futility\". Armies appointed alternative military \"recruiters\" to bring in the out-of-uniform 17–50-year-old conscripts and deserters. Nearly 3,000 officers were tasked with the job. By late 1864, Lee was calling for more troops. \"Our ranks are constantly diminishing by battle and disease, and few recruits are received; the consequences are inevitable.\" By March 1865 conscription was to be administered by generals of the state reserves calling out men over 45 and under 18 years old. All exemptions were abolished. These regiments were assigned to recruit conscripts ages 17–50, recover deserters, and repel enemy cavalry raids. The service retained men who had lost but one arm or a leg in home guards. Ultimately, conscription was a failure, and its main value was in goading men to volunteer.\n\nThe survival of the Confederacy depended on a strong base of civilians and soldiers devoted to victory. The soldiers performed well, though increasing numbers deserted in the last year of fighting, and the Confederacy never succeeded in replacing casualties as the Union could. The civilians, although enthusiastic in 1861–62, seem to have lost faith in the future of the Confederacy by 1864, and instead looked to protect their homes and communities. As Rable explains, \"This contraction of civic vision was more than a crabbed libertarianism; it represented an increasingly widespread disillusionment with the Confederate experiment.\"\n\nThe American Civil War broke out in April 1861 with a Confederate victory at the Battle of Fort Sumter in Charleston.\n\nIn January, President James Buchanan had attempted to resupply the garrison with the steamship, \"Star of the West\", but Confederate artillery drove it away. In March, President Lincoln notified South Carolina Governor Pickens that without Confederate resistance to the resupply there would be no military reinforcement without further notice, but Lincoln prepared to force resupply if it were not allowed. Confederate President Davis, in cabinet, decided to seize Fort Sumter before the relief fleet arrived, and on April 12, 1861, General Beauregard forced its surrender.\n\nFollowing Sumter, Lincoln directed states to provide 75,000 troops for three months to recapture the Charleston Harbor forts and all other federal property. This emboldened secessionists in Virginia, Arkansas, Tennessee and North Carolina to secede rather than provide troops to march into neighboring Southern states. In May, Federal troops crossed into Confederate territory along the entire border from the Chesapeake Bay to New Mexico. The first battles were Confederate victories at Big Bethel (Bethel Church, Virginia), First Bull Run (First Manassas) in Virginia July and in August, Wilson's Creek (Oak Hills) in Missouri. At all three, Confederate forces could not follow up their victory due to inadequate supply and shortages of fresh troops to exploit their successes. Following each battle, Federals maintained a military presence and occupied Washington, DC; Fort Monroe, Virginia; and Springfield, Missouri. Both North and South began training up armies for major fighting the next year. Union General George B. McClellan's forces gained possession of much of northwestern Virginia in mid-1861, concentrating on towns and roads; the interior was too large to control and became the center of guerrilla activity. General Robert E. Lee was defeated at Cheat Mountain in September and no serious Confederate advance in western Virginia occurred until the next year.\n\nMeanwhile, the Union Navy seized control of much of the Confederate coastline from Virginia to South Carolina. It took over plantations and the abandoned slaves. Federals there began a war-long policy of burning grain supplies up rivers into the interior wherever they could not occupy. The Union Navy began a blockade of the major southern ports and prepared an invasion of Louisiana to capture New Orleans in early 1862.\n\nThe victories of 1861 were followed by a series of defeats east and west in early 1862. To restore the Union by military force, the Federal strategy was to (1) secure the Mississippi River, (2) seize or close Confederate ports, and (3) march on Richmond. To secure independence, the Confederate intent was to (1) repel the invader on all fronts, costing him blood and treasure, and (2) carry the war into the North by two offensives in time to affect the mid-term elections.\nMuch of northwestern Virginia was under Federal control.\nIn February and March, most of Missouri and Kentucky were Union \"occupied, consolidated, and used as staging areas for advances further South\". Following the repulse of Confederate counter-attack at the Battle of Shiloh, Tennessee, permanent Federal occupation expanded west, south and east. Confederate forces repositioned south along the Mississippi River to Memphis, Tennessee, where at the naval Battle of Memphis, its River Defense Fleet was sunk. Confederates withdrew from northern Mississippi and northern Alabama. New Orleans was captured April 29 by a combined Army-Navy force under U.S. Admiral David Farragut, and the Confederacy lost control of the mouth of the Mississippi River. It had to concede extensive agricultural resources that had supported the Union's sea-supplied logistics base.\n\nAlthough Confederates had suffered major reverses everywhere, as of the end of April the Confederacy still controlled territory holding 72% of its population. Federal forces disrupted Missouri and Arkansas; they had broken through in western Virginia, Kentucky, Tennessee and Louisiana. Along the Confederacy's shores, Union forces had closed ports and made garrisoned lodgments on every coastal Confederate state except Alabama and Texas. Although scholars sometimes assess the Union blockade as ineffectual under international law until the last few months of the war, from the first months it disrupted Confederate privateers, making it \"almost impossible to bring their prizes into Confederate ports\". British firms developed small fleets of blockade running companies, such as John Fraser and Company, and the Ordnance Department secured its own blockade runners for dedicated munitions cargoes.\n\nDuring the Civil War fleets of armored warships were deployed for the first time in sustained blockades at sea. After some success against the Union blockade, in March the ironclad CSS \"Virginia\" was forced into port and burned by Confederates at their retreat. Despite several attempts mounted from their port cities, CSA naval forces were unable to break the Union blockade. Attempts were made by Commodore Josiah Tattnall's ironclads from Savannah in 1862 with the CSS \"Atlanta\". Secretary of the Navy Stephen Mallory placed his hopes in a European-built ironclad fleet, but they were never realized. On the other hand, four new English-built commerce raiders served the Confederacy, and several fast blockade runners were sold in Confederate ports. They were converted into commerce-raiding cruisers, and manned by their British crews.\n\nIn the east, Union forces could not close on Richmond. General McClellan landed his army on the Lower Peninsula of Virginia. Lee subsequently ended that threat from the east, then Union General John Pope attacked overland from the north only to be repulsed at Second Bull Run (Second Manassas). Lee's strike north was turned back at Antietam MD, then Union Major General Ambrose Burnside's offensive was disastrously ended at Fredericksburg VA in December. Both armies then turned to winter quarters to recruit and train for the coming spring.\n\nIn an attempt to seize the initiative, reprovision, protect farms in mid-growing season and influence U.S. Congressional elections, two major Confederate incursions into Union territory had been launched in August and September 1862. Both Braxton Bragg's invasion of Kentucky and Lee's invasion of Maryland were decisively repulsed, leaving Confederates in control of but 63% of its population. Civil War scholar Allan Nevins argues that 1862 was the strategic high-water mark of the Confederacy. The failures of the two invasions were attributed to the same irrecoverable shortcomings: lack of manpower at the front, lack of supplies including serviceable shoes, and exhaustion after long marches without adequate food. Also in September Confederate General William W. Loring pushed Federal forces from Charleston, Virginia, and the Kanawha Valley in western Virginia, but lacking re-inforcements Loring abandoned his position and by November the region was back in Federal control.\n\nThe failed Middle Tennessee campaign was ended January 2, 1863, at the inconclusive Battle of Stones River (Murfreesboro), both sides losing the largest percentage of casualties suffered during the war. It was followed by another strategic withdrawal by Confederate forces. The Confederacy won a significant victory April 1863, repulsing the Federal advance on Richmond at Chancellorsville, but the Union consolidated positions along the Virginia coast and the Chesapeake Bay.\n\nWithout an effective answer to Federal gunboats, river transport and supply, the Confederacy lost the Mississippi River following the capture of Vicksburg, Mississippi, and Port Hudson in July, ending Southern access to the trans-Mississippi West. July brought short-lived counters, Morgan's Raid into Ohio and the New York City draft riots. Robert E. Lee's strike into Pennsylvania was repulsed at Gettysburg, Pennsylvania despite Pickett's famous charge and other acts of valor. Southern newspapers assessed the campaign as \"The Confederates did not gain a victory, neither did the enemy.\"\n\nSeptember and November left Confederates yielding Chattanooga, Tennessee, the gateway to the lower south. For the remainder of the war fighting was restricted inside the South, resulting in a slow but continuous loss of territory. In early 1864, the Confederacy still controlled 53% of its population, but it withdrew further to reestablish defensive positions. Union offensives continued with Sherman's March to the Sea to take Savannah and Grant's Wilderness Campaign to encircle Richmond and besiege Lee's army at Petersburg.\n\nIn April 1863, the C.S. Congress authorized a uniformed Volunteer Navy, many of whom were British. Wilmington and Charleston had more shipping while \"blockaded\" than before the beginning of hostilities. The Confederacy had altogether eighteen commerce destroying cruisers, which seriously disrupted Federal commerce at sea and increased shipping insurance rates 900%. Commodore Tattnall unsuccessfully attempted to break the Union blockade on the Savannah River in Georgia with an ironclad again in 1863. Beginning in April 1864 the ironclad CSS \"Albemarle\" engaged Union gunboats and sank or cleared them for six months on the Roanoke River North Carolina. The Federals closed Mobile Bay by sea-based amphibious assault in August, ending Gulf coast trade east of the Mississippi River. In December, the Battle of Nashville ended Confederate operations in the western theater.\n\nLarge numbers of families relocated to safer places, usually remote rural areas, bringing along household slaves if they had any. Mary Massey argues these elite exiles introduced an element of defeatism into the southern outlook.\n\nThe first three months of 1865 saw the Federal Carolinas Campaign, devastating a wide swath of the remaining Confederate heartland. The \"breadbasket of the Confederacy\" in the Great Valley of Virginia was occupied by Philip Sheridan. The Union Blockade captured Fort Fisher in North Carolina, and Sherman finally took Charleston, South Carolina, by land attack.\n\nThe Confederacy controlled no ports, harbors or navigable rivers. Railroads were captured or had ceased operating. Its major food producing regions had been war-ravaged or occupied. Its administration survived in only three pockets of territory holding only one-third of its population. Its armies were defeated or disbanding. At the February 1865 Hampton Roads Conference with Lincoln, senior Confederate officials rejected his invitation to restore the Union with compensation for emancipated slaves. The three pockets of unoccupied Confederacy were southern Virginia - North Carolina, central Alabama - Florida, and Texas, the latter two areas less from any notion of resistance than from the disinterest of Federal forces to occupy them. The Davis policy was independence or nothing, while Lee's army was wracked by disease and desertion, barely holding the trenches defending Jefferson Davis' capital.\n\nThe Confederacy's last remaining blockade-running port, Wilmington, North Carolina, was lost. When the Union broke through Lee's lines at Petersburg, Richmond fell immediately. Lee surrendered a remnant of 50,000 from the Army of Northern Virginia at Appomattox Court House, Virginia, on April 9, 1865. \"The Surrender\" marked the end of the Confederacy.\nThe CSS \"Stonewall\" sailed from Europe to break the Union blockade in March; on making Havana, Cuba, it surrendered. Some high officials escaped to Europe, but President Davis was captured May 10; all remaining Confederate land forces surrendered by June 1865. The U.S. Army took control of the Confederate areas without post-surrender insurgency or guerrilla warfare against them, but peace was subsequently marred by a great deal of local violence, feuding and revenge killings. The last confederate military unit, the commerce raider CSS \"Shenandoah\", surrendered on November 6, 1865 in Liverpool.\n\nHistorian Gary Gallagher concluded that the Confederacy capitulated in early 1865 because northern armies crushed \"organized southern military resistance\". The Confederacy's population, soldier and civilian, had suffered material hardship and social disruption. They had expended and extracted a profusion of blood and treasure until collapse; \"the end had come\". Jefferson Davis' assessment in 1890 determined, \"With the capture of the capital, the dispersion of the civil authorities, the surrender of the armies in the field, and the arrest of the President, the Confederate States of America disappeared ... their history henceforth became a part of the history of the United States.\"\n\nWhen the war ended over 14,000 Confederates petitioned President Johnson for a pardon; he was generous in giving them out. He issued a general amnesty to all Confederate participants in the \"late Civil War\" in 1868. Congress passed additional Amnesty Acts in May 1866 with restrictions on office holding, and the Amnesty Act in May 1872 lifting those restrictions. There was a great deal of discussion in 1865 about bringing treason trials, especially against Jefferson Davis. There was no consensus in President Johnson's cabinet and there were no treason trials against anyone. In the case of Davis there was a strong possibility of acquittal which would have been humiliating for the government.\n\nDavis was indicted for treason but never tried; he was released from prison on bail in May 1867. The amnesty of December 25, 1868, by President Johnson eliminated any possibility of Jefferson Davis (or anyone else associated with the Confederacy) standing trial for treason.\n\nHenry Wirz, the commandant of a notorious prisoner-of-war camp near Andersonville, Georgia, was tried and convicted by a military court, and executed on November 10, 1865. The charges against him involved conspiracy and cruelty, not treason.\n\nThe U.S. government began a decade-long process known as Reconstruction which attempted to resolve the political and constitutional issues of the Civil War. The priorities were: to guarantee that Confederate nationalism and slavery were ended, to ratify and enforce the Thirteenth Amendment which outlawed slavery; the Fourteenth which guaranteed dual U.S. and state citizenship to all native-born residents, regardless of race; and the Fifteenth, which made it illegal to deny the right to vote because of race.\n\nBy 1877, the Compromise of 1877 ended Reconstruction in the former Confederate states. Federal troops were withdrawn from the South, where conservative white Democrats had already regained political control of state governments, often through extreme violence and fraud to suppress black voting. The prewar South had many rich areas; the war left the entire region economically devastated by military action, ruined infrastructure, and exhausted resources. Still dependent on an agricultural economy and resisting investment in infrastructure, it remained dominated by the planter elite into the next century. Confederate veterans had been temporarily disenfranchised by Reconstruction policy, and Democrat-dominated legislatures passed new constitutions and amendments to now exclude most blacks and many poor whites. This exclusion and a weakened Republican Party remained the norm until the Voting Rights Act of 1965. The Solid South of the early 20th century did not achieve national levels of prosperity until long after World War II.\n\nIn \"Texas v. White\", the United States Supreme Court ruled – by a 5–3 majority – that Texas had remained a state ever since it first joined the Union, despite claims that it joined the Confederate States of America. In this case, the court held that the Constitution did not permit a state to unilaterally secede from the United States. Further, that the ordinances of secession, and all the acts of the legislatures within seceding states intended to give effect to such ordinances, were \"absolutely null\", under the Constitution. This case settled the law that applied to all questions regarding state legislation during the war. Furthermore, it decided one of the \"central constitutional questions\" of the Civil War: The Union is perpetual and indestructible, as a matter of constitutional law. In declaring that no state could leave the Union, \"except through revolution or through consent of the States\", it was \"explicitly repudiating the position of the Confederate states that the United States was a voluntary compact between sovereign states\".\n\nHistorian Frank Lawrence Owsley argued that the Confederacy \"died of states' rights\". The central government was denied requisitioned soldiers and money by governors and state legislatures because they feared that Richmond would encroach on the rights of the states. Georgia's governor Joseph Brown warned of a secret conspiracy by Jefferson Davis to destroy states' rights and individual liberty. The first conscription act in North America authorizing Davis to draft soldiers was said to be the \"essence of military despotism\".\nVice President Alexander H. Stephens feared losing the very form of republican government. Allowing President Davis to threaten \"arbitrary arrests\" to draft hundreds of governor-appointed \"bomb-proof\" bureaucrats conferred \"more power than the English Parliament had ever bestowed on the king. History proved the dangers of such unchecked authority.\" The abolishment of draft exemptions for newspaper editors was interpreted as an attempt by the Confederate government to muzzle presses, such as the Raleigh NC \"Standard\", to control elections and to suppress the peace meetings there. As Rable concludes, \"For Stephens, the essence of patriotism, the heart of the Confederate cause, rested on an unyielding commitment to traditional rights\" without considerations of military necessity, pragmatism or compromise.\n\nIn 1863 governor Pendleton Murrah of Texas determined that state troops were required for defense against Plains Indians and Union forces that might attack from Kansas. He refused to send his soldiers to the East. Governor Zebulon Vance of North Carolina showed intense opposition to conscription, limiting recruitment success. Vance's faith in states' rights drove him into repeated, stubborn opposition to the Davis administration.\n\nDespite political differences within the Confederacy, no national political parties were formed because they were seen as illegitimate. \"Anti-partyism became an article of political faith.\" Without a two-party system building alternative sets of national leaders, electoral protests tended to be narrowly state-based, \"negative, carping and petty\". The 1863 mid-term elections became mere expressions of futile and frustrated dissatisfaction. According to historian David M. Potter, this lack of a functioning two-party system caused \"real and direct damage\" to the Confederate war effort since it prevented the formulation of any effective alternatives to the conduct of the war by the Davis administration.\n\nThe enemies of President Davis proposed that the Confederacy \"died of Davis\". He was unfavorably compared to George Washington by critics such as Edward Alfred Pollard, editor of the most influential newspaper the \"Richmond Examiner\". E. Merton Coulter summarizes, \"The American Revolution had its Washington; the Southern Revolution had its Davis ... one succeeded and the other failed.\" Beyond the early honeymoon period, Davis was never popular. He unwittingly caused much internal dissension from early on. His ill health and temporary bouts of blindness disabled him for days at a time.\n\nCoulter says Davis was heroic and his will was indomitable. But his \"tenacity, determination, and will power\" stirred up lasting opposition of enemies Davis could not shake. He failed to overcome \"petty leaders of the states\" who made the term \"Confederacy\" into a label for tyranny and oppression, denying the \"Stars and Bars\" from becoming a symbol of larger patriotic service and sacrifice. Instead of campaigning to develop nationalism and gain support for his administration, he rarely courted public opinion, assuming an aloofness, \"almost like an Adams\".\n\nEscott argues that Davis was unable to mobilize Confederate nationalism in support of his government effectively, and especially failed to appeal to the small farmers who comprised the bulk of the population. In addition to the problems caused by states rights, Escott also emphasizes that the widespread opposition to any strong central government combined with the vast difference in wealth between the slave-owning class and the small farmers created insolvable dilemmas when the Confederate survival presupposed a strong central government backed by a united populace. The prewar claim that white solidarity was necessary to provide a unified Southern voice in Washington no longer held. Davis failed to build a network of supporters who would speak up when he came under criticism, and he repeatedly alienated governors and other state-based leaders by demanding centralized control of the war effort.\n\nAccording to Coulter, Davis was not an efficient administrator as he attended to too many details, protected his friends after their failures were obvious, and spent too much time on military affairs versus his civic responsibilities. Coulter concludes he was not the ideal leader for the Southern Revolution, but he showed \"fewer weaknesses than any other\" contemporary character available for the role. Robert E. Lee's assessment of Davis as President was, \"I knew of none that could have done as well.\"\n\nThe Southern leaders met in Montgomery, Alabama, to write their constitution. Much of the Confederate States Constitution replicated the United States Constitution verbatim, but it contained several explicit protections of the institution of slavery including provisions for the recognition and protection of slavery in any territory of the Confederacy. It maintained the ban on international slave-trading while protecting the existing internal trade of slaves among slaveholding states.\n\nIn certain areas, the Confederate Constitution gave greater powers to the states (or curtailed the powers of the central government more) than the U.S. Constitution of the time did, but in other areas, the states lost rights they had under the U.S. Constitution. Although the Confederate Constitution, like the U.S. Constitution, contained a commerce clause, the Confederate version prohibited the central government from using revenues collected in one state for funding internal improvements in another state. The Confederate Constitution's equivalent to the U.S. Constitution's general welfare clause prohibited protective tariffs (but allowed tariffs for providing domestic revenue), and spoke of \"carry[ing] on the Government of the Confederate States\" rather than providing for the \"general welfare\". State legislatures had the power to impeach officials of the Confederate government in some cases. On the other hand, the Confederate Constitution contained a Necessary and Proper Clause and a Supremacy Clause that essentially duplicated the respective clauses of the U.S. Constitution. The Confederate Constitution also incorporated each of the 12 amendments to the U.S. Constitution that had been ratified up to that point.\n\nThe Confederate Constitution did not specifically include a provision allowing states to secede; the Preamble spoke of each state \"acting in its sovereign and independent character\" but also of the formation of a \"permanent federal government\". During the debates on drafting the Confederate Constitution, one proposal would have allowed states to secede from the Confederacy. The proposal was tabled with only the South Carolina delegates voting in favor of considering the motion. The Confederate Constitution also explicitly denied States the power to bar slaveholders from other parts of the Confederacy from bringing their slaves into any state of the Confederacy or to interfere with the property rights of slave owners traveling between different parts of the Confederacy. In contrast with the language of the United States Constitution, the Confederate Constitution overtly asked God's blessing (\"... invoking the favor and guidance of Almighty God ...\").\n\nThe Montgomery Convention to establish the Confederacy and its executive met on February 4, 1861. Each state as a sovereignty had one vote, with the same delegation size as it held in the U.S. Congress, and generally 41 to 50 members attended. Offices were \"provisional\", limited to a term not to exceed one year. One name was placed in nomination for president, one for vice president. Both were elected unanimously, 6–0.\n\nJefferson Davis was elected provisional president. His U.S. Senate resignation speech greatly impressed with its clear rationale for secession and his pleading for a peaceful departure from the Union to independence. Although he had made it known that he wanted to be commander-in-chief of the Confederate armies, when elected, he assumed the office of Provisional President. Three candidates for provisional Vice President were under consideration the night before the February 9 election. All were from Georgia, and the various delegations meeting in different places determined two would not do, so Alexander H. Stephens was elected unanimously provisional Vice President, though with some privately held reservations. Stephens was inaugurated February 11, Davis February 18.\n\nDavis and Stephens were elected President and Vice President, unopposed on November 6, 1861. They were inaugurated on February 22, 1862.\n\nHistorian E. M. Coulter observed, \"No president of the U.S. ever had a more difficult task.\" Washington was inaugurated in peacetime. Lincoln inherited an established government of long standing. The creation of the Confederacy was accomplished by men who saw themselves as fundamentally conservative. Although they referred to their \"Revolution\", it was in their eyes more a counter-revolution against changes away from their understanding of U.S. founding documents. In Davis' inauguration speech, he explained the Confederacy was not a French-like revolution, but a transfer of rule. The Montgomery Convention had assumed all the laws of the United States until superseded by the Confederate Congress.\n\nThe Permanent Constitution provided for a President of the Confederate States of America, elected to serve a six-year term but without the possibility of re-election. Unlike the United States Constitution, the Confederate Constitution gave the president the ability to subject a bill to a line item veto, a power also held by some state governors.\n\nThe Confederate Congress could overturn either the general or the line item vetoes with the same two-thirds votes required in the U.S. Congress. In addition, appropriations not specifically requested by the executive branch required passage by a two-thirds vote in both houses of Congress. The only person to serve as president was Jefferson Davis, due to the Confederacy being defeated before the completion of his term.\n! style=\"background:#DCDCDC; text-align:center;\" style=\"font-size:90%; colspan=\"3\"  The Davis Cabinet\n!\n!\n!style=\"background:#000;\" colspan=\"3\" \nPresident\n!Jefferson Davis\n1861–65\nVice President\n!Alexander H. Stephens\n1861–65\n!style=\"background:#000;\" colspan=\"3\" \nSecretary of State\n!Robert Toombs\n1861\n\n!Robert M.T. Hunter\n1861–62\n\n!Judah P. Benjamin\n1862–65\n!style=\"background:#D1D1D1;\" colspan=\"3\" \nSecretary of the Treasury\n!Christopher Memminger\n1861–64\n\n!George Trenholm\n1864–65\n\n!John H. Reagan\n1865\n!style=\"background:#D1D1D1;\" colspan=\"3\" \nSecretary of War\n!Leroy Pope Walker\n1861\n\n!Judah P. Benjamin\n1861–62\n\n!George W. Randolph\n1862\n\n!James Seddon\n1862–65\n\n!John C. Breckinridge\n1865\nSecretary of the Navy\n!Stephen Mallory\n1861–65\n!style=\"background:#D1D1D1;\" colspan=\"3\" \nPostmaster General\n!John H. Reagan\n1861–65\n!style=\"background:#D1D1D1;\" colspan=\"3\" \nAttorney General\n!Judah P. Benjamin\n1861\n\n!Thomas Bragg\n1861–62\n\n!Thomas H. Watts\n1862–63\n\n!George Davis\n1864–65\n\nThe only two \"formal, national, functioning, civilian administrative bodies\" in the Civil War South were the Jefferson Davis administration and the Confederate Congresses. The Confederacy was begun by the Provisional Congress in Convention at Montgomery, Alabama on February 28, 1861. It had one vote per state in a unicameral assembly.\n\nThe Permanent Confederate Congress was elected and began its first session February 18, 1862. The Permanent Congress for the Confederacy followed the United States forms with a bicameral legislature. The Senate had two per state, twenty-six Senators. The House numbered 106 representatives apportioned by free and slave populations within each state. Two Congresses sat in six sessions until March 18, 1865.\n\nThe political influences of the civilian, soldier vote and appointed representatives reflected divisions of political geography of a diverse South. These in turn changed over time relative to Union occupation and disruption, the war impact on local economy, and the course of the war. Without political parties, key candidate identification related to adopting secession before or after Lincoln's call for volunteers to retake Federal property. Previous party affiliation played a part in voter selection, predominantly secessionist Democrat or unionist Whig.\n\nThe absence of political parties made individual roll call voting all the more important, as the Confederate \"freedom of roll-call voting [was] unprecedented in American legislative history. Key issues throughout the life of the Confederacy related to (1) suspension of habeas corpus, (2) military concerns such as control of state militia, conscription and exemption, (3) economic and fiscal policy including impressment of slaves, goods and scorched earth, and (4) support of the Jefferson Davis administration in its foreign affairs and negotiating peace.\n\nBULLET::::- Provisional Congress\nFor the first year, the unicameral Provisional Confederate Congress functioned as the Confederacy's legislative branch.\n\nBULLET::::- President of the Provisional Congress\nBULLET::::- Howell Cobb, Sr. of Georgia, February 4, 1861 – February 17, 1862\n\nBULLET::::- Presidents pro tempore of the Provisional Congress\nBULLET::::- Robert Woodward Barnwell of South Carolina, February 4, 1861\nBULLET::::- Thomas Stanhope Bocock of Virginia, December 10–21, 1861 and January 7–8, 1862\nBULLET::::- Josiah Abigail Patterson Campbell of Mississippi, December 23–24, 1861 and January 6, 1862\n\nBULLET::::- Sessions of the Confederate Congress\nBULLET::::- Provisional Congress\nBULLET::::- 1st Congress\nBULLET::::- 2nd Congress\n\nBULLET::::- Tribal Representatives to Confederate Congress\nBULLET::::- Elias Cornelius Boudinot 1862–65, Cherokee\nBULLET::::- Samuel Benton Callahan Unknown years, Creek, Seminole\nBULLET::::- Burton Allen Holder 1864–65, Chickasaw\nBULLET::::- Robert McDonald Jones 1863–65, Choctaw\nThe Confederate Constitution outlined a judicial branch of the government, but the ongoing war and resistance from states-rights advocates, particularly on the question of whether it would have appellate jurisdiction over the state courts, prevented the creation or seating of the \"Supreme Court of the Confederate States;\" the state courts generally continued to operate as they had done, simply recognizing the Confederate States as the national government.\n\nConfederate district courts were authorized by Article III, Section 1, of the Confederate Constitution, and President Davis appointed judges within the individual states of the Confederate States of America. In many cases, the same US Federal District Judges were appointed as Confederate States District Judges. Confederate district courts began reopening in early 1861, handling many of the same type cases as had been done before. Prize cases, in which Union ships were captured by the Confederate Navy or raiders and sold through court proceedings, were heard until the blockade of southern ports made this impossible. After a Sequestration Act was passed by the Confederate Congress, the Confederate district courts heard many cases in which enemy aliens (typically Northern absentee landlords owning property in the South) had their property sequestered (seized) by Confederate Receivers.\n\nWhen the matter came before the Confederate court, the property owner could not appear because he was unable to travel across the front lines between Union and Confederate forces. Thus, the District Attorney won the case by default, the property was typically sold, and the money used to further the Southern war effort. Eventually, because there was no Confederate Supreme Court, sharp attorneys like South Carolina's Edward McCrady began filing appeals. This prevented their clients' property from being sold until a supreme court could be constituted to hear the appeal, which never occurred. Where Federal troops gained control over parts of the Confederacy and re-established civilian government, US district courts sometimes resumed jurisdiction.\n\nSupreme Court – not established.\n\nDistrict Courts – judges\nBULLET::::- Alabama William G. Jones 1861–65\nBULLET::::- Arkansas Daniel Ringo 1861–65\nBULLET::::- Florida Jesse J. Finley 1861–62\nBULLET::::- Georgia Henry R. Jackson 1861, Edward J. Harden 1861–65\nBULLET::::- Louisiana Edwin Warren Moise 1861–65\nBULLET::::- Mississippi Alexander Mosby Clayton 1861–65\nBULLET::::- North Carolina Asa Biggs 1861–65\n\nBULLET::::- South Carolina Andrew G. Magrath 1861–64, Benjamin F. Perry 1865\nBULLET::::- Tennessee West H. Humphreys 1861–65\nBULLET::::- Texas-East William Pinckney Hill 1861–65\nBULLET::::- Texas-West Thomas J. Devine 1861–65\nBULLET::::- Virginia-East James D. Halyburton 1861–65\nBULLET::::- Virginia-West John W. Brockenbrough 1861–65\n\nWhen the Confederacy was formed and its seceding states broke from the Union, it was at once confronted with the arduous task of providing its citizens with a mail delivery system, and, in the midst of the American Civil War, the newly formed Confederacy created and established the Confederate Post Office. One of the first undertakings in establishing the Post Office was the appointment of John H. Reagan to the position of Postmaster General, by Jefferson Davis in 1861, making him the first Postmaster General of the Confederate Post Office as well as a member of Davis' presidential cabinet. Through Reagan's resourcefulness and remarkable industry, he had his department assembled, organized and in operation before the other Presidential cabinet members had their departments fully operational.\n\nWhen the war began, the US Post Office still delivered mail from the secessionist states for a brief period of time. Mail that was postmarked after the date of a state's admission into the Confederacy through May 31, 1861, and bearing US postage was still delivered. After this time, private express companies still managed to carry some of the mail across enemy lines. Later, mail that crossed lines had to be sent by 'Flag of Truce' and was allowed to pass at only two specific points. Mail sent from the South to the North states was received, opened and inspected at Fortress Monroe on the Virginia coast before being passed on into the U.S. mail stream. Mail sent from the North to the South passed at City Point, also in Virginia, where it was also inspected before being sent on.\n\nWith the chaos of the war, a working postal system was more important than ever for the Confederacy. The Civil War had divided family members and friends and consequently letter writing increased dramatically across the entire divided nation, especially to and from the men who were away serving in an army. Mail delivery was also important for the Confederacy for a myriad of business and military reasons. Because of the Union blockade, basic supplies were always in demand and so getting mailed correspondence out of the country to suppliers was imperative to the successful operation of the Confederacy. Volumes of material have been written about the Blockade runners who evaded Union ships on blockade patrol, usually at night, and who moved cargo and mail in and out of the Confederate States throughout the course of the war. Of particular interest to students and historians of the American Civil War is \"Prisoner of War mail\" and \"Blockade mail\" as these items were often involved with a variety of military and other war time activities. The postal history of the Confederacy along with has helped historians document the various people, places and events that were involved in the American Civil War as it unfolded.\nThe Confederacy actively used the army to arrest people suspected of loyalty to the United States. Historian Mark Neely found 4,108 names of men arrested and estimated a much larger total. The Confederacy arrested pro-Union civilians in the South at about the same rate as the Union arrested pro-Confederate civilians in the North. Neely argues:\n\nAcross the South, widespread rumors alarmed the whites by predicting the slaves were planning some sort of insurrection. Patrols were stepped up. The slaves did become increasingly independent, and resistant to punishment, but historians agree there were no insurrections. In the invaded areas, insubordination was more the norm than loyalty to the old master; Bell Wiley says, \"It was not disloyalty, but the lure of freedom.\" Many slaves became spies for the North, and large numbers ran away to federal lines.\n\nLincoln's Emancipation Proclamation, an executive order of the U.S. government on January 1, 1863, changed the legal status of 3 million slaves in designated areas of the Confederacy from \"slave\" to \"free\". The long-term effect was that the Confederacy could not preserve the institution of slavery, and lost the use of the core element of its plantation labor force. Slaves were legally freed by the Proclamation, and became free by escaping to federal lines, or by advances of federal troops. Over 200,000 freed slaves were hired by the federal army as teamsters, cooks, launderers and laborers, and eventually as soldiers. Plantation owners, realizing that emancipation would destroy their economic system, sometimes moved their slaves as far as possible out of reach of the Union army. By \"Juneteenth\" (June 19, 1865, in Texas), the Union Army controlled all of the Confederacy and had liberated all its slaves. Their owners never received compensation.\n\nMost whites were subsistence farmers who traded their surpluses locally. The plantations of the South, with white ownership and an enslaved labor force, produced substantial wealth from cash crops. It supplied two-thirds of the world's cotton, which was in high demand for textiles, along with tobacco, sugar, and naval stores (such as turpentine). These raw materials were exported to factories in Europe and the Northeast. Planters reinvested their profits in more slaves and fresh land, for cotton and tobacco depleted the soil. There was little manufacturing or mining; shipping was controlled by outsiders.\n\nThe plantations that enslaved over three million black people were the principal source of wealth. Most were concentrated in \"black belt\" plantation areas (because few white families in the poor regions owned slaves.) For decades there had been widespread fear of slave revolts. During the war extra men were assigned to \"home guard\" patrol duty and governors sought to keep militia units at home for protection. Historian William Barney reports, \"no major slave revolts erupted during the Civil War.\" Nevertheless, slaves took the opportunity to enlarge their sphere of independence, and when union forces were nearby, many ran off to join them.\n\nSlave labor was applied in industry in a limited way in the Upper South and in a few port cities. One reason for the regional lag in industrial development was top-heavy income distribution. Mass production requires mass markets, and slaves living in small cabins, using self-made tools and outfitted with one suit of work clothes each year of inferior fabric, did not generate consumer demand to sustain local manufactures of any description in the same way a mechanized family farm of free labor did in the North. The Southern economy was \"pre-capitalist\" in that slaves were put to work in the largest revenue-producing enterprises, not free labor market. That labor system as practiced in the American South encompassed paternalism, whether abusive or indulgent, and that meant labor management considerations apart from productivity.\n\nApproximately 85% of both North and South white populations lived on family farms, both regions were predominantly agricultural, and mid-century industry in both was mostly domestic. But the Southern economy was pre-capitalist in its overwhelming reliance on the agriculture of cash crops to produce wealth, while the great majority of farmers fed themselves and supplied a small local market. Southern cities and industries grew faster than ever before, but the thrust of the rest of the country's exponential growth elsewhere was toward urban industrial development along transportation systems of canals and railroads. The South was following the dominant currents of the American economic mainstream, but at a \"great distance\" as it lagged in the all-weather modes of transportation that brought cheaper, speedier freight shipment and forged new, expanding inter-regional markets.\n\nA third count of southern pre-capitalist economy relates to the cultural setting. The South and southerners did not adopt a work ethic, nor the habits of thrift that marked the rest of the country. It had access to the tools of capitalism, but it did not adopt its culture. The Southern Cause as a national economy in the Confederacy was grounded in \"slavery and race, planters and patricians, plain folk and folk culture, cotton and plantations\".\n\nThe Confederacy started its existence as an agrarian economy with exports, to a world market, of cotton, and, to a lesser extent, tobacco and sugarcane. Local food production included grains, hogs, cattle, and gardens. The cash came from exports but the Southern people spontaneously stopped exports in early 1861 to hasten the impact of \"King Cotton\". When the blockade was announced, commercial shipping practically ended (the ships could not get insurance), and only a trickle of supplies came via blockade runners. The cutoff of exports was an economic disaster for the South, rendering useless its most valuable properties, its plantations and their enslaved workers. Many planters kept growing cotton, which piled up everywhere, but most turned to food production. All across the region, the lack of repair and maintenance wasted away the physical assets.\n\nThe eleven states had produced $155 million in manufactured goods in 1860, chiefly from local grist-mills, and lumber, processed tobacco, cotton goods and naval stores such as turpentine. The main industrial areas were border cities such as Baltimore, Wheeling, Louisville and St. Louis, that were never under Confederate control. The government did set up munitions factories in the Deep South. Combined with captured munitions and those coming via blockade runners, the armies were kept minimally supplied with weapons. The soldiers suffered from reduced rations, lack of medicines, and the growing shortages of uniforms, shoes and boots. Shortages were much worse for civilians, and the prices of necessities steadily rose.\n\nThe Confederacy adopted a tariff or tax on imports of 15%, and imposed it on all imports from other countries, including the United States. The tariff mattered little; the Union blockade minimized commercial traffic through the Confederacy's ports, and very few people paid taxes on goods smuggled from the North. The Confederate government in its entire history collected only $3.5 million in tariff revenue. The lack of adequate financial resources led the Confederacy to finance the war through printing money, which led to high inflation. The Confederacy underwent an economic revolution by centralization and standardization, but it was too little too late as its economy was systematically strangled by blockade and raids.\n\nIn peacetime, the South's extensive and connected systems of navigable rivers and coastal access allowed for cheap and easy transportation of agricultural products. The railroad system in the South had developed as a supplement to the navigable rivers to enhance the all-weather shipment of cash crops to market. Railroads tied plantation areas to the nearest river or seaport and so made supply more dependable, lowered costs and increased profits. In the event of invasion, the vast geography of the Confederacy made logistics difficult for the Union. Wherever Union armies invaded, they assigned many of their soldiers to garrison captured areas and to protect rail lines.\n\nAt the onset of the Civil War the South had a rail network disjointed and plagued by changes in track gauge as well as lack of interchange. Locomotives and freight cars had fixed axles and could not use tracks of different gauges (widths). Railroads of different gauges leading to the same city required all freight to be off-loaded onto wagons for transport to the connecting railroad station, where it had to await freight cars and a locomotive before proceeding. Centers requiring off-loading included Vicksburg, New Orleans, Montgomery, Wilmington and Richmond. In addition, most rail lines led from coastal or river ports to inland cities, with few lateral railroads. Due to this design limitation, the relatively primitive railroads of the Confederacy were unable to overcome the Union naval blockade of the South's crucial intra-coastal and river routes.\n\nThe Confederacy had no plan to expand, protect or encourage its railroads. Southerners' refusal to export the cotton crop in 1861 left railroads bereft of their main source of income. Many lines had to lay off employees; many critical skilled technicians and engineers were permanently lost to military service. In the early years of the war the Confederate government had a hands-off approach to the railroads. Only in mid-1863 did the Confederate government initiate a national policy, and it was confined solely to aiding the war effort. Railroads came under the \"de facto\" control of the military. In contrast, the U.S. Congress had authorized military administration of Union-controlled railroad and telegraph systems in January 1862, imposed a standard gauge, and built railroads into the South using that gauge. Confederate armies successfully reoccupying territory could not be resupplied directly by rail as they advanced. The C.S. Congress formally authorized military administration of railroads in February 1865.\n\nIn the last year before the end of the war, the Confederate railroad system stood permanently on the verge of collapse. There was no new equipment and raids on both sides systematically destroyed key bridges, as well as locomotives and freight cars. Spare parts were cannibalized; feeder lines were torn up to get replacement rails for trunk lines, and rolling stock wore out through heavy use.\n\nThe Confederate army experienced a persistent shortage of horses and mules, and requisitioned them with dubious promissory notes given to local farmers and breeders. Union forces paid in real money and found ready sellers in the South. Both armies needed horses for cavalry and for artillery. Mules pulled the wagons. The supply was undermined by an unprecedented epidemic of glanders, a fatal disease that baffled veterinarians. After 1863 the invading Union forces had a policy of shooting all the local horses and mules they did not need – in order to keep them out of Confederate hands. The Confederate armies and farmers experienced a growing shortage of horses and mules, which hurt the Southern economy and the war effort. The South lost half of its 2.5 million horses and mules; many farmers ended the war with none left. Army horses were used up by hard work, malnourishment, disease and battle wounds; they had a life expectancy of about seven months.\n\nBoth the individual Confederate states and later the Confederate government printed Confederate States of America dollars as paper currency in various denominations, with a total face value of $1.5 billion. Much of it was signed by Treasurer Edward C. Elmore. Inflation became rampant as the paper money depreciated and eventually became worthless. The state governments and some localities printed their own paper money, adding to the runaway inflation. Many bills still exist, although in recent years counterfeit copies have proliferated.\n\nThe Confederate government initially wanted to finance its war mostly through tariffs on imports, export taxes, and voluntary donations of gold. After the spontaneous imposition of an embargo on cotton sales to Europe in 1861, these sources of revenue dried up and the Confederacy increasingly turned to issuing debt and printing money to pay for war expenses. The Confederate States politicians were worried about angering the general population with hard taxes. A tax increase might disillusion many Southerners, so the Confederacy resorted to printing more money. As a result, inflation increased and remained a problem for the southern states throughout the rest of the war. By April 1863, for example, the cost of flour in Richmond had risen to $100 a barrel and housewives were rioting.\n\nThe Confederate government took over the three national mints in its territory: the Charlotte Mint in North Carolina, the Dahlonega Mint in Georgia, and the New Orleans Mint in Louisiana. During 1861 all of these facilities produced small amounts of gold coinage, and the latter half dollars as well. Since the mints used the current dies on hand, all appear to be U.S. issues. However, by comparing slight differences in the dies specialists can distinguish 1861-O half dollars that were minted either under the authority of the U.S. government, the State of Louisiana, or finally the Confederate States. Unlike the gold coins, this issue was produced in significant numbers (over 2.5 million) and is inexpensive in lower grades, although fakes have been made for sale to the public. However, before the New Orleans Mint ceased operation in May, 1861, the Confederate government used its own reverse design to strike four half dollars. This made one of the great rarities of American numismatics. A lack of silver and gold precluded further coinage. The Confederacy apparently also experimented with issuing one cent coins, although only 12 were produced by a jeweler in Philadelphia, who was afraid to send them to the South. Like the half dollars, copies were later made as souvenirs.\n\nUS coinage was hoarded and did not have any general circulation. U.S. coinage was admitted as legal tender up to $10, as were British sovereigns, French Napoleons and Spanish and Mexican doubloons at a fixed rate of exchange. Confederate money was paper and postage stamps.\n\nBy mid-1861, the Union naval blockade virtually shut down the export of cotton and the import of manufactured goods. Food that formerly came overland was cut off.\n\nWomen had charge of making do. They cut back on purchases, brought out old spinning wheels and enlarged their gardens with flax and peas to provide clothing and food. They used ersatz substitutes when possible, but there was no real coffee and it was hard to develop a taste for the okra or chicory substitutes used. The households were severely hurt by inflation in the cost of everyday items like flour and the shortages of food, fodder for the animals, and medical supplies for the wounded.\n\nState governments pleaded with planters to grow less cotton and more food. Most refused. When cotton prices soared in Europe, expectations were that Europe would soon intervene to break the blockade and make them rich. The myth of omnipotent \"King Cotton\" died hard. The Georgia legislature imposed cotton quotas, making it a crime to grow an excess. But food shortages only worsened, especially in the towns.\n\nThe overall decline in food supplies, made worse by the inadequate transportation system, led to serious shortages and high prices in urban areas. When bacon reached a dollar a pound in 1863, the poor women of Richmond, Atlanta and many other cities began to riot; they broke into shops and warehouses to seize food. The women expressed their anger at ineffective state relief efforts, speculators, and merchants. As wives and widows of soldiers they were hurt by the inadequate welfare system.\n\nBy the end of the war deterioration of the Southern infrastructure was widespread. The number of civilian deaths is unknown. Every Confederate state was affected, but most of the war was fought in Virginia and Tennessee, while Texas and Florida saw the least military action. Much of the damage was caused by direct military action, but most was caused by lack of repairs and upkeep, and by deliberately using up resources. Historians have recently estimated how much of the devastation was caused by military action. Paul Paskoff calculates that Union military operations were conducted in 56% of 645 counties in nine Confederate states (excluding Texas and Florida). These counties contained 63% of the 1860 white population and 64% of the slaves. By the time the fighting took place, undoubtedly some people had fled to safer areas, so the exact population exposed to war is unknown.\n\nThe eleven Confederate States in the 1860 United States Census had 297 towns and cities with 835,000 people; of these 162 with 681,000 people were at one point occupied by Union forces. Eleven were destroyed or severely damaged by war action, including Atlanta (with an 1860 population of 9,600), Charleston, Columbia, and Richmond (with prewar populations of 40,500, 8,100, and 37,900, respectively); the eleven contained 115,900 people in the 1860 census, or 14% of the urban South. Historians have not estimated what their actual population was when Union forces arrived. The number of people (as of 1860) who lived in the destroyed towns represented just over 1% of the Confederacy's 1860 population. In addition, 45 court houses were burned (out of 830). The South's agriculture was not highly mechanized. The value of farm implements and machinery in the 1860 Census was $81 million; by 1870, there was 40% less, worth just $48 million. Many old tools had broken through heavy use; new tools were rarely available; even repairs were difficult.\n\nThe economic losses affected everyone. Banks and insurance companies were mostly bankrupt. Confederate currency and bonds were worthless. The billions of dollars invested in slaves vanished. Most debts were also left behind. Most farms were intact but most had lost their horses, mules and cattle; fences and barns were in disrepair. Paskoff shows the loss of farm infrastructure was about the same whether or not fighting took place nearby. The loss of infrastructure and productive capacity meant that rural widows throughout the region faced not only the absence of able-bodied men, but a depleted stock of material resources that they could manage and operate themselves. During four years of warfare, disruption, and blockades, the South used up about half its capital stock. The North, by contrast, absorbed its material losses so effortlessly that it appeared richer at the end of the war than at the beginning.\n\nThe rebuilding took years and was hindered by the low price of cotton after the war. Outside investment was essential, especially in railroads. One historian has summarized the collapse of the transportation infrastructure needed for economic recovery:\n\nAbout 250,000 men never came home, some 30 percent of all white men aged 18 to 40, in 1860. Widows who were overwhelmed often abandoned the farm and merged into the households of relatives, or even became refugees living in camps with high rates of disease and death. In the Old South, being an \"old maid\" was something of an embarrassment to the woman and her family. After the war it became almost a norm. Some women welcomed the freedom of not having to marry. Divorce, while never fully accepted, became more common. The concept of the \"New Woman\" emerged – she was self-sufficient and independent, and stood in sharp contrast to the \"Southern Belle\" of antebellum lore.\n\nThe first official flag of the Confederate States of America – called the \"Stars and Bars\" – originally had seven stars, representing the first seven states that initially formed the Confederacy. As more states joined, more stars were added, until the total was 13 (two stars were added for the divided states of Kentucky and Missouri). During the First Battle of Bull Run, (First Manassas) it sometimes proved difficult to distinguish the Stars and Bars from the Union flag. To rectify the situation, a separate \"Battle Flag\" was designed for use by troops in the field. Also known as the \"Southern Cross\", many variations sprang from the original square configuration. Although it was never officially adopted by the Confederate government, the popularity of the Southern Cross among both soldiers and the civilian population was a primary reason why it was made the main color feature when a new national flag was adopted in 1863. This new standard – known as the \"Stainless Banner\" – consisted of a lengthened white field area with a Battle Flag canton. This flag too had its problems when used in military operations as, on a windless day, it could easily be mistaken for a flag of truce or surrender. Thus, in 1865, a modified version of the Stainless Banner was adopted. This final national flag of the Confederacy kept the Battle Flag canton, but shortened the white field and added a vertical red bar to the fly end.\n\nBecause of its depiction in the 20th-century and popular media, many people consider the rectangular battle flag with the dark blue bars as being synonymous with \"the Confederate Flag\", but this flag was never adopted as a Confederate national flag. The \"Confederate Flag\" has a color scheme similar to the most common Battle Flag design, but is rectangular, not square. The \"Confederate Flag\" is a highly recognizable symbol of the South in the United States today, and continues to be a controversial icon.\n\nThe Confederate States of America claimed a total of of coastline, thus a large part of its territory lay on the seacoast with level and often sandy or marshy ground. Most of the interior portion consisted of arable farmland, though much was also hilly and mountainous, and the far western territories were deserts. The lower reaches of the Mississippi River bisected the country, with the western half often referred to as the Trans-Mississippi. The highest point (excluding Arizona and New Mexico) was Guadalupe Peak in Texas at .\n\nClimate\n\nMuch of the area claimed by the Confederate States of America had a humid subtropical climate with mild winters and long, hot, humid summers. The climate and terrain varied from vast swamps (such as those in Florida and Louisiana) to semi-arid steppes and arid deserts west of longitude 100 degrees west. The subtropical climate made winters mild but allowed infectious diseases to flourish. Consequently, on both sides more soldiers died from disease than were killed in combat, a fact hardly atypical of pre-World War I conflicts.\n\nThe United States Census of 1860 gives a picture of the overall 1860 population of the areas that joined the Confederacy. Note that population-numbers exclude non-assimilated Indian tribes.\n\n! scope=\"col\"  State\n! scope=\"col\"  Totalpopulation\n! scope=\"col\"  Totalnumber ofslaves\n! scope=\"col\"  Totalnumber ofhouseholds\n! scope=\"col\"  Totalfreepopulation\n! scope=\"col\"  Total numberslaveholders\n! scope=\"col\"  % of Freepopulationowningslaves\n! scope=\"col\"  Slavesas % ofpopulation\n! scope=\"col\"  Totalfreecolored\n! scope=\"row\"  Alabama\n! scope=\"row\"  Arkansas\n! scope=\"row\"  Florida\n! scope=\"row\"  Georgia\n! scope=\"row\"  Louisiana\n! scope=\"row\"  Mississippi\n! scope=\"row\"  North Carolina\n! scope=\"row\"  South Carolina\n! scope=\"row\"  Tennessee\n! scope=\"row\"  Texas\n! scope=\"row\"  Virginia\n! scope=\"row\"  Total\n\n! scope=\"col\"  Age structure\n! scope=\"col\"  0–14 years\n! scope=\"col\"  15–59 years\n! scope=\"col\"  60 years and over\n! scope=\"row\"  White males\n! scope=\"row\"  White females\n! scope=\"row\"  Male slaves\n! scope=\"row\"  Female slaves\n! scope=\"row\"  Free black males\n! scope=\"row\"  Free black females\n! scope=\"row\"  Total population\n\nIn 1860 the areas that later formed the eleven Confederate States (and including the future West Virginia) had 132,760 (1.46%) free blacks. Males made up 49.2% of the total population and females 50.8% (whites: 48.60% male, 51.40% female; slaves: 50.15% male, 49.85% female; free blacks: 47.43% male, 52.57% female).\n\nThe CSA was overwhelmingly rural. Few towns had populations of more than 1,000 – the typical county seat had a population of fewer than 500. Cities were rare. Of the twenty largest U.S. cities in the 1860 census, only New Orleans lay in Confederate territory – and the Union captured New Orleans in 1862. Only 13 Confederate-controlled cities ranked among the top 100 U.S. cities in 1860, most of them ports whose economic activities vanished or suffered severely in the Union blockade. The population of Richmond swelled after it became the Confederate capital, reaching an estimated 128,000 in 1864. Other Southern cities in the Border slave-holding states such as Baltimore, Washington, D.C., Wheeling, Alexandria, Louisville, and St. Louis never came under the control of the Confederate government.\n\nThe cities of the Confederacy included most prominently in order of size of population:\n\n! scope=\"col\"  #\n! scope=\"col\"  City\n! scope=\"col\"  1860 population\n! scope=\"col\"  1860 U.S. rank\n! Return to U.S. control\n! scope=\"row\"  1.\n! scope=\"row\"  2.\n! scope=\"row\"  3.\n! scope=\"row\"  4.\n! scope=\"row\"  5.\n! scope=\"row\"  6.\n! scope=\"row\"  7.\n! scope=\"row\"  8.\n! scope=\"row\"  9.\n! scope=\"row\"  10.\n! scope=\"row\"  11.\n! scope=\"row\"  12.\n! scope=\"row\"  13.\n\n\"(See also Atlanta in the Civil War, Charleston, South Carolina, in the Civil War, Nashville in the Civil War, New Orleans in the Civil War, Wilmington, North Carolina, in the American Civil War, and Richmond in the Civil War).\"\n\nThe CSA was overwhelmingly Protestant. Both free and enslaved populations identified with evangelical Protestantism. Baptists and Methodists together formed majorities of both the white and the slave population (see \"Black church\"). Freedom of religion and separation of church and state were fully ensured by Confederate laws. Church attendance was very high and chaplains played a major role in the Army.\n\nMost large denominations experienced a North–South split in the prewar era on the issue of slavery. The creation of a new country necessitated independent structures. For example, the Presbyterian Church in the United States split, with much of the new leadership provided by Joseph Ruggles Wilson (father of President Woodrow Wilson). In 1861, he organized the meeting that formed General Assembly of the Southern Presbyterian Church and served as its chief executive for thirty-seven years. Baptists and Methodists both broke off from their Northern coreligionists over the slavery issue, forming the Southern Baptist Convention and the Methodist Episcopal Church, South, respectively. Elites in the southeast favored the Protestant Episcopal Church in the Confederate States of America, which reluctantly split off the Episcopal Church (USA) in 1861. Other elites were Presbyterians belonging to the 1861-founded Presbyterian Church in the United States. Catholics included an Irish working class element in coastal cities and an old French element in southern Louisiana. Other insignificant and scattered religious populations included Lutherans, the Holiness movement, other Reformed, other Christian fundamentalists, the Stone-Campbell Restoration Movement, the Churches of Christ, the Latter Day Saint movement, Adventists, Muslims, Jews, Native American animists, deists and irreligious people.\n\nThe southern churches met the shortage of Army chaplains by sending missionaries. The Southern Baptists started in 1862 and had a total of 78 missionaries. Presbyterians were even more active with 112 missionaries in January 1865. Other missionaries were funded and supported by the Episcopalians, Methodists, and Lutherans. One result was wave after wave of revivals in the Army.\n\nMilitary leaders of the Confederacy (with their state or country of birth and highest rank) included:\nBULLET::::- Robert E. Lee (Virginia) – General & General in Chief\nBULLET::::- P. G. T. Beauregard (Louisiana) – General\nBULLET::::- Braxton Bragg (North Carolina) – General\nBULLET::::- Samuel Cooper (New York) – General\nBULLET::::- Albert Sidney Johnston (Kentucky) – General\nBULLET::::- Joseph E. Johnston (Virginia) – General\nBULLET::::- Edmund Kirby Smith (Florida)General\nBULLET::::- Simon Bolivar Buckner, Sr. (Kentucky)Lieutenant General\nBULLET::::- Jubal Early (Virginia) – Lieutenant-General\nBULLET::::- Richard S. Ewell (Virginia) – Lieutenant-General\nBULLET::::- Nathan Bedford Forrest (Tennessee) – Lieutenant-General\nBULLET::::- Wade Hampton III (South Carolina) – Lieutenant-General\nBULLET::::- William J. Hardee (Georgia)Lieutenant-General\nBULLET::::- A. P. Hill (Virginia) – Lieutenant-General\nBULLET::::- Theophilus H. Holmes (North Carolina) Lieutenant-General\nBULLET::::- John Bell Hood (Kentucky) – Lieutenant-General (temporary General)\nBULLET::::- Thomas J. \"Stonewall\" Jackson (Virginia) – Lieutenant-General\nBULLET::::- Stephen D. Lee (South Carolina)Lieutenant-General\nBULLET::::- James Longstreet (South Carolina) – Lieutenant-General\nBULLET::::- John C. Pemberton (Pennsylvania)Lieutenant-General\nBULLET::::- Leonidas Polk (North Carolina) – Lieutenant-General\nBULLET::::- Alexander P. Stewart (North Carolina)Lieutenant-General\nBULLET::::- Richard Taylor (Kentucky) – Lieutenant-General (son of U.S. President Zachary Taylor)\nBULLET::::- Joseph Wheeler (Georgia)Lieutenant-General\nBULLET::::- John C. Breckinridge (Kentucky)Major-General & Secretary of War\nBULLET::::- Richard H. Anderson (South Carolina)Major-General (temporary Lieutenant-General)\nBULLET::::- Patrick Cleburne (Arkansas) – Major-General\nBULLET::::- John Brown Gordon (Georgia)Major-General\nBULLET::::- Henry Heth (Virginia)Major-General\nBULLET::::- Daniel Harvey Hill (South Carolina)Major-General\nBULLET::::- Edward Johnson (Virginia)Major-General\nBULLET::::- Joseph B. Kershaw (South Carolina)Major-General\nBULLET::::- Fitzhugh Lee (Virginia)Major-General\nBULLET::::- George Washington Custis Lee (Virginia)Major-General\nBULLET::::- William Henry Fitzhugh Lee (Virginia)Major-General\nBULLET::::- William Mahone (Virginia)Major-General\nBULLET::::- George Pickett (Virginia)Major-General\nBULLET::::- Camillus J. Polignac (France) – Major-General\nBULLET::::- Sterling Price (Missouri) – Major-General\nBULLET::::- Stephen Dodson Ramseur (North Carolina) – Major-General\nBULLET::::- Thomas L. Rosser (Virginia) – Major-General\nBULLET::::- J. E. B. Stuart (Virginia) – Major-General\nBULLET::::- Earl Van Dorn (Mississippi)Major-General\nBULLET::::- John A. Wharton (Tennessee) – Major-General\nBULLET::::- Edward Porter Alexander (Georgia) – Brigadier-General\nBULLET::::- Francis Marion Cockrell (Missouri) – Brigadier-General\nBULLET::::- Clement A. Evans (Georgia)Brigadier-General\nBULLET::::- John Hunt Morgan (Kentucky) – Brigadier-General\nBULLET::::- William N. Pendleton (Virginia) – Brigadier-General\nBULLET::::- Stand Watie (Georgia) – Brigadier-General (last to surrender)\nBULLET::::- Lawrence Sullivan Ross (Texas) – Brigadier-General\nBULLET::::- John S. Mosby, the \"Grey Ghost of the Confederacy\" (Virginia) – Colonel\nBULLET::::- Franklin Buchanan (Maryland) – Admiral\nBULLET::::- Raphael Semmes (Maryland) – Rear Admiral\nBULLET::::- Cabinet of the Confederate States\nBULLET::::- Commemoration of the American Civil War\nBULLET::::- Commemoration of the American Civil War on postage stamps\nBULLET::::- Confederados\nBULLET::::- Confederate colonies\nBULLET::::- Confederate Patent Office\nBULLET::::- Confederate States Army\nBULLET::::- Confederate States Congress\nBULLET::::- Confederate war finance\nBULLET::::- \"2004 film\"\nBULLET::::- Flags of the Confederate States\nBULLET::::- Golden Circle (proposed country)\nBULLET::::- History of the Southern United States\nBULLET::::- List of Confederate arms manufacturers\nBULLET::::- List of Confederate arsenals and armories\nBULLET::::- List of Confederate monuments\nBULLET::::- List of treaties of the Confederate States\nBULLET::::- National Civil War Naval Museum\nBULLET::::- Postage stamps and postal history of the Confederate States\nBULLET::::- President of the Confederate States\nBULLET::::- Prisoner of war camps\nBULLET::::- Seal of the Confederate States\n\nBULLET::::- Bowman, John S. (ed), \"The Civil War Almanac\", New York: Bison Books, 1983\nBULLET::::- Eicher, John H., & Eicher, David J., \"Civil War High Commands\", Stanford University Press, 2001,\nBULLET::::- Martis, Kenneth C. \"The Historical Atlas of the Congresses of the Confederate States of America 1861–1865\" (1994)\n\nBULLET::::- \"American Annual Cyclopaedia for 1861\" (N.Y.: Appleton's, 1864), an encyclopedia of events in the U.S. and CSA (and other countries); covers each state in detail\nBULLET::::- \"Appletons' annual cyclopedia and register of important events: Embracing political, military, and ecclesiastical affairs; public documents; biography, statistics, commerce, finance, literature, science, agriculture, and mechanical industry, Volume 3 1863\" (1864), thorough coverage of the events of 1863\nBULLET::::- Beringer, Richard E., Herman Hattaway, Archer Jones, and William N. Still Jr. \"Why the South Lost the Civil War\". Athens: University of Georgia Press, 1986. .\nBULLET::::- Boritt, Gabor S., and others., \"Why the Confederacy Lost\", (1992)\nBULLET::::- Coulter, E. Merton \"The Confederate States of America, 1861–1865\", 1950\nBULLET::::- Current, Richard N., ed. \"Encyclopedia of the Confederacy\" (4 vol), 1993. 1900 pages, articles by scholars.\nBULLET::::- Eaton, Clement \"A History of the Southern Confederacy\", 1954\nBULLET::::- Faust, Patricia L., ed. \"Historical Times Illustrated History of the Civil War\". New York: Harper & Row, 1986. .\nBULLET::::- Gallagher, Gary W. \"The Confederate War\". Cambridge, MA: Harvard University Press, 1997. .\nBULLET::::- Heidler, David S., and Jeanne T. Heidler, eds. \"Encyclopedia of the American Civil War: A Political, Social, and Military History\". New York: W. W. Norton & Company, 2000. . 2740 pages.\nBULLET::::- McPherson, James M. \"Battle Cry of Freedom: The Civil War Era\". Oxford History of the United States. New York: Oxford University Press, 1988. . standard military history of the war; Pulitzer Prize\nBULLET::::- Nevins, Allan. \"The War for the Union\". Vol. 1, \"The Improvised War 1861–1862\". New York: Charles Scribner's Sons, 1959. ; \"The War for the Union\". Vol. 2, \"War Becomes Revolution 1862–1863\". New York: Charles Scribner's Sons, 1960. ; \"The War for the Union\". Vol. 3, \"The Organized War 1863–1864\". New York: Charles Scribner's Sons, 1971. ; \"The War for the Union\". Vol. 4, \"The Organized War to Victory 1864–1865\". New York: Charles Scribner's Sons, 1971. . The most detailed history of the war.\nBULLET::::- Roland, Charles P. \"The Confederacy\", (1960) brief survey\nBULLET::::- Thomas, Emory M. \"The Confederate Nation, 1861–1865\". New York: Harper & Row, 1979. . Standard political-economic-social history\nBULLET::::- Wakelyn, Jon L. \"Biographical Dictionary of the Confederacy\" Greenwood Press\nBULLET::::- Weigley, Russell F. \"A Great Civil War: A Military and Political History, 1861–1865\". Bloomington and Indianapolis: Indiana University Press, 2000. .\nBULLET::::- Boles, John B. and Evelyn Thomas Nolen, eds. \"Interpreting Southern History: Historiographical Essays in Honor of Sanford W. Higginbotham\" (1987)\nBULLET::::- Grant, Susan-Mary, and Brian Holden Reid, eds. \"The American civil war: explorations and reconsiderations\" (Longman, 2000.)\nBULLET::::- Link, Arthur S. and Rembert W. Patrick, eds. \"Writing Southern History: Essays in Historiography in Honor of Fletcher M. Green\" (1965)\nBULLET::::- Woodworth, Steven E. ed. \"The American Civil War: A Handbook of Literature and Research\", 1996 750 pages of historiography and bibliography\n\nBULLET::::- Tucker, Spencer, ed. \"American Civil War: A State-by-State Encyclopedia\" (2 vol 2015) 1019pp\n\nBULLET::::- Ash, Stephen V. \"Middle Tennessee society transformed, 1860–1870: war and peace in the Upper South\" (2006)\nBULLET::::- Cooling, Benjamin Franklin. \"Fort Donelson's Legacy: War and Society in Kentucky and Tennessee, 1862–1863\" (1997)\nBULLET::::- Cottrell, Steve. \"Civil War in Tennessee\" (2001) 142pp\nBULLET::::- Crofts, Daniel W. \"Reluctant Confederates: Upper South Unionists in the Secession Crisis\". (1989) .\nBULLET::::- Dollar, Kent, and others. \"Sister States, Enemy States: The Civil War in Kentucky and Tennessee\" (2009)\nBULLET::::- Durham, Walter T. \"Nashville: The Occupied City, 1862–1863\" (1985); \"Reluctant Partners: Nashville and the Union, 1863–1865\" (1987)\nBULLET::::- Mackey, Robert R. \"The Uncivil War: Irregular Warfare in the Upper South, 1861–1865\" (University of Oklahoma Press, 2014)\nBULLET::::- Temple, Oliver P. \"East Tennessee and the civil war\" (1899) 588pp online edition\n\nBULLET::::- Fleming, Walter L. \"Civil War and Reconstruction in Alabama\" (1905). the most detailed study; Dunning School full text online from Project Gutenberg\nBULLET::::- Rainwater, Percy Lee. \"Mississippi: storm center of secession, 1856–1861\" (1938)\nBULLET::::- Rigdon, John. \"A Guide to Alabama Civil War Research\" (2011)\nBULLET::::- Smith, Timothy B. \"Mississippi in the Civil War: The Home Front\" University Press of Mississippi, (2010) 265 pages; Examines the declining morale of Mississippians as they witnessed extensive destruction and came to see victory as increasingly improbable\nBULLET::::- Sterkx, H. E. \"Partners in Rebellion: Alabama Women in the Civil War\" (Fairleigh Dickinson University Press, 1970)\nBULLET::::- Storey, Margaret M. \"Civil War Unionists and the Political Culture of Loyalty in Alabama, 1860–1861\". \"Journal of Southern History\" (2003): 71–106. in JSTOR\nBULLET::::- Storey, Margaret M., \"Loyalty and Loss: Alabama's Unionists in the Civil War and Reconstruction\". Baton Rouge: Louisiana State University Press, 2004.\nBULLET::::- Towns, Peggy Allen. \"Duty Driven: The Plight of North Alabama's African Americans During the Civil War\" (2012)\n\nBULLET::::- DeCredico, Mary A. \"Patriotism for Profit: Georgia's Urban Entrepreneurs and the Confederate War Effort\" (1990)\nBULLET::::- Fowler, John D. and David B. Parker, eds. \"Breaking the Heartland: The Civil War in Georgia\" (2011)\nBULLET::::- Hill, Louise Biles. \"Joseph E. Brown and the Confederacy\". (1972); He was the governor\nBULLET::::- Johns, John Edwin. \"Florida During the Civil War\" (University of Florida Press, 1963)\nBULLET::::- Johnson, Michael P. \"Toward A Patriarchal Republic: The Secession of Georgia\" (1977)\nBULLET::::- Mohr, Clarence L. \"On the Threshold of Freedom: Masters and Slaves in Civil War Georgia\" (1986)\nBULLET::::- Nulty, William H. \"Confederate Florida: The Road to Olustee\" (University of Alabama Press, 1994)\nBULLET::::- Parks, Joseph H. \"Joseph E. Brown of Georgia\" (LSU Press, 1977) 612 pages; Governor\nBULLET::::- Wetherington, Mark V. \"Plain Folk's Fight: The Civil War and Reconstruction in Piney Woods Georgia\" (2009)\n\nBULLET::::- Bailey, Anne J., and Daniel E. Sutherland, eds. \"Civil War Arkansas: beyond battles and leaders\" (Univ of Arkansas Pr, 2000)\nBULLET::::- Ferguson, John Lewis, ed. \"Arkansas and the Civil War\" (Pioneer Press, 1965)\nBULLET::::- Ripley, C. Peter. \"Slaves and Freedmen in Civil War Louisiana\" (LSU Press, 1976)\nBULLET::::- Snyder, Perry Anderson. \"Shreveport, Louisiana, during the Civil War and Reconstruction\" (1979)\nBULLET::::- Underwood, Rodman L. \"Waters of Discord: The Union Blockade of Texas During the Civil War\" (McFarland, 2003)\nBULLET::::- Winters, John D. \"The Civil War in Louisiana\" (LSU Press, 1991)\nBULLET::::- Woods, James M. \"Rebellion and Realignment: Arkansas's Road to Secession\". (1987)\nBULLET::::- Wooster, Ralph A. \"Civil War Texas\" (Texas A&M University Press, 2014)\n\nBULLET::::- Barrett, John G. \"The Civil War in North Carolina\" (1995)\nBULLET::::- Carbone, John S. \"The Civil War in Coastal North Carolina\" (2001)\nBULLET::::- Cauthen, Charles Edward; Power, J. Tracy. \"South Carolina goes to war, 1860–1865\" (1950)\nBULLET::::- Hardy, Michael C. \"North Carolina in the Civil War\" (2011)\nBULLET::::- Inscoe, John C. \"The Heart of Confederate Appalachia: Western North Carolina in the Civil War\" (2003)\nBULLET::::- Lee, Edward J. and Ron Chepesiuk, eds. \"South Carolina in the Civil War: The Confederate Experience in Letters and Diaries\" (2004), primary sources\n\nBULLET::::- Ayers, Edward L. and others. \"Crucible of the Civil War: Virginia from Secession to Commemoration\" (2008)\nBULLET::::- Bryan, T. Conn. \"Confederate Georgia\" (1953), the standard scholarly survey\nBULLET::::- Davis, William C. and James I. Robertson, Jr., eds. \"Virginia at War 1861\". Lexington, KY: University of Kentucky Press, 2005. ; \"Virginia at War 1862\" (2007); \"Virginia at War 1863\" (2009); \"Virginia at War 1864\" (2009); \"Virginia at War 1865\" (2012)\nBULLET::::- Snell, Mark A. \"West Virginia and the Civil War, Mountaineers Are Always Free\", (2011) .\nBULLET::::- Wallenstein, Peter, and Bertram Wyatt-Brown, eds. \"Virginia's Civil War\" (2008)\nBULLET::::- Furgurson, Ernest B. \"Ashes of Glory: Richmond at War\" (1997)\n\nBULLET::::- Ash, Stephen V. \"The Black Experience in the Civil War South\" (2010) online\nBULLET::::- Bartek, James M. \"The Rhetoric of Destruction: Racial Identity and Noncombatant Immunity in the Civil War Era.\" (PhD Dissertation, University of Kentucky, 2010). online; Bibliography pp 515–52.\nBULLET::::- Brown, Alexis Girardin. \"The Women Left Behind: Transformation of the Southern Belle, 1840–1880\" (2000) \"Historian\" 62#4 pp 759–778.\nBULLET::::- Cashin, Joan E. \"Torn Bonnets and Stolen Silks: Fashion, Gender, Race, and Danger in the Wartime South.\" \"Civil War History\" 61#4 (2015): 338–361. online\nBULLET::::- Chesson, Michael B. \"Harlots or Heroines? A New Look at the Richmond Bread Riot.\" \"Virginia Magazine of History and Biography\" 92#2 (1984): 131–175. in JSTOR\nBULLET::::- Clinton, Catherine, and Silber, Nina, eds. \"Divided Houses: Gender and the Civil War\" (1992)\nBULLET::::- Davis, William C. and James I. Robertson Jr., eds. \"Virginia at War, 1865\" (2012) online\nBULLET::::- Elliot, Jane Evans. \"Diary of Mrs. Jane Evans Elliot, 1837–1882\" (1908)\nBULLET::::- Faust, Drew Gilpin. \"Mothers of Invention: Women of the Slaveholding South in the American Civil War\" (1996)\nBULLET::::- Faust, Drew Gilpin. \"This Republic of Suffering: Death and the American Civil War\" (2008)\nBULLET::::- Frank, Lisa Tendrich, ed. \"Women in the American Civil War\" (2008)\nBULLET::::- Frankel, Noralee. \"Freedom's Women: Black Women and Families in Civil War Era Mississippi\" (1999)\nBULLET::::- Gleeson. David T. \"The Green and the Gray: The Irish in the Confederate States of America\" (U of North Carolina Press, 2013); online review\nBULLET::::- Levine, Bruce. \"The Fall of the House of Dixie: The Civil War and the Social Revolution That Transformed the South\" (2013)\nBULLET::::- Lowry, Thomas P. \"The Story the Soldiers Wouldn't Tell: Sex in the Civil War\" (Stackpole Books, 1994).\nBULLET::::- Litwack, Leon F. \"Been in the Storm So Long: The Aftermath of Slavery\" (1979), on freed slaves\nBULLET::::- Massey, Mary Elizabeth \"Bonnet Brigades: American Women and the Civil War\" (1966)\nBULLET::::- Massey, Mary Elizabeth \"Refugee Life in the Confederacy\", (1964)\nBULLET::::- Rable, George C. \"Civil Wars: Women and the Crisis of Southern Nationalism\" (1989)\nBULLET::::- Slap, Andrew L. and Frank Towers, eds. \"Confederate Cities: The Urban South during the Civil War Era\" (U of Chicago Press, 2015). 302 pp.\nBULLET::::- Stokes, Karen. \"South Carolina Civilians in Sherman's Path: Stories of Courage Amid Civil War Destruction\" (The History Press, 2012).\nBULLET::::- Whites, LeeAnn. \"The Civil War as a Crisis in Gender: Augusta, Georgia, 1860–1890\" (1995)\nBULLET::::- Wiley, Bell Irwin \"Southern Negroes: 1861–1865\" (1938)\nBULLET::::- Wiley, Bell Irwin \"Confederate Women\" (1975)\nBULLET::::- Wiley, Bell Irwin \"The Plain People of the Confederacy\" (1944)\nBULLET::::- Woodward, C. Vann, ed. \"Mary Chesnut's Civil War\", 1981, detailed diary; primary source\n\nBULLET::::- Bernath, Michael T. \"Confederate Minds: The Struggle for Intellectual Independence in the Civil War South\" (University of North Carolina Press; 2010) 412 pages. Examines the efforts of writers, editors, and other \"cultural nationalists\" to free the South from the dependence on Northern print culture and educational systems.\nBULLET::::- Bonner, Robert E., \"Proslavery Extremism Goes to War: The Counterrevolutionary Confederacy and Reactionary Militarism\", \"Modern Intellectual History\", 6 (August 2009), 261–85.\nBULLET::::- Downing, David C. \"A South Divided: Portraits of Dissent in the Confederacy\". (2007).\nBULLET::::- Faust, Drew Gilpin. \"The Creation of Confederate Nationalism: Ideology and Identity in the Civil War South\". (1988)\nBULLET::::- Hutchinson, Coleman. \"Apples and Ashes: Literature, Nationalism, and the Confederate States of America\". Athens, Georgia: University of Georgia Press, 2012.\nBULLET::::- Lentz, Perry Carlton \"Our Missing Epic: A Study in the Novels about the American Civil War\", 1970\nBULLET::::- Rubin, Anne Sarah. \"A Shattered Nation: The Rise and Fall of the Confederacy, 1861–1868\", 2005 A cultural study of Confederates' self images\n\nBULLET::::- Alexander, Thomas B., and Beringer, Richard E. \"The Anatomy of the Confederate Congress: A Study of the Influences of Member Characteristics on Legislative Voting Behavior, 1861–1865\", (1972)\nBULLET::::- Cooper, William J, \"Jefferson Davis, American\" (2000), standard biography\nBULLET::::- Davis, William C. \"A Government of Our Own: The Making of the Confederacy\". New York: The Free Press, a division of Macmillan, Inc., 1994. .\nBULLET::::- Eckenrode, H. J., \"Jefferson Davis: President of the South\", 1923\nBULLET::::- Levine, Bruce. \"Confederate Emancipation: Southern Plans to Free and Arm Slaves during the Civil War\". (2006)\nBULLET::::- Martis, Kenneth C., \"The Historical Atlas of the Congresses of the Confederate States of America 1861–1865\" (1994)\nBULLET::::- Neely, Mark E. Jr., \"Confederate Bastille: Jefferson Davis and Civil Liberties\" (1993)\nBULLET::::- Neely, Mark E. Jr. \"Southern Rights: Political Prisoners and the Myth of Confederate Constitutionalism\". (1999)\nBULLET::::- George C. Rable \"The Confederate Republic: A Revolution against Politics\", 1994\nBULLET::::- Rembert, W. Patrick \"Jefferson Davis and His Cabinet\" (1944).\nBULLET::::- Williams, William M. \"Justice in Grey: A History of the Judicial System of the Confederate States of America\" (1941)\nBULLET::::- Yearns, Wilfred Buck \"The Confederate Congress\" (1960)\n\nBULLET::::- Blumenthal, Henry. \"Confederate Diplomacy: Popular Notions and International Realities\", \"Journal of Southern History\", Vol. 32, No. 2 (May 1966), pp. 151–171 in JSTOR\nBULLET::::- Daddysman, James W. \"The Matamoros Trade: Confederate Commerce, Diplomacy, and Intrigue\". (1984)\nBULLET::::- Foreman, Amanda. \"A World on Fire: Britain's Crucial Role in the American Civil War\" (2011) especially on Brits inside the Confederacy;\nBULLET::::- Hubbard, Charles M. \"The Burden of Confederate Diplomacy\" (1998)\nBULLET::::- Jones, Howard. \"Blue and Gray Diplomacy: A History of Union and Confederate Foreign Relations\" (2009)\nBULLET::::- Jones, Howard. \"Union in Peril: The Crisis Over British Intervention in the Civil War\". Lincoln, NE: University of Nebraska Press, Bison Books, 1997. . Originally published: Chapel Hill: University of North Carolina Press, 1992.\nBULLET::::- Mahin, Dean B. \"One War at a Time: The International Dimensions of the American Civil War\". Washington, DC: Brassey's, 2000. . Originally published: Washington, DC: Brassey's, 1999.\nBULLET::::- Merli, Frank J. \"The Alabama, British Neutrality, and the American Civil War\" (2004). 225 pp.\nBULLET::::- Owsley, Frank. \"King Cotton Diplomacy: Foreign Relations of the Confederate States of America\" (2nd ed. 1959)\nBULLET::::- Sainlaude, Steve. \"La France et la Confédération sudiste\" (2011)\nBULLET::::- Sainlaude, Steve. \"Le gouvernement impérial et la guerre de Sécession\" (2011)\n\nBULLET::::- Black, III, Robert C. \"The Railroads of the Confederacy\". Chapel Hill: University of North Carolina Press, 1952, 1988. .\nBULLET::::- Bonner, Michael Brem. \"Expedient Corporatism and Confederate Political Economy\", \"Civil War History\", 56 (March 2010), 33–65.\nBULLET::::- Dabney, Virginius \"Richmond: The Story of a City\". Charlottesville: The University of Virginia Press, 1990\nBULLET::::- Grimsley, Mark \"The Hard Hand of War: Union Military Policy toward Southern Civilians, 1861–1865\", 1995\nBULLET::::- Hurt, R. Douglas. \"Agriculture and the Confederacy: Policy, Productivity, and Power in the Civil War South\" (2015)\nBULLET::::- Massey, Mary Elizabeth \"Ersatz in the Confederacy: Shortages and Substitutes on the Southern Homefront\" (1952)\nBULLET::::- Paskoff, Paul F. \"Measures of War: A Quantitative Examination of the Civil War's Destructiveness in the Confederacy\", \"Civil War History\" (2008) 54#1 pp 35–62 in Project MUSE\nBULLET::::- Ramsdell, Charles. \"Behind the Lines in the Southern Confederacy\", 1994.\nBULLET::::- Roark, James L. \"Masters without Slaves: Southern Planters in the Civil War and Reconstruction\", 1977.\nBULLET::::- Thomas, Emory M. \"The Confederacy as a Revolutionary Experience\", 1992\n\nBULLET::::- Carter, Susan B., ed. \"The Historical Statistics of the United States: Millennial Edition\" (5 vols), 2006\nBULLET::::- Commager, Henry Steele. \"The Blue and the Gray: The Story of the Civil War As Told by Participants\". 2 vols. Indianapolis and New York: The Bobbs-Merrill Company, Inc., 1950. . Many reprints.\nBULLET::::- Davis, Jefferson. \"The Rise of the Confederate Government\". New York: Barnes & Noble, 2010. Original edition: 1881. .\nBULLET::::- Davis, Jefferson. \"The Fall of the Confederate Government\". New York: Barnes & Noble, 2010. Original edition: 1881. .\nBULLET::::- Harwell, Richard B., \"The Confederate Reader\" (1957)\nBULLET::::- Hettle, Wallace, ed. \"The Confederate Homefront: A History in Documents\" (LSU Press, 2017) 214 pp\nBULLET::::- Jones, John B. \"A Rebel War Clerk's Diary at the Confederate States Capital\", edited by Howard Swiggert, [1935] 1993. 2 vols.\nBULLET::::- Richardson, James D., ed. \"A Compilation of the Messages and Papers of the Confederacy, Including the Diplomatic Correspondence 1861–1865\", 2 volumes, 1906.\nBULLET::::- Yearns, W. Buck and Barret, John G., eds. \"North Carolina Civil War Documentary\", 1980.\nBULLET::::- Confederate official government documents major online collection of complete texts in HTML format, from University of North Carolina\nBULLET::::- \"Journal of the Congress of the Confederate States of America, 1861–1865\" (7 vols), 1904. Available online at the Library of Congress0\n\nBULLET::::- Confederate offices Index of Politicians by Office Held or Sought\nBULLET::::- Civil War Research & Discussion Group -*\"Confederate States of Am. Army and Navy Uniforms\", 1861\nBULLET::::- \"The Countryman\", 1862–1866, published weekly by Turnwold, Ga., edited by J.A. Turner\nBULLET::::- \"The Federal and the Confederate Constitution Compared\"\nBULLET::::- \"Confederate Postage Stamps\"\nBULLET::::- Photographs of the original Confederate Constitution and other Civil War documents owned by the Hargrett Rare Book and Manuscript Library at the University of Georgia Libraries.\nBULLET::::- \"Photographic History of the Civil War\", 10 vols., 1912.\nBULLET::::- DocSouth: Documenting the American South – numerous online text, image, and audio collections.\nBULLET::::- The Boston Athenæum has over 4000 Confederate imprints, including rare books, pamphlets, government documents, manuscripts, serials, broadsides, maps, and sheet music that have been conserved and digitized.\nBULLET::::- Oklahoma Digital Maps: Digital Collections of Oklahoma and Indian Territory\nBULLET::::- Confederate States of America Collection at the Library of Congress\n"}
{"id": "7025", "url": "https://en.wikipedia.org/wiki?curid=7025", "title": "Cranberry", "text": "Cranberry\n\nCranberries are a group of evergreen dwarf shrubs or trailing vines in the subgenus Oxycoccus of the genus \"Vaccinium\". In Britain, cranberry may refer to the native species \"Vaccinium oxycoccos\", while in North America, cranberry may refer to \"Vaccinium macrocarpon\". \"Vaccinium oxycoccos\" is cultivated in central and northern Europe, while \"Vaccinium macrocarpon\" is cultivated throughout the northern United States, Canada and Chile. In some methods of classification, \"Oxycoccus\" is regarded as a genus in its own right. They can be found in acidic bogs throughout the cooler regions of the Northern Hemisphere.\n\nCranberries are low, creeping shrubs or vines up to long and in height; they have slender, wiry stems that are not thickly woody and have small evergreen leaves. The flowers are dark pink, with very distinct \"reflexed\" petals, leaving the style and stamens fully exposed and pointing forward. They are pollinated by bees. The fruit is a berry that is larger than the leaves of the plant; it is initially light green, turning red when ripe. It is edible, but with an acidic taste that usually overwhelms its sweetness.\n\nIn 2016, 98% of world production of cranberries resulted from the United States, Canada, and Chile. Most cranberries are processed into products such as juice, sauce, jam, and sweetened dried cranberries, with the remainder sold fresh to consumers. Cranberry sauce is a traditional accompaniment to turkey at Christmas dinner in the United Kingdom, and at Christmas and Thanksgiving dinners in the United States and Canada.\n\nThere are three to four species of cranberry, classified into two sections:\n\nBULLET::::- Subgenus \"Oxycoccus\", sect. \"Oxycoccus\"\nBULLET::::- \"Vaccinium oxycoccos\" or \"Oxycoccus palustris\" (common cranberry, northern cranberry or cranberry) is widespread throughout the cool temperate Northern Hemisphere, including northern Europe, northern Asia, and northern North America. It has small leaves. The flowers are dark pink, with a purple central spike, produced on finely hairy stems. The fruit is a small pale pink berry, with a refreshing sharp acidic flavor.\nBULLET::::- \"Vaccinium microcarpum\" or \"Oxycoccus microcarpus\" (small cranberry) occurs in northern North America, northern Europe and northern Asia, and differs from \"V. oxycoccos\" in the leaves being more triangular, and the flower stems hairless; additionally, their stems can also be smaller and produce a smaller number of flowers than \"V. ocycoccos\". They also differ in the fact that their leaves can also be smaller in size, even though the main difference is their triangular shape. Some botanists include it within \"V. oxycoccos\".\nBULLET::::- \"Vaccinium macrocarpon\" or \"Oxycoccus macrocarpus\" (large cranberry, American cranberry, bearberry) native to northern North America across Canada, and eastern United States, south to North Carolina at high altitudes). It differs from \"V. oxycoccos\" in the leaves being larger, long, and in its slightly apple-like taste.\nBULLET::::- Subgenus \"Oxycoccus\", sect. \"Oxycoccoides\"\nBULLET::::- \"Vaccinium erythrocarpum\" or \"Oxycoccus erythrocarpus\" (southern mountain cranberry) native to southeastern North America at high altitudes in the southern Appalachian Mountains, and also in eastern Asia.\n\nCranberries are related to bilberries, blueberries, and huckleberries, all in \"Vaccinium\" subgenus \"Vaccinium\". These differ in having bell-shaped flowers, the petals not being reflexed, and woodier stems, forming taller shrubs.\n\nSome plants of the completely unrelated genus \"Viburnum\" are sometimes called \"highbush cranberries\" (e.g. \"Viburnum trilobum\").\n\nCranberries are susceptible to false blossom, a harmful but controllable phytoplasma disease common in the eastern production areas of Massachusetts and New Jersey.\n\nThe name, \"cranberry\", derives from the German, \"kraanbere\" (English translation, \"craneberry\"), first named as \"cranberry\" in English by the missionary John Eliot in 1647. Around 1694, German and Dutch colonists in New England used the word, cranberry, to represent the expanding flower, stem, calyx, and petals resembling the neck, head, and bill of a crane. The traditional English name for the plant more common in Europe, \"Vaccinium oxycoccos\", , originated from plants with small red berries found growing in fen (marsh) lands of England.\n\nIn North America, the Narragansett people of the Algonquian nation in the regions of New England appeared to be using cranberries in pemmican for food and for dye. Calling the red berries, \"sasemineash\", the Narragansett people may have introduced cranberries to colonists in Massachusetts. In 1550, James White Norwood made reference to Native Americans using cranberries, and it was the first reference to American cranberries up until this point. In James Rosier's book \"The Land of Virginia\" there is an account of Europeans coming ashore and being met with Native Americans bearing bark cups full of cranberries. In Plymouth, Massachusetts, there is a 1633 account of the husband of Mary Ring auctioning her cranberry-dyed petticoat for 16 shillings. In 1643, Roger Williams's book \"A Key Into the Language of America\" described cranberries, referring to them as \"bearberries\" because bears ate them. In 1648, preacher John Elliott was quoted in Thomas Shepard's book \"Clear Sunshine of the Gospel\" with an account of the difficulties the Pilgrims were having in using the Indians to harvest cranberries as they preferred to hunt and fish. In 1663, the Pilgrim cookbook appears with a recipe for cranberry sauce. In 1667, New Englanders sent to King Charles ten barrels of cranberries, three barrels of codfish and some Indian corn as a means of appeasement for his anger over their local coining of the Pine-tree shilling. In 1669, Captain Richard Cobb had a banquet in his house (to celebrate both his marriage to Mary Gorham and his election to the Convention of Assistance), serving wild turkey with sauce made from wild cranberries. In the 1672 book \"New England Rarities Discovered\" author John Josselyn described cranberries, writing:\n\nSauce for the Pilgrims, cranberry or bearberry, is a small trayling [\"sic\"] plant that grows in salt marshes that are overgrown with moss. The berries are of a pale yellow color, afterwards red, as big as a cherry, some perfectly round, others oval, all of them hollow with sower [\"sic\"] astringent taste; they are ripe in August and September. They are excellent against the Scurvy. They are also good to allay the fervor of hoof diseases. The Indians and English use them mush, boyling [\"sic\"] them with sugar for sauce to eat with their meat; and it is a delicate sauce, especially with roasted mutton. Some make tarts with them as with gooseberries.\n\n\"The Compleat Cook's Guide\", published in 1683, made reference to cranberry juice. In 1703, cranberries were served at the Harvard University commencement dinner. In 1787, James Madison wrote Thomas Jefferson in France for background information on constitutional government to use at the Constitutional Convention. Jefferson sent back a number of books on the subject and in return asked for a gift of apples, pecans and cranberries. William Aiton, a Scottish botanist, included an entry for the cranberry in volume II of his 1789 work \"Hortus Kewensis\". He notes that \"Vaccinium macrocarpon\" (American cranberry) was cultivated by James Gordon in 1760. In 1796, cranberries were served at the first celebration of the landing of the Pilgrims, and Amelia Simmons (an American orphan) wrote a book entitled \"American Cookery\" which contained a recipe for cranberry tarts.\n\nAmerican Revolutionary War veteran Henry Hall first cultivated cranberries in the Cape Cod town of Dennis around 1816. In the 1820s, Hall was shipping cranberries to New York City and Boston from which shipments were also sent to Europe. In 1843, Eli Howes planted his own crop of cranberries on Cape Cod, using the \"Howes\" variety. In 1847, Cyrus Cahoon planted a crop of \"Early Black\" variety near Pleasant Lake, Harwich, Massachusetts.\n\nBy 1900, were under cultivation in the New England region. In 2014, the total area of cranberries harvested in the United States was , with Massachusetts as the second largest producer after Wisconsin.\n\nHistorically, cranberry beds were constructed in wetlands. Today's cranberry beds are constructed in upland areas with a shallow water table. The topsoil is scraped off to form dykes around the bed perimeter. Clean sand is hauled in and spread to a depth of four to eight inches (10 to 20 centimeters). The surface is laser leveled flat to provide even drainage. Beds are frequently drained with socked tile in addition to the perimeter ditch. In addition to making it possible to hold water, the dykes allow equipment to service the beds without driving on the vines. Irrigation equipment is installed in the bed to provide irrigation for vine growth and for spring and autumn frost protection.\n\nA common misconception about cranberry production is that the beds remain flooded throughout the year. During the growing season cranberry beds are not flooded, but are irrigated regularly to maintain soil moisture. Beds are flooded in the autumn to facilitate harvest and again during the winter to protect against low temperatures. In cold climates like Wisconsin, New England, and eastern Canada, the winter flood typically freezes into ice, while in warmer climates the water remains liquid. When ice forms on the beds, trucks can be driven onto the ice to spread a thin layer of sand to control pests and rejuvenate the vines. Sanding is done every three to five years. Additionally, climate change could prove to be an issue for the cultivation of cranberries in the future. It is possible that, given rising temperatures over the next 50 years, chilling temperatures for harvesting cranberries may be insufficient for the process.\n\nCranberry vines are propagated by moving vines from an established bed. The vines are spread on the surface of the sand of the new bed and pushed into the sand with a blunt disk. The vines are watered frequently during the first few weeks until roots form and new shoots grow. Beds are given frequent, light application of nitrogen fertilizer during the first year. The cost of renovating cranberry beds is estimated to be between .\n\nCranberries are harvested in the fall when the fruit takes on its distinctive deep red color. Berries that receive sun turn a deep red when fully ripe, while those that do not fully mature are a pale pink or white color. This is usually in September through the first part of November. To harvest cranberries, the beds are flooded with six to eight inches (15 to 20 centimeters) of water above the vines. A harvester is driven through the beds to remove the fruit from the vines. For the past 50 years, water reel type harvesters have been used. Harvested cranberries float in the water and can be corralled into a corner of the bed and conveyed or pumped from the bed. From the farm, cranberries are taken to receiving stations where they are cleaned, sorted, and stored prior to packaging or processing. While cranberries are harvested when they take on their deep red color, they can also be harvested beforehand when they are still white, which is how white cranberry juice is made. Yields are lower on beds harvested early and the early flooding tends to damage vines, but not severely. Vines can also be trained through dry picking to help avoid damage in subsequent harvests.\n\nAlthough most cranberries are wet-picked as described above, 5–10% of the US crop is still dry-picked. This entails higher labor costs and lower yield, but dry-picked berries are less bruised and can be sold as fresh fruit instead of having to be immediately frozen or processed. Originally performed with two-handed comb scoops, dry picking is today accomplished by motorized, walk-behind harvesters which must be small enough to traverse beds without damaging the vines.\n\nCranberries for fresh market are stored in shallow bins or boxes with perforated or slatted bottoms, which deter decay by allowing air to circulate. Because harvest occurs in late autumn, cranberries for fresh market are frequently stored in thick walled barns without mechanical refrigeration. Temperatures are regulated by opening and closing vents in the barn as needed. Cranberries destined for processing are usually frozen in bulk containers shortly after arriving at a receiving station.\n\n! colspan=2Cranberry production – 2017\n! style=\"background:#ddf; width:75%;\" Country\n! style=\"background:#ddf; width:25%;\" \ncolspan=2\n\nIn 2017, world production of cranberry was 625,181 tonnes, mainly by the United States, Canada, and Chile, which collectively accounted for 97% of the global total (table). Wisconsin (65% of US production) and Quebec were the two largest regional producers of cranberries in North America. Cranberries are also a major commercial crop in Massachusetts (23% of US production), New Jersey, Oregon, and Washington, as well as in the Canadian provinces of British Columbia, New Brunswick, Ontario, Nova Scotia, Prince Edward Island, and Newfoundland.\n\nAs fresh cranberries are hard, sour, and bitter, about 95% of cranberries are processed and used to make cranberry juice and sauce. They are also sold dried and sweetened. Cranberry juice is usually sweetened or blended with other fruit juices to reduce its natural tartness. At one teaspoon of sugar per ounce, cranberry juice cocktail is more highly sweetened than even soda drinks that have been linked to obesity.\n\nUsually cranberries as fruit are cooked into a compote or jelly, known as cranberry sauce. Such preparations are traditionally served with roast turkey, as a staple of English Christmas dinners, and Thanksgiving (both in Canada and in the United States). The berry is also used in baking (muffins, scones, cakes and breads). In baking it is often combined with orange or orange zest. Less commonly, cranberries are used to add tartness to savory dishes such as soups and stews.\n\nFresh cranberries can be frozen at home, and will keep up to nine months; they can be used directly in recipes without thawing.\n\nThere are several alcoholic cocktails, including the Cosmopolitan, that include cranberry juice.\n\nRaw cranberries are 87% water, 12% carbohydrates, and contain negligible protein and fat (table). In a 100 gram reference amount, raw cranberries supply 46 calories and moderate levels of vitamin C, dietary fiber, and the essential dietary mineral, manganese, each with more than 10% of its Daily Value. Other micronutrients have low content (table).\n\nA comprehensive review in 2012 of available research concluded there is no evidence that cranberry juice or cranberry extract as tablets or capsules are effective in preventing urinary tract infections (UTIs). The European Food Safety Authority reviewed the evidence for one brand of cranberry extract and concluded a cause and effect relationship has not been established between the consumption of the product and reducing the risk of UTIs.\n\nOne systematic review in 2017 showed that cranberry products reduced the incidence of UTIs, indicating that cranberry products may be effective for individuals with recurrent infections. When the quality of meta-analyses on the efficacy of cranberry products for preventing or treating UTIs is examined, large variation and uncertainty of effect are seen, resulting from inconsistencies of clinical research.\n\nRaw cranberries, cranberry juice and cranberry extracts are a source of polyphenols – including proanthocyanidins, flavonols and quercetin. These phytochemical compounds are being studied in vivo and in vitro for possible effects on the cardiovascular system, immune system and cancer. However, there is no confirmation from human studies that consuming cranberry polyphenols provides anti-cancer, immune, or cardiovascular benefits. Potential is limited by poor absorption and rapid excretion.\n\nCranberry juice contains a high molecular weight non-dializable material that is under research for its potential to affect formation of plaque by \"Streptococcus mutans\" pathogens that cause tooth decay. Cranberry juice components are also being studied for possible effects on kidney stone formation.\n\nProblems may arise with the lack of validation for quantifying of A-type proanthocyanidins (PAC) extracted from cranberries. For instance, PAC extract quality and content can be performed using different methods including the European Pharmacopoeia method, liquid chromatography–mass spectrometry, or a modified 4-dimethylaminocinnamaldehyde colorimetric method. Variations in extract analysis can lead to difficulties in assessing the quality of PAC extracts from different cranberry starting material, such as by regional origin, ripeness at time of harvest and post-harvest processing. Assessments show that quality varies greatly from one commercial PAC extract product to another.\n\nThe anticoagulant effects of warfarin may be increased by consuming cranberry juice, resulting in adverse effects such as increased incidence of bleeding and bruising. Other safety concerns from consuming large quantities of cranberry juice or using cranberry supplements include potential for nausea, increasing stomach inflammation, sugar intake or kidney stone formation.\n\nCranberry sales in the United States have traditionally been associated with holidays of Thanksgiving and Christmas.\n\nIn the U.S., large-scale cranberry cultivation has been developed as opposed to other countries. American cranberry growers have a long history of cooperative marketing. As early as 1904, John Gaynor, a Wisconsin grower, and A.U. Chaney, a fruit broker from Des Moines, Iowa, organized Wisconsin growers into a cooperative called the Wisconsin Cranberry Sales Company to receive a uniform price from buyers. Growers in New Jersey and Massachusetts were also organized into cooperatives, creating the National Fruit Exchange that marketed fruit under the Eatmor brand. The success of cooperative marketing almost led to its failure. With consistent and high prices, area and production doubled between 1903 and 1917 and prices fell.\n\nWith surplus cranberries and changing American households some enterprising growers began canning cranberries that were below-grade for fresh market. Competition between canners was fierce because profits were thin. The Ocean Spray cooperative was established in 1930 through a merger of three primary processing companies: Ocean Spray Preserving company, Makepeace Preserving Co, and Cranberry Products Co. The new company was called Cranberry Canners, Inc. and used the Ocean Spray label on their products. Since the new company represented over 90% of the market, it would have been illegal (cf. antitrust) had attorney John Quarles not found an exemption for agricultural cooperatives. Morris April Brothers were the producers of Eatmor brand cranberry sauce, in Tuckahoe, New Jersey; Morris April Brothers brought an action against Ocean Spray for violation of the Sherman Antitrust Act and won $200,000 in real damages plus triple damages, in 1958, just in time for the Great Cranberry Scare of 1959. , about 65% of the North American industry belongs to the Ocean Spray cooperative. (The percentage may be slightly higher in Canada than in the U.S.)\n\nA turning point for the industry occurred on 9 November 1959, when the secretary of the United States Department of Health, Education, and Welfare Arthur S. Flemming announced that some of the 1959 crop was tainted with traces of the herbicide aminotriazole. The market for cranberries collapsed and growers lost millions of dollars. However, the scare taught the industry that they could not be completely dependent on the holiday market for their products: they had to find year-round markets for their fruit. They also had to be exceedingly careful about their use of pesticides. After the aminotriazole scare, Ocean Spray reorganized and spent substantial sums on product development. New products such as cranberry/apple juice blends were introduced, followed by other juice blends.\n\nA Federal Marketing Order that is authorized to synchronize supply and demand was approved in 1962. The order has been renewed and modified slightly in subsequent years, but it has allowed for more stable marketing. The market order has been invoked during six crop years: 1962 (12%), 1963 (5%), 1970 (10%), 1971 (12%), 2000 (15%), and 2001 (35%). Even though supply still exceeds demand, there is little will to invoke the Federal Marketing Order out of the realization that any pullback in supply by U.S. growers would easily be filled by Canadian production.\n\nPrices and production increased steadily during the 1980s and 1990s. Prices peaked at about $65.00 per barrel ()—a cranberry barrel equals . in 1996 then fell to $18.00 per barrel () in 2001. The cause for the precipitous drop was classic oversupply. Production had outpaced consumption leading to substantial inventory in freezers or as concentrate.\n\nCranberry handlers (processors) include Ocean Spray, Cliffstar Corporation, Northland Cranberries Inc.[Sun Northland LLC], Clement Pappas & Co., and Decas Cranberry Products as well as a number of small handlers and processors.\n\nThe Cranberry Marketing Committee is an organization that represents United States cranberry growers in four marketing order districts. The committee was established in 1962 as a Federal Marketing Order to ensure a stable, orderly supply of good quality product. The Cranberry Marketing Committee, based in Wareham, Massachusetts, represents more than 1,100 cranberry growers and 60 cranberry handlers across Massachusetts, Rhode Island, Connecticut, New Jersey, Wisconsin, Michigan, Minnesota, Oregon, Washington and New York (Long Island). The authority for the actions taken by the Cranberry Marketing Committee is provided in Chapter IX, Title 7, Code of Federal Regulations which is called the Federal Cranberry Marketing Order. The Order is part of the Agricultural Marketing Agreement Act of 1937, identifying cranberries as a commodity good that can be regulated by Congress. The Federal Cranberry Marketing Order has been altered over the years to expand the Cranberry Marketing Committee's ability to develop projects in the United States and around the world. The Cranberry Marketing Committee currently runs promotional programs in the United States, China, India, Mexico, Pan-Europe, and South Korea.\n\n, the European Union was the largest importer of American cranberries, followed individually by Canada, China, Mexico, and South Korea. From 2013 to 2017, US cranberry exports to China grew exponentially, making China the second largest country importer, reaching $36 million in cranberry products. The China–United States trade war resulted in many Chinese businesses cutting off ties with their U.S. cranberry suppliers.\n\nBULLET::::- Books\nBULLET::::- Cole, S. & Gifford, L. (2009). \"The Cranberry: Hard Work and Holiday Sauce\". Tilbury House Publishers.\nBULLET::::- Trehane, J. (2009). \"Blueberries, Cranberries and Other Vacciniums\". Timber Press.\n\nBULLET::::- Germplasm Resources Information Network: Sect. \"Oxycoccus\" and Sect. \"Oxycoccoides\"\nBULLET::::- University of Massachusetts Amherst Cranberry Station for information on cranberry research\nBULLET::::- Cranberry Library Page Hosted by the University of Wisconsin-Madison\nBULLET::::- Wikimapia An overhead view of a cranberry farm near Wisconsin Rapids, Wisconsin\nBULLET::::- Cranberry research at Rutgers, The State University of New Jersey\nBULLET::::- University of Massachusetts Cranberry Station Hosted by the University of Massachusetts - Amherst\n"}
{"id": "7030", "url": "https://en.wikipedia.org/wiki?curid=7030", "title": "Code coverage", "text": "Code coverage\n\nIn computer science, test coverage is a measure used to describe the degree to which the source code of a program is executed when a particular test suite runs. A program with high test coverage, measured as a percentage, has had more of its source code executed during testing, which suggests it has a lower chance of containing undetected software bugs compared to a program with low test coverage. Many different metrics can be used to calculate test coverage; some of the most basic are the percentage of program subroutines and the percentage of program statements called during execution of the test suite.\n\nTest coverage was among the first methods invented for systematic software testing. The first published reference was by Miller and Maloney in \"Communications of the ACM\" in 1963.\n\nTo measure what percentage of code has been exercised by a test suite, one or more \"coverage criteria\" are used. Coverage criteria are usually defined as rules or requirements, which a test suite needs to satisfy.\n\nThere are a number of coverage criteria, the main ones being:\nBULLET::::- Function coverageHas each function (or subroutine) in the program been called?\nBULLET::::- Statement coverageHas each statement in the program been executed?\nBULLET::::- Edge coveragehas every edge in the Control flow graph been executed?\nBULLET::::- Branch coverageHas each branch (also called DD-path) of each control structure (such as in \"if\" and \"case\" statements) been executed? For example, given an \"if\" statement, have both the true and false branches been executed? Notice that this one is a subset of Edge coverage.\nBULLET::::- Condition coverage (or predicate coverage)Has each Boolean sub-expression evaluated both to true and false?\n\nFor example, consider the following C function:\nAssume this function is a part of some bigger program and this program was run with some test suite. \nBULLET::::- If during this execution function 'foo' was called at least once, then \"function coverage\" for this function is satisfied.\nBULLET::::- \"Statement coverage\" for this function will be satisfied if it was called e.g. as codice_1, as in this case, every line in the function is executed including codice_2.\nBULLET::::- Tests calling codice_1 and codice_4 will satisfy \"branch coverage\" because, in the first case, both codice_5 conditions are met and codice_2 is executed, while in the second case, the first condition codice_7 is not satisfied, which prevents executing codice_2.\nBULLET::::- \"Condition coverage\" can be satisfied with tests that call codice_9 and codice_4. These are necessary because in the first cases, codice_7 evaluates to codice_12, while in the second, it evaluates codice_13. At the same time, the first case makes codice_14 codice_13, while the second makes it codice_12.\n\nCondition coverage does not necessarily imply branch coverage. For example, consider the following fragment of code:\n\nCondition coverage can be satisfied by two tests:\nBULLET::::- codice_17, codice_18\nBULLET::::- codice_19, codice_20\nHowever, this set of tests does not satisfy branch coverage since neither case will meet the codice_5 condition.\n\nFault injection may be necessary to ensure that all conditions and branches of exception handling code have adequate coverage during testing.\n\nA combination of function coverage and branch coverage is sometimes also called\ndecision coverage. This criterion requires that every point of entry and exit in the program has been invoked at least once, and every decision in the program has taken on all possible outcomes at least once. In this context the decision is a boolean expression composed of conditions and zero or more boolean operators. This definition is not the same as branch coverage, however, some do use the term \"decision coverage\" as a synonym for \"branch coverage\".\n\nCondition/decision coverage requires that both decision and condition coverage be satisfied. However, for safety-critical applications (e.g., for avionics software) it is often required that modified condition/decision coverage (MC/DC) be satisfied. This criterion extends condition/decision criteria with requirements that each condition should affect the decision outcome independently. For example, consider the following code:\n\nThe condition/decision criteria will be satisfied by the following set of tests:\nBULLET::::- a=true, b=true, c=true\nBULLET::::- a=false, b=false, c=false\nHowever, the above tests set will not satisfy modified condition/decision coverage, since in the first test, the value of 'b' and in the second test the value of 'c' would not influence the output. So, the following test set is needed to satisfy MC/DC:\nBULLET::::- a=false, b=true, c=false\nBULLET::::- a=false, b=true, c=true\nBULLET::::- a=false, b=false, c=true\nBULLET::::- a=true, b=false, c=true\n\nThis criterion requires that all combinations of conditions inside each decision are tested. For example, the code fragment from the previous section will require eight tests:\nBULLET::::- a=false, b=false, c=false\nBULLET::::- a=false, b=false, c=true\nBULLET::::- a=false, b=true, c=false\nBULLET::::- a=false, b=true, c=true\nBULLET::::- a=true, b=false, c=false\nBULLET::::- a=true, b=false, c=true\nBULLET::::- a=true, b=true, c=false\nBULLET::::- a=true, b=true, c=true\n\nParameter value coverage (PVC) requires that in a method taking parameters, all the common values for such parameters be considered. \nThe idea is that all common possible values for a parameter are tested. For example, common values for a string are: 1) null, 2) empty, 3) whitespace (space, tabs, newline), 4) valid string, 5) invalid string, 6) single-byte string, 7) double-byte string. It may also be appropriate to use very long strings. Failure to test each possible parameter value may leave a bug. Testing only one of these could result in 100% code coverage as each line is covered, but as only one of seven options are tested, there is only 14.2% PVC.\n\nThere are further coverage criteria, which are used less often:\nBULLET::::- Linear Code Sequence and Jump (LCSAJ) coverage a.k.a. JJ-Path coverage has every LCSAJ/JJ-path been executed?\nBULLET::::- Path coverageHas every possible route through a given part of the code been executed?\nBULLET::::- Entry/exit coverageHas every possible call and return of the function been executed?\nBULLET::::- Loop coverageHas every possible loop been executed zero times, once, and more than once?\nBULLET::::- State coverageHas each state in a finite-state machine been reached and explored?\nBULLET::::- Data-flow coverageHas each variable definition and its usage been reached and explored?\n\nSafety-critical or dependable applications are often required to demonstrate 100% of some form of test coverage.\nFor example, the ECSS-E-ST-40C standard demands 100% statement and decision coverage for two out of four different criticality levels; for the other ones, target coverage values are up to negotiation between supplier and customer.\nHowever, setting specific target values - and, in particular, 100% - has been criticized by practitioners for various reasons (cf.)\nMartin Fowler writes: \"I would be suspicious of anything like 100% - it would smell of someone writing tests to make the coverage numbers happy, but not thinking about what they are doing\".\n\nSome of the coverage criteria above are connected. For instance, path coverage implies decision, statement and entry/exit coverage. Decision coverage implies statement coverage, because every statement is part of a branch.\n\nFull path coverage, of the type described above, is usually impractical or impossible. Any module with a succession of formula_1 decisions in it can have up to formula_2 paths within it; loop constructs can result in an infinite number of paths. Many paths may also be infeasible, in that there is no input to the program under test that can cause that particular path to be executed. However, a general-purpose algorithm for identifying infeasible paths has been proven to be impossible (such an algorithm could be used to solve the halting problem). Basis path testing is for instance a method of achieving complete branch coverage without achieving complete path coverage.\n\nMethods for practical path coverage testing instead attempt to identify classes of code paths that differ only in the number of loop executions, and to achieve \"basis path\" coverage the tester must cover all the path classes.\n\nThe target software is built with special options or libraries and run under a controlled environment, to map every executed function to the function points in the source code. This allows testing parts of the target software that are rarely or never accessed under normal conditions, and helps reassure that the most important conditions (function points) have been tested. The resulting output is then analyzed to see what areas of code have not been exercised and the tests are updated to include these areas as necessary. Combined with other test coverage methods, the aim is to develop a rigorous, yet manageable, set of regression tests.\n\nIn implementing test coverage policies within a software development environment, one must consider the following:\n\nBULLET::::- What are coverage requirements for the end product certification and if so what level of test coverage is required? The typical level of rigor progression is as follows: Statement, Branch/Decision, Modified Condition/Decision Coverage(MC/DC), LCSAJ (Linear Code Sequence and Jump)\nBULLET::::- Will coverage be measured against tests that verify requirements levied on the system under test (DO-178B)?\nBULLET::::- Is the object code generated directly traceable to source code statements? Certain certifications, (i.e. DO-178B Level A) require coverage at the assembly level if this is not the case: \"Then, additional verification should be performed on the object code to establish the correctness of such generated code sequences\" (DO-178B) para-6.4.4.2.\n\nSoftware authors can look at test coverage results to devise additional tests and input or configuration sets to increase the coverage over vital functions. Two common forms of test coverage are statement (or line) coverage and branch (or edge) coverage. Line coverage reports on the execution footprint of testing in terms of which lines of code were executed to complete the test. Edge coverage reports which branches or code decision points were executed to complete the test. They both report a coverage metric, measured as a percentage. The meaning of this depends on what form(s) of coverage have been used, as 67% branch coverage is more comprehensive than 67% statement coverage.\n\nGenerally, test coverage tools incur computation and logging in addition to the actual program thereby slowing down the application, so typically this analysis is not done in production. As one might expect, there are classes of software that cannot be feasibly subjected to these coverage tests, though a degree of coverage mapping can be approximated through analysis rather than direct testing.\n\nThere are also some sorts of defects which are affected by such tools. In particular, some race conditions or similar real time sensitive operations can be masked when run under test environments; though conversely, some of these defects may become easier to find as a result of the additional overhead of the testing code.\n\nTest coverage is one consideration in the safety certification of avionics equipment. The guidelines by which avionics gear is certified by the Federal Aviation Administration (FAA) is documented in DO-178B and DO-178C.\n\nTest coverage is also a requirement in part 6 of the automotive safety standard ISO 26262 \"Road Vehicles - Functional Safety\".\n\nBULLET::::- Cyclomatic complexity\nBULLET::::- Intelligent verification\nBULLET::::- Linear Code Sequence and Jump\nBULLET::::- Modified Condition/Decision Coverage\nBULLET::::- Mutation testing\nBULLET::::- Regression testing\nBULLET::::- Software metric\nBULLET::::- Static code analysis\nBULLET::::- White box testing\nBULLET::::- Java code coverage tools\n"}
{"id": "7033", "url": "https://en.wikipedia.org/wiki?curid=7033", "title": "Caitlin Clarke", "text": "Caitlin Clarke\n\nCaitlin Clarke (May 3, 1952 – September 9, 2004) was an American theater and film actress best known for her role as Valerian in the 1981 fantasy film \"Dragonslayer\" and for her role as Charlotte Cardoza in the 1998–1999 Broadway musical \"Titanic\".\n\nClarke was born Katherine Anne Clarke in Pittsburgh, the oldest of five sisters, the youngest of whom is Victoria Clarke. Her family moved to Sewickley when she was ten.\n\nClarke received her B.A. in theater arts from Mount Holyoke College in 1974 and her M.F.A. from the Yale School of Drama in 1978. During her final year at Yale Clarke performed with the Yale Repertory Theater in such plays as Tales from the Vienna Woods.\n\nThe first few years of Clarke's professional career were largely theatrical, apart from her role in \"Dragonslayer\". After appearing in three Broadway plays in 1985, Clarke moved to Los Angeles for several years as a film and television actress. She appeared in the 1986 film \"Crocodile Dundee\" as Simone, a friendly prostitute. She returned to theater in the early 1990s, and to Broadway as Charlotte Cardoza in \"Titanic\".\n\nClarke was diagnosed with ovarian cancer in 2000. She returned to Pittsburgh to teach theater at the University of Pittsburgh and at the Pittsburgh Musical Theater's Rauh Conservatory as well as to perform in Pittsburgh theatre until her death on September 9, 2004.\n\nBULLET::::- 1983 - \"Teaneck Tanzi: The Venus Flytrap\"\nBULLET::::- 1985 - \"The Marriage of Figaro\"\nBULLET::::- 1985 - \"Arms and the Man\"\nBULLET::::- 1985 - \"Strange Interlude\"\nBULLET::::- 1998 - \"Titanic: A New Musical\"\n\nBULLET::::- 1979 - \"Othello\"\nBULLET::::- 1981 - \"No End of Blame\"\nBULLET::::- 1983 - \"Summer\"\nBULLET::::- 1984 - \"Total Eclipse\"\nBULLET::::- 1984 - \"Quartermaine's Terms\"\nBULLET::::- 1984 - \"Thin Ice\"\nBULLET::::- 1994 - \"Three Birds Alighting On A Field\"\nBULLET::::- 1994 - \"Unexpected Tenderness\"\n\nBULLET::::- 1978 - \"Tales from the Vienna Woods\" (New Haven)\nBULLET::::- 1979 - \"The Winter's Tale\" (Washington)\nBULLET::::- 1980 - \"Bal\" (Chicago)\nBULLET::::- 1981 - \"Plenty\" (Chicago)\nBULLET::::- 1982 - \"Summer Vacation Madness\" (Minneapolis)\nBULLET::::- 1984 - \"As You Like It\" (San Diego)\nBULLET::::- 1984 - \"Not Quite Jerusalem\" (New Haven)\nBULLET::::- 1989 - \"Our Country's Good\" (Los Angeles)\nBULLET::::- 1991 - \"The Queen And The Rebels\" (Baltimore)\nBULLET::::- 1996 - \"Mrs. Warren's Profession\" (New Haven)\nBULLET::::- 1997 - \"Indiscretions\" (Dallas)\nBULLET::::- 1997 - \"The Glass Menagerie\" (Portland, Maine)\nBULLET::::- 1999 - \"Griller\" (Baltimore)\nBULLET::::- 2000 - \"Who's Afraid of Virginia Woolf\" (Rochester, NY)\nBULLET::::- 2002 - \"The Gigli Concert\" (Pittsburgh)\nBULLET::::- 2002 - \"Aristocrats\" (Pittsburgh)\n\nBULLET::::- \"Dragonslayer\" (1981) - Valerian\nBULLET::::- \"Crocodile Dundee\" (1986) - Simone\nBULLET::::- \"Kenny\" (a.k.a. \"The Kid Brother\") (1987) - Sharon\nBULLET::::- \"The Big Picture\" (1989) - Sharon\nBULLET::::- \"Penn & Teller Get Killed\" (1989) - Carlotta / Officer McNamara\nBULLET::::- \"Blown Away\" (1994) - Rita\nBULLET::::- \"Cost of Living\" (1997) - Annie\nBULLET::::- \"A Cure For Serpents\" (1997, Short) - Mother\nBULLET::::- \"Joe the King\" (1999) - Pat\nBULLET::::- \"Never Again\" (2001) - Allison (final film role)\n\nSeries: \"Northern Exposure\", \"The Equalizer\", \"Once a Hero\", \"Moonlighting\", \"Sex And The City\", \"Law & Order\" (\"Menace\", \"Juvenile\", \"Stiff\"), \"Matlock\".\n\nMovies: \"Mayflower Madam\" (1986), \"Love, Lies and Murder\" (1991), \"The Stepford Husbands\" (1996).\n\n"}
{"id": "7034", "url": "https://en.wikipedia.org/wiki?curid=7034", "title": "Cruiser", "text": "Cruiser\n\nA cruiser is a type of warship. Modern cruisers are generally the largest ships in a fleet after aircraft carriers and amphibious assault ships, and can usually perform several roles.\n\nThe term has been in use for several hundred years, and has had different meanings throughout this period. During the Age of Sail, the term \"cruising\" referred to certain kinds of missions—independent scouting, commerce protection, or raiding—fulfilled by a frigate or sloop-of-war, which were the \"cruising warships\" of a fleet.\n\nIn the middle of the 19th century, \"cruiser\" came to be a classification for the ships intended for cruising distant waters, commerce raiding, and scouting for the battle fleet. Cruisers came in a wide variety of sizes, from the medium-sized protected cruiser to large armored cruisers that were nearly as big (although not as powerful or as well-armored) as a pre-dreadnought battleship. With the advent of the dreadnought battleship before World War I, the armored cruiser evolved into a vessel of similar scale known as the battlecruiser. The very large battlecruisers of the World War I era that succeeded armored cruisers were now classified, along with dreadnought battleships, as capital ships.\n\nBy the early 20th century after World War I, the direct successors to protected cruisers could be placed on a consistent scale of warship size, smaller than a battleship but larger than a destroyer. In 1922, the Washington Naval Treaty placed a formal limit on these cruisers, which were defined as warships of up to 10,000 tons displacement carrying guns no larger than 8 inches in calibre; heavy cruisers had 8-inch guns, while those with guns of 6.1 inches or less were light cruisers, which shaped cruiser design until the end of World War II. Some variations on the Treaty cruiser design included the German \"pocket battleships\" which had heavier armament at the expense of speed compared to standard heavy cruisers, and the American , which was a scaled-up heavy cruiser design designated as a \"cruiser-killer\".\n\nIn the later 20th century, the obsolescence of the battleship left the cruiser as the largest and most powerful surface combatant after the aircraft carrier. The role of the cruiser varied according to ship and navy, often including air defense and shore bombardment. During the Cold War, the Soviet Navy's cruisers had heavy anti-ship missile armament designed to sink NATO carrier task forces via saturation attack. The U.S. Navy built guided-missile cruisers upon destroyer-style hulls (some called \"destroyer leaders\" or \"frigates\" prior to the 1975 reclassification) primarily designed to provide air defense while often adding anti-submarine capabilities, being larger and having longer-range surface-to-air missiles (SAMs) than early \"Charles F. Adams\" guided-missile destroyers tasked with the short-range air defense role. By the end of the Cold War, the line between cruisers and destroyers had blurred, with the cruiser using the hull of the destroyer but receiving the cruiser designation due to their enhanced mission and combat systems. Indeed, the newest U.S. and Chinese destroyers (for instance the and Type 055) are more heavily armed than some of the cruisers that they succeeded.\n\nCurrently only two nations operate cruisers: the United States and Russia, and in both cases the vessels are primarily armed with guided missiles. was the last gun cruiser in service, serving with the Peruvian Navy until 2017.\n\nThe term \"cruiser\" or \"cruizer\" was first commonly used in the 17th century to refer to an independent warship. \"Cruiser\" meant the purpose or mission of a ship, rather than a category of vessel. However, the term was nonetheless used to mean a smaller, faster warship suitable for such a role. In the 17th century, the ship of the line was generally too large, inflexible, and expensive to be dispatched on long-range missions (for instance, to the Americas), and too strategically important to be put at risk of fouling and foundering by continual patrol duties.\n\nThe Dutch navy was noted for its cruisers in the 17th century, while the Royal Navy—and later French and Spanish navies—subsequently caught up in terms of their numbers and deployment. The British Cruiser and Convoy Acts were an attempt by mercantile interests in Parliament to focus the Navy on commerce defence and raiding with cruisers, rather than the more scarce and expensive ships of the line. During the 18th century the frigate became the preeminent type of cruiser. A frigate was a small, fast, long range, lightly armed (single gun-deck) ship used for scouting, carrying dispatches, and disrupting enemy trade. The other principal type of cruiser was the sloop, but many other miscellaneous types of ship were used as well.\n\nDuring the 19th century, navies began to use steam power for their fleets. The 1840s saw the construction of experimental steam-powered frigates and sloops. By the middle of the 1850s, the British and U.S. Navies were both building steam frigates with very long hulls and a heavy gun armament, for instance or .\n\nThe 1860s saw the introduction of the ironclad. The first ironclads were frigates, in the sense of having one gun deck; however, they were also clearly the most powerful ships in the navy, and were principally to serve in the line of battle. In spite of their great speed, they would have been wasted in a cruising role.\n\nThe French constructed a number of smaller ironclads for overseas cruising duties, starting with the , commissioned 1865. These \"station ironclads\" were the beginning of the development of the armored cruisers, a type of ironclad specifically for the traditional cruiser missions of fast, independent raiding and patrol.\n\nThe first true armored cruiser was the Russian , completed in 1874, and followed by the British a few years later.\n\nUntil the 1890s armored cruisers were still built with masts for a full sailing rig, to enable them to operate far from friendly coaling stations.\n\nUnarmored cruising warships, built out of wood, iron, steel or a combination of those materials, remained popular until towards the end of the 19th century. The ironclad's armor often meant that they were limited to short range under steam, and many ironclads were unsuited to long-range missions or for work in distant colonies. The unarmored cruiser – often a screw sloop or screw frigate – could continue in this role. Even though mid- to late-19th century cruisers typically carried up-to-date guns firing explosive shells, they were unable to face ironclads in combat. This was evidenced by the clash between , a modern British cruiser, and the Peruvian monitor \"Huáscar\". Even though the Peruvian vessel was obsolete by the time of the encounter, it stood up well to roughly 50 hits from British shells.\n\nIn the 1880s, naval engineers began to use steel as a material for construction and armament. A steel cruiser could be lighter and faster than one built of iron or wood. The \"Jeune Ecole\" school of naval doctrine suggested that a fleet of fast unprotected steel cruisers were ideal for commerce raiding, while the torpedo boat would be able to destroy an enemy battleship fleet.\n\nSteel also offered the cruiser a way of acquiring the protection needed to survive in combat. Steel armor was considerably stronger, for the same weight, than iron. By putting a relatively thin layer of steel armor above the vital parts of the ship, and by placing the coal bunkers where they might stop shellfire, a useful degree of protection could be achieved without slowing the ship too much. Protected cruisers generally had an armored deck with sloped sides, providing similar protection to a light armored belt at less weight and expense.\n\nThe first protected cruiser was the Chilean ship \"Esmeralda\", launched in 1883. Produced by a shipyard at Elswick, in Britain, owned by Armstrong, she inspired a group of protected cruisers produced in the same yard and known as the \"Elswick cruisers\". Her forecastle, poop deck and the wooden board deck had been removed, replaced with an armored deck.\n\n\"Esmeralda\"s armament consisted of fore and aft 10-inch (25.4 cm) guns and 6-inch (15.2 cm) guns in the midships positions. It could reach a speed of , and was propelled by steam alone. It also had a displacement of less than 3,000 tons. During the two following decades, this cruiser type came to be the inspiration for combining heavy artillery, high speed and low displacement.\n\nThe torpedo cruiser (known in the Royal Navy as the torpedo gunboat) was a smaller unarmored cruiser, which emerged in the 1880s–1890s. These ships could reach speeds up to and were armed with medium to small calibre guns as well as torpedoes. These ships were tasked with guard and reconnaissance duties, to repeat signals and all other fleet duties for which smaller vessels were suited. These ships could also function as flagships of torpedo boat flotillas. After the 1900s, these ships were usually traded for faster ships with better sea going qualities.\n\nSteel also affected the construction and role of armored cruisers. Steel meant that new designs of battleship, later known as pre-dreadnought battleships, would be able to combine firepower and armor with better endurance and speed than ever before. The armored cruisers of the 1890s greatly resembled the battleships of the day; they tended to carry slightly smaller main armament ( rather than 12-inch) and have somewhat thinner armor in exchange for a faster speed (perhaps rather than 18). Because of their similarity, the lines between battleships and armored cruisers became blurred.\n\nShortly after the turn of the 20th century there were difficult questions about the design of future cruisers. Modern armored cruisers, almost as powerful as battleships, were also fast enough to outrun older protected and unarmored cruisers. In the Royal Navy, Jackie Fisher cut back hugely on older vessels, including many cruisers of different sorts, calling them \"a miser's hoard of useless junk\" that any modern cruiser would sweep from the seas. The scout cruiser also appeared in this era; this was a small, fast, lightly armed and armored type designed primarily for reconnaissance. The Royal Navy and the Italian Navy were the primary developers of this type.\n\nThe growing size and power of the armored cruiser resulted in the battlecruiser, with an armament and size similar to the revolutionary new dreadnought battleship; the brainchild of British admiral Jackie Fisher. He believed that to ensure British naval dominance in its overseas colonial possessions, a fleet of large, fast, powerfully armed vessels which would be able to hunt down and mop up enemy cruisers and armored cruisers with overwhelming fire superiority was needed. They were equipped with the same gun types as battleships, though usually with fewer guns, and were intended to engage enemy capital ships as well. This type of vessel came to be known as the \"battlecruiser\", and the first were commissioned into the Royal Navy in 1907. The British battlecruisers sacrificed protection for speed, as they were intended to \"choose their range\" (to the enemy) with superior speed and only engage the enemy at long range. When engaged at moderate ranges, the lack of protection combined with unsafe ammunition handling practices became tragic with the loss of three of them at the Battle of Jutland. Germany and eventually Japan followed suit to build these vessels, replacing armored cruisers in most frontline roles. German battlecruisers were generally better protected but slower than British battlecruisers. Battlecruisers were in many cases larger and more expensive than contemporary battleships, due to their much-larger propulsion plants.\n\nAt around the same time as the battlecruiser was developed, the distinction between the armored and the unarmored cruiser finally disappeared. By the British , the first of which was launched in 1909, it was possible for a small, fast cruiser to carry both belt and deck armor, particularly when turbine engines were adopted. These light armored cruisers began to occupy the traditional cruiser role once it became clear that the battlecruiser squadrons were required to operate with the battle fleet.\n\nSome light cruisers were built specifically to act as the leaders of flotillas of destroyers.\n\nThese vessels were essentially large coastal patrol boats armed with multiple light guns. One such warship was \"Grivița\" of the Romanian Navy. She displaced 110 tons, measured 60 meters in length and was armed with four light guns.\n\nThe auxiliary cruiser was a merchant ship hastily armed with small guns on the outbreak of war. Auxiliary cruisers were used to fill gaps in their long-range lines or provide escort for other cargo ships, although they generally proved to be useless in this role because of their low speed, feeble firepower and lack of armor. In both world wars the Germans also used small merchant ships armed with cruiser guns to surprise Allied merchant ships.\n\nSome large liners were armed in the same way. In British service these were known as Armed Merchant Cruisers (AMC). The Germans and French used them in World War I as raiders because of their high speed (around 30 knots (56 km/h)), and they were used again as raiders early in World War II by the Germans and Japanese. In both the First World War and in the early part of the Second, they were used as convoy escorts by the British.\n\nCruisers were one of the workhorse types of warship during World War I. By the time of World War I, cruisers had accelerated their development and improved their quality significantly, with drainage volume reaching 3000–4000 tons, a speed of 25–30 knots and a calibre of 127–152 mm.\n\nNaval construction in the 1920s and 1930s was limited by international treaties designed to prevent the repetition of the Dreadnought arms race of the early 20th century. The Washington Naval Treaty of 1922 placed limits on the construction of ships with a standard displacement of more than 10,000 tons and an armament of guns larger than 8-inch (203 mm). A number of navies commissioned classes of cruisers at the top end of this limit, known as \"treaty cruisers\".\n\nThe London Naval Treaty in 1930 then formalised the distinction between these \"heavy\" cruisers and light cruisers: a \"heavy\" cruiser was one with guns of more than 6.1-inch (155 mm) calibre. The Second London Naval Treaty attempted to reduce the tonnage of new cruisers to 8,000 or less, but this had little effect; Japan and Germany were not signatories, and some navies had already begun to evade treaty limitations on warships. The first London treaty did touch off a period of the major powers building 6-inch or 6.1-inch gunned cruisers, nominally of 10,000 tons and with up to fifteen guns, the treaty limit. Thus, most light cruisers ordered after 1930 were the size of heavy cruisers but with more and smaller guns. The Imperial Japanese Navy began this new race with the , launched in 1934. After building smaller light cruisers with six or eight 6-inch guns launched 1931–35, the British Royal Navy followed with the 12-gun in 1936. To match foreign developments and potential treaty violations, in the 1930s the US developed a series of new guns firing \"super-heavy\" armor piercing ammunition; these included the 6-inch (152 mm)/47 caliber gun Mark 16 introduced with the 15-gun s in 1936, and the 8-inch (203 mm)/55 caliber gun Mark 12 introduced with in 1937.\n\nThe heavy cruiser was a type of cruiser designed for long range, high speed and an armament of naval guns around 203 mm (8 in) in calibre. The first heavy cruisers were built in 1915, although it only became a widespread classification following the London Naval Treaty in 1930. The heavy cruiser's immediate precursors were the light cruiser designs of the 1910s and 1920s; the US lightly armored 8-inch \"treaty cruisers\" of the 1920s (built under the Washington Naval Treaty) were originally classed as light cruisers until the London Treaty forced their redesignation.\n\nInitially, all cruisers built under the Washington treaty had torpedo tubes, regardless of nationality. However, in 1930, results of war games caused the US Naval War College to conclude that only perhaps half of cruisers would use their torpedoes in action. In a surface engagement, long-range gunfire and destroyer torpedoes would decide the issue, and under air attack numerous cruisers would be lost before getting within torpedo range. Thus, beginning with launched in 1933, new cruisers were built without torpedoes, and torpedoes were removed from older heavy cruisers due to the perceived hazard of their being exploded by shell fire. The Japanese took exactly the opposite approach with cruiser torpedoes, and this proved crucial to their tactical victories in most of the numerous cruiser actions of 1942. Beginning with the launched in 1925, every Japanese heavy cruiser was armed with torpedoes, larger than any other cruisers'. By 1933 Japan had developed the Type 93 torpedo for these ships, eventually nicknamed \"Long Lance\" by the Allies. This type used compressed oxygen instead of compressed air, allowing it to achieve ranges and speeds unmatched by other torpedoes. It could achieve a range of at , compared with the US Mark 15 torpedo with at . The Mark 15 had a maximum range of at , still well below the \"Long Lance\". The Japanese were able to keep the Type 93's performance and oxygen power secret until the Allies recovered one in early 1943, thus the Allies faced a great threat they were not aware of in 1942. The Type 93 was also fitted to Japanese post-1930 light cruisers and the majority of their World War II destroyers.\n\nHeavy cruisers continued in use until after World War II, with some converted to guided missile cruisers for air defense or strategic attack and some used for shore bombardment by the United States in the Korean War and the Vietnam War.\n\nThe German was a series of three \"Panzerschiffe\" (\"armored ships\"), a form of heavily armed cruiser, designed and built by the German Reichsmarine in nominal accordance with restrictions imposed by the Treaty of Versailles. All three ships were launched between 1931 and 1934, and served with Germany's Kriegsmarine during World War II. Within the Kriegsmarine, the Panzerschiffe had the propaganda value of capital ships: heavy cruisers with battleship guns, torpedoes, and scout aircraft. The similar Swedish \"Panzerschiffe\" were tactically used as centers of battlefleets and not as cruisers. They were deployed by Nazi Germany in support of the German interests in the Spanish Civil War. Panzerschiff \"Admiral Graf Spee\" represented Germany in the 1937 Cornation Fleet Review.\n\nThe British press referred to the vessels as pocket battleships, in reference to the heavy firepower contained in the relatively small vessels; they were considerably smaller than contemporary battleships, though at 28 knots were slower than battlecruisers. At up to 16,000 tons at full load, they were not treaty compliant 10,000 ton cruisers. And although their displacement and scale of armor protection were that of a heavy cruiser, their main armament was heavier than the guns of other nations' heavy cruisers, and the latter two members of the class also had tall conning towers resembling battleships. The Panzerschiffe were listed as Ersatz replacements for retiring Reichsmarine coastal defense battleships, which added to their propaganda status in the Kriegsmarine as Ersatz battleships; within the Royal Navy, only battlecruisers HMS \"Hood\", \"Repulse\" and \"Renown\" were capable of both outrunning and outgunning the Panzerschiffe. They were seen in the 1930s as a new and serious threat by both Britain and France. While the Kriegsmarine reclassified them as heavy cruisers in 1940, \"Deutschland\"-class ships continued to be called \"pocket battleships\" in the popular press.\n\nThe American represented the supersized cruiser design. Due to the German pocket battleships, the , and rumored Japanese \"super cruisers\", all of which carried guns larger than the standard heavy cruiser's 8-inch size dictated by naval treaty limitations, the \"Alaska\"s were intended to be \"cruiser-killers\". While superficially appearing similar to a battleship/battlecruiser and mounting three triple turrets of 12-inch guns, their actual protection scheme and design resembled a scaled-up heavy cruiser design. Their hull classification symbol of CB (cruiser, big) reflected this.\n\nA precursor to the anti-aircraft cruiser was the Romanian British-built protected cruiser \"Elisabeta\". After the start of World War I, her four 120 mm main guns were landed and her four 75 mm (12-pounder) secondary guns were modified for anti-aircraft fire.\n\nThe development of the anti-aircraft cruiser began in 1935 when the Royal Navy re-armed and . Torpedo tubes and low-angle guns were removed from these World War I light cruisers and replaced with ten high-angle guns, with appropriate fire-control equipment to provide larger warships with protection against high-altitude bombers.\n\nA tactical shortcoming was recognised after completing six additional conversions of s. Having sacrificed anti-ship weapons for anti-aircraft armament, the converted anti-aircraft cruisers might themselves need protection against surface units. New construction was undertaken to create cruisers of similar speed and displacement with dual-purpose guns, which offered good anti-aircraft protection with anti-surface capability for the traditional light cruiser role of defending capital ships from destroyers.\n\nThe first purpose built anti-aircraft cruiser was the British , completed in 1940–42. The US Navy's cruisers (CLAA: light cruiser with anti-aircraft capability) were designed to match the capabilities of the Royal Navy. Both \"Dido\" and \"Atlanta\" cruisers initially carried torpedo tubes; the \"Atlanta\" cruisers at least were originally designed as destroyer leaders, were originally designated CL (light cruiser), and did not receive the CLAA designation until 1949.\n\nThe concept of the quick-firing dual-purpose gun anti-aircraft cruiser was embraced in several designs completed too late to see combat, including: , completed in 1948; , completed in 1949; two s, completed in 1947; two s, completed in 1953; , completed in 1955; , completed in 1959; and , and , all completed between 1959 and 1961.\n\nMost post-World War II cruisers were tasked with air defense roles. In the early 1950s, advances in aviation technology forced the move from anti-aircraft artillery to anti-aircraft missiles. Therefore, most modern cruisers are equipped with surface-to-air missiles as their main armament. Today's equivalent of the anti-aircraft cruiser is the guided missile cruiser (CAG/CLG/CG/CGN).\n\nCruisers participated in a number of surface engagements in the early part of World War II, along with escorting carrier and battleship groups throughout the war. In the later part of the war, Allied cruisers primarily provided anti-aircraft (AA) escort for carrier groups and performed shore bombardment. Japanese cruisers similarly escorted carrier and battleship groups in the later part of the war, notably in the disastrous Battle of the Philippine Sea and Battle of Leyte Gulf. In 1937–41 the Japanese, having withdrawn from all naval treaties, upgraded or completed the \"Mogami\" and es as heavy cruisers by replacing their triple turrets with twin turrets. Torpedo refits were also made to most heavy cruisers, resulting in up to sixteen tubes per ship, plus a set of reloads. In 1941 the 1920s light cruisers and were converted to torpedo cruisers with four guns and forty torpedo tubes. In 1944 \"Kitakami\" was further converted to carry up to eight \"Kaiten\" human torpedoes in place of ordinary torpedoes. Before World War II, cruisers were mainly divided into three types: heavy cruisers, light cruisers and auxiliary cruisers. Heavy cruiser tonnage reached 20–30,000 tons, speed 32–34 knots, endurance of more than 10,000 nautical miles, armor thickness of 127–203 mm. Heavy cruisers were equipped with eight or nine guns with a range of more than 20 nautical miles. They were mainly used to attack enemy surface ships and shore-based targets. In addition, there were 10–16 secondary guns with a caliber of less than . Also, dozens of automatic antiaircraft guns were installed to fight aircraft and small vessels such as torpedo boats. For example, in World War II, American Alaska-class cruisers were more than 30,000 tons, equipped with nine guns. Some cruisers could also carry three or four seaplanes to correct the accuracy of gunfire and perform reconnaissance. Together with battleships, these heavy cruisers formed powerful naval task forces, which dominated the world's oceans for more than a century. After the signing of the Washington Treaty on Arms Limitation in 1922, the tonnage and quantity of battleships, aircraft carriers and cruisers were severely restricted. In order not to violate the treaty, countries began to develop light cruisers. Light cruisers of the 1920s had displacements of less than 10,000 tons and a speed of up to 35 knots. They were equipped with 6–12 main guns with a caliber of 127–133 mm (5–5.5 inches). In addition, they were equipped with 8–12 secondary guns under 127 mm (5 in) and dozens of small caliber cannons, as well as torpedoes and mines. Some ships also carried 2–4 seaplanes, mainly for reconnaissance. In 1930 the London Naval Treaty allowed large light cruisers to be built, with the same tonnage as heavy cruisers and armed with up to fifteen guns. The Japanese \"Mogami\" class were built to this treaty's limit, the Americans and British also built similar ships. However, in 1939 the \"Mogami\"s were refitted as heavy cruisers with ten guns.\n\nIn December 1939, three British cruisers engaged the German \"pocket battleship\" \"Admiral Graf Spee\" (which was on a commerce raiding mission) in the Battle of the River Plate; \"Admiral Graf Spee\" then took refuge in neutral Montevideo, Uruguay. By broadcasting messages indicating capital ships were in the area, the British caused \"Admiral Graf Spee\"s captain to think he faced a hopeless situation while low on ammunition and order his ship scuttled. On 8 June 1940 the German capital ships and , classed as battleships but with large cruiser armament, sank the aircraft carrier with gunfire. From October 1940 through March 1941 the German heavy cruiser (also known as \"pocket battleship\", see above) conducted a successful commerce-raiding voyage in the Atlantic and Indian Oceans.\n\nOn 27 May 1941, attempted to finish off the German battleship with torpedoes, probably causing the Germans to scuttle the ship. \"Bismarck\" (accompanied by the heavy cruiser ) previously sank the battlecruiser and damaged the battleship with gunfire in the Battle of the Denmark Strait.\n\nOn 19 November 1941 sank in a mutually fatal engagement with the German raider \"Kormoran\" in the Indian Ocean near Western Australia.\n\nTwenty-three British cruisers were lost to enemy action, mostly to air attack and submarines, in operations in the Atlantic, Mediterranean, and Indian Ocean. Sixteen of these losses were in the Mediterranean. The British included cruisers and anti-aircraft cruisers among convoy escorts in the Mediterranean and to northern Russia due to the threat of surface and air attack. Almost all cruisers in World War II were vulnerable to submarine attack due to a lack of anti-submarine sonar and weapons. Also, until 1943–44 the light anti-aircraft armament of most cruisers was weak.\n\nIn July 1942 an attempt to intercept Convoy PQ 17 with surface ships, including the heavy cruiser \"Admiral Scheer\", failed due to multiple German warships grounding, but air and submarine attacks sank 2/3 of the convoy's ships. In August 1942 \"Admiral Scheer\" conducted Operation Wunderland, a solo raid into northern Russia's Kara Sea. She bombarded Dikson Island but otherwise had little success.\n\nOn 31 December 1942 the Battle of the Barents Sea was fought, a rare action for a Murmansk run because it involved cruisers on both sides. Four British destroyers and five other vessels were escorting Convoy JW 51B from the UK to the Murmansk area. Another British force of two cruisers ( and ) and two destroyers were in the area. Two heavy cruisers (one the \"pocket battleship\" \"Lützow\"), accompanied by six destroyers, attempted to intercept the convoy near North Cape after it was spotted by a U-boat. Although the Germans sank a British destroyer and a minesweeper (also damaging another destroyer), they failed to damage any of the convoy's merchant ships. A German destroyer was lost and a heavy cruiser damaged. Both sides withdrew from the action for fear of the other side's torpedoes.\n\nOn 26 December 1943 the German capital ship \"Scharnhorst\" was sunk while attempting to intercept a convoy in the Battle of the North Cape. The British force that sank her was led by Vice Admiral Bruce Fraser in the battleship , accompanied by four cruisers and nine destroyers. One of the cruisers was the preserved .\n\n\"Scharnhorst\"s sister \"Gneisenau\", damaged by a mine and a submerged wreck in the Channel Dash of 13 February 1942 and repaired, was further damaged by a British air attack on 27 February 1942. She began a conversion process to mount six guns instead of nine guns, but in early 1943 Hitler (angered by the recent failure at the Battle of the Barents Sea) ordered her disarmed and her armament used as coast defence weapons. One 28 cm triple turret survives near Trondheim, Norway.\n\nThe attack on Pearl Harbor on 7 December 1941 brought the United States into the war, but with eight battleships sunk or damaged by air attack. On 10 December 1941 HMS \"Prince of Wales\" and the battlecruiser were sunk by land-based torpedo bombers northeast of Singapore. It was now clear that surface ships could not operate near enemy aircraft in daylight without air cover; most surface actions of 1942–43 were fought at night as a result. Generally, both sides avoided risking their battleships until the Japanese attack at Leyte Gulf in 1944.\n\nSix of the battleships from Pearl Harbor were eventually returned to service, but no US battleships engaged Japanese surface units at sea until the Naval Battle of Guadalcanal in November 1942, and not thereafter until the Battle of Surigao Strait in October 1944. was on hand for the initial landings at Guadalcanal on 7 August 1942, and escorted carriers in the Battle of the Eastern Solomons later that month. However, on 15 September she was torpedoed while escorting a carrier group and had to return to the US for repairs.\n\nGenerally, the Japanese held their capital ships out of all surface actions in the 1941–42 campaigns or they failed to close with the enemy; the Naval Battle of Guadalcanal in November 1942 was the sole exception. The four ships performed shore bombardment in Malaya, Singapore, and Guadalcanal and escorted the raid on Ceylon and other carrier forces in 1941–42. Japanese capital ships also participated ineffectively (due to not being engaged) in the Battle of Midway and the simultaneous Aleutian diversion; in both cases they were in battleship groups well to the rear of the carrier groups. Sources state that sat out the entire Guadalcanal Campaign due to lack of high-explosive bombardment shells, poor nautical charts of the area, and high fuel consumption. It is likely that the poor charts affected other battleships as well. Except for the \"Kongō\" class, most Japanese battleships spent the critical year of 1942, in which most of the war's surface actions occurred, in home waters or at the fortified base of Truk, far from any risk of attacking or being attacked.\n\nFrom 1942 through mid-1943, US and other Allied cruisers were the heavy units on their side of the numerous surface engagements of the Dutch East Indies campaign, the Guadalcanal Campaign, and subsequent Solomon Islands fighting; they were usually opposed by strong Japanese cruiser-led forces equipped with Long Lance torpedoes. Destroyers also participated heavily on both sides of these battles and provided essentially all the torpedoes on the Allied side, with some battles in these campaigns fought entirely between destroyers.\n\nAlong with lack of knowledge of the capabilities of the Long Lance torpedo, the US Navy was hampered by a deficiency it was initially unaware of – the unreliability of the Mark 15 torpedo used by destroyers. This weapon shared the Mark 6 exploder and other problems with the more famously unreliable Mark 14 torpedo; the most common results of firing either of these torpedoes were a dud or a miss. The problems with these weapons were not solved until mid-1943, after almost all of the surface actions in the Solomon Islands had taken place. Another factor that shaped the early surface actions was the pre-war training of both sides. The US Navy concentrated on long-range 8-inch gunfire as their primary offensive weapon, leading to rigid battle line tactics, while the Japanese trained extensively for nighttime torpedo attacks. Since all post-1930 Japanese cruisers had 8-inch guns by 1941, almost all of the US Navy's cruisers in the South Pacific in 1942 were the 8-inch-gunned (203 mm) \"treaty cruisers\"; most of the 6-inch-gunned (152 mm) cruisers were deployed in the Atlantic.\n\nAlthough their battleships were held out of surface action, Japanese cruiser-destroyer forces rapidly isolated and mopped up the Allied naval forces in the Dutch East Indies campaign of February–March 1942. In three separate actions, they sank five Allied cruisers (two Dutch and one each British, Australian, and American) with torpedoes and gunfire, against one Japanese cruiser damaged. With one other Allied cruiser withdrawn for repairs, the only remaining Allied cruiser in the area was the damaged . Despite their rapid success, the Japanese proceeded methodically, never leaving their air cover and rapidly establishing new air bases as they advanced.\n\nAfter the key carrier battles of the Coral Sea and Midway in mid-1942, Japan had lost four of the six fleet carriers that launched the Pearl Harbor raid and was on the strategic defensive. On 7 August 1942 US Marines were landed on Guadalcanal and other nearby islands, beginning the Guadalcanal Campaign. This campaign proved to be a severe test for the Navy as well as the Marines. Along with two carrier battles, several major surface actions occurred, almost all at night between cruiser-destroyer forces.\n\nBattle of Savo Island<br>\nOn the night of 8–9 August 1942 the Japanese counterattacked near Guadalcanal in the Battle of Savo Island with a cruiser-destroyer force. In a controversial move, the US carrier task forces were withdrawn from the area on the 8th due to heavy fighter losses and low fuel. The Allied force included six heavy cruisers (two Australian), two light cruisers (one Australian), and eight US destroyers. Of the cruisers, only the Australian ships had torpedoes. The Japanese force included five heavy cruisers, two light cruisers, and one destroyer. Numerous circumstances combined to reduce Allied readiness for the battle. The results of the battle were three American heavy cruisers sunk by torpedoes and gunfire, one Australian heavy cruiser disabled by gunfire and scuttled, one heavy cruiser damaged, and two US destroyers damaged. The Japanese had three cruisers lightly damaged. This was the most lopsided outcome of the surface actions in the Solomon Islands. Along with their superior torpedoes, the opening Japanese gunfire was accurate and very damaging. Subsequent analysis showed that some of the damage was due to poor housekeeping practices by US forces. Stowage of boats and aircraft in midships hangars with full gas tanks contributed to fires, along with full and unprotected ready-service ammunition lockers for the open-mount secondary armament. These practices were soon corrected, and US cruisers with similar damage sank less often thereafter. Savo was the first surface action of the war for almost all the US ships and personnel; few US cruisers and destroyers were targeted or hit at Coral Sea or Midway.\n\nBattle of the Eastern Solomons<br>\nOn 24–25 August 1942 the Battle of the Eastern Solomons, a major carrier action, was fought. Part of the action was a Japanese attempt to reinforce Guadalcanal with men and equipment on troop transports. The Japanese troop convoy was attacked by Allied aircraft, resulting in the Japanese subsequently reinforcing Guadalcanal with troops on fast warships at night. These convoys were called the \"Tokyo Express\" by the Allies. Although the Tokyo Express often ran unopposed, most surface actions in the Solomons revolved around Tokyo Express missions. Also, US air operations had commenced from Henderson Field, the airfield on Guadalcanal. Fear of air power on both sides resulted in all surface actions in the Solomons being fought at night.\n\nBattle of Cape Esperance<br>\nThe Battle of Cape Esperance occurred on the night of 11–12 October 1942. A Tokyo Express mission was underway for Guadalcanal at the same time as a separate cruiser-destroyer bombardment group loaded with high explosive shells for bombarding Henderson Field. A US cruiser-destroyer force was deployed in advance of a convoy of US Army troops for Guadalcanal that was due on 13 October. The Tokyo Express convoy was two seaplane tenders and six destroyers; the bombardment group was three heavy cruisers and two destroyers, and the US force was two heavy cruisers, two light cruisers, and five destroyers. The US force engaged the Japanese bombardment force; the Tokyo Express convoy was able to unload on Guadalcanal and evade action. The bombardment force was sighted at close range () and the US force opened fire. The Japanese were surprised because their admiral was anticipating sighting the Tokyo Express force, and withheld fire while attempting to confirm the US ships' identity. One Japanese cruiser and one destroyer were sunk and one cruiser damaged, against one US destroyer sunk with one light cruiser and one destroyer damaged. The bombardment force failed to bring its torpedoes into action, and turned back. The next day US aircraft from Henderson Field attacked several of the Japanese ships, sinking two destroyers and damaging a third. The US victory resulted in overconfidence in some later battles, reflected in the initial after-action report claiming two Japanese heavy cruisers, one light cruiser, and three destroyers sunk by the gunfire of alone. The battle had little effect on the overall situation, as the next night two Kongō-class battleships bombarded and severely damaged Henderson Field unopposed, and the following night another Tokyo Express convoy delivered 4,500 troops to Guadalcanal. The US convoy delivered the Army troops as scheduled on the 13th.\n\nBattle of the Santa Cruz Islands<br>\nThe Battle of the Santa Cruz Islands took place 25–27 October 1942. It was a pivotal battle, as it left the US and Japanese with only two large carriers each in the South Pacific (another large Japanese carrier was damaged and under repair until May 1943). Due to the high carrier attrition rate with no replacements for months, for the most part both sides stopped risking their remaining carriers until late 1943, and each side sent in a pair of battleships instead. The next major carrier operations for the US were the carrier raid on Rabaul and support for the invasion of Tarawa, both in November 1943.\n\nNaval Battle of Guadalcanal<br>\nThe Naval Battle of Guadalcanal occurred 12–15 November 1942 in two phases. A night surface action on 12–13 November was the first phase. The Japanese force consisted of two Kongō-class battleships with high explosive shells for bombarding Henderson Field, one small light cruiser, and 11 destroyers. Their plan was that the bombardment would neutralize Allied airpower and allow a force of 11 transport ships and 12 destroyers to reinforce Guadalcanal with a Japanese division the next day. However, US reconnaissance aircraft spotted the approaching Japanese on the 12th and the Americans made what preparations they could. The American force consisted of two heavy cruisers, one light cruiser, two anti-aircraft cruisers, and eight destroyers. The Americans were outgunned by the Japanese that night, and a lack of pre-battle orders by the US commander led to confusion. The destroyer closed with the battleship , firing all torpedoes (though apparently none hit or detonated) and raking the battleship's bridge with gunfire, wounding the Japanese admiral and killing his chief of staff. The Americans initially lost four destroyers including \"Laffey\", with both heavy cruisers, most of the remaining destroyers, and both anti-aircraft cruisers damaged. The Japanese initially had one battleship and four destroyers damaged, but at this point they withdrew, possibly unaware that the US force was unable to further oppose them. At dawn US aircraft from Henderson Field, , and Espiritu Santo found the damaged battleship and two destroyers in the area. The battleship (\"Hiei\") was sunk by aircraft (or possibly scuttled), one destroyer was sunk by the damaged , and the other destroyer was attacked by aircraft but was able to withdraw. Both of the damaged US anti-aircraft cruisers were lost on 13 November, one () torpedoed by a Japanese submarine, and the other sank on the way to repairs. \"Juneau\"s loss was especially tragic; the submarine's presence prevented immediate rescue, over 100 survivors of a crew of nearly 700 were adrift for eight days, and all but ten died. Among the dead were the five Sullivan brothers.\n\nThe Japanese transport force was rescheduled for the 14th and a new cruiser-destroyer force (belatedly joined by the surviving battleship ) was sent to bombard Henderson Field the night of 13 November. Only two cruisers actually bombarded the airfield, as \"Kirishima\" had not arrived yet and the remainder of the force was on guard for US warships. The bombardment caused little damage. The cruiser-destroyer force then withdrew, while the transport force continued towards Guadalcanal. Both forces were attacked by US aircraft on the 14th. The cruiser force lost one heavy cruiser sunk and one damaged. Although the transport force had fighter cover from the carrier , six transports were sunk and one heavily damaged. All but four of the destroyers accompanying the transport force picked up survivors and withdrew. The remaining four transports and four destroyers approached Guadalcanal at night, but stopped to await the results of the night's action.\n\nOn the night of 14–15 November a Japanese force of \"Kirishima\", two heavy and two light cruisers, and nine destroyers approached Guadalcanal. Two US battleships ( and ) were there to meet them, along with four destroyers. This was one of only two battleship-on-battleship encounters during the Pacific War; the other was the lopsided Battle of Surigao Strait in October 1944, part of the Battle of Leyte Gulf. The battleships had been escorting \"Enterprise\", but were detached due to the urgency of the situation. With nine 16-inch (406 mm) guns apiece against eight 14-inch (356 mm) guns on \"Kirishima\", the Americans had major gun and armor advantages. All four destroyers were sunk or severely damaged and withdrawn shortly after the Japanese attacked them with gunfire and torpedoes. Although her main battery remained in action for most of the battle, \"South Dakota\" spent much of the action dealing with major electrical failures that affected her radar, fire control, and radio systems. Although her armor was not penetrated, she was hit by 26 shells of various calibers and temporarily rendered, in a US admiral's words, \"deaf, dumb, blind, and impotent\". \"Washington\" went undetected by the Japanese for most of the battle, but withheld shooting to avoid \"friendly fire\" until \"South Dakota\" was illuminated by Japanese fire, then rapidly set \"Kirishima\" ablaze with a jammed rudder and other damage. \"Washington\", finally spotted by the Japanese, then headed for the Russell Islands to hopefully draw the Japanese away from Guadalcanal and \"South Dakota\", and was successful in evading several torpedo attacks. Unusually, only a few Japanese torpedoes scored hits in this engagement. \"Kirishima\" sank or was scuttled before the night was out, along with two Japanese destroyers. The remaining Japanese ships withdrew, except for the four transports, which beached themselves in the night and started unloading. However, dawn (and US aircraft, US artillery, and a US destroyer) found them still beached, and they were destroyed.\n\nBattle of Tassafaronga<br>\nThe Battle of Tassafaronga took place on the night of 30 November-1 December 1942. The US had four heavy cruisers, one light cruiser, and four destroyers. The Japanese had eight destroyers on a Tokyo Express run to deliver food and supplies in drums to Guadalcanal. The Americans achieved initial surprise, damaging one destroyer with gunfire which later sank, but the Japanese torpedo counterattack was devastating. One American heavy cruiser was sunk and three others heavily damaged, with the bows blown off of two of them. It was significant that these two were not lost to Long Lance hits as happened in previous battles; American battle readiness and damage control had improved. Despite defeating the Americans, the Japanese withdrew without delivering the crucial supplies to Guadalcanal. Another attempt on 3 December dropped 1,500 drums of supplies near Guadalcanal, but Allied strafing aircraft sank all but 300 before the Japanese Army could recover them. On 7 December PT boats interrupted a Tokyo Express run, and the following night sank a Japanese supply submarine. The next day the Japanese Navy proposed stopping all destroyer runs to Guadalcanal, but agreed to do just one more. This was on 11 December and was also intercepted by PT boats, which sank a destroyer; only 200 of 1,200 drums dropped off the island were recovered. The next day the Japanese Navy proposed abandoning Guadalcanal; this was approved by the Imperial General Headquarters on 31 December and the Japanese left the island in early February 1943.\n\nAfter the Japanese abandoned Guadalcanal in February 1943, Allied operations in the Pacific shifted to the New Guinea campaign and isolating Rabaul. The Battle of Kula Gulf was fought on the night of 5–6 July. The US had three light cruisers and four destroyers; the Japanese had ten destroyers loaded with 2,600 troops destined for Vila to oppose a recent US landing on Rendova. Although the Japanese sank a cruiser, they lost two destroyers and were able to deliver only 850 troops. On the night of 12–13 July, the Battle of Kolombangara occurred. The Allies had three light cruisers (one New Zealand) and ten destroyers; the Japanese had one small light cruiser and five destroyers, a Tokyo Express run for Vila. All three Allied cruisers were heavily damaged, with the New Zealand cruiser put out of action for 25 months by a Long Lance hit. The Allies sank only the Japanese light cruiser, and the Japanese landed 1,200 troops at Vila. Despite their tactical victory, this battle caused the Japanese to use a different route in the future, where they were more vulnerable to destroyer and PT boat attacks.\n\nThe Battle of Empress Augusta Bay was fought on the night of 1–2 November 1943, immediately after US Marines invaded Bougainville in the Solomon Islands. A Japanese heavy cruiser was damaged by a nighttime air attack shortly before the battle; it is likely that Allied airborne radar had progressed far enough to allow night operations. The Americans had four of the new cruisers and eight destroyers. The Japanese had two heavy cruisers, two small light cruisers, and six destroyers. Both sides were plagued by collisions, shells that failed to explode, and mutual skill in dodging torpedoes. The Americans suffered significant damage to three destroyers and light damage to a cruiser, but no losses. The Japanese lost one light cruiser and a destroyer, with four other ships damaged. The Japanese withdrew; the Americans pursued them until dawn, then returned to the landing area to provide anti-aircraft cover.\n\nAfter the Battle of the Santa Cruz Islands in October 1942, both sides were short of large aircraft carriers. The US suspended major carrier operations until sufficient carriers could be completed to destroy the entire Japanese fleet at once should it appear. The Central Pacific carrier raids and amphibious operations commenced in November 1943 with a carrier raid on Rabaul (preceded and followed by Fifth Air Force attacks) and the bloody but successful invasion of Tarawa. The air attacks on Rabaul crippled the Japanese cruiser force, with four heavy and two light cruisers damaged; they were withdrawn to Truk. The US had built up a force in the Central Pacific of six large, five light, and six escort carriers prior to commencing these operations.\n\nFrom this point on, US cruisers primarily served as anti-aircraft escorts for carriers and in shore bombardment. The only major Japanese carrier operation after Guadalcanal was the disastrous (for Japan) Battle of the Philippine Sea in June 1944, nicknamed the \"Marianas Turkey Shoot\" by the US Navy.\n\nThe Imperial Japanese Navy's last major operation was the Battle of Leyte Gulf, an attempt to dislodge the American invasion of the Philippines in October 1944. The two actions at this battle in which cruisers played a significant role were the Battle off Samar and the Battle of Surigao Strait.\n\nBattle of Surigao Strait<br>\nThe Battle of Surigao Strait was fought on the night of 24–25 October, a few hours before the Battle off Samar. The Japanese had a small battleship group composed of and , one heavy cruiser, and four destroyers. They were followed at a considerable distance by another small force of two heavy cruisers, a small light cruiser, and four destroyers. Their goal was to head north through Surigao Strait and attack the invasion fleet off Leyte. The Allied force, known as the 7th Fleet Support Force, guarding the strait was overwhelming. It included six battleships (all but one previously damaged in 1941 at Pearl Harbor), four heavy cruisers (one Australian), four light cruisers, and 28 destroyers, plus a force of 39 PT boats. The only advantage to the Japanese was that most of the battleships and cruisers were loaded mainly with high explosive shells, although a significant number of armor-piercing shells were also loaded. The lead Japanese force evaded the PT boats' torpedoes, but were hit hard by the destroyers' torpedoes, losing a battleship. Then they encountered the battleship and cruiser guns. Only one destroyer survived. The engagement is notable for being one of only two occasions in which battleships fired on battleships in the Pacific Theater, the other being the Naval Battle of Guadalcanal. Due to the starting arrangement of the opposing forces, the Allied force was in a \"crossing the T\" position, so this was the last battle in which this occurred, but it was not a planned maneuver. The following Japanese cruiser force had several problems, including a light cruiser damaged by a PT boat and two heavy cruisers colliding, one of which fell behind and was sunk by air attack the next day. An American veteran of Surigao Strait, , was transferred to Argentina in 1951 as , becoming most famous for being sunk by in the Falklands War on 2 May 1982. She was the first ship sunk by a nuclear submarine outside of accidents, and only the second ship sunk by a submarine since World War II.\n\nBattle off Samar<br>\nAt the Battle off Samar, a Japanese battleship group moving towards the invasion fleet off Leyte engaged a minuscule American force known as \"Taffy 3\" (formally Task Unit 77.4.3), composed of six escort carriers with about 28 aircraft each, three destroyers, and four destroyer escorts. The biggest guns in the American force were /38 caliber guns, while the Japanese had , , and guns. Aircraft from six additional escort carriers also participated for a total of around 330 US aircraft, a mix of F6F Hellcat fighters and TBF Avenger torpedo bombers. The Japanese had four battleships including \"Yamato\", six heavy cruisers, two small light cruisers, and 11 destroyers. The Japanese force had earlier been driven off by air attack, losing \"Yamato\"s sister . Admiral Halsey then decided to use his Third Fleet carrier force to attack the Japanese carrier group, located well to the north of Samar, which was actually a decoy group with few aircraft. The Japanese were desperately short of aircraft and pilots at this point in the war, and Leyte Gulf was the first battle in which \"kamikaze\" attacks were used. Due to a tragedy of errors, Halsey took the American battleship force with him, leaving San Bernardino Strait guarded only by the small Seventh Fleet escort carrier force. The battle commenced at dawn on 25 October 1944, shortly after the Battle of Surigao Strait. In the engagement that followed, the Americans exhibited uncanny torpedo accuracy, blowing the bows off several Japanese heavy cruisers. The escort carriers' aircraft also performed very well, attacking with machine guns after their carriers ran out of bombs and torpedoes. The unexpected level of damage, and maneuvering to avoid the torpedoes and air attacks, disorganized the Japanese and caused them to think they faced at least part of the Third Fleet's main force. They had also learned of the defeat a few hours before at Surigao Strait, and did not hear that Halsey's force was busy destroying the decoy fleet. Convinced that the rest of the Third Fleet would arrive soon if it hadn't already, the Japanese withdrew, eventually losing three heavy cruisers sunk with three damaged to air and torpedo attacks. The Americans lost two escort carriers, two destroyers, and one destroyer escort sunk, with three escort carriers, one destroyer, and two destroyer escorts damaged, thus losing over one-third of their engaged force sunk with nearly all the remainder damaged.\n\nThe US built cruisers in quantity through the end of the war, notably 14 heavy cruisers and 27 \"Cleveland\"-class light cruisers, along with eight \"Atlanta\"-class anti-aircraft cruisers. The \"Cleveland\" class was the largest cruiser class ever built in number of ships completed, with nine additional \"Cleveland\"s completed as light aircraft carriers. The large number of cruisers built was probably due to the significant cruiser losses of 1942 in the Pacific theater (seven American and five other Allied) and the perceived need for several cruisers to escort each of the numerous s being built. Losing four heavy and two small light cruisers in 1942, the Japanese built only five light cruisers during the war; these were small ships with six guns each. Losing 20 cruisers in 1940–42, the British completed no heavy cruisers, thirteen light cruisers ( and classes), and sixteen anti-aircraft cruisers (\"Dido\" class) during the war.\n\nThe rise of air power during World War II dramatically changed the nature of naval combat. Even the fastest cruisers could not maneuver quickly enough to evade aerial attack, and aircraft now had torpedoes, allowing moderate-range standoff capabilities. This change led to the end of independent operations by single ships or very small task groups, and for the second half of the 20th century naval operations were based on very large fleets believed able to fend off all but the largest air attacks, though this was not tested by any war in that period. The US Navy became centered around carrier groups, with cruisers and battleships primarily providing anti-aircraft defense and shore bombardment. Until the Harpoon missile entered service in the late 1970s, the US Navy was almost entirely dependent on carrier-based aircraft and submarines for conventionally attacking enemy warships. Lacking aircraft carriers, the Soviet Navy depended on anti-ship cruise missiles; in the 1950s these were primarily delivered from heavy land-based bombers. Soviet submarine-launched cruise missiles at the time were primarily for land attack; but by 1964 anti-ship missiles were deployed in quantity on cruisers, destroyers, and submarines.\n\nThe US Navy was aware of the potential missile threat as soon as World War II ended, and had considerable related experience due to Japanese \"kamikaze\" attacks in that war. The initial response was to upgrade the light AA armament of new cruisers from 40 mm and 20 mm weapons to twin 3-inch (76 mm)/50 caliber gun mounts. For the longer term, it was thought that gun systems would be inadequate to deal with the missile threat, and by the mid-1950s three naval SAM systems were developed: Talos (long range), Terrier (medium range), and Tartar (short range). Talos and Terrier were nuclear-capable and this allowed their use in anti-ship or shore bombardment roles in the event of nuclear war. Chief of Naval Operations Admiral Arleigh Burke is credited with speeding the development of these systems.\n\nTerrier was initially deployed on two converted \"Baltimore\"-class cruisers (CAG), with conversions completed in 1955–56. Further conversions of six \"Cleveland\"-class cruisers (CLG) ( and classes), redesign of the as guided missile \"frigates\" (DLG), and development of the DDGs resulted in the completion of numerous additional guided missile ships deploying all three systems in 1959–1962. Also completed during this period was the nuclear-powered , with two Terrier and one Talos launchers, plus an ASROC anti-submarine launcher the World War II conversions lacked. The converted World War II cruisers up to this point retained one or two main battery turrets for shore bombardment. However, in 1962–1964 three additional \"Baltimore\" and cruisers were more extensively converted as the . These had two Talos and two Tartar launchers plus ASROC and two 5-inch (127 mm) guns for self-defense, and were primarily built to get greater numbers of Talos launchers deployed. Of all these types, only the \"Farragut\" DLGs were selected as the design basis for further production, although their successors were significantly larger (5,670 tons standard versus 4,150 tons standard) due to a second Terrier launcher and greater endurance. An economical crew size compared with World War II conversions was probably a factor, as the \"Leahy\"s required a crew of only 377 versus 1,200 for the \"Cleveland\"-class conversions. Through 1980, the ten \"Farragut\"s were joined by four additional classes and two one-off ships for a total of 36 guided missile frigates, eight of them nuclear-powered (DLGN). In 1975 the \"Farragut\"s were reclassified as guided missile destroyers (DDG) due to their small size, and the remaining DLG/DLGN ships became guided missile cruisers (CG/CGN). The World War II conversions were gradually retired between 1970 and 1980; the Talos missile was withdrawn in 1980 as a cost-saving measure and the \"Albany\"s were decommissioned. \"Long Beach\" had her Talos launcher removed in a refit shortly thereafter; the deck space was used for Harpoon missiles. Around this time the Terrier ships were upgraded with the RIM-67 Standard ER missile. The guided missile frigates and cruisers served in the Cold War and the Vietnam War; off Vietnam they performed shore bombardment and shot down enemy aircraft or, as Positive Identification Radar Advisory Zone (PIRAZ) ships, guided fighters to intercept enemy aircraft. By 1995 the former guided missile frigates were replaced by the s and s.\n\nThe U.S. Navy's guided-missile cruisers were built upon destroyer-style hulls (some called \"destroyer leaders\" or \"frigates\" prior to the 1975 reclassification). As the U.S. Navy's strike role was centered around aircraft carriers, cruisers were primarily designed to provide air defense while often adding anti-submarine capabilities. These U.S. cruisers that were built in the 1960s and 1970s were larger, often nuclear-powered for extended endurance in escorting nuclear-powered fleet carriers, and carried longer-range surface-to-air missiles (SAMs) than early \"Charles F. Adams\" guided-missile destroyers that were tasked with the short-range air defense role. The U.S. cruiser was a major contrast to their contemporaries, Soviet \"rocket cruisers\" that were armed with large numbers of anti-ship cruise missiles (ASCMs) as part of the combat doctrine of saturation attack, though in the early 1980s the U.S. Navy retrofitted some of these existing cruisers to carry a small number of Harpoon anti-ship missiles and Tomahawk cruise missiles.\n\nThe line between U.S. Navy cruisers and destroyers blurred with the . While originally designed for anti-submarine warfare, a \"Spruance\" destroyer was comparable in size to existing U.S. cruisers, while having the advantage of an enclosed hangar (with space for up to two medium-lift helicopters) which was a considerable improvement over the basic aviation facilities of earlier cruisers. The \"Spruance\" hull design was used as the basis for two classes; the which had comparable anti-air capabilities to cruisers at the time, and then the DDG-47-class destroyers which were redesignated as the \"Ticonderoga\"-class guided missile cruisers to emphasize the additional capability provided by the ships' Aegis combat systems, and their flag facilities suitable for an admiral and his staff. In addition, 24 members of the \"Spruance\" class were upgraded with the vertical launch system (VLS) for Tomahawk cruise missiles due to its modular hull design, along with the similarly VLS-equipped \"Ticonderoga\" class, these ships had anti-surface strike capabilities beyond the 1960s–1970s cruisers that received Tomahawk armored-box launchers as part of the New Threat Upgrade. Like the \"Ticonderoga\" ships with VLS, the \"Arleigh Burke\" and , despite being classified as destroyers, actually have much heavier anti-surface armament than previous U.S. ships classified as cruisers.\n\nPrior to the introduction of the \"Ticonderoga\"s, the US Navy used odd naming conventions that left its fleet seemingly without many cruisers, although a number of their ships were cruisers in all but name. From the 1950s to the 1970s, US Navy cruisers were large vessels equipped with heavy offensive missiles (mostly surface-to-air, but for several years including the Regulus nuclear cruise missile) for wide-ranging combat against land-based and sea-based targets. All save one— USS \"Long Beach\"—were converted from World War II cruisers of the \"Oregon City\", \"Baltimore\" and \"Cleveland\" classes. \"Long Beach\" was also the last cruiser built with a World War II-era cruiser style hull (characterized by a long lean hull); later new-build cruisers were actually converted frigates (DLG/CG , , and the \"Leahy\", , , and classes) or uprated destroyers (the DDG/CG \"Ticonderoga\" class was built on a \"Spruance\"-class destroyer hull).\n\nFrigates under this scheme were almost as large as the cruisers and optimized for anti-aircraft warfare, although they were capable anti-surface warfare combatants as well. In the late 1960s, the US government perceived a \"cruiser gap\"—at the time, the US Navy possessed six ships designated as cruisers, compared to 19 for the Soviet Union, even though the USN had 21 ships designated as frigates with equal or superior capabilities to the Soviet cruisers at the time. Because of this, in 1975 the Navy performed a massive redesignation of its forces:\nBULLET::::- CVA/CVAN (Attack Aircraft Carrier/Nuclear-powered Attack Aircraft Carrier) were redesignated CV/CVN (although and never embarked anti-submarine squadrons).\nBULLET::::- DLG/DLGN (Frigates/Nuclear-powered Frigates) of the \"Leahy\", \"Belknap\", and \"California\" classes along with USS \"Bainbridge\" and USS \"Truxtun\" were redesignated CG/CGN (Guided Missile Cruiser/Nuclear-powered Guided Missile Cruiser).\nBULLET::::- \"Farragut\"-class guided missile frigates (DLG), being smaller and less capable than the others, were redesignated to DDGs ( was the first ship of this class to be re-numbered; because of this the class is sometimes called the \"Coontz\" class);\nBULLET::::- DE/DEG (Ocean Escort/Guided Missile Ocean Escort) were redesignated to FF/FFG (Guided Missile Frigates), bringing the US \"Frigate\" designation into line with the rest of the world.\n\nAlso, a series of Patrol Frigates of the , originally designated PFG, were redesignated into the FFG line. The cruiser-destroyer-frigate realignment and the deletion of the Ocean Escort type brought the US Navy's ship designations into line with the rest of the world's, eliminating confusion with foreign navies. In 1980, the Navy's then-building DDG-47-class destroyers were redesignated as cruisers (\"Ticonderoga\" guided missile cruisers) to emphasize the additional capability provided by the ships' Aegis combat systems, and their flag facilities suitable for an admiral and his staff.\n\nIn the Soviet Navy, cruisers formed the basis of combat groups. In the immediate post-war era it built a fleet of gun-armed light cruisers, but replaced these beginning in the early 1960s with large ships called \"rocket cruisers\", carrying large numbers of anti-ship cruise missiles (ASCMs) and anti-aircraft missiles. The Soviet combat doctrine of saturation attack meant that their cruisers (as well as destroyers and even missile boats) mounted multiple missiles in large container/launch tube housings and carried far more ASCMs than their NATO counterparts, while NATO combatants instead used individually smaller and lighter missiles (while appearing under-armed when compared to Soviet ships).\n\nIn 1962–1965 the four s entered service; these had launchers for eight long-range SS-N-3 Shaddock ASCMs with a full set of reloads; these had a range of up to with mid-course guidance. The four more modest s, with launchers for four SS-N-3 ASCMs and no reloads, entered service in 1967–69. In 1969–79 Soviet cruiser numbers more than tripled with ten s and seven s entering service. These had launchers for eight large-diameter missiles whose purpose was initially unclear to NATO. This was the SS-N-14 Silex, an over/under rocket-delivered heavyweight torpedo primarily for the anti-submarine role, but capable of anti-surface action with a range of up to . Soviet doctrine had shifted; powerful anti-submarine vessels (these were designated \"Large Anti-Submarine Ships\", but were listed as cruisers in most references) were needed to destroy NATO submarines to allow Soviet ballistic missile submarines to get within range of the United States in the event of nuclear war. By this time Long Range Aviation and the Soviet submarine force could deploy numerous ASCMs. Doctrine later shifted back to overwhelming carrier group defenses with ASCMs, with the \"Slava\" and \"Kirov\" classes.\n\nThe most recent Soviet/Russian rocket cruisers, the four s, were built in the 1970s and 1980s. Two of the \"Kirov\" class are in refit until 2020, and one was scheduled to leave refit in 2018, with the in active service. Russia also operates three s and one \"Admiral Kuznetsov\"-class carrier which is officially designated as a cruiser.\n\nCurrently, the \"Kirov\"-class heavy missile cruisers are used for command purposes, as \"Pyotr Velikiy\" is the flagship of the Northern Fleet. However, their air defense capabilities are still powerful, as shown by the array of point defense missiles they carry, from 44 OSA-MA missiles to 196 9K311 Tor missiles. For longer range targets, the S-300 is used. For closer range targets, AK-630 or Kashtan CIWSs are used. Aside from that, \"Kirov\"s have 20 P-700 Granit missiles for anti-ship warfare. For target acquisition beyond the radar horizon, three helicopters can be used. Besides a vast array of armament, \"Kirov\"-class cruisers are also outfitted with many sensors and communications equipment, allowing them to lead the fleet.\n\nThe United States Navy has centered on the aircraft carrier since World War II. The \"Ticonderoga\"-class cruisers, built in the 1980s, were originally designed and designated as a class of destroyer, intended to provide a very powerful air-defense in these carrier-centered fleets.\n\nOutside the US and Soviet navies, new cruisers were rare following World War II. Most navies use guided missile destroyers for fleet air defense, and destroyers and frigates for cruise missiles. The need to operate in task forces has led most navies to change to fleets designed around ships dedicated to a single role, anti-submarine or anti-aircraft typically, and the large \"generalist\" ship has disappeared from most forces. The United States Navy and the Russian Navy are the only remaining navies which operate cruisers. Italy used until 2003; France operated a single helicopter cruiser until May 2010, , for training purposes only. While Type 055 of the Chinese Navy is classified as a cruiser by the U.S. Department of Defense, the Chinese consider it a guided missile destroyer.\n\nIn the years since the launch of in 1981, the class has received a number of upgrades that have dramatically improved its members' capabilities for anti-submarine and land attack (using the Tomahawk missile). Like their Soviet counterparts, the modern \"Ticonderoga\"s can also be used as the basis for an entire battle group. Their cruiser designation was almost certainly deserved when first built, as their sensors and combat management systems enable them to act as flagships for a surface warship flotilla if no carrier is present, but newer ships rated as destroyers and also equipped with Aegis approach them very closely in capability, and once more blur the line between the two classes.\n\nFrom time to time, some navies have experimented with aircraft-carrying cruisers. One example is the Swedish . Another was the Japanese \"Mogami\", which was converted to carry a large floatplane group in 1942. Another variant is the \"helicopter cruiser\". The last example in service was the Soviet Navy's , whose last unit was converted to a pure aircraft carrier and sold to India as . The Russian Navy's is nominally designated as an aviation cruiser but otherwise resembles a standard medium aircraft carrier, albeit with a surface-to-surface missile battery. The Royal Navy's aircraft-carrying and the Italian Navy's aircraft-carrying vessels were originally designated 'through-deck cruisers', but have since been designated as small aircraft carriers. Similarly, the Japan Maritime Self-Defense Force's and \"helicopter destroyers\" are really more along the lines of helicopter cruisers in function and aircraft complement, but due to the Treaty of San Francisco, must be designated as destroyers.\n\nOne cruiser alternative studied in the late 1980s by the United States was variously entitled a Mission Essential Unit (MEU) or CG V/STOL. In a return to the thoughts of the independent operations cruiser-carriers of the 1930s and the Soviet \"Kiev\" class, the ship was to be fitted with a hangar, elevators, and a flight deck. The mission systems were Aegis, SQS-53 sonar, 12 SV-22 ASW aircraft and 200 VLS cells. The resulting ship would have had a waterline length of 700 feet, a waterline beam of 97 feet, and a displacement of about 25,000 tons. Other features included an integrated electric drive and advanced computer systems, both stand-alone and networked. It was part of the U.S. Navy's \"Revolution at Sea\" effort. The project was curtailed by the sudden end of the Cold War and its aftermath, otherwise the first of class would have been likely ordered in the early 1990s.\n\nFew cruisers are still operational in the world navies. Those that remain in service today are:\nBULLET::::- : The is kept in ceremonial commission as the flagship of the Hellenic Navy due to her historical significance.\nBULLET::::- : 1 , 3 guided missile cruisers; and the was ceremonially recommissioned as the flagship of the Russian Navy due to her historical significance.\nBULLET::::- : 22 \"Ticonderoga\"-class guided missile cruisers.\n\nThe following is under construction/in layup:\nBULLET::::- : The is a \"Slava\"-class cruiser that was under construction during the breakup of the Soviet Union. Ukraine inherited the ship following its independence. Progress to complete the ship has been slow and currently stands at 95% complete. It is estimated that an additional US$30 million are needed to complete the ship, but her ultimate fate is uncertain.\n\nThe following are classified as destroyers by their respective operators, but, due to their size, are considered to be cruisers by some:\nBULLET::::- : The first Type 055 destroyer was launched by China in June 2017 and is expected to enter service in early 2019. Despite its classification as a destroyer, many naval analysts believe that it is far too large and too well armed to be considered a destroyer, and thus is in fact a cruiser, and is even classified by the United States Defense Department as such.\nBULLET::::- : 3 s. Despite their classification as a destroyer, many naval analysts feel they are in fact cruisers due to their size and armament, which are both greater than most of the world's destroyer classes. Three more are under construction.\n\nAs of 2019, several decommissioned cruisers have been saved from scrapping and exist worldwide as museum ships. They are:\n\nBULLET::::- A floating replica of the is on display in Dandong, China.\nBULLET::::- The is currently undergoing restoration for preservation in Taranto, Italy\nBULLET::::- in Athens, Greece; still active as the flagship of the Hellenic Navy\nBULLET::::- will be preserved in Lima, Peru - was the world's last operational gun cruiser.\nBULLET::::- in St. Petersburg, Russia; still active as the flagship of the Russian Navy\nBULLET::::- in Novorossiysk, Russia; the last surviving\nBULLET::::- in London, England\nBULLET::::- in Belfast, Northern Ireland; the last surviving ship from the Battle of Jutland\nBULLET::::- in Philadelphia, Pennsylvania; the world's oldest steel-hulled warship afloat.\nBULLET::::- in Buffalo, New York\nBULLET::::- in Quincy, Massachusetts; the world's last heavy cruiser.\n\nBULLET::::- Future museums:\nBULLET::::- Many interest groups favor preserving a \"Ticonderoga\"-class cruiser as the class retires from service\n\nBULLET::::- Former museums:\nBULLET::::- The was on display in Bordeaux, France until 2006, when she was forced to close due to financial difficulties. She sat in the French Navy's mothball fleet in Landevennec until she was sold for scrap in 2014.\n\nBULLET::::- List of cruisers\nBULLET::::- List of ships of the Second World War\nBULLET::::- List of battlecruisers of the Second World War\nBULLET::::- List of cruisers of the Second World War\n\nBULLET::::- Parkes, Oscar \"British Battleships\" (2nd Edition). Leo Cooper, London, 1990. .\n"}
{"id": "7037", "url": "https://en.wikipedia.org/wiki?curid=7037", "title": "Chlamydia", "text": "Chlamydia\n\nChlamydia, or more specifically a chlamydia infection, is a sexually transmitted infection caused by the bacterium \"Chlamydia trachomatis\". Most people who are infected have no symptoms. When symptoms do develop this can take a few weeks following infection to occur. Symptoms in women may include vaginal discharge or burning with urination. Symptoms in men may include discharge from the penis, burning with urination, or pain and swelling of one or both testicles. The infection can spread to the upper genital tract in women causing pelvic inflammatory disease which may result in future infertility or ectopic pregnancy. Repeated infections of the eyes that go without treatment can result in trachoma, a common cause of blindness in the developing world.\nChlamydia can be spread during vaginal, anal, or oral sex, and can be passed from an infected mother to her baby during childbirth. The eye infections may also be spread by personal contact, flies, and contaminated towels in areas with poor sanitation. \"Chlamydia trachomatis\" only occurs in humans. Diagnosis is often by screening which is recommended yearly in sexually active women under the age of twenty-five, others at higher risk, and at the first prenatal visit. Testing can be done on the urine or a swab of the cervix, vagina, or urethra. Rectal or mouth swabs are required to diagnose infections in those areas.\nPrevention is by not having sex, the use of condoms, or having sex with only one other person, who is not infected. Chlamydia can be cured by antibiotics with typically either azithromycin or doxycycline being used. Erythromycin or azithromycin is recommended in babies and during pregnancy. Sexual partners should also be treated and the infected people advised not to have sex for seven days and until symptom free. Gonorrhea, syphilis, and HIV should be tested for in those who have been infected. Following treatment people should be tested again after three months.\nChlamydia is one of the most common sexually transmitted infections, affecting about 4.2% of women and 2.7% of men worldwide. In 2015 about 61 million new cases occurred globally. In the United States about 1.4 million cases were reported in 2014. Infections are most common among those between the ages of 15 and 25 and are more common in women than men. In 2015 infections resulted in about 200 deaths. The word \"chlamydia\" is from the Greek, χλαμύδα meaning \"cloak\".\n\nChlamydial infection of the cervix (neck of the womb) is a sexually transmitted infection which has no symptoms for 50–70% of women infected. The infection can be passed through vaginal, anal, or oral sex. Of those who have an asymptomatic infection that is not detected by their doctor, approximately half will develop pelvic inflammatory disease (PID), a generic term for infection of the uterus, fallopian tubes, and/or ovaries. PID can cause scarring inside the reproductive organs, which can later cause serious complications, including chronic pelvic pain, difficulty becoming pregnant, ectopic (tubal) pregnancy, and other dangerous complications of pregnancy.\n\nChlamydia is known as the \"silent epidemic\", as in women it may not cause any symptoms in 70–80% of cases, and can linger for months or years before being discovered. Signs and symptoms may include abnormal vaginal bleeding or discharge, abdominal pain, painful sexual intercourse, fever, painful urination or the urge to urinate more often than usual (urinary urgency).\n\nFor sexually active women who are not pregnant, screening is recommended in those under 25 and others at risk of infection. Risk factors include a history of chlamydial or other sexually transmitted infection, new or multiple sexual partners, and inconsistent condom use. Guidelines recommend all women attending for emergency contraceptive are offered Chlamydia testing, with studies showing up to 9% of women aged <25 years had Chlamydia.\n\nIn men, those with a chlamydial infection show symptoms of infectious inflammation of the urethra in about 50% of cases. Symptoms that may occur include: a painful or burning sensation when urinating, an unusual discharge from the penis, testicular pain or swelling, or fever. If left untreated, chlamydia in men can spread to the testicles causing epididymitis, which in rare cases can lead to sterility if not treated. Chlamydia is also a potential cause of prostatic inflammation in men, although the exact relevance in prostatitis is difficult to ascertain due to possible contamination from urethritis.\n\nChlamydia conjunctivitis or trachoma was once the most important cause of blindness worldwide, but its role diminished from 15% of blindness cases by trachoma in 1995 to 3.6% in 2002. The infection can be spread from eye to eye by fingers, shared towels or cloths, coughing and sneezing and eye-seeking flies. Newborns can also develop chlamydia eye infection through childbirth (see below). Using the SAFE strategy (acronym for surgery for in-growing or in-turned lashes, antibiotics, facial cleanliness, and environmental improvements), the World Health Organization aims for the global elimination of trachoma by 2020 (GET 2020 initiative).\n\nChlamydia may also cause reactive arthritis—the triad of arthritis, conjunctivitis and urethral inflammation—especially in young men. About 15,000 men develop reactive arthritis due to chlamydia infection each year in the U.S., and about 5,000 are permanently affected by it. It can occur in both sexes, though is more common in men.\n\nAs many as half of all infants born to mothers with chlamydia will be born with the disease. Chlamydia can affect infants by causing spontaneous abortion; premature birth; conjunctivitis, which may lead to blindness; and pneumonia. Conjunctivitis due to chlamydia typically occurs one week after birth (compared with chemical causes (within hours) or gonorrhea (2–5 days)).\n\nA different serovar of Chlamydia trachomatis is also the cause of lymphogranuloma venereum, an infection of the lymph nodes and lymphatics. It usually presents with genital ulceration and swollen lymph nodes in the groin, but it may also manifest as rectal inflammation, fever or swollen lymph nodes in other regions of the body.\n\nChlamydia can be transmitted during vaginal, anal, or oral sex or direct contact with infected tissue such as conjunctiva. Chlamydia can also be passed from an infected mother to her baby during vaginal childbirth.\n\n\"Chlamydiae\" have the ability to establish long-term associations with host cells. When an infected host cell is starved for various nutrients such as amino acids (for example, tryptophan), iron, or vitamins, this has a negative consequence for \"Chlamydiae\" since the organism is dependent on the host cell for these nutrients. Long-term cohort studies indicate that approximately 50% of those infected clear within a year, 80% within two years, and 90% within three years.\n\nThe starved chlamydiae enter a persistent growth state wherein they stop cell division and become morphologically aberrant by increasing in size. Persistent organisms remain viable as they are capable of returning to a normal growth state once conditions in the host cell improve.\n\nThere is debate as to whether persistence has relevance. Some believe that persistent chlamydiae are the cause of chronic chlamydial diseases. Some antibiotics such as β-lactams have been found to induce a persistent-like growth state.\n\nThe diagnosis of genital chlamydial infections evolved rapidly from the 1990s through 2006. Nucleic acid amplification tests (NAAT), such as polymerase chain reaction (PCR), transcription mediated amplification (TMA), and the DNA strand displacement amplification (SDA) now are the mainstays. NAAT for chlamydia may be performed on swab specimens sampled from the cervix (women) or urethra (men), on self-collected vaginal swabs, or on voided urine. NAAT has been estimated to have a sensitivity of approximately 90% and a specificity of approximately 99%, regardless of sampling from a cervical swab or by urine specimen. In women seeking an STI clinic and a urine test is negative, a subsequent cervical swab has been estimated to be positive in approximately 2% of the time.\n\nAt present, the NAATs have regulatory approval only for testing urogenital specimens, although rapidly evolving research indicates that they may give reliable results on rectal specimens.\n\nBecause of improved test accuracy, ease of specimen management, convenience in specimen management, and ease of screening sexually active men and women, the NAATs have largely replaced culture, the historic gold standard for chlamydia diagnosis, and the non-amplified probe tests. The latter test is relatively insensitive, successfully detecting only 60–80% of infections in asymptomatic women, and often giving falsely positive results. Culture remains useful in selected circumstances and is currently the only assay approved for testing non-genital specimens. Other method also exist including: ligase chain reaction (LCR), direct fluorescent antibody resting, enzyme immunoassay, and cell culture.\n\nPrevention is by not having sex, the use of condoms, or having sex with only one other person, who is not infected.\n\nFor sexually active women who are not pregnant, screening is recommended in those under 25 and others at risk of infection. Risk factors include a history of chlamydial or other sexually transmitted infection, new or multiple sexual partners, and inconsistent condom use. For pregnant women, guidelines vary: screening women with age or other risk factors is recommended by the U.S. Preventive Services Task Force (USPSTF) (which recommends screening women under 25) and the American Academy of Family Physicians (which recommends screening women aged 25 or younger). The American College of Obstetricians and Gynecologists recommends screening all at risk, while the Centers for Disease Control and Prevention recommend universal screening of pregnant women. The USPSTF acknowledges that in some communities there may be other risk factors for infection, such as ethnicity. Evidence-based recommendations for screening initiation, intervals and termination are currently not possible. For men, the USPSTF concludes evidence is currently insufficient to determine if regular screening of men for chlamydia is beneficial. They recommend regular screening of men who are at increased risk for HIV or syphilis infection. A Cochrane review found that the effects of screening are uncertain in terms of chlamydia transmission but that screening probably reduces the risk of pelvic inflammatory disease in women.\n\nIn the United Kingdom the National Health Service (NHS) aims to:\n\nBULLET::::1. Prevent and control chlamydia infection through early detection and treatment of asymptomatic infection;\nBULLET::::2. Reduce onward transmission to sexual partners;\nBULLET::::3. Prevent the consequences of untreated infection;\nBULLET::::4. Test at least 25 percent of the sexually active under 25 population annually.\nBULLET::::5. Retest after treatment.\n\n\"C. trachomatis\" infection can be effectively cured with antibiotics. Guidelines recommend azithromycin, doxycycline, erythromycin, levofloxacin or ofloxacin. Agents recommended during pregnancy include erythromycin or amoxicillin.\n\nAn option for treating sexual partners of those with chlamydia or gonorrhea includes patient-delivered partner therapy (PDT or PDPT), which is the practice of treating the sex partners of index cases by providing prescriptions or medications to the patient to take to his/her partner without the health care provider first examining the partner.\n\nFollowing treatment people should be tested again after three months to check for reinfection.\n\nGlobally, as of 2015, sexually transmitted chlamydia affects approximately 61 million people. It is more common in women (3.8%) than men (2.5%). In 2015 it resulted in about 200 deaths.\n\nIn the United States about 1.6 million cases were reported in 2016. The CDC estimates that if one includes unreported cases there are about 2.9 million each year. It affects around 2% of young people. Chlamydial infection is the most common bacterial sexually transmitted infection in the UK.\n\nChlamydia causes more than 250,000 cases of epididymitis in the U.S. each year. Chlamydia causes 250,000 to 500,000 cases of PID every year in the United States. Women infected with chlamydia are up to five times more likely to become infected with HIV, if exposed.\n\nBULLET::::- Chlamydia Fact Sheet from the CDC\nBULLET::::- Links to chlamydia pictures at University of Iowa\n"}
{"id": "7038", "url": "https://en.wikipedia.org/wiki?curid=7038", "title": "Candidiasis", "text": "Candidiasis\n\nCandidiasis is a fungal infection due to any type of \"Candida\" (a type of yeast). When it affects the mouth, it is commonly called thrush. Signs and symptoms include white patches on the tongue or other areas of the mouth and throat. Other symptoms may include soreness and problems swallowing. When it affects the vagina, it is commonly called a yeast infection. Signs and symptoms include genital itching, burning, and sometimes a white \"cottage cheese-like\" discharge from the vagina. Yeast infections of the penis are less common and typically present with an itchy rash. Very rarely, yeast infections may become invasive, spreading to other parts of the body. This may result in fevers along with other symptoms depending on the parts involved.\nMore than 20 types of \"Candida\" can cause infection with \"Candida albicans\" being the most common. Infections of the mouth are most common among children less than one month old, the elderly, and those with weak immune systems. Conditions that result in a weak immune system include HIV/AIDS, the medications used after organ transplantation, diabetes, and the use of corticosteroids. Other risks include dentures, following antibiotic therapy, and breastfeeding. Vaginal infections occur more commonly during pregnancy, in those with weak immune systems, and following antibiotic use. Individuals at risk for invasive candidiasis include low birth weight babies, people recovering from surgery, people admitted to an intensive care units, and those with an otherwise compromised immune systems.\nEfforts to prevent infections of the mouth include the use of chlorhexidine mouth wash in those with poor immune function and washing out the mouth following the use of inhaled steroids. Little evidence supports probiotics for either prevention or treatment even among those with frequent vaginal infections. For infections of the mouth, treatment with topical clotrimazole or nystatin is usually effective. By mouth or intravenous fluconazole, itraconazole, or amphotericin B may be used if these do not work. A number of topical antifungal medications may be used for vaginal infections including clotrimazole. In those with widespread disease, an echinocandin such as caspofungin or micafungin is used. A number of weeks of intravenous amphotericin B may be used as an alternative. In certain groups at very high risk, antifungal medications may be used preventatively.\nInfections of the mouth occur in about 6% of babies less than a month old. About 20% of those receiving chemotherapy for cancer and 20% of those with AIDS also develop the disease. About three-quarters of women have at least one yeast infection at some time during their lives. Widespread disease is rare except in those who have risk factors.\n\nSigns and symptoms of candidiasis vary depending on the area affected. Most candidal infections result in minimal complications such as redness, itching, and discomfort, though complications may be severe or even fatal if left untreated in certain populations. In healthy (immunocompetent) persons, candidiasis is usually a localized infection of the skin, fingernails or toenails (onychomycosis), or mucosal membranes, including the oral cavity and pharynx (thrush), esophagus, and the genitalia (vagina, penis, etc.); less commonly in healthy individuals, the gastrointestinal tract, urinary tract, and respiratory tract are sites of candida infection.\n\nIn immunocompromised individuals, \"Candida\" infections in the esophagus occur more frequently than in healthy individuals and have a higher potential of becoming systemic, causing a much more serious condition, a fungemia called candidemia. Symptoms of esophageal candidiasis include difficulty swallowing, painful swallowing, abdominal pain, nausea, and vomiting.\n\nInfection in the mouth is characterized by white discolorations in the tongue, around the mouth, and throat. Irritation may also occur, causing discomfort when swallowing.\n\nThrush is commonly seen in infants. It is not considered abnormal in infants unless it lasts longer than a few weeks.\n\nInfection of the vagina or vulva may cause severe itching, burning, soreness, irritation, and a whitish or whitish-gray cottage cheese-like discharge. Symptoms of infection of the male genitalia (balanitis thrush) include red skin around the head of the penis, swelling, irritation, itchiness and soreness of the head of the penis, thick, lumpy discharge under the foreskin, unpleasant odour, difficulty retracting the foreskin (phimosis), and pain when passing urine or during sex.\n\nSigns and symptoms of candidiasis in the skin include itching, irritation, and chafing or broken skin.\n\nCommon symptoms of gastrointestinal candidiasis in healthy individuals are anal itching, belching, bloating, indigestion, nausea, diarrhea, gas, intestinal cramps, vomiting, and gastric ulcers. Perianal candidiasis can cause anal itching; the lesion can be red, papular, or ulcerative in appearance, and it is not considered to be a sexually transmissible disease. Abnormal proliferation of the candida in the gut may lead to dysbiosis. While it is not yet clear, this alteration may be the source of symptoms generally described as the irritable bowel syndrome, and other gastrointestinal diseases.\n\n\"Candida\" yeasts are generally present in healthy humans, frequently part of the human body's normal oral and intestinal flora, and particularly on the skin; however, their growth is normally limited by the human immune system and by competition of other microorganisms, such as bacteria occupying the same locations in the human body. \n\"Candida\" requires moisture for growth, notably on the skin. For example, wearing wet swimwear for long periods of time is believed to be a risk factor. Additionally, candida can also cause diaper rashes in babies. In extreme cases, superficial infections of the skin or mucous membranes may enter into the bloodstream and cause systemic \"Candida\" infections.\n\nFactors that increase the risk of candidiasis include HIV/AIDS, mononucleosis, cancer treatments, steroids, stress, antibiotic usage, diabetes, and nutrient deficiency. Hormone replacement therapy and infertility treatments may also be predisposing factors. Use of inhaled corticosteroids increases risk of candidiasis of the mouth. Inhaled corticosteroids with other risk factors such as antibiotics, oral glucocorticoids, not rinsing mouth after use of inhaled corticosteroids or high dose of inhaled corticosteroids put people at even higher risk. Treatment with antibiotics can lead to eliminating the yeast's natural competitors for resources in the oral and intestinal flora; thereby increasing the severity of the condition. A weakened or undeveloped immune system or metabolic illnesses are significant predisposing factors of candidiasis. Almost 15% of people with weakened immune systems develop a systemic illness caused by \"Candida\" species. Diets high in simple carbohydrates have been found to affect rates of oral candidiases.\n\n\"C. albicans\" was isolated from the vaginas of 19% of apparently healthy women, i.e., those who experienced few or no symptoms of infection. External use of detergents or douches or internal disturbances (hormonal or physiological) can perturb the normal vaginal flora, consisting of lactic acid bacteria, such as lactobacilli, and result in an overgrowth of \"Candida\" cells, causing symptoms of infection, such as local inflammation. Pregnancy and the use of oral contraceptives have been reported as risk factors. Diabetes mellitus and the use of antibiotics are also linked to increased rates of yeast infections.\n\nIn penile candidiasis, the causes include sexual intercourse with an infected individual, low immunity, antibiotics, and diabetes. Male genital yeast infections are less common, but a yeast infection on the penis caused from direct contact via sexual intercourse with an infected partner is not uncommon.\n\nBreast-feeding mothers may also develop candidiasis on and around the nipple as a result of moisture created by excessive milk-production.\n\nVaginal candidiasis can cause congenital candidiasis in newborns.\n\nIn oral candidiasis, simply inspecting the person's mouth for white patches and irritation may make the diagnosis. They may also take a sample of the infected area to determine what organism is causing the infection.\n\nSymptoms of vaginal candidiasis are also present in the more common bacterial vaginosis; aerobic vaginitis is distinct and should be excluded in the differential diagnosis. In a 2002 study, only 33% of women who were self-treating for a yeast infection actually had such an infection, while most had either bacterial vaginosis or a mixed-type infection.\n\nDiagnosis of a yeast infection is done either via microscopic examination or culturing. For identification by light microscopy, a scraping or swab of the affected area is placed on a microscope slide. A single drop of 10% potassium hydroxide (KOH) solution is then added to the specimen. The KOH dissolves the skin cells, but leaves the \"Candida\" cells intact, permitting visualization of pseudohyphae and budding yeast cells typical of many \"Candida\" species.\n\nFor the culturing method, a sterile swab is rubbed on the infected skin surface. The swab is then streaked on a culture medium. The culture is incubated at 37 °C (98.6 °F) for several days, to allow development of yeast or bacterial colonies. The characteristics (such as morphology and colour) of the colonies may allow initial diagnosis of the organism causing disease symptoms.\n\nRespiratory, gastrointestinal, and esophageal candidiasis require an endoscopy to diagnose. For gastrointestinal candidiasis, it is necessary to obtain a 3–5 milliliter sample of fluid from the duodenum for fungal culture. The diagnosis of gastrointestinal candidiasis is based upon the culture containing in excess of 1,000 colony-forming units per milliliter.\n\nCandidiasis may be divided into these types:\nBULLET::::- Mucosal candidiasis\nBULLET::::- Oral candidiasis (thrush, oropharyngeal candidiasis)\nBULLET::::- Pseudomembranous candidiasis\nBULLET::::- Erythematous candidiasis\nBULLET::::- Hyperplastic candidiasis\nBULLET::::- Denture-related stomatitis — \"Candida\" organisms are involved in about 90% of cases\nBULLET::::- Angular cheilitis — \"Candida\" species are responsible for about 20% of cases, mixed infection of \"C. albicans\" and \"Staphylococcus aureus\" for about 60% of cases.\nBULLET::::- Median rhomboid glossitis\nBULLET::::- Candidal vulvovaginitis (vaginal yeast infection)\nBULLET::::- Candidal balanitis — infection of the glans penis, almost exclusively occurring in uncircumcised males\nBULLET::::- Esophageal candidiasis (candidal esophagitis)\nBULLET::::- Gastrointestinal candidiasis\nBULLET::::- Respiratory candidiasis\nBULLET::::- Cutaneous candidiasis\nBULLET::::- Candidial folliculitis\nBULLET::::- Candidal intertrigo\nBULLET::::- Candidal paronychia\nBULLET::::- Perianal candidiasis, may present as pruritus ani\nBULLET::::- Candidid\nBULLET::::- Chronic mucocutaneous candidiasis\nBULLET::::- Congenital cutaneous candidiasis\nBULLET::::- Diaper candidiasis: an infection of a child's diaper area\nBULLET::::- Erosio interdigitalis blastomycetica\nBULLET::::- Candidial onychomycosis (nail infection) caused by \"Candida\"\nBULLET::::- Systemic candidiasis\nBULLET::::- Candidemia, a form of fungemia which may lead to sepsis\nBULLET::::- Invasive candidiasis (disseminated candidiasis) — organ infection by \"Candida\"\nBULLET::::- Chronic systemic candidiasis (hepatosplenic candidiasis) — sometimes arises during recovery from neutropenia\nBULLET::::- Antibiotic candidiasis (iatrogenic candidiasis)\n\nA diet that supports the immune system and is not high in simple carbohydrates contributes to a healthy balance of the oral and intestinal flora. While yeast infections are associated with diabetes, the level of blood sugar control may not affect the risk. Wearing cotton underwear may help to reduce the risk of developing skin and vaginal yeast infections, along with not wearing wet clothes for long periods of time. For women who experience recurrent yeast infections, there is limited evidence that oral or intravaginal probiotics help to prevent future infections. This includes either as pills or as yogurt.\n\nOral hygiene can help prevent oral candidiasis when people have a weakened immune system. For people undergoing cancer treatment, chlorhexidine mouthwash can prevent or reduce thrush. People who use inhaled corticosteroids can reduce the risk of developing oral candidiasis by rinsing the mouth with water or mouthwash after using the inhaler. People with dentures should also disinfect their dentures regularly to prevent oral candidiasis.\n\nCandidiasis is treated with antifungal medications; these include clotrimazole, nystatin, fluconazole, voriconazole, amphotericin B, and echinocandins. Intravenous fluconazole or an intravenous echinocandin such as caspofungin are commonly used to treat immunocompromised or critically ill individuals.\n\nThe 2016 revision of the clinical practice guideline for the management of candidiasis lists a large number of specific treatment regimens for \"Candida\" infections that involve different \"Candida\" species, forms of antifungal drug resistance, immune statuses, and infection localization and severity. Gastrointestinal candidiasis in immunocompetent individuals is treated with 100–200 mg fluconazole per day for 2–3 weeks.\n\nMouth and throat candidiasis are treated with antifungal medication. Oral candidiasis usually responds to topical treatments; otherwise, systemic antifungal medication may be needed for oral infections. Candidal skin infections in the skin folds (candidal intertrigo) typically respond well to topical antifungal treatments (e.g., nystatin or miconazole). For breastfeeding mothers topical miconazole is the most effective treatment for treating candidiasis on the breasts. Gentian violet can be used for thrush in breastfeeding babies. Systemic treatment with antifungals by mouth is reserved for severe cases or if treatment with topical therapy is unsuccessful. Candida esophagitis may be treated orally or intravenously; for severe or azole-resistant esophageal candidiasis, treatment with amphotericin B may be necessary.\n\nVaginal yeast infections are typically treated with topical antifungal agents. A one-time dose of fluconazole by mouth is 90% effective in treating a vaginal yeast infection. For severe nonrecurring cases, several doses of fluconazole is recommended. Local treatment may include vaginal suppositories or medicated douches. Other types of yeast infections require different dosing. \"C. albicans\" can develop resistance to fluconazole, this being more of an issue in those with HIV/AIDS who are often treated with multiple courses of fluconazole for recurrent oral infections.\n\nFor vaginal yeast infection in pregnancy, topical imidazole or triazole antifungals are considered the therapy of choice owing to available safety data. Systemic absorption of these topical formulations is minimal, posing little risk of transplacental transfer. In vaginal yeast infection in pregnancy, treatment with topical azole antifungals is recommended for 7 days instead of a shorter duration.\n\nFor vaginal yeast infections, many complementary treatments are proposed, however a number have side effects. No benefit from probiotics has been found for active infections.\n\nTreatment typically consists of oral or intravenous antifungal medications. In candidal infections of the blood, intravenous fluconazole or an echinocandin such as caspofungin may be used. Amphotericin B is another option.\n\nAmong individuals being treated in intensive care units, the mortality rate is about 30–50% when systemic candidiasis develops.\n\nOral candidiasis is the most common fungal infection of the mouth, and it also represents the most common opportunistic oral infection in humans. Infections of the mouth occur in about 6% of babies less than a month old. About 20% of those receiving chemotherapy for cancer and 20% of those with AIDS also develop the disease.\n\nIt is estimated that 20% of women may be asymptomatically colonized by vaginal yeast. In the United States there are approximately 1.4 million doctor office visits every year for candidiasis. About three-quarters of women have at least one yeast infection at some time during their lives.\n\nEsophageal candidiasis is the most common esophageal infection in persons with AIDS and accounts for about 50% of all esophageal infections, often coexisting with other esophageal diseases. About two-thirds of people with AIDS and esophageal candidiasis also have oral candidiasis.\n\nCandidal sepsis is rare. Candida is the fourth most common cause of bloodstream infections among hospital patients in the United States.\n\nDescriptions of what sounds like oral thrush go back to the time of Hippocrates \"circa\" 460–370 BCE.\n\nVulvovaginal candidiasis was first described in 1849 by Wilkinson. In 1875, Haussmann demonstrated the causative organism in both vulvovaginal and oral candidiasis is the same.\n\nWith the advent of antibiotics following World War II, the rates of candidiasis increased. The rates then decreased in the 1950s following the development of nystatin.\n\nThe colloquial term \"thrush\" refers to the resemblance of the white flecks present in some forms of candidiasis (\"e.g.\" pseudomembranous candidiasis) with the breast of the bird of the same name.<ref name=\"emedicine/Medscape\"></ref> The term candidosis is largely used in British English, and candidiasis in American English. \"Candida\" is also pronounced differently; in American English, the stress is on the \"i\", whereas in British English the stress is on the first syllable.\n\nThe genus \"Candida\" and species \"C. albicans\" were described by botanist Christine Marie Berkhout in her doctoral thesis at the University of Utrecht in 1923. Over the years, the classification of the genera and species has evolved. Obsolete names for this genus include \"Mycotorula\" and \"Torulopsis\". The species has also been known in the past as \"Monilia albicans\" and \"Oidium albicans\". The current classification is \"nomen conservandum\", which means the name is authorized for use by the International Botanical Congress (IBC).\n\nThe genus \"Candida\" includes about 150 different species; however, only a few are known to cause human infections. \"C. albicans\" is the most significant pathogenic species. Other species pathogenic in humans include \"C. auris\", \"C. tropicalis\", \"C. glabrata\", \"C. krusei\", \"C. parapsilosis\", \"C. dubliniensis\", and \"C. lusitaniae\".\n\nThe name \"Candida\" was proposed by Berkhout. It is from the Latin word \"toga candida\", referring to the white toga (robe) worn by candidates for the Senate of the ancient Roman republic. The specific epithet \"albicans\" also comes from Latin, \"albicare\" meaning \"to whiten\". These names refer to the generally white appearance of \"Candida\" species when cultured.\n\nA 2005 publication noted that \"a large pseudoscientific cult\" has developed around the topic of \"Candida\", with claims up to one in three people are affected by yeast-related illness, particularly a condition called \"Candidiasis hypersensitivity\". Some practitioners of alternative medicine have promoted these purported conditions and sold dietary supplements as supposed cures; a number of them have been prosecuted. In 1990, alternative health vendor Nature's Way signed an FTC consent agreement not to misrepresent in advertising any self-diagnostic test concerning yeast conditions or to make any unsubstantiated representation concerning any food or supplement's ability to control yeast conditions, with a fine of $30,000 payable to the National Institutes of Health for research in genuine candidiasis.\n\nHigh level \"Candida\" colonization is linked to several diseases of the gastrointestinal tract including Crohn's disease.\n\nThere has been an increase in resistance to antifungals worldwide over the past 30–40 years.\n"}
{"id": "7039", "url": "https://en.wikipedia.org/wiki?curid=7039", "title": "Control theory", "text": "Control theory\n\nControl theory in control systems engineering is a subfield of mathematics that deals with the control of continuously operating dynamical systems in engineered processes and machines. The objective is to develop a control model for controlling such systems using a control action in an optimum manner without \"delay or overshoot\" and ensuring control stability.\n\nTo do this, a \"controller\" with the requisite corrective behaviour is required. This controller monitors the controlled process variable (PV), and compares it with the reference or set point (SP). The difference between actual and desired value of the process variable, called the \"error\" signal, or SP-PV error, is applied as feedback to generate a control action to bring the controlled process variable to the same value as the set point. Other aspects which are also studied are controllability and observability. This is the basis for the advanced type of automation that revolutionized manufacturing, aircraft, communications and other industries. This is \"feedback control\", which is usually \"continuous\" and involves taking measurements using a sensor and making calculated adjustments to keep the measured variable within a set range by means of a \"final control element\", such as a control valve.\n\nExtensive use is usually made of a diagrammatic style known as the block diagram. In it the transfer function, also known as the system function or network function, is a mathematical model of the relation between the input and output based on the differential equations describing the system.\n\nControl theory dates from the 19th century, when the theoretical basis for the operation of governors was first described by James Clerk Maxwell. Control theory was further advanced by Edward Routh in 1874, Charles Sturm and in 1895, Adolf Hurwitz, who all contributed to the establishment of control stability criteria; and from 1922 onwards, the development of PID control theory by Nicolas Minorsky.\nAlthough a major application of control theory is in control systems engineering, which deals with the design of process control systems for industry, other applications range far beyond this. As the general theory of feedback systems, control theory is useful wherever feedback occurs.\n\nAlthough control systems of various types date back to antiquity, a more formal analysis of the field began with a dynamics analysis of the centrifugal governor, conducted by the physicist James Clerk Maxwell in 1868, entitled \"On Governors\". A centrifugal governor was already used to regulate the velocity of windmills. Maxwell described and analyzed the phenomenon of self-oscillation, in which lags in the system may lead to overcompensation and unstable behavior. This generated a flurry of interest in the topic, during which Maxwell's classmate, Edward John Routh, abstracted Maxwell's results for the general class of linear systems. Independently, Adolf Hurwitz analyzed system stability using differential equations in 1877, resulting in what is now known as the Routh–Hurwitz theorem.\n\nA notable application of dynamic control was in the area of manned flight. The Wright brothers made their first successful test flights on December 17, 1903 and were distinguished by their ability to control their flights for substantial periods (more so than the ability to produce lift from an airfoil, which was known). Continuous, reliable control of the airplane was necessary for flights lasting longer than a few seconds.\n\nBy World War II, control theory was becoming an important area of research. Irmgard Flügge-Lotz developed the theory of discontinuous automatic control systems, and applied the bang-bang principle to the development of automatic flight control equipment for aircraft. Other areas of application for discontinuous controls included fire-control systems, guidance systems and electronics.\n\nSometimes, mechanical methods are used to improve the stability of systems. For example, ship stabilizers are fins mounted beneath the waterline and emerging laterally. In contemporary vessels, they may be gyroscopically controlled active fins, which have the capacity to change their angle of attack to counteract roll caused by wind or waves acting on the ship.\n\nThe Space Race also depended on accurate spacecraft control, and control theory has also seen an increasing use in fields such as economics and artificial intelligence. Here, one might say that the goal is to find an internal model that obeys the good regulator theorem. So, for example, in economics, the more accurately a (stock or commodities) trading model represents the actions of the market, the more easily it can control that market (and extract \"useful work\" (profits) from it). In AI, an example might be a chatbot modelling the discourse state of humans: the more accurately it can model the human state (e.g. on a telephone voice-support hotline), the better it can manipulate the human (e.g. into performing the corrective actions to resolve the problem that caused the phone call to the help-line). These last two examples take the narrow historical interpretation of control theory as a set of differential equations modeling and regulating kinetic motion, and broaden it into a vast generalization of a regulator interacting with a plant.\n\nFundamentally, there are two types of control loops: open loop control and closed loop (feedback) control.\n\nIn open loop control, the control action from the controller is independent of the \"process output\" (or \"controlled process variable\" - PV). A good example of this is a central heating boiler controlled only by a timer, so that heat is applied for a constant time, regardless of the temperature of the building. The control action is the timed switching on/off of the boiler, the process variable is the building temperature, but neither is linked.\n\nIn closed loop control, the control action from the controller is dependent on feedback from the process in the form of the value of the process variable (PV). In the case of the boiler analogy, a closed loop would include a thermostat to compare the building temperature (PV) with the temperature set on the thermostat (the set point - SP). This generates a controller output to maintain the building at the desired temperature by switching the boiler on and off. A closed loop controller, therefore, has a feedback loop which ensures the controller exerts a control action to manipulate the process variable to be the same as the \"Reference input\" or \"set point\". For this reason, closed loop controllers are also called feedback controllers.\n\nThe definition of a closed loop control system according to the British Standard Institution is \"a control system possessing monitoring feedback, the deviation signal formed as a result of this feedback being used to control the action of a final control element in such a way as to tend to reduce the deviation to zero.\" \n\nLikewise; \"A \"Feedback Control System\" is a system which tends to maintain a prescribed relationship of one system variable to another by comparing functions of these variables and using the difference as a means of control.\"\n\nAn example of a control system is a car's cruise control, which is a device designed to maintain vehicle speed at a constant \"desired\" or \"reference\" speed provided by the driver. The \"controller\" is the cruise control, the \"plant\" is the car, and the \"system\" is the car and the cruise control. The system output is the car's speed, and the control itself is the engine's throttle position which determines how much power the engine delivers.\n\nA primitive way to implement cruise control is simply to lock the throttle position when the driver engages cruise control. However, if the cruise control is engaged on a stretch of flat road, then the car will travel slower going uphill and faster when going downhill. This type of controller is called an \"open-loop controller\" because there is no feedback; no measurement of the system output (the car's speed) is used to alter the control (the throttle position.) As a result, the controller cannot compensate for changes acting on the car, like a change in the slope of the road.\n\nIn a \"closed-loop control system\", data from a sensor monitoring the car's speed (the system output) enters a controller which continuously compares the quantity representing the speed with the reference quantity representing the desired speed. The difference, called the error, determines the throttle position (the control). The result is to match the car's speed to the reference speed (maintain the desired system output). Now, when the car goes uphill, the difference between the input (the sensed speed) and the reference continuously determines the throttle position. As the sensed speed drops below the reference, the difference increases, the throttle opens, and engine power increases, speeding up the vehicle. In this way, the controller dynamically counteracts changes to the car's speed. The central idea of these control systems is the \"feedback loop\", the controller affects the system output, which in turn is measured and fed back to the controller.\n\nTo overcome the limitations of the open-loop controller, control theory introduces feedback.\nA closed-loop controller uses feedback to control states or outputs of a dynamical system. Its name comes from the information path in the system: process inputs (e.g., voltage applied to an electric motor) have an effect on the process outputs (e.g., speed or torque of the motor), which is measured with sensors and processed by the controller; the result (the control signal) is \"fed back\" as input to the process, closing the loop.\n\nClosed-loop controllers have the following advantages over open-loop controllers:\nBULLET::::- disturbance rejection (such as hills in the cruise control example above)\nBULLET::::- guaranteed performance even with model uncertainties, when the model structure does not match perfectly the real process and the model parameters are not exact\nBULLET::::- unstable processes can be stabilized\nBULLET::::- reduced sensitivity to parameter variations\nBULLET::::- improved reference tracking performance\n\nIn some systems, closed-loop and open-loop control are used simultaneously. In such systems, the open-loop control is termed feedforward and serves to further improve reference tracking performance.\n\nA common closed-loop controller architecture is the PID controller.\n\nThe output of the system \"y(t)\" is fed back through a sensor measurement \"F\" to a comparison with the reference value \"r(t)\". The controller \"C\" then takes the error \"e\" (difference) between the reference and the output to change the inputs \"u\" to the system under control \"P\". This is shown in the figure. This kind of controller is a closed-loop controller or feedback controller.\n\nThis is called a single-input-single-output (\"SISO\") control system; \"MIMO\" (i.e., Multi-Input-Multi-Output) systems, with more than one input/output, are common. In such cases variables are represented through vectors instead of simple scalar values. For some distributed parameter systems the vectors may be infinite-dimensional (typically functions).\n\nIf we assume the controller \"C\", the plant \"P\", and the sensor \"F\" are linear and time-invariant (i.e., elements of their transfer function \"C(s)\", \"P(s)\", and \"F(s)\" do not depend on time), the systems above can be analysed using the Laplace transform on the variables. This gives the following relations:\n\nSolving for \"Y\"(\"s\") in terms of \"R\"(\"s\") gives\n\nThe expression formula_5 is referred to as the \"closed-loop transfer function\" of the system. The numerator is the forward (open-loop) gain from \"r\" to \"y\", and the denominator is one plus the gain in going around the feedback loop, the so-called loop gain. If formula_6, i.e., it has a large norm with each value of \"s\", and if formula_7, then \"Y(s)\" is approximately equal to \"R(s)\" and the output closely tracks the reference input.\n\nA proportional–integral–derivative controller (PID controller) is a control loop feedback mechanism control technique widely used in control systems.\n\nA PID controller continuously calculates an \"error value\" formula_8 as the difference between a desired setpoint and a measured process variable and applies a correction based on proportional, integral, and derivative terms. \"PID\" is an initialism for \"Proportional-Integral-Derivative\", referring to the three terms operating on the error signal to produce a control signal.\n\nThe theoretical understanding and application dates from the 1920s, and they are implemented in nearly all analogue control systems; originally in mechanical controllers, and then using discrete electronics and latterly in industrial process computers.\nThe PID controller is probably the most-used feedback control design.\n\nIf \"u(t)\" is the control signal sent to the system, \"y(t)\" is the measured output and \"r(t)\" is the desired output, and formula_9 is the tracking error, a PID controller has the general form\n\nThe desired closed loop dynamics is obtained by adjusting the three parameters formula_11, formula_12 and formula_13, often iteratively by \"tuning\" and without specific knowledge of a plant model. Stability can often be ensured using only the proportional term. The integral term permits the rejection of a step disturbance (often a striking specification in process control). The derivative term is used to provide damping or shaping of the response. PID controllers are the most well-established class of control systems: however, they cannot be used in several more complicated cases, especially if MIMO systems are considered.\n\nApplying Laplace transformation results in the transformed PID controller equation\n\nwith the PID controller transfer function\n\nAs an example of tuning a PID controller in the closed-loop system formula_17, consider a 1st order plant given by\n\nwhere formula_19 and formula_20 are some constants. The plant output is fed back through\n\nwhere formula_22 is also a constant. Now if we set formula_23, formula_24, and formula_25, we can express the PID controller transfer function in series form as\n\nPlugging formula_27, formula_28, and formula_29 into the closed-loop transfer function formula_17, we find that by setting\n\nformula_32. With this tuning in this example, the system output follows the reference input exactly.\n\nHowever, in practice, a pure differentiator is neither physically realizable nor desirable due to amplification of noise and resonant modes in the system. Therefore, a phase-lead compensator type approach or a differentiator with low-pass roll-off are used instead.\n\nThe field of control theory can be divided into two branches:\nBULLET::::- \"Linear control theory\" – This applies to systems made of devices which obey the superposition principle, which means roughly that the output is proportional to the input. They are governed by linear differential equations. A major subclass is systems which in addition have parameters which do not change with time, called \"linear time invariant\" (LTI) systems. These systems are amenable to powerful frequency domain mathematical techniques of great generality, such as the Laplace transform, Fourier transform, Z transform, Bode plot, root locus, and Nyquist stability criterion. These lead to a description of the system using terms like bandwidth, frequency response, eigenvalues, gain, resonant frequencies, zeros and poles, which give solutions for system response and design techniques for most systems of interest.\nBULLET::::- \"Nonlinear control theory\" – This covers a wider class of systems that do not obey the superposition principle, and applies to more real-world systems because all real control systems are nonlinear. These systems are often governed by nonlinear differential equations. The few mathematical techniques which have been developed to handle them are more difficult and much less general, often applying only to narrow categories of systems. These include limit cycle theory, Poincaré maps, Lyapunov stability theorem, and describing functions. Nonlinear systems are often analyzed using numerical methods on computers, for example by simulating their operation using a simulation language. If only solutions near a stable point are of interest, nonlinear systems can often be linearized by approximating them by a linear system using perturbation theory, and linear techniques can be used.\n\nMathematical techniques for analyzing and designing control systems fall into two different categories:\nBULLET::::- \"Frequency domain\" – In this type the values of the state variables, the mathematical variables representing the system's input, output and feedback are represented as functions of frequency. The input signal and the system's transfer function are converted from time functions to functions of frequency by a transform such as the Fourier transform, Laplace transform, or Z transform. The advantage of this technique is that it results in a simplification of the mathematics; the \"differential equations\" that represent the system are replaced by \"algebraic equations\" in the frequency domain which is much simpler to solve. However, frequency domain techniques can only be used with linear systems, as mentioned above.\nBULLET::::- \"Time-domain state space representation\" – In this type the values of the state variables are represented as functions of time. With this model, the system being analyzed is represented by one or more differential equations. Since frequency domain techniques are limited to linear systems, time domain is widely used to analyze real-world nonlinear systems. Although these are more difficult to solve, modern computer simulation techniques such as simulation languages have made their analysis routine.\nIn contrast to the frequency domain analysis of the classical control theory, modern control theory utilizes the time-domain state space representation, a mathematical model of a physical system as a set of input, output and state variables related by first-order differential equations. To abstract from the number of inputs, outputs, and states, the variables are expressed as vectors and the differential and algebraic equations are written in matrix form (the latter only being possible when the dynamical system is linear). The state space representation (also known as the \"time-domain approach\") provides a convenient and compact way to model and analyze systems with multiple inputs and outputs. With inputs and outputs, we would otherwise have to write down Laplace transforms to encode all the information about a system. Unlike the frequency domain approach, the use of the state-space representation is not limited to systems with linear components and zero initial conditions. \"State space\" refers to the space whose axes are the state variables. The state of the system can be represented as a point within that space.\n\nControl systems can be divided into different categories depending on the number of inputs and outputs.\nBULLET::::- Single-input single-output (SISO) – This is the simplest and most common type, in which one output is controlled by one control signal. Examples are the cruise control example above, or an audio system, in which the control input is the input audio signal and the output is the sound waves from the speaker.\nBULLET::::- Multiple-input multiple-output (MIMO) – These are found in more complicated systems. For example, modern large telescopes such as the Keck and MMT have mirrors composed of many separate segments each controlled by an actuator. The shape of the entire mirror is constantly adjusted by a MIMO active optics control system using input from multiple sensors at the focal plane, to compensate for changes in the mirror shape due to thermal expansion, contraction, stresses as it is rotated and distortion of the wavefront due to turbulence in the atmosphere. Complicated systems such as nuclear reactors and human cells are simulated by a computer as large MIMO control systems.\n\nThe \"stability\" of a general dynamical system with no input can be described with Lyapunov stability criteria.\nBULLET::::- A linear system is called bounded-input bounded-output (BIBO) stable if its output will stay bounded for any bounded input.\nBULLET::::- Stability for nonlinear systems that take an input is input-to-state stability (ISS), which combines Lyapunov stability and a notion similar to BIBO stability.\n\nFor simplicity, the following descriptions focus on continuous-time and discrete-time linear systems.\n\nMathematically, this means that for a causal linear system to be stable all of the poles of its transfer function must have negative-real values, i.e. the real part of each pole must be less than zero. Practically speaking, stability requires that the transfer function complex poles reside\nBULLET::::- in the open left half of the complex plane for continuous time, when the Laplace transform is used to obtain the transfer function.\nBULLET::::- inside the unit circle for discrete time, when the Z-transform is used.\nThe difference between the two cases is simply due to the traditional method of plotting continuous time versus discrete time transfer functions. The continuous Laplace transform is in Cartesian coordinates where the formula_33 axis is the real axis and the discrete Z-transform is in circular coordinates where the formula_34 axis is the real axis.\n\nWhen the appropriate conditions above are satisfied a system is said to be asymptotically stable; the variables of an asymptotically stable control system always decrease from their initial value and do not show permanent oscillations. Permanent oscillations occur when a pole has a real part exactly equal to zero (in the continuous time case) or a modulus equal to one (in the discrete time case). If a simply stable system response neither decays nor grows over time, and has no oscillations, it is marginally stable; in this case the system transfer function has non-repeated poles at the complex plane origin (i.e. their real and complex component is zero in the continuous time case). Oscillations are present when poles with real part equal to zero have an imaginary part not equal to zero.\n\nIf a system in question has an impulse response of\n\nthen the Z-transform (see this example), is given by\n\nwhich has a pole in formula_37 (zero imaginary part). This system is BIBO (asymptotically) stable since the pole is \"inside\" the unit circle.\n\nHowever, if the impulse response was\n\nthen the Z-transform is\n\nwhich has a pole at formula_40 and is not BIBO stable since the pole has a modulus strictly greater than one.\n\nNumerous tools exist for the analysis of the poles of a system. These include graphical systems like the root locus, Bode plots or the Nyquist plots.\n\nMechanical changes can make equipment (and control systems) more stable. Sailors add ballast to improve the stability of ships. Cruise ships use antiroll fins that extend transversely from the side of the ship for perhaps 30 feet (10 m) and are continuously rotated about their axes to develop forces that oppose the roll.\n\nControllability and observability are main issues in the analysis of a system before deciding the best control strategy to be applied, or whether it is even possible to control or stabilize the system. Controllability is related to the possibility of forcing the system into a particular state by using an appropriate control signal. If a state is not controllable, then no signal will ever be able to control the state. If a state is not controllable, but its dynamics are stable, then the state is termed \"stabilizable\". Observability instead is related to the possibility of \"observing\", through output measurements, the state of a system. If a state is not observable, the controller will never be able to determine the behavior of an unobservable state and hence cannot use it to stabilize the system. However, similar to the stabilizability condition above, if a state cannot be observed it might still be detectable.\n\nFrom a geometrical point of view, looking at the states of each variable of the system to be controlled, every \"bad\" state of these variables must be controllable and observable to ensure a good behavior in the closed-loop system. That is, if one of the eigenvalues of the system is not both controllable and observable, this part of the dynamics will remain untouched in the closed-loop system. If such an eigenvalue is not stable, the dynamics of this eigenvalue will be present in the closed-loop system which therefore will be unstable. Unobservable poles are not present in the transfer function realization of a state-space representation, which is why sometimes the latter is preferred in dynamical systems analysis.\n\nSolutions to problems of an uncontrollable or unobservable system include adding actuators and sensors.\n\nSeveral different control strategies have been devised in the past years. These vary from extremely general ones (PID controller), to others devoted to very particular classes of systems (especially robotics or aircraft cruise control).\n\nA control problem can have several specifications. Stability, of course, is always present. The controller must ensure that the closed-loop system is stable, regardless of the open-loop stability. A poor choice of controller can even worsen the stability of the open-loop system, which must normally be avoided. Sometimes it would be desired to obtain particular dynamics in the closed loop: i.e. that the poles have formula_41, where formula_42 is a fixed value strictly greater than zero, instead of simply asking that formula_43.\n\nAnother typical specification is the rejection of a step disturbance; including an integrator in the open-loop chain (i.e. directly before the system under control) easily achieves this. Other classes of disturbances need different types of sub-systems to be included.\n\nOther \"classical\" control theory specifications regard the time-response of the closed-loop system. These include the rise time (the time needed by the control system to reach the desired value after a perturbation), peak overshoot (the highest value reached by the response before reaching the desired value) and others (settling time, quarter-decay). Frequency domain specifications are usually related to robustness (see after).\n\nModern performance assessments use some variation of integrated tracking error (IAE,ISA,CQI).\n\nA control system must always have some robustness property. A robust controller is such that its properties do not change much if applied to a system slightly different from the mathematical one used for its synthesis. This requirement is important, as no real physical system truly behaves like the series of differential equations used to represent it mathematically. Typically a simpler mathematical model is chosen in order to simplify calculations, otherwise, the true system dynamics can be so complicated that a complete model is impossible.\n\nBULLET::::- System identification\n\nThe process of determining the equations that govern the model's dynamics is called system identification. This can be done off-line: for example, executing a series of measures from which to calculate an approximated mathematical model, typically its transfer function or matrix. Such identification from the output, however, cannot take account of unobservable dynamics. Sometimes the model is built directly starting from known physical equations, for example, in the case of a system we know that formula_44. Even assuming that a \"complete\" model is used in designing the controller, all the parameters included in these equations (called \"nominal parameters\") are never known with absolute precision; the control system will have to behave correctly even when connected to a physical system with true parameter values away from nominal.\n\nSome advanced control techniques include an \"on-line\" identification process (see later). The parameters of the model are calculated (\"identified\") while the controller itself is running. In this way, if a drastic variation of the parameters ensues, for example, if the robot's arm releases a weight, the controller will adjust itself consequently in order to ensure the correct performance.\n\nBULLET::::- Analysis\nAnalysis of the robustness of a SISO (single input single output) control system can be performed in the frequency domain, considering the system's transfer function and using Nyquist and Bode diagrams. Topics include gain and phase margin and amplitude margin. For MIMO (multi-input multi output) and, in general, more complicated control systems, one must consider the theoretical results devised for each control technique (see next section). I.e., if particular robustness qualities are needed, the engineer must shift his attention to a control technique by including them in its properties.\n\nBULLET::::- Constraints\nA particular robustness issue is the requirement for a control system to perform properly in the presence of input and state constraints. In the physical world every signal is limited. It could happen that a controller will send control signals that cannot be followed by the physical system, for example, trying to rotate a valve at excessive speed. This can produce undesired behavior of the closed-loop system, or even damage or break actuators or other subsystems. Specific control techniques are available to solve the problem: model predictive control (see later), and anti-wind up systems. The latter consists of an additional control block that ensures that the control signal never exceeds a given threshold.\n\nFor MIMO systems, pole placement can be performed mathematically using a state space representation of the open-loop system and calculating a feedback matrix assigning poles in the desired positions. In complicated systems this can require computer-assisted calculation capabilities, and cannot always ensure robustness. Furthermore, all system states are not in general measured and so observers must be included and incorporated in pole placement design.\n\nProcesses in industries like robotics and the aerospace industry typically have strong nonlinear dynamics. In control theory it is sometimes possible to linearize such classes of systems and apply linear techniques, but in many cases it can be necessary to devise from scratch theories permitting control of nonlinear systems. These, e.g., feedback linearization, backstepping, sliding mode control, trajectory linearization control normally take advantage of results based on Lyapunov's theory. Differential geometry has been widely used as a tool for generalizing well-known linear control concepts to the non-linear case, as well as showing the subtleties that make it a more challenging problem. Control theory has also been used to decipher the neural mechanism that directs cognitive states.\n\nWhen the system is controlled by multiple controllers, the problem is one of decentralized control. Decentralization is helpful in many ways, for instance, it helps control systems to operate over a larger geographical area. The agents in decentralized control systems can interact using communication channels and coordinate their actions.\n\nA stochastic control problem is one in which the evolution of the state variables is subjected to random shocks from outside the system. A deterministic control problem is not subject to external random shocks.\n\nEvery control system must guarantee first the stability of the closed-loop behavior. For linear systems, this can be obtained by directly placing the poles. Non-linear control systems use specific theories (normally based on Aleksandr Lyapunov's Theory) to ensure stability without regard to the inner dynamics of the system. The possibility to fulfill different specifications varies from the model considered and the control strategy chosen.\n\nBULLET::::- List of the main control techniques\n\nBULLET::::- Adaptive control uses on-line identification of the process parameters, or modification of controller gains, thereby obtaining strong robustness properties. Adaptive controls were applied for the first time in the aerospace industry in the 1950s, and have found particular success in that field.\nBULLET::::- A hierarchical control system is a type of control system in which a set of devices and governing software is arranged in a hierarchical tree. When the links in the tree are implemented by a computer network, then that hierarchical control system is also a form of networked control system.\nBULLET::::- Intelligent control uses various AI computing approaches like artificial neural networks, Bayesian probability, fuzzy logic, machine learning, evolutionary computation and genetic algorithms to control a dynamic system.\nBULLET::::- Optimal control is a particular control technique in which the control signal optimizes a certain \"cost index\": for example, in the case of a satellite, the jet thrusts needed to bring it to desired trajectory that consume the least amount of fuel. Two optimal control design methods have been widely used in industrial applications, as it has been shown they can guarantee closed-loop stability. These are Model Predictive Control (MPC) and linear-quadratic-Gaussian control (LQG). The first can more explicitly take into account constraints on the signals in the system, which is an important feature in many industrial processes. However, the \"optimal control\" structure in MPC is only a means to achieve such a result, as it does not optimize a true performance index of the closed-loop control system. Together with PID controllers, MPC systems are the most widely used control technique in process control.\nBULLET::::- Robust control deals explicitly with uncertainty in its approach to controller design. Controllers designed using \"robust control\" methods tend to be able to cope with small differences between the true system and the nominal model used for design. The early methods of Bode and others were fairly robust; the state-space methods invented in the 1960s and 1970s were sometimes found to lack robustness. Examples of modern robust control techniques include H-infinity loop-shaping developed by Duncan McFarlane and Keith Glover, Sliding mode control (SMC) developed by Vadim Utkin, and safe protocols designed for control of large heterogeneous populations of electric loads in Smart Power Grid applications. Robust methods aim to achieve robust performance and/or stability in the presence of small modeling errors.\nBULLET::::- Stochastic control deals with control design with uncertainty in the model. In typical stochastic control problems, it is assumed that there exist random noise and disturbances in the model and the controller, and the control design must take into account these random deviations.\nBULLET::::- Energy-shaping control view the plant and the controller as energy-transformation devices. The control strategy is formulated in terms of interconnection (in a power-preserving manner) in order to achieve a desired behavior.\nBULLET::::- Self-organized criticality control may be defined as attempts to interfere in the processes by which the self-organized system dissipates energy.\n\nMany active and historical figures made significant contribution to control theory including\nBULLET::::- Pierre-Simon Laplace invented the Z-transform in his work on probability theory, now used to solve discrete-time control theory problems. The Z-transform is a discrete-time equivalent of the Laplace transform which is named after him.\nBULLET::::- Irmgard Flugge-Lotz developed the theory of discontinuous automatic control and applied it to automatic aircraft control systems.\nBULLET::::- Alexander Lyapunov in the 1890s marks the beginning of stability theory.\nBULLET::::- Harold S. Black invented the concept of negative feedback amplifiers in 1927. He managed to develop stable negative feedback amplifiers in the 1930s.\nBULLET::::- Harry Nyquist developed the Nyquist stability criterion for feedback systems in the 1930s.\nBULLET::::- Richard Bellman developed dynamic programming since the 1940s.\nBULLET::::- Andrey Kolmogorov co-developed the Wiener–Kolmogorov filter in 1941.\nBULLET::::- Norbert Wiener co-developed the Wiener–Kolmogorov filter and coined the term cybernetics in the 1940s.\nBULLET::::- John R. Ragazzini introduced digital control and the use of Z-transform in control theory (invented by Laplace) in the 1950s.\nBULLET::::- Lev Pontryagin introduced the maximum principle and the bang-bang principle.\nBULLET::::- Pierre-Louis Lions developed viscosity solutions into stochastic control and optimal control methods.\nBULLET::::- Rudolf Kalman pioneered the state-space approach to systems and control. Introduced the notions of controllability and observability. Developed the Kalman filter for linear estimation.\nBULLET::::- Ali H. Nayfeh who was one of the main contributors to Non-Linear Control Theory and published many books on Perturbation Methods\nBULLET::::- Jan C. Willems Introduced the concept of dissipativity, as a generalization of Lyapunov function to input/state/output systems.The construction of the storage function, as the analogue of a Lyapunov function is called, led to the study of the linear matrix inequality (LMI) in control theory. He pioneered the behavioral approach to mathematical systems theory.\n\nBULLET::::- Examples of control systems\n\nBULLET::::- Automation\nBULLET::::- Deadbeat controller\nBULLET::::- Distributed parameter systems\nBULLET::::- Fractional-order control\nBULLET::::- H-infinity loop-shaping\nBULLET::::- Hierarchical control system\nBULLET::::- Model predictive control\nBULLET::::- PID controller\nBULLET::::- Process control\nBULLET::::- Robust control\nBULLET::::- Servomechanism\nBULLET::::- State space (controls)\nBULLET::::- Vector control\n\nBULLET::::- Topics in control theory\n\nBULLET::::- Coefficient diagram method\nBULLET::::- Control reconfiguration\nBULLET::::- Cut-insertion theorem\nBULLET::::- Feedback\nBULLET::::- H infinity\nBULLET::::- Hankel singular value\nBULLET::::- Krener's theorem\nBULLET::::- Lead-lag compensator\nBULLET::::- Minor loop feedback\nBULLET::::- Multi-loop feedback\nBULLET::::- Positive systems\nBULLET::::- Radial basis function\nBULLET::::- Root locus\nBULLET::::- Signal-flow graphs\nBULLET::::- Stable polynomial\nBULLET::::- State space representation\nBULLET::::- Steady state\nBULLET::::- Transient response\nBULLET::::- Transient state\nBULLET::::- Underactuation\nBULLET::::- Youla–Kucera parametrization\nBULLET::::- Markov chain approximation method\n\nBULLET::::- Other related topics\n\nBULLET::::- Automation and remote control\nBULLET::::- Bond graph\nBULLET::::- Control engineering\nBULLET::::- Control–feedback–abort loop\nBULLET::::- Controller (control theory)\nBULLET::::- Cybernetics\nBULLET::::- Intelligent control\nBULLET::::- Mathematical system theory\nBULLET::::- Negative feedback amplifier\nBULLET::::- People in systems and control\nBULLET::::- Perceptual control theory\nBULLET::::- Systems theory\nBULLET::::- Time scale calculus\nBULLET::::- For Chemical Engineering\n\nBULLET::::- Control Tutorials for Matlab, a set of worked-through control examples solved by several different methods.\nBULLET::::- Control Tuning and Best Practices\nBULLET::::- Advanced control structures, free on-line simulators explaining the control theory\nBULLET::::- \"Applying control theory to manage flash erasures/lifespan\"\nBULLET::::- The Dark Side of Loop Control Theory, a professional seminar taught at APEC in 2012 (Orlando, FL).\n"}
{"id": "7042", "url": "https://en.wikipedia.org/wiki?curid=7042", "title": "Cracking joints", "text": "Cracking joints\n\nCracking joints is manipulating one's joints to produce a distinct cracking or popping sound. It is sometimes performed by physical therapists, chiropractors, osteopaths, and masseurs in Turkish baths.\n\nThe cracking of joints, especially knuckles, was long believed to lead to arthritis and other joint problems. However, medical research has not demonstrated such a connection.\n\nThe cracking mechanism and the resulting sound is caused by carbon dioxide cavitation bubbles suddenly partially collapsing inside the joints.\n\nFor many decades, the physical mechanism that causes the cracking sound as a result of bending, twisting, or compressing joints was uncertain. Suggested causes included:\n\nBULLET::::- Formation of bubbles of joint air as the joint is expanded.\nBULLET::::- Cavitation within the joint—small cavities of partial vacuum form in the synovial fluid and then rapidly collapse, producing a sharp sound.\nBULLET::::- Rapid stretching of ligaments.\nBULLET::::- Intra-articular (within-joint) adhesions being broken.\n\nThere were several theories to explain the cracking of joints. Synovial fluid cavitation has some evidence to support it. When a spinal manipulation is performed, the applied force separates the articular surfaces of a fully encapsulated synovial joint, which in turn creates a reduction in pressure within the joint cavity. In this low-pressure environment, some of the gases that are dissolved in the synovial fluid (which are naturally found in all bodily fluids) leave the solution, making a bubble, or cavity, which rapidly collapses upon itself, resulting in a \"clicking\" sound. The contents of the resultant gas bubble are thought to be mainly carbon dioxide, oxygen and nitrogen. The effects of this process will remain for a period of time known as the \"refractory period,\" during which the joint cannot be \"re-cracked,\" which lasts about twenty minutes, while the gases are slowly reabsorbed into the synovial fluid. There is some evidence that ligament laxity may be associated with an increased tendency to cavitate.\n\nIn 2015, research showed that bubbles remained in the fluid after cracking, suggesting that the cracking sound was produced when the bubble within the joint was formed, not when it collapsed. In 2018, a team in France created a mathematical simulation of what happens in a joint just before it cracks. The team concluded that the sound is caused by bubbles' collapse, and bubbles observed in the fluid are the result of a partial collapse. Due to the theoretical basis and lack of physical experimentation, the scientific community is still not fully convinced of this conclusion.\n\nThe snapping of tendons or scar tissue over a prominence (as in snapping hip syndrome) can also generate a loud snapping or popping sound.\n\nThe common claim that cracking one's knuckles causes arthritis is not supported by evidence. A study published in 2011 examined the hand radiographs of 215 people (aged 50 to 89) and compared the joints of those who regularly cracked their knuckles to those who did not. The study concluded that knuckle-cracking did not cause hand osteoarthritis, no matter how many years or how often a person cracked their knuckles. A 1990 study also concluded that there was no increased preponderance of arthritis of the hand of chronic knuckle-crackers but that habitual knuckle-crackers were more likely to have hand swelling and lowered grip strength. It claimed further that habitual knuckle-cracking was associated with manual labour, biting of the nails, smoking, and drinking alcohol and suggested it resulted in functional hand impairment. This early study has been criticized for not taking into consideration the possibility of confounding factors, such as whether the ability to crack one's knuckles is associated with impaired hand functioning rather than being a cause of it.\n\nBULLET::::- Crepitus—sounds made by joints\n"}
{"id": "7043", "url": "https://en.wikipedia.org/wiki?curid=7043", "title": "Chemical formula", "text": "Chemical formula\n\n-\\overset{\\displaystyle H \\atop }{\\underset{ \\atop \\displaystyle H}{C}}-\\overset{\\displaystyle H \\atop }{\\underset{ \\atop \\displaystyle H}{C}}-\\overset{\\displaystyle H \\atop }{\\underset{ \\atop \\displaystyle H}{C}}-H</chem>\n\nA chemical formula is a way of presenting information about the chemical proportions of atoms that constitute a particular chemical compound or molecule, using chemical element symbols, numbers, and sometimes also other symbols, such as parentheses, dashes, brackets, commas and \"plus\" (+) and \"minus\" (−) signs. These are limited to a single typographic line of symbols, which may include subscripts and superscripts. A chemical formula is not a chemical name, and it contains no words. Although a chemical formula may imply certain simple chemical structures, it is not the same as a full chemical structural formula. Chemical formulas can fully specify the structure of only the simplest of molecules and chemical substances, and are generally more limited in power than are chemical names and structural formulas.\n\nThe simplest types of chemical formulas are called \"empirical formulas\", which use letters and numbers indicating the numerical \"proportions\" of atoms of each type. Molecular formulas indicate the simple numbers of each type of atom in a molecule, with no information on structure. For example, the empirical formula for glucose is CHO (twice as many hydrogen atoms as carbon and oxygen), while its molecular formula is CHO (12 hydrogen atoms, six carbon and oxygen atoms).\n\nSometimes a chemical formula is complicated by being written as a condensed formula (or condensed molecular formula, occasionally called a \"semi-structural formula\"), which conveys additional information about the particular ways in which the atoms are chemically bonded together, either in covalent bonds, ionic bonds, or various combinations of these types. This is possible if the relevant bonding is easy to show in one dimension. An example is the condensed molecular/chemical formula for ethanol, which is CH-CH-OH or CHCHOH. However, even a condensed chemical formula is necessarily limited in its ability to show complex bonding relationships between atoms, especially atoms that have bonds to four or more different substituents.\n\nSince a chemical formula must be expressed as a single line of chemical element symbols, it often cannot be as informative as a true structural formula, which is a graphical representation of the spatial relationship between atoms in chemical compounds (see for example the figure for butane structural and chemical formulas, at right). For reasons of structural complexity, a single condensed chemical formula (or semi-structural formula) may correspond to different molecules, known as isomers. For example glucose shares its molecular formula CHO with a number of other sugars, including fructose, galactose and mannose. Linear equivalent chemical \"names\" exist that can and do specify uniquely any complex structural formula (see chemical nomenclature), but such names must use many terms (words), rather than the simple element symbols, numbers, and simple typographical symbols that define a chemical formula.\n\nChemical formulas may be used in chemical equations to describe chemical reactions and other chemical transformations, such as the dissolving of ionic compounds into solution. While, as noted, chemical formulas do not have the full power of structural formulas to show chemical relationships between atoms, they are sufficient to keep track of numbers of atoms and numbers of electrical charges in chemical reactions, thus balancing chemical equations so that these equations can be used in chemical problems involving conservation of atoms, and conservation of electric charge.\n\nA chemical formula identifies each constituent element by its chemical symbol and indicates the proportionate number of atoms of each element. In empirical formulas, these proportions begin with a key element and then assign numbers of atoms of the other elements in the compound, by ratios to the key element. For molecular compounds, these ratio numbers can all be expressed as whole numbers. For example, the empirical formula of ethanol may be written CHO because the molecules of ethanol all contain two carbon atoms, six hydrogen atoms, and one oxygen atom. Some types of ionic compounds, however, cannot be written with entirely whole-number empirical formulas. An example is boron carbide, whose formula of CB is a variable non-whole number ratio with n ranging from over 4 to more than 6.5.\n\nWhen the chemical compound of the formula consists of simple molecules, chemical formulas often employ ways to suggest the structure of the molecule. These types of formulas are variously known as \"molecular formulas\" and \"condensed formulas\". A molecular formula enumerates the number of atoms to reflect those in the molecule, so that the molecular formula for glucose is CHO rather than the glucose empirical formula, which is CHO. However, except for very simple substances, molecular chemical formulas lack needed structural information, and are ambiguous.\n\nFor simple molecules, a condensed (or semi-structural) formula is a type of chemical formula that may fully imply a correct structural formula. For example, ethanol may be represented by the condensed chemical formula CHCHOH, and dimethyl ether by the condensed formula CHOCH. These two molecules have the same empirical and molecular formulas (CHO), but may be differentiated by the condensed formulas shown, which are sufficient to represent the full structure of these simple organic compounds.\n\nCondensed chemical formulas may also be used to represent ionic compounds that do not exist as discrete molecules, but nonetheless do contain covalently bound clusters within them. These polyatomic ions are groups of atoms that are covalently bound together and have an overall ionic charge, such as the sulfate ion. Each polyatomic ion in a compound is written individually in order to illustrate the separate groupings. For example, the compound dichlorine hexoxide has an empirical formula , and molecular formula , but in liquid or solid forms, this compound is more correctly shown by an ionic condensed formula , which illustrates that this compound consists of ions and ions. In such cases, the condensed formula only need be complex enough to show at least one of each ionic species.\n\nChemical formulas as described here are distinct from the far more complex chemical systematic names that are used in various systems of chemical nomenclature. For example, one systematic name for glucose is (2\"R\",3\"S\",4\"R\",5\"R\")-2,3,4,5,6-pentahydroxyhexanal. This name, interpreted by the rules behind it, fully specifies glucose's structural formula, but the name is not a chemical formula as usually understood, and uses terms and words not used in chemical formulas. Such names, unlike basic formulas, may be able to represent full structural formulas without graphs.\n\nIn chemistry, the empirical formula of a chemical is a simple expression of the relative number of each type of atom or ratio of the elements in the compound. Empirical formulas are the standard for ionic compounds, such as , and for macromolecules, such as . An empirical formula makes no reference to isomerism, structure, or absolute number of atoms. The term \"empirical\" refers to the process of elemental analysis, a technique of analytical chemistry used to determine the relative percent composition of a pure chemical substance by element.\n\nFor example, hexane has a molecular formula of , or structurally , implying that it has a chain structure of 6 carbon atoms, and 14 hydrogen atoms. However, the empirical formula for hexane is . Likewise the empirical formula for hydrogen peroxide, , is simply HO expressing the 1:1 ratio of component elements. Formaldehyde and acetic acid have the same empirical formula, . This is the actual chemical formula for formaldehyde, but acetic acid has double the number of atoms.\n\nMolecular formulas indicate the simple numbers of each type of atom in a molecule of a molecular substance. They are the same as empirical formulas for molecules that only have one atom of a particular type, but otherwise may have larger numbers. An example of the difference is the empirical formula for glucose, which is CHO (\"ratio\" 1:2:1), while its molecular formula is CHO (\"number of atoms\" 6:12:6). For water, both formulas are HO. A molecular formula provides more information about a molecule than its empirical formula, but is more difficult to establish.\n\nA molecular formula shows the number of elements in a molecule, and determines whether it is a binary compound, ternary compound, quaternary compound, or has even more elements.\n\nThe connectivity of a molecule often has a strong influence on its physical and chemical properties and behavior. Two molecules composed of the same numbers of the same types of atoms (i.e. a pair of isomers) might have completely different chemical and/or physical properties if the atoms are connected differently or in different positions. In such cases, a structural formula is useful, as it illustrates which atoms are bonded to which other ones. From the connectivity, it is often possible to deduce the approximate shape of the molecule.\n\nA condensed chemical formula may represent the types and spatial arrangement of bonds in a simple chemical substance, though it does not necessarily specify isomers or complex structures. For example, ethane consists of two carbon atoms single-bonded to each other, with each carbon atom having three hydrogen atoms bonded to it. Its chemical formula can be rendered as CHCH. In ethylene there is a double bond between the carbon atoms (and thus each carbon only has two hydrogens), therefore the chemical formula may be written: CHCH, and the fact that there is a double bond between the carbons is implicit because carbon has a valence of four. However, a more explicit method is to write HC=CH or less commonly HC::CH. The two lines (or two pairs of dots) indicate that a double bond connects the atoms on either side of them.\n\nA triple bond may be expressed with three lines (HC≡CH) or three pairs of dots (HC:::CH), and if there may be ambiguity, a single line or pair of dots may be used to indicate a single bond.\n\nMolecules with multiple functional groups that are the same may be expressed by enclosing the repeated group in round brackets. For example, isobutane may be written (CH)CH. This condensed structural formula implies a different connectivity from other molecules that can be formed using the same atoms in the same proportions (isomers). The formula (CH)CH implies a central carbon atom connected to one hydrogen atom and three CH groups. The same number of atoms of each element (10 hydrogens and 4 carbons, or CH) may be used to make a straight chain molecule, \"n\"-butane: CHCHCHCH.\n\nIn any given chemical compound, the elements always combine in the same proportion with each other. This is the law of constant composition.\n\nThe law of constant composition says that, in any particular chemical compound, all samples of that compound will be made up of the same elements in the same proportion or ratio. For example, any water molecule is always made up of two hydrogen atoms and one oxygen atom in a 2:1 ratio. If we look at the relative masses of oxygen and hydrogen in a water molecule, we see that 94% of the mass of a water molecule is accounted for by oxygen and the remaining 6% is the mass of hydrogen. This mass proportion will be the same for any water molecule.\n\nThe alkene called but-2-ene has two isomers, which the chemical formula CHCH=CHCH does not identify. The relative position of the two methyl groups must be indicated by additional notation denoting whether the methyl groups are on the same side of the double bond (\"cis\" or \"Z\") or on the opposite sides from each other (\"trans\" or \"E\"). Such extra symbols violate the rules for chemical formulas, and begin to enter the territory of more complex naming systems.\n\nAs noted above, in order to represent the full structural formulas of many complex organic and inorganic compounds, chemical nomenclature may be needed which goes well beyond the available resources used above in simple condensed formulas. See IUPAC nomenclature of organic chemistry and IUPAC nomenclature of inorganic chemistry 2005 for examples. In addition, linear naming systems such as International Chemical Identifier (InChI) allow a computer to construct a structural formula, and simplified molecular-input line-entry system (SMILES) allows a more human-readable ASCII input. However, all these nomenclature systems go beyond the standards of chemical formulas, and technically are chemical naming systems, not formula systems.\n\nFor polymers in condensed chemical formulas, parentheses are placed around the repeating unit. For example, a hydrocarbon molecule that is described as CH(CH)CH, is a molecule with fifty repeating units. If the number of repeating units is unknown or variable, the letter \"n\" may be used to indicate this formula: CH(CH)CH.\n\nFor ions, the charge on a particular atom may be denoted with a right-hand superscript. For example, Na, or Cu. The total charge on a charged molecule or a polyatomic ion may also be shown in this way. For example: HO or SO. Note that + and - are used in place of +1 and -1, respectively.\n\nFor more complex ions, brackets [ ] are often used to enclose the ionic formula, as in [BH], which is found in compounds such as Cs[BH]. Parentheses ( ) can be nested inside brackets to indicate a repeating unit, as in [Co(NH)]Cl. Here, (NH) indicates that the ion contains six NH groups bonded to cobalt, and [ ] encloses the entire formula of the ion with charge +3. \n\nThis is strictly optional; a chemical formula is valid with or without ionization information, and Hexamminecobalt(III) chloride may be written as [Co(NH)]Cl or [Co(NH)]Cl. Brackets, like parentheses, behave in chemistry as they do in mathematics, grouping terms together they are not specifically employed only for ionization states. In the latter case here, the parentheses indicate 6 groups all of the same shape, bonded to another group of size 1 (the cobalt atom), and then the entire bundle, as a group, is bonded to 3 chlorine atoms. In the former case, it is clearer that the bond connecting the chlorines is ionic, rather than covalent.\n\nAlthough isotopes are more relevant to nuclear chemistry or stable isotope chemistry than to conventional chemistry, different isotopes may be indicated with a prefixed superscript in a chemical formula. For example, the phosphate ion containing radioactive phosphorus-32 is [PO]. Also a study involving stable isotope ratios might include the molecule OO.\n\nA left-hand subscript is sometimes used redundantly to indicate the atomic number. For example, O for dioxygen, and for the most abundant isotopic species of dioxygen. This is convenient when writing equations for nuclear reactions, in order to show the balance of charge more clearly.\n\nThe @ symbol (at sign) indicates an atom or molecule trapped inside a cage but not chemically bound to it. For example, a buckminsterfullerene (C) with an atom (M) would simply be represented as MC regardless of whether M was inside the fullerene without chemical bonding or outside, bound to one of the carbon atoms. Using the @ symbol, this would be denoted M@C if M was inside the carbon network. A non-fullerene example is [As@NiAs], an ion in which one As atom is trapped in a cage formed by the other 32 atoms.\n\nThis notation was proposed in 1991 with the discovery of fullerene cages (endohedral fullerenes), which can trap atoms such as La to form, for example, La@C or La@C. The choice of the symbol has been explained by the authors as being concise, readily printed and transmitted electronically (the at sign is included in ASCII, which most modern character encoding schemes are based on), and the visual aspects suggesting the structure of an endohedral fullerene.\n\nChemical formulas most often use integers for each element. However, there is a class of compounds, called non-stoichiometric compounds, that cannot be represented by small integers. Such a formula might be written using decimal fractions, as in FeO, or it might include a variable part represented by a letter, as in FeO, where x is normally much less than 1.\n\nA chemical formula used for a series of compounds that differ from each other by a constant unit is called a \"general formula\". It generates a homologous series of chemical formulas. For example, alcohols may be represented by the formula CHOH (\"n\" ≥ 1), giving the homologs methanol, ethanol, propanol for \"n\"=1–3.\n\nThe Hill system (or Hill notation) is a system of writing empirical chemical formulas, molecular chemical formulas and components of a condensed formula such that the number of carbon atoms in a molecule is indicated first, the number of hydrogen atoms next, and then the number of all other chemical elements subsequently, in alphabetical order of the chemical symbols. When the formula contains no carbon, all the elements, including hydrogen, are listed alphabetically.\n\nBy sorting formulas according to the number of atoms of each element present in the formula according to these rules, with differences in earlier elements or numbers being treated as more significant than differences in any later element or number—like sorting text strings into lexicographical order—it is possible to collate chemical formulas into what is known as Hill system order.\n\nThe Hill system was first published by Edwin A. Hill of the United States Patent and Trademark Office in 1900. It is the most commonly used system in chemical databases and printed indexes to sort lists of compounds.\n\nA list of formulas in Hill system order is arranged alphabetically, as above, with single-letter elements coming before two-letter symbols when the symbols begin with the same letter (so \"B\" comes before \"Be\", which comes before \"Br\").\n\nThe following example formulae are written using the Hill system, and listed in Hill order:\n\nBULLET::::- BrI\nBULLET::::- CCl\nBULLET::::- CHI\nBULLET::::- CHBr\nBULLET::::- HOS\n\nBULLET::::- Dictionary of chemical formulas\nBULLET::::- Element symbol\nBULLET::::- Nuclear notation\nBULLET::::- Periodic table\nBULLET::::- IUPAC nomenclature of inorganic chemistry\nBULLET::::- Formula unit\n\n\nBULLET::::- Hill notation example, from the University of Massachusetts Lowell libraries, including how to sort into Hill system order\nBULLET::::- Molecular formula calculation applying Hill notation. The library calculating Hill notation is available on npm.\n"}
{"id": "7044", "url": "https://en.wikipedia.org/wiki?curid=7044", "title": "Beetle", "text": "Beetle\n\nBeetles are a group of insects that form the order Coleoptera, in the superorder Endopterygota. Their front pair of wings are hardened into wing-cases, elytra, distinguishing them from most other insects. The Coleoptera, with about 400,000 species, is the largest of all orders, constituting almost 40% of described insects and 25% of all known animal life-forms; new species are discovered frequently. The largest of all families, the Curculionidae (weevils), with some 83,000 member species,\nbelongs to this order. Found in almost every habitat except the sea and the polar regions, they interact with their ecosystems in several ways: beetles often feed on plants and fungi, break down animal and plant debris, and eat other invertebrates. Some species are serious agricultural pests, such as the Colorado potato beetle, while others such as Coccinellidae (ladybirds or ladybugs) eat aphids, scale insects, thrips, and other plant-sucking insects that damage crops.\n\nBeetles typically have a particularly hard exoskeleton including the elytra, though some such as the rove beetles have very short elytra while blister beetles have softer elytra. The general anatomy of a beetle is quite uniform and typical of insects, although there are several examples of novelty, such as adaptations in water beetles which trap air bubbles under the elytra for use while diving. Beetles are endopterygotes, which means that they undergo complete metamorphosis, with a series of conspicuous and relatively abrupt changes in body structure between hatching and becoming adult after a relatively immobile pupal stage. Some, such as stag beetles, have a marked sexual dimorphism, the males possessing enormously enlarged mandibles which they use to fight other males. Many beetles are aposematic, with bright colours and patterns warning of their toxicity, while others are harmless Batesian mimics of such insects. Many beetles, including those that live in sandy places, have effective camouflage.\n\nBeetles are prominent in human culture, from the sacred scarabs of ancient Egypt to beetlewing art and use as pets or fighting insects for entertainment and gambling. Many beetle groups are brightly and attractively coloured making them objects of collection and decorative displays. Over 300 species are used as food, mostly as larvae; species widely consumed include mealworms and rhinoceros beetle larvae. However, the major impact of beetles on human life is as agricultural, forestry, and horticultural pests. Serious pests include the boll weevil of cotton, the Colorado potato beetle, the coconut hispine beetle, and the mountain pine beetle. Most beetles, however, do not cause economic damage and many, such as the lady beetles and dung beetles are beneficial by helping to control insect pests.\n\nThe name of the taxonomic order, Coleoptera, comes from the Greek \"koleopteros\" (κολεόπτερος), given to the group by Aristotle for their elytra, hardened shield-like forewings, from \"koleos\", sheath, and \"pteron\", wing. The English name beetle comes from the Old English word \"bitela\", little biter, related to \"bītan\" (to bite), leading to Middle English \"betylle\". Another Old English name for beetle is \"ċeafor\", chafer, used in names such as cockchafer, from the Proto-Germanic *\"kebrô\" (\"beetle\"; compare German \"Käfer\", Dutch \"kever\").\n\nBeetles are by far the largest order of insects: the roughly 400,000 species make up about 40% of all insect species so far described, and about 25% of all animals. A 2015 study provided four independent estimates of the total number of beetle species, giving a mean estimate of some 1.5 million with a \"surprisingly narrow range\" spanning all four estimates from a minimum of 0.9 to a maximum of 2.1 million beetle species. The four estimates made use of host-specificity relationships (1.5 to 1.9 million), ratios with other taxa (0.9 to 1.2 million), plant:beetle ratios (1.2 to 1.3), and extrapolations based on body size by year of description (1.7 to 2.1 million).\n\nBeetles are found in nearly all habitats, including freshwater and coastal habitats, wherever vegetative foliage is found, from trees and their bark to flowers, leaves, and underground near roots - even inside plants in galls, in every plant tissue, including dead or decaying ones. Tropical forest canopies have a large and diverse fauna of beetles, including Carabidae, Chrysomelidae, and Scarabaeidae.\n\nThe heaviest beetle, indeed the heaviest insect stage, is the larva of the goliath beetle, \"Goliathus goliatus\", which can attain a mass of at least and a length of . Adult male goliath beetles are the heaviest beetle in its adult stage, weighing and measuring up to . Adult elephant beetles, \"Megasoma elephas\" and \"Megasoma actaeon\" often reach and .\n\nThe longest beetle is the Hercules beetle \"Dynastes hercules\", with a maximum overall length of at least 16.7 cm (6.6 in) including the very long pronotal horn. The smallest recorded beetle and the smallest free-living insect (), is the featherwing beetle \"Scydosella musawasensis\" which may measure as little as 325 µm in length.\n\nThe oldest known fossil insect that unequivocally resembles a Coleopteran is from the Lower Permian Period about (mya), though these members of the family Tshekardocoleidae have 13-segmented antennae, elytra with more fully developed venation and more irregular longitudinal ribbing, and abdomen and ovipositor extending beyond the apex of the elytra. In the Permian–Triassic extinction event at the end of the Permian, some 30% of all insect species became extinct, so the fossil record of insects only includes beetles from the Lower Triassic . Around this time, during the Late Triassic, fungus-feeding species such as Cupedidae appear in the fossil record. In the stages of the Upper Triassic, alga-feeding insects such as Triaplidae and Hydrophilidae begin to appear, alongside predatory water beetles. The first weevils, including the Obrienidae, appear alongside the first rove beetles (Staphylinidae), which closely resemble recent species. Some entomologists are sceptical that such early insects are so closely related to present-day species, arguing that this is extremely unlikely; for example, the structure of the metepisternum suggests that the Obrienidae could be Archostemata, not weevils at all, despite fossils with weevil-like snouts.\n\nIn 2009, a fossil beetle was described from the Pennsylvanian of Mazon Creek, Illinois, pushing the origin of the beetles to an earlier date, . Fossils from this time have been found in Asia and Europe, for instance in the red slate fossil beds of Niedermoschel near Mainz, Germany. Further fossils have been found in Obora, Czech Republic and Tshekarda in the Ural mountains, Russia. However, there are only a few fossils from North America before the middle Permian, although both Asia and North America had been united to Euramerica. The first discoveries from North America made in the Wellington formation of Oklahoma were published in 2005 and 2008.\n\nAs a consequence of the Permian–Triassic extinction event, the fossil record of insects is scant, including beetles from the Lower Triassic. However, there are a few exceptions, such as in Eastern Europe. At the Babiy Kamen site in the Kuznetsk Basin, numerous beetle fossils were discovered, including entire specimens of the infraorders Archostemata (e.g. Ademosynidae, Schizocoleidae), Adephaga (e.g., Triaplidae, Trachypachidae) and Polyphaga (e.g. Hydrophilidae, Byrrhidae, Elateroidea). However, species from the families Cupedidae and Schizophoroidae are not present at this site, whereas they dominate at other fossil sites from the Lower Triassic such as Khey-Yaga, Russia, in the Korotaikha Basin.\n\nDuring the Jurassic (), there was a dramatic increase in the diversity of beetle families, including the development and growth of carnivorous and herbivorous species. The Chrysomeloidea diversified around the same time, feeding on a wide array of plant hosts from cycads and conifers to angiosperms. Close to the Upper Jurassic, the Cupedidae decreased, but the diversity of the early plant-eating species increased. Most recent plant-eating beetles feed on flowering plants or angiosperms, whose success contributed to a doubling of plant-eating species during the Middle Jurassic. However, the increase of the number of beetle families during the Cretaceous does not correlate with the increase of the number of angiosperm species. Around the same time, numerous primitive weevils (e.g. Curculionoidea) and click beetles (e.g. Elateroidea) appeared. The first jewel beetles (e.g. Buprestidae) are present, but they remained rare until the Cretaceous. The first scarab beetles were not coprophagous but presumably fed on rotting wood with the help of fungus; they are an early example of a mutualistic relationship.\n\nThere are more than 150 important fossil sites from the Jurassic, the majority in Eastern Europe and North Asia. Outstanding sites include Solnhofen in Upper Bavaria, Germany, Karatau in South Kazakhstan, the Yixian formation in Liaoning, North China, as well as the Jiulongshan formation and further fossil sites in Mongolia. In North America there are only a few sites with fossil records of insects from the Jurassic, namely the shell limestone deposits in the Hartford basin, the Deerfield basin and the Newark basin.\n\nThe Cretaceous saw the fragmenting of the southern landmass, with the opening of the southern Atlantic Ocean and the isolation of New Zealand, while South America, Antarctica, and Australia grew more distant. The diversity of Cupedidae and Archostemata decreased considerably. Predatory ground beetles (Carabidae) and rove beetles (Staphylinidae) began to distribute into different patterns; the Carabidae predominantly occurred in the warm regions, while the Staphylinidae and click beetles (Elateridae) preferred temperate climates. Likewise, predatory species of Cleroidea and Cucujoidea hunted their prey under the bark of trees together with the jewel beetles (Buprestidae). The diversity of jewel beetles increased rapidly, as they were the primary consumers of wood, while longhorn beetles (Cerambycidae) were rather rare: their diversity increased only towards the end of the Upper Cretaceous. The first coprophagous beetles are from the Upper Cretaceous and may have lived on the excrement of herbivorous dinosaurs. The first species where both larvae and adults are adapted to an aquatic lifestyle are found. Whirligig beetles (Gyrinidae) were moderately diverse, although other early beetles (e.g. Dytiscidae) were less, with the most widespread being the species of Coptoclavidae, which preyed on aquatic fly larvae.\n\nMany fossil sites worldwide contain beetles from the Cretaceous. Most are in Europe and Asia and belong to the temperate climate zone during the Cretaceous. Lower Cretaceous sites include the Crato fossil beds in the Araripe basin in the Ceará, North Brazil, as well as overlying Santana formation; the latter was near the equator at that time. In Spain, important sites are near Montsec and Las Hoyas. In Australia, the Koonwarra fossil beds of the Korumburra group, South Gippsland, Victoria, are noteworthy. Major sites from the Upper Cretaceous include Kzyl-Dzhar in South Kazakhstan and Arkagala in Russia.\n\nBeetle fossils are abundant in the Cenozoic; by the Quaternary (up to 1.6 mya), fossil species are identical to living ones, while from the Late Miocene (5.7 mya) the fossils are still so close to modern forms that they are most likely the ancestors of living species. The large oscillations in climate during the Quaternary caused beetles to change their geographic distributions so much that current location gives little clue to the biogeographical history of a species. It is evident that geographic isolation of populations must often have been broken as insects moved under the influence of changing climate, causing mixing of gene pools, rapid evolution, and extinctions, especially in middle latitudes.\n\nThe very large number of beetle species poses special problems for classification. Some families contain tens of thousands of species, and need to be divided into subfamilies and tribes. This immense number led the evolutionary biologist J. B. S. Haldane to quip, when some theologians asked him what could be inferred about the mind of the Creator from the works of His Creation, \"An inordinate fondness for beetles\".\nPolyphaga is the largest suborder, containing more than 300,000 described species in more than 170 families, including rove beetles (Staphylinidae), scarab beetles (Scarabaeidae), blister beetles (Meloidae), stag beetles (Lucanidae) and true weevils (Curculionidae). These polyphagan beetle groups can be identified by the presence of cervical sclerites (hardened parts of the head used as points of attachment for muscles) absent in the other suborders.\nAdephaga contains about 10 families of largely predatory beetles, includes ground beetles (Carabidae), water beetles (Dytiscidae) and whirligig beetles (Gyrinidae). In these insects, the testes are tubular and the first abdominal sternum (a plate of the exoskeleton) is divided by the hind coxae (the basal joints of the beetle's legs).\nArchostemata contains four families of mainly wood-eating beetles, including reticulated beetles (Cupedidae) and the telephone-pole beetle.\nThe Archostemata have an exposed plate called the metatrochantin in front of the basal segment or coxa of the hind leg. Myxophaga contains about 65 described species in four families, mostly very small, including Hydroscaphidae and the genus \"Sphaerius\". The myxophagan beetles are small and mostly alga-feeders. Their mouthparts are characteristic in lacking galeae and having a mobile tooth on their left mandible.\n\nThe consistency of beetle morphology, in particular their possession of elytra, has long suggested that Coleoptera is monophyletic, though there have been doubts about the arrangement of the suborders, namely the Adephaga, Archostemata, Myxophaga and Polyphaga within that clade. The twisted-wing parasites, Strepsiptera, are thought to be a sister group to the beetles, having split from them in the Early Permian.\n\nMolecular phylogenetic analysis confirms that the Coleoptera are monophyletic. Duane McKenna et al. (2015) used eight nuclear genes for 367 species from 172 of 183 Coleopteran families. They split the Adephaga into 2 clades, Hydradephaga and Geadephaga, broke up the Cucujoidea into 3 clades, and placed the Lymexyloidea within the Tenebrionoidea. The Polyphaga appear to date from the Triassic. Most extant beetle families appear to have arisen in the Cretaceous. The cladogram is based on McKenna (2015). The number of species in each group (mainly superfamilies) is shown in parentheses, and boldface if over 10,000. English common names are given where possible. Dates of origin of major groups are shown in italics in millions of years ago (mya).\nBeetles are generally characterized by a particularly hard exoskeleton and hard forewings (elytra) not usable for flying. Almost all beetles have mandibles that move in a horizontal plane. The mouthparts are rarely suctorial, though they are sometimes reduced; the maxillae always bear palps. The antennae usually have 11 or fewer segments, except in some groups like the Cerambycidae (longhorn beetles) and the Rhipiceridae (cicada parasite beetles). The coxae of the legs are usually located recessed within a coxal cavity. The genitalic structures are telescoped into the last abdominal segment in all extant beetles. Beetle larvae can often be confused with those of other endopterygote groups. The beetle's exoskeleton is made up of numerous plates, called sclerites, separated by thin sutures. This design provides armored defenses while maintaining flexibility. The general anatomy of a beetle is quite uniform, although specific organs and appendages vary greatly in appearance and function between the many families in the order. Like all insects, beetles' bodies are divided into three sections: the head, the thorax, and the abdomen. Because there are so many species, identification is quite difficult, and relies on attributes including the shape of the antennae, the tarsal formulae and shapes of these small segments on the legs, the mouthparts, and the ventral plates (sterna, pleura, coxae). In many species accurate identification can only be made by examination of the unique male genitalic structures.\n\nThe head, having mouthparts projecting forward or sometimes downturned, is usually heavily sclerotized and is sometimes very large. The eyes are compound and may display remarkable adaptability, as in the case of the aquatic whirligig beetles (Gyrinidae), where they are split to allow a view both above and below the waterline. A few Longhorn beetles (Cerambycidae) and weevils as well as some fireflies (Rhagophthalmidae) have divided eyes, while many have eyes that are notched, and a few have ocelli, small, simple eyes usually farther back on the head (on the vertex); these are more common in larvae than in adults. The anatomical organization of the compound eyes may be modified and depends on whether a species is primarily crepuscular, or diurnally or nocturnally active. Ocelli are found in the adult carpet beetle (Dermestidae), some rove beetles (Omaliinae), and the Derodontidae.\n\nBeetle antennae are primarily organs of sensory perception and can detect motion, odour and chemical substances, but may also be used to physically feel a beetle's environment. Beetle families may use antennae in different ways. For example, when moving quickly, tiger beetles may not be able to see very well and instead hold their antennae rigidly in front of them in order to avoid obstacles.\nCertain Cerambycidae use antennae to balance, and blister beetles may use them for grasping. Some aquatic beetle species may use antennae for gathering air and passing it under the body whilst submerged. Equally, some families use antennae during mating, and a few species use them for defence. In the cerambycid \"Onychocerus albitarsis\", the antennae have venom injecting structures used in defence, which is unique among arthropods. Antennae vary greatly in form, sometimes between the sexes, but are often similar within any given family. Antennae may be , , , , (either on one side or both, bipectinate), or . The physical variation of antennae is important for the identification of many beetle groups. The Curculionidae have elbowed or geniculate antennae. Feather like flabellate antennae are a restricted form found in the Rhipiceridae and a few other families. The Silphidae have a capitate antennae with a spherical head at the tip. The Scarabaeidae typically have lamellate antennae with the terminal segments extended into long flat structures stacked together. The Carabidae typically have thread-like antennae. The antennae arises between the eye and the mandibles and in the Tenebrionidae, the antennae rise in front of a notch that breaks the usually circular outline of the compound eye. They are segmented and usually consist of 11 parts, the first part is called the scape and the second part is the pedicel. The other segments are jointly called the flagellum.\n\nBeetles have mouthparts like those of grasshoppers. The mandibles appear as large pincers on the front of some beetles. The mandibles are a pair of hard, often tooth-like structures that move horizontally to grasp, crush, or cut food or enemies (see defence, below). Two pairs of finger-like appendages, the maxillary and labial palpi, are found around the mouth in most beetles, serving to move food into the mouth. In many species, the mandibles are sexually dimorphic, with those of the males enlarged enormously compared with those of females of the same species.\n\nThe thorax is segmented into the two discernible parts, the pro- and pterothorax. The pterothorax is the fused meso- and metathorax, which are commonly separated in other insect species, although flexibly articulate from the prothorax. When viewed from below, the thorax is that part from which all three pairs of legs and both pairs of wings arise. The abdomen is everything posterior to the thorax. When viewed from above, most beetles appear to have three clear sections, but this is deceptive: on the beetle's upper surface, the middle section is a hard plate called the pronotum, which is only the front part of the thorax; the back part of the thorax is concealed by the beetle's wings. This further segmentation is usually best seen on the abdomen.\n\nThe multisegmented legs end in two to five small segments called tarsi. Like many other insect orders, beetles have claws, usually one pair, on the end of the last tarsal segment of each leg. While most beetles use their legs for walking, legs have been variously adapted for other uses. Aquatic beetles including the Dytiscidae (diving beetles), Haliplidae, and many species of Hydrophilidae, the legs, often the last pair, are modified for swimming, typically with rows of long hairs. Male diving beetles have suctorial cups on their forelegs that they use to grasp females. Other beetles have fossorial legs widened and often spined for digging. Species with such adaptations are found among the scarabs, ground beetles, and clown beetles (Histeridae). The hind legs of some beetles, such as flea beetles (within Chrysomelidae) and flea weevils (within Curculionidae), have enlarged femurs that help them leap.\n\nThe forewings of beetles are not used for flight, but form elytra which cover the hind part of the body and protect the hindwings. The elytra are usually hard shell-like structures which must be raised to allow the hind wings to move for flight. However, in the soldier beetles (Cantharidae), the elytra are soft, earning this family the name of leatherwings. Other soft wing beetles include the net-winged beetle \"Calopteron discrepans\", which has brittle wings that rupture easily in order to release chemicals for defence.\n\nBeetles' flight wings are crossed with veins and are folded after landing, often along these veins, and stored below the elytra. A fold (\"jugum\") of the membrane at the base of each wing is characteristic. Some beetles have lost the ability to fly. These include some ground beetles (Carabidae) and some true weevils (Curculionidae), as well as desert- and cave-dwelling species of other families. Many have the two elytra fused together, forming a solid shield over the abdomen. In a few families, both the ability to fly and the elytra have been lost, as in the glow-worms (Phengodidae), where the females resemble larvae throughout their lives. The presence of elytra and wings does not always indicate that the beetle will fly. For example, the tansy beetle walks between habitats despite being physically capable of flight.\n\nThe abdomen is the section behind the metathorax, made up of a series of rings, each with a hole for breathing and respiration, called a spiracle, composing three different segmented sclerites: the tergum, pleura, and the sternum. The tergum in almost all species is membranous, or usually soft and concealed by the wings and elytra when not in flight. The pleura are usually small or hidden in some species, with each pleuron having a single spiracle. The sternum is the most widely visible part of the abdomen, being a more or less sclerotized segment. The abdomen itself does not have any appendages, but some (for example, Mordellidae) have articulating sternal lobes.\n\nThe digestive system of beetles is primarily adapted for a herbivorous diet. Digestion takes place mostly in the anterior midgut, although in predatory groups like the Carabidae, most digestion occurs in the crop by means of midgut enzymes. In the Elateridae, the larvae are liquid feeders that extraorally digest their food by secreting enzymes. The alimentary canal basically consists of a short, narrow pharynx, a widened expansion, the crop, and a poorly developed gizzard. This is followed by the midgut, that varies in dimensions between species, with a large amount of cecum, and the hindgut, with varying lengths. There are typically four to six Malpighian tubules.\n\nThe nervous system in beetles contains all the types found in insects, varying between different species, from three thoracic and seven or eight abdominal ganglia which can be distinguished to that in which all the thoracic and abdominal ganglia are fused to form a composite structure.\n\nLike most insects, beetles inhale air, for the oxygen it contains, and exhale carbon dioxide, via a tracheal system. Air enters the body through spiracles, and circulates within the haemocoel in a system of tracheae and tracheoles, through whose walls the gases can diffuse.\n\nDiving beetles, such as the Dytiscidae, carry a bubble of air with them when they dive. Such a bubble may be contained under the elytra or against the body by specialized hydrophobic hairs. The bubble covers at least some of the spiracles, permitting air to enter the tracheae. The function of the bubble is not only to contain a store of air but to act as a physical gill. The air that it traps is in contact with oxygenated water, so as the animal's consumption depletes the oxygen in the bubble, more oxygen can diffuse in to replenish it. Carbon dioxide is more soluble in water than either oxygen or nitrogen, so it readily diffuses out faster than in. Nitrogen is the most plentiful gas in the bubble, and the least soluble, so it constitutes a relatively static component of the bubble and acts as a stable medium for respiratory gases to accumulate in and pass through. Occasional visits to the surface are sufficient for the beetle to re-establish the constitution of the bubble.\n\nLike other insects, beetles have open circulatory systems, based on hemolymph rather than blood. As in other insects, a segmented tube-like heart is attached to the dorsal wall of the hemocoel. It has paired inlets or \"ostia\" at intervals down its length, and circulates the hemolymph from the main cavity of the haemocoel and out through the anterior cavity in the head.\n\nDifferent glands are specialized for different pheromones to attract mates. Pheromones from species of Rutelinae are produced from epithelial cells lining the inner surface of the apical abdominal segments; amino acid-based pheromones of Melolonthinae are produced from eversible glands on the abdominal apex. Other species produce different types of pheromones. Dermestids produce esters, and species of Elateridae produce fatty acid-derived aldehydes and acetates. To attract a mate, fireflies (Lampyridae) use modified fat body cells with transparent surfaces backed with reflective uric acid crystals to produce light by bioluminescence. Light production is highly efficient, by oxidation of luciferin catalyzed by enzymes (luciferases) in the presence of adenosine triphosphate (ATP) and oxygen, producing oxyluciferin, carbon dioxide, and light.\n\nTympanal organs or hearing organs consist of a membrane (tympanum) stretched across a frame backed by an air sac and associated sensory neurons, are found in two families. Several species of the genus \"Cicindela\" (Carabidae) have hearing organs on the dorsal surfaces of their first abdominal segments beneath the wings; two tribes in the Dynastinae (within the Scarabaeidae) have hearing organs just beneath their pronotal shields or neck membranes. Both families are sensitive to ultrasonic frequencies, with strong evidence indicating they function to detect the presence of bats by their ultrasonic echolocation.\n\nBeetles are members of the superorder Endopterygota, and accordingly most of them undergo complete metamorphosis. The typical form of metamorphosis in beetles passes through four main stages: the egg, the larva, the pupa, and the imago or adult. The larvae are commonly called grubs and the pupa sometimes is called the chrysalis. In some species, the pupa may be enclosed in a cocoon constructed by the larva towards the end of its final instar. Some beetles, such as typical members of the families Meloidae and Rhipiphoridae, go further, undergoing hypermetamorphosis in which the first instar takes the form of a triungulin.\n\nSome beetles have intricate mating behaviour. Pheromone communication is often important in locating a mate.\nDifferent species use different pheromones. Scarab beetles such as the Rutelinae use pheromones derived from fatty acid synthesis, while other scarabs such as the Melolonthinae use amino acids and terpenoids. Another way beetles find mates is seen in the fireflies (Lampyridae) which are bioluminescent, with abdominal light-producing organs. The males and females engage in a complex dialogue before mating; each species has a unique combination of flight patterns, duration, composition, and intensity of the light produced.\n\nBefore mating, males and females may stridulate, or vibrate the objects they are on. In the Meloidae, the male climbs onto the dorsum of the female and strokes his antennae on her head, palps, and antennae. In \"Eupompha\", the male draws his antennae along his longitudinal vertex. They may not mate at all if they do not perform the precopulatory ritual. This mating behaviour may be different amongst dispersed populations of the same species. For example, the mating of a Russian population of tansy beetle (\"Chysolina graminis\") is preceded by an elaborate ritual involving the male tapping the female's eyes, pronotum and antennae with its antennae, which is not evident in the population of this species in the United Kingdom.\n\nCompetition can play a part in the mating rituals of species such as burying beetles (\"Nicrophorus\"), the insects fighting to determine which can mate. Many male beetles are territorial and fiercely defend their territories from intruding males. In such species, the male often has horns on the head or thorax, making its body length greater than that of a female. Copulation is generally quick, but in some cases lasts for several hours. During copulation, sperm cells are transferred to the female to fertilize the egg.\n\nEssentially all beetles lay eggs, though some myrmecophilous Aleocharinae and some Chrysomelinae which live in mountains or the subarctic are ovoviviparous, laying eggs which hatch almost immediately. Beetle eggs generally have smooth surfaces and are soft, though the Cupedidae have hard eggs. Eggs vary widely between species: the eggs tend to be small in species with many instars (larval stages), and in those that lay large numbers of eggs.\nA female may lay from several dozen to several thousand eggs during her lifetime, depending on the extent of parental care. This ranges from the simple laying of eggs under a leaf, to the parental care provided by scarab beetles, which house, feed and protect their young. The Attelabidae roll leaves and lay their eggs inside the roll for protection.\n\nThe larva is usually the principal feeding stage of the beetle life cycle. Larvae tend to feed voraciously once they emerge from their eggs. Some feed externally on plants, such as those of certain leaf beetles, while others feed within their food sources. Examples of internal feeders are most Buprestidae and longhorn beetles. The larvae of many beetle families are predatory like the adults (ground beetles, ladybirds, rove beetles). The larval period varies between species, but can be as long as several years. The larvae of skin beetles undergo a degree of reversed development when starved, and later grow back to the previously attained level of maturity. The cycle can be repeated many times (see Biological immortality). Larval morphology is highly varied amongst species, with well-developed and sclerotized heads, distinguishable thoracic and abdominal segments (usually the tenth, though sometimes the eighth or ninth).\n\nBeetle larvae can be differentiated from other insect larvae by their hardened, often darkened heads, the presence of chewing mouthparts, and spiracles along the sides of their bodies. Like adult beetles, the larvae are varied in appearance, particularly between beetle families. Beetles with somewhat flattened, highly mobile larvae include the ground beetles and rove beetles; their larvae are described as campodeiform. Some beetle larvae resemble hardened worms with dark head capsules and minute legs. These are elateriform larvae, and are found in the click beetle (Elateridae) and darkling beetle (Tenebrionidae) families. Some elateriform larvae of click beetles are known as wireworms. Beetles in the Scarabaeoidea have short, thick larvae described as scarabaeiform, more commonly known as grubs.\n\nAll beetle larvae go through several instars, which are the developmental stages between each moult. In many species, the larvae simply increase in size with each successive instar as more food is consumed. In some cases, however, more dramatic changes occur. Among certain beetle families or genera, particularly those that exhibit parasitic lifestyles, the first instar (the planidium) is highly mobile to search out a host, while the following instars are more sedentary and remain on or within their host. This is known as hypermetamorphosis; it occurs in the Meloidae, Micromalthidae, and Ripiphoridae. The blister beetle \"Epicauta vittata\" (Meloidae), for example, has three distinct larval stages. Its first stage, the triungulin, has longer legs to go in search of the eggs of grasshoppers. After feeding for a week it moults to the second stage, called the caraboid stage, which resembles the larva of a carabid beetle. In another week it moults and assumes the appearance of a scarabaeid larva – the scarabaeidoid stage. Its penultimate larval stage is the pseudo-pupa or the coarcate larva, which will overwinter and pupate until the next spring.\n\nThe larval period can vary widely. A fungus feeding staphylinid \"Phanerota fasciata\" undergoes three moults in 3.2 days at room temperature while \"Anisotoma\" sp. (Leiodidae) completes its larval stage in the fruiting body of slime mold in 2 days and possibly represents the fastest growing beetles. Dermestid beetles, \"Trogoderma inclusum\" can remain in an extended larval state under unfavourable conditions, even reducing their size between moults. A larva is reported to have survived for 3.5 years in an enclosed container.\n\nAs with all endopterygotes, beetle larvae pupate, and from these pupae emerge fully formed, sexually mature adult beetles, or imagos. Pupae never have mandibles (they are adecticous). In most pupae, the appendages are not attached to the body and are said to be exarate; in a few beetles (Staphylinidae, Ptiliidae etc.) the appendages are fused with the body (termed as obtect pupae).\n\nAdults have extremely variable lifespans, from weeks to years, depending on the species. Some wood-boring beetles can have extremely long life-cycles. It is believed that when furniture or house timbers are infested by beetle larvae, the timber already contained the larvae when it was first sawn up. A birch bookcase 40 years old released adult \"Eburia quadrigeminata\" (Cerambycidae), while \"Buprestis aurulenta\" and other Buprestidae have been documented as emerging as much as 51 years after manufacture of wooden items.\n\nThe elytra allow beetles to both fly and move through confined spaces, doing so by folding the delicate wings under the elytra while not flying, and folding their wings out just before takeoff. The unfolding and folding of the wings is operated by muscles attached to the wing base; as long as the tension on the radial and cubital veins remains, the wings remain straight. In some day-flying species (for example, Buprestidae, Scarabaeidae), flight does not include large amounts of lifting of the elytra, having the metathorac wings extended under the lateral elytra margins. The altitude reached by beetles in flight varies. One study investigating the flight altitude of the ladybird species \"Coccinella septempunctata\" and \"Harmonia axyridis\" using radar showed that, whilst the majority in flight over a single location were at 150–195 m above ground level, some reached altitudes of over 1100 m.\n\nMany rove beetles have greatly reduced elytra, and while they are capable of flight, they most often move on the ground: their soft bodies and strong abdominal muscles make them flexible, easily able to wriggle into small cracks.\n\nAquatic beetles use several techniques for retaining air beneath the water's surface. Diving beetles (Dytiscidae) hold air between the abdomen and the elytra when diving. Hydrophilidae have hairs on their under surface that retain a layer of air against their bodies. Adult crawling water beetles use both their elytra and their hind coxae (the basal segment of the back legs) in air retention, while whirligig beetles simply carry an air bubble down with them whenever they dive.\n\nBeetles have a variety of ways to communicate, including the use of pheromones. The mountain pine beetle emits a pheromone to attract other beetles to a tree. The mass of beetles are able to overcome the chemical defenses of the tree. After the tree's defenses have been exhausted, the beetles emit an anti-aggregation pheromone. This species can stridulate to communicate, but others may use sound to defend themselves when attacked.\n\nParental care is found in a few families of beetle, perhaps for protection against adverse conditions and predators. The rove beetle \"Bledius spectabilis\" lives in salt marshes, so the eggs and larvae are endangered by the rising tide. The maternal beetle patrols the eggs and larvae, burrowing to keep them from flooding and asphyxiating, and protects them from the predatory carabid beetle \"Dicheirotrichus gustavi\" and from the parasitoidal wasp \"Barycnemis blediator\", which kills some 15% of the larvae.\n\nBurying beetles are attentive parents, and participate in cooperative care and feeding of their offspring. Both parents work to bury small animal carcass to serve as a food resource for their young and build a brood chamber around it. The parents prepare the carcass and protect it from competitors and from early decomposition. After their eggs hatch, the parents keep the larvae clean of fungus and bacteria and help the larvae feed by regurgitating food for them.\n\nSome dung beetles provide parental care, collecting herbivore dung and laying eggs within that food supply, an instance of mass provisioning. Some species do not leave after this stage, but remain to safeguard their offspring.\n\nMost species of beetles do not display parental care behaviors after the eggs have been laid.\n\nSubsociality, where females guard their offspring, is well-documented in two families of Chrysomelidae, Cassidinae and Chrysomelinae.\n\nEusociality involves cooperative brood care (including brood care of offspring from other individuals), overlapping generations within a colony of adults, and a division of labour into reproductive and non-reproductive groups. Few organisms outside Hymenoptera exhibit this behavior; the only beetle to do so is the weevil \"Austroplatypus incompertus\". This Australian species lives in horizontal networks of tunnels, in the heartwood of \"Eucalyptus\" trees. It is one of more than 300 species of wood-boring Ambrosia beetles which distribute the spores of ambrosia fungi. The fungi grow in the beetles' tunnels, providing food for the beetles and their larvae; female offspring remain in the tunnels and maintain the fungal growth, probably never reproducing. Cooperative brood care is also found in the bess beetles (Passalidae) where the larvae feed on the semi-digested faeces of the adults.\n\nBeetles are able to exploit a wide diversity of food sources available in their many habitats. Some are omnivores, eating both plants and animals. Other beetles are highly specialized in their diet. Many species of leaf beetles, longhorn beetles, and weevils are very host-specific, feeding on only a single species of plant. Ground beetles and rove beetles (Staphylinidae), among others, are primarily carnivorous and catch and consume many other arthropods and small prey, such as earthworms and snails. While most predatory beetles are generalists, a few species have more specific prey requirements or preferences.\n\nDecaying organic matter is a primary diet for many species. This can range from dung, which is consumed by coprophagous species (such as certain scarab beetles in the Scarabaeidae), to dead animals, which are eaten by necrophagous species (such as the carrion beetles, Silphidae). Some beetles found in dung and carrion are in fact predatory. These include members of the Histeridae and Silphidae, preying on the larvae of coprophagous and necrophagous insects. Many beetles feed under bark, some feed on wood while others feed on fungi growing on wood or leaf-litter. Some beetles have special mycangia, structures for the transport of fungal spores.\n\nBeetles, both adults and larvae, are the prey of many animal predators including mammals from bats to rodents, birds, lizards, amphibians, fishes, dragonflies, robberflies, reduviid bugs, ants, other beetles, and spiders. Beetles use a variety of anti-predator adaptations to defend themselves. These include camouflage and mimicry against predators that hunt by sight, toxicity, and defensive behaviour.\n\nCamouflage is common and widespread among beetle families, especially those that feed on wood or vegetation, such as leaf beetles (Chrysomelidae, which are often green) and weevils. In some species, sculpturing or various coloured scales or hairs cause beetles such as the avocado weevil \"Heilipus apiatus\" to resemble bird dung or other inedible objects. Many beetles that live in sandy environments blend in with the coloration of that substrate.\n\nSome longhorn beetles (Cerambycidae) are effective Batesian mimics of wasps. Beetles may combine coloration with behavioural mimicry, acting like the wasps they already closely resemble. Many other beetles, including ladybirds, blister beetles, and lycid beetles secrete distasteful or toxic substances to make them unpalatable or poisonous, and are often aposematic, where bright or contrasting coloration warn off predators; many beetles and other insects mimic these chemically protected species.\n\nChemical defense is important in some species, usually being advertised by bright aposematic colours. Some Tenebrionidae use their posture for releasing noxious chemicals to warn off predators. Chemical defences may serve purposes other than just protection from vertebrates, such as protection from a wide range of microbes. Some species sequester chemicals from the plants they feed on, incorporating them into their own defenses.\n\nOther species have special glands to produce deterrent chemicals. The defensive glands of carabid ground beetles produce a variety of hydrocarbons, aldehydes, phenols, quinones, esters, and acids released from an opening at the end of the abdomen. African carabid beetles (for example, \"Anthia\" and \"Thermophilum\" – \"Thermophilum\" is sometimes included within \"Anthia\") employ the same chemicals as ants: formic acid. Bombardier beetles have well-developed pygidial glands that empty from the sides of the intersegment membranes between the seventh and eighth abdominal segments. The gland is made of two containing chambers, one for hydroquinones and hydrogen peroxide, the other holding hydrogen peroxide and catalase enzymes. These chemicals mix and result in an explosive ejection, reaching a temperature of around , with the breakdown of hydroquinone to hydrogen, oxygen, and quinone. The oxygen propels the noxious chemical spray as a jet that can be aimed accurately at predators.\n\nLarge ground-dwelling beetles such as Carabidae, the rhinoceros beetle and the longhorn beetles defend themselves using strong mandibles, or heavily sclerotised (armored) spines or horns to deter or fight off predators. Many species of weevil that feed out in the open on leaves of plants react to attack by employing a drop-off reflex. Some combine it with thanatosis, in which they close up their appendages and \"play dead\". The click beetles (Elateridae) can suddenly catapult themselves out of danger by releasing the energy stored by a click mechanism, which consists of a stout spine on the prosternum and a matching groove in the mesosternum. Some species startle an attacker by producing sounds through a process known as stridulation.\n\nA few species of beetles are ectoparasitic on mammals. One such species, \"Platypsyllus castoris\", parasitises beavers (\"Castor\" spp.). This beetle lives as a parasite both as a larva and as an adult, feeding on epidermal tissue and possibly on skin secretions and wound exudates. They are strikingly flattened dorsoventrally, no doubt as an adaptation for slipping between the beavers' hairs. They are wingless and eyeless, as are many other ectoparasites. Others are kleptoparasites of other invertebrates, such as the small hive beetle (\"Aethina tumida\") that infests honey bee nests, while many species are parasitic inquilines or commensal in the nests of ants. A few groups of beetles are primary parasitoids of other insects, feeding off of, and eventually killing their hosts.\n\nBeetle-pollinated flowers are usually large, greenish or off-white in color, and heavily scented. Scents may be spicy, fruity, or similar to decaying organic material. Beetles were most likely the first insects to pollinate flowers. Most beetle-pollinated flowers are flattened or dish-shaped, with pollen easily accessible, although they may include traps to keep the beetle longer. The plants' ovaries are usually well protected from the biting mouthparts of their pollinators. The beetle families that habitually pollinate flowers are the Buprestidae, Cantharidae, Cerambycidae, Cleridae, Dermestidae, Lycidae, Melyridae, Mordellidae, Nitidulidae and Scarabaeidae. Beetles may be particularly important in some parts of the world such as semiarid areas of southern Africa and southern California and the montane grasslands of KwaZulu-Natal in South Africa.\n\nMutualism is well known in a few beetles, such as the ambrosia beetle, which partners with fungi to digest the wood of dead trees. The beetles excavate tunnels in dead trees in which they cultivate fungal gardens, their sole source of nutrition. After landing on a suitable tree, an ambrosia beetle excavates a tunnel in which it releases spores of its fungal symbiont. The fungus penetrates the plant's xylem tissue, digests it, and concentrates the nutrients on and near the surface of the beetle gallery, so the weevils and the fungus both benefit. The beetles cannot eat the wood due to toxins, and uses its relationship with fungi to help overcome the defenses of its host tree in order to provide nutrition for their larvae. Chemically mediated by a bacterially produced polyunsaturated peroxide, this mutualistic relationship between the beetle and the fungus is coevolved.\n\nAbout 90% of beetle species enter a period of adult diapause, a quiet phase with reduced metabolism to tide unfavourable environmental conditions. Adult diapause is the most common form of diapause in Coleoptera. To endure the period without food (often lasting many months) adults prepare by accumulating reserves of lipids, glycogen, proteins and other substances needed for resistance to future hazardous changes of environmental conditions. This diapause is induced by signals heralding the arrival of the unfavourable season; usually the cue is photoperiodic. Short (decreasing) day length serves as a signal of approaching winter and induces winter diapause (hibernation). A study of hibernation in the Arctic beetle \"Pterostichus brevicorni\" showed that the body fat levels of adults were highest in autumn with the alimentary canal filled with food, but empty by the end of January. This loss of body fat was a gradual process, occurring in combination with dehydration.\n\nAll insects are poikilothermic, so the ability of a few beetles to live in extreme environments depends on their resilience to unusually high or low temperatures. The bark beetle \"Pityogenes chalcographus\" can survive whilst overwintering beneath tree bark; the Alaskan beetle \"Cucujus clavipes puniceus\" is able to withstand ; its larvae may survive . At these low temperatures, the formation of ice crystals in internal fluids is the biggest threat to survival to beetles, but this is prevented through the production of antifreeze proteins that stop water molecules from grouping together. The low temperatures experienced by \"Cucujus clavipes\" can be survived through their deliberate dehydration in conjunction with the antifreeze proteins. This concentrates the antifreezes several fold. The hemolymph of the mealworm beetle \"Tenebrio molitor\" contains several antifreeze proteins. The Alaskan beetle \"Upis ceramboides\" can survive −60 °C: its cryoprotectants are xylomannan, a molecule consisting of a sugar bound to a fatty acid, and the sugar-alcohol, threitol.\n\nConversely, desert dwelling beetles are adapted to tolerate high temperatures. For example, the Tenebrionid beetle \"Onymacris rugatipennis\" can withstand . Tiger beetles in hot, sandy areas are often whitish (for example, \"Habroscelimorpha dorsalis\"), to reflect more heat than a darker colour would. These beetles also exhibits behavioural adaptions to tolerate the heat: they are able to stand erect on their tarsi to hold their bodies away from the hot ground, seek shade, and turn to face the sun so that only the front parts of their heads are directly exposed.\n\nThe fogstand beetle of the Namib Desert, \"Stenocara gracilipes\", is able to collect water from fog, as its elytra have a textured surface combining hydrophilic (water-loving) bumps and waxy, hydrophobic troughs. The beetle faces the early morning breeze, holding up its abdomen; droplets condense on the elytra and run along ridges towards their mouthparts. Similar adaptations are found in several other Namib desert beetles such as \"Onymacris unguicularis\".\n\nSome terrestrial beetles that exploit shoreline and floodplain habitats have physiological adaptations for surviving floods. In the event of flooding, adult beetles may be mobile enough to move away from flooding, but larvae and pupa often cannot. Adults of \"Cicindela togata\" are unable to survive immersion in water, but larvae are able to survive a prolonged period, up to 6 days, of anoxia during floods. Anoxia tolerance in the larvae may have been sustained by switching to anaerobic metabolic pathways or by reducing metabolic rate. Anoxia tolerance in the adult Carabid beetle \"Pelophilia borealis\" was tested in laboratory conditions and it was found that they could survive a continuous period of up to 127 days in an atmosphere of 99.9% nitrogen at 0 °C.\n\nMany beetle species undertake annual mass movements which are termed as migrations. These include the pollen beetle \"Meligethes aeneus\" and many species of coccinellids. These mass movements may also be opportunistic, in search of food, rather than seasonal. A 2008 study of an unusually large outbreak of Mountain Pine Beetle (\"Dendroctonus ponderosae\") in British Columbia found that beetles were capable of flying 30–110 km per day in densities of up to 18, 600 beetles per hectare.\n\nSeveral species of dung beetle, especially the sacred scarab, \"Scarabaeus sacer\", were revered in Ancient Egypt. The hieroglyphic image of the beetle may have had existential, fictional, or ontologic significance. Images of the scarab in bone, ivory, stone, Egyptian faience, and precious metals are known from the Sixth Dynasty and up to the period of Roman rule. The scarab was of prime significance in the funerary cult of ancient Egypt. The scarab was linked to Khepri, the god of the rising sun, from the supposed resemblance of the rolling of the dung ball by the beetle to the rolling of the sun by the god. Some of ancient Egypt's neighbors adopted the scarab motif for seals of varying types. The best-known of these are the Judean LMLK seals, where eight of 21 designs contained scarab beetles, which were used exclusively to stamp impressions on storage jars during the reign of Hezekiah. Beetles are mentioned as a symbol of the sun, as in ancient Egypt, in Plutarch's 1st century \"Moralia\". The Greek Magical Papyri of the 2nd century BC to the 5th century AD describe scarabs as an ingredient in a spell.\n\nPliny the Elder discusses beetles in his \"Natural History\", describing the stag beetle: \"Some insects, for the preservation of their wings, are covered with (elytra) – the beetle, for instance, the wing of which is peculiarly fine and frail. To these insects a sting has been denied by Nature; but in one large kind we find horns of a remarkable length, two-pronged at the extremities, and forming pincers, which the animal closes when it is its intention to bite.\" The stag beetle is recorded in a Greek myth by Nicander and recalled by Antoninus Liberalis in which Cerambus is turned into a beetle: \"He can be seen on trunks and has hook-teeth, ever moving his jaws together. He is black, long and has hard wings like a great dung beetle\". The story concludes with the comment that the beetles were used as toys by young boys, and that the head was removed and worn as a pendant.\n\nAbout 75% of beetle species are phytophagous in both the larval and adult stages. Many feed on economically important plants and stored plant products, including trees, cereals, tobacco, and dried fruits. Some, such as the boll weevil, which feeds on cotton buds and flowers, can cause extremely serious damage to agriculture. The boll weevil crossed the Rio Grande near Brownsville, Texas, to enter the United States from Mexico around 1892, and had reached southeastern Alabama by 1915. By the mid-1920s, it had entered all cotton-growing regions in the US, traveling per year. It remains the most destructive cotton pest in North America. Mississippi State University has estimated, since the boll weevil entered the United States, it has cost cotton producers about $13 billion, and in recent times about $300 million per year.\n\nThe bark beetle, elm leaf beetle and the Asian longhorned beetle (\"Anoplophora glabripennis\") are among the species that attack elm trees. Bark beetles (Scolytidae) carry Dutch elm disease as they move from infected breeding sites to healthy trees. The disease has devastated elm trees across Europe and North America.\n\nSome species of beetle have evolved immunity to insecticides. For example, the Colorado potato beetle, \"Leptinotarsa decemlineata\", is a destructive pest of potato plants. Its hosts include other members of the Solanaceae, such as nightshade, tomato, eggplant and capsicum, as well as the potato. Different populations have between them developed resistance to all major classes of insecticide. The Colorado potato beetle was evaluated as a tool of entomological warfare during World War II, the idea being to use the beetle and its larvae to damage the crops of enemy nations. Germany tested its Colorado potato beetle weaponisation program south of Frankfurt, releasing 54,000 beetles.\n\nThe death watch beetle, \"Xestobium rufovillosum\" (Ptinidae), is a serious pest of older wooden buildings in Europe. It attacks hardwoods such as oak and chestnut, always where some fungal decay has taken or is taking place. The actual introduction of the pest into buildings is thought to take place at the time of construction.\n\nOther pests include the coconut hispine beetle, \"Brontispa longissima\", which feeds on young leaves, seedlings and mature coconut trees, causing serious economic damage in the Philippines. The mountain pine beetle is a destructive pest of mature or weakened lodgepole pine, sometimes affecting large areas of Canada.\n\nBeetles can be beneficial to human economics by controlling the populations of pests. The larvae and adults of some species of lady beetles (Coccinellidae) feed on aphids that are pests. Other lady beetles feed on scale insects, whitefly and mealybugs. If normal food sources are scarce, they may feed on small caterpillars, young plant bugs, or honeydew and nectar. Ground beetles (Carabidae) are common predators of many insect pests, including fly eggs, caterpillars, and wireworms. Ground beetles can help to control weeds by eating their seeds in the soil, reducing the need for herbicides to protect crops. The effectiveness of some species in reducing certain plant populations has resulted in the deliberate introduction of beetles in order to control weeds. For example, the genus \"Zygogramma\" is native to North America but has been used to control \"Parthenium hysterophorus\" in India and \"Ambrosia artemisiifolia\" in Russia.\n\nDung beetles (Scarabidae) have been successfully used to reduce the populations of pestilent flies, such as \"Musca vetustissima\" and \"Haematobia exigua\" which are serious pests of cattle in Australia. The beetles make the dung unavailable to breeding pests by quickly rolling and burying it in the soil, with the added effect of improving soil fertility, tilth, and nutrient cycling. The Australian Dung Beetle Project (1965–1985), introduced species of dung beetle to Australia from South Africa and Europe to reduce populations of \"Musca vetustissima\", following successful trials of this technique in Hawaii. The American Institute of Biological Sciences reports that dung beetles save the United States cattle industry an estimated US$380 million annually through burying above-ground livestock feces.\n\nThe Dermestidae are often used in taxidermy and in the preparation of scientific specimens, to clean soft tissue from bones. Larvae feed on and remove cartilage along with other soft tissue.\n\nBeetles are the most widely eaten insects, with about 344 species used as food, usually at the larval stage. The mealworm (the larva of the darkling beetle) and the rhinoceros beetle are among the species commonly eaten. A wide range of species is also used in folk medicine to treat those suffering from a variety of disorders and illnesses, though this is done without clinical studies supporting the efficacy of such treatments.\n\nDue to their habitat specificity, many species of beetles have been suggested as suitable as indicators, their presence, numbers, or absence providing a measure of habitat quality. Predatory beetles such as the tiger beetles (Cicindelidae) have found scientific use as an indicator taxon for measuring regional patterns of biodiversity. They are suitable for this as their taxonomy is stable; their life history is well described; they are large and simple to observe when visiting a site; they occur around the world in many habitats, with species specialised to particular habitats; and their occurrence by species accurately indicates other species, both vertebrate and invertebrate. According to the habitats, many other groups such as the rove beetles in human-modified habitats, dung beetles in savannas and saproxylic beetles in forests have been suggested as potential indicator species.\n\nMany beetles have beautiful and durable elytra that have been used as material in arts, with beetlewing the best example. Sometimes, they are incorporated into ritual objects for their religious significance. Whole beetles, either as-is or encased in clear plastic, are made into objects ranging from cheap souvenirs such as key chains to expensive fine-art jewellery. In parts of Mexico, beetles of the genus \"Zopherus\" are made into living brooches by attaching costume jewelry and golden chains, which is made possible by the incredibly hard elytra and sedentary habits of the genus.\n\nFighting beetles are used for entertainment and gambling. This sport exploits the territorial behavior and mating competition of certain species of large beetles. In the Chiang Mai district of northern Thailand, male \"Xylotrupes\" rhinoceros beetles are caught in the wild and trained for fighting. Females are held inside a log to stimulate the fighting males with their pheromones. These fights may be competitive and involve gambling both money and property. In South Korea the Dytiscidae species \"Cybister tripunctatus\" is used in a roulette-like game.\n\nBeetles are sometimes used as instruments: the Onabasulu of Papua New Guinea historically used the weevil \"Rhynchophorus ferrugineus\" as a musical instrument by letting the human mouth serve as a variable resonance chamber for the wing vibrations of the live adult beetle.\n\nSome species of beetle are kept as pets, for example diving beetles (Dytiscidae) may be kept in a domestic fresh water tank.\n\nIn Japan the practice of keeping horned rhinoceros beetles (Dynastinae) and stag beetles (Lucanidae) is particularly popular amongst young boys. Such is the popularity in Japan that vending machines dispensing live beetles were developed in 1999, each holding up to 100 stag beetles.\n\nBeetle collecting became extremely popular in the Victorian era. The naturalist Alfred Russel Wallace collected (by his own count) a total of 83,200 beetles during the eight years described in his 1869 book \"The Malay Archipelago\", including 2,000 species new to science.\n\nSeveral coleopteran adaptations have attracted interest in biomimetics with possible commercial applications. The bombardier beetle's powerful repellent spray has inspired the development of a fine mist spray technology, claimed to have a low carbon impact compared to aerosol sprays. Moisture harvesting behavior by the Namib desert beetle (\"Stenocara gracilipes\") has inspired a self-filling water bottle which utilises hydrophilic and hydrophobic materials to benefit people living in dry regions with no regular rainfall.\n\nLiving beetles have been used as cyborgs. A Defense Advanced Research Projects Agency funded project implanted electrodes into \"Mecynorhina torquata\" beetles, allowing them to be remotely controlled via a radio receiver held on its back, as proof-of-concept for surveillance work. Similar technology has been applied to enable a human operator to control the free-flight steering and walking gaits of \"Mecynorhina torquata\" as well as graded turning and backward walking of \"Zophobas morio\".\n\nSince beetles form such a large part of the world's biodiversity, their conservation is important, and equally, loss of habitat and biodiversity is essentially certain to impact on beetles. Many species of beetles have very specific habitats and long life cycles that make them vulnerable. Some species are highly threatened while others are already feared extinct. Island species tend to be more susceptible as in the case of \"Helictopleurus undatus\" of Madagascar which is thought to have gone extinct during the late 20th century. Conservationists have attempted to arouse a liking for beetles with flagship species like the stag beetle, \"Lucanus cervus\", and tiger beetles (Cicindelidae). In Japan the Genji firefly, \"Luciola cruciata\", is extremely popular, and in South Africa the Addo elephant dung beetle offers promise for broadening ecotourism beyond the big five tourist mammal species. Popular dislike of pest beetles, too, can be turned into public interest in insects, as can unusual ecological adaptations of species like the fairy shrimp hunting beetle, \"Cicinis bruchi\".\n\n\nBULLET::::- Coleoptera from the Tree of Life Web Project\nBULLET::::- \"Käfer der Welt\"\nBULLET::::- Coleoptera Atlas\nBULLET::::- Beetles – Coleoptera\n"}
{"id": "7045", "url": "https://en.wikipedia.org/wiki?curid=7045", "title": "Concorde", "text": "Concorde\n\nThe Aérospatiale/BAC Concorde () is a British-French turbojet-powered supersonic passenger airliner that was operated until 2003. It had a maximum speed over twice the speed of sound at Mach 2.04 ( at cruise altitude), with seating for 92 to 128 passengers. First flown in 1969, Concorde entered service in 1976 and continued flying for the next 27 years. It is one of only two supersonic transports to have been operated commercially; the other is the Soviet-built Tupolev Tu-144, which operated in the late 1970s.\n\nConcorde was jointly developed and manufactured by Sud Aviation (later Aérospatiale) and the British Aircraft Corporation (BAC) under an Anglo-French treaty. Twenty aircraft were built, including six prototypes and development aircraft. Air France (AF) and British Airways (BA) were the only airlines to purchase and fly Concorde. The aircraft was used mainly by wealthy passengers who could afford to pay a high price in exchange for the aircraft's speed and luxury service. For example, in 1997, the round-trip ticket price from New York to London was $7,995 ($ in dollars), more than 30 times the cost of the cheapest option to fly this route.\n\nThe original program cost estimate of £70 million met huge overruns and delays, with the program eventually costing £1.3 billion. It was this extreme cost that became the main factor in the production run being much smaller than anticipated. Later, another factor which affected the viability of all supersonic transport programmes was that supersonic flight could only be used on ocean-crossing routes, to prevent sonic boom disturbance over populated areas. With only seven airframes each being operated by the British and French, the per-unit cost was impossible to recoup, so the French and British governments absorbed the development costs. British Airways and Air France were able to operate Concorde at a profit, in spite of very high maintenance costs, because the aircraft was able to sustain a high ticket price.\n\nAmong other destinations, Concorde flew regular transatlantic flights from London's Heathrow Airport and Paris's Charles de Gaulle Airport to John F. Kennedy International Airport in New York, Washington Dulles International Airport in Virginia, and Grantley Adams International Airport in Barbados; it flew these routes in less than half the time of other airliners.\n\nConcorde won the 2006 Great British Design Quest organised by the BBC and the Design Museum, beating other well-known designs such as the BMC Mini, the miniskirt, the Jaguar E-Type, the London Tube map and the Supermarine Spitfire. The type was retired in 2003, three years after the crash of Air France Flight 4590, in which all passengers and crew were killed. The general downturn in the commercial aviation industry after the September 11 attacks in 2001 and the end of maintenance support for Concorde by Airbus (the successor company of both Aérospatiale and BAC) also contributed.\n\nThe origins of the Concorde project date to the early 1950s, when Arnold Hall, director of the Royal Aircraft Establishment (RAE) asked Morien Morgan to form a committee to study the supersonic transport (SST) concept. The group met for the first time in February 1954 and delivered their first report in April 1955.\n\nAt the time it was known that the drag at supersonic speeds was strongly related to the span of the wing. This led to the use of very short-span, very thin trapezoidal wings such as those seen on the control surfaces of many missiles, or in aircraft like the Lockheed F-104 Starfighter or the Avro 730 that the team studied. The team outlined a baseline configuration that looked like an enlarged Avro 730.\n\nThis same short span produced very little lift at low speed, which resulted in extremely long take-off runs and frighteningly high landing speeds. In an SST design, this would have required enormous engine power to lift off from existing runways, and to provide the fuel needed, \"some horribly large aeroplanes\" resulted. Based on this, the group considered the concept of an SST infeasible, and instead suggested continued low-level studies into supersonic aerodynamics.\n\nSoon after, Johanna Weber and Dietrich Küchemann at the RAE published a series of reports on a new wing planform, known in the UK as the \"slender delta\" concept. The team, including Eric Maskell whose report \"Flow Separation in Three Dimensions\" contributed to an understanding of the physical nature of separated flow, worked with the fact that delta wings can produce strong vortices on their upper surfaces at high angles of attack. The vortex will lower the air pressure and cause lift to be greatly increased. This effect had been noticed earlier, notably by Chuck Yeager in the Convair XF-92, but its qualities had not been fully appreciated. Weber suggested that this was no mere curiosity, and the effect could be deliberately used to improve low speed performance.\n\nKüchemann's and Weber's papers changed the entire nature of supersonic design almost overnight. Although the delta had already been used on aircraft prior to this point, these designs used planforms that were not much different from a swept wing of the same span. Weber noted that the lift from the vortex was increased by the length of the wing it had to operate over, which suggested that the effect would be maximised by extending the wing along the fuselage as far as possible. Such a layout would still have good supersonic performance inherent to the short span, while also offering reasonable take-off and landing speeds using vortex generation. The only downside to such a design is that the aircraft would have to take off and land very \"nose high\" to generate the required vortex lift, which led to questions about the low speed handling qualities of such a design. It would also need to have long landing gear to produce the required angle of attack while still on the runway.\n\nKüchemann presented the idea at a meeting where Morgan was also present. Test pilot Eric Brown recalls Morgan's reaction to the presentation, saying that he immediately seized on it as the solution to the SST problem. Brown considers this moment as being the true birth of the Concorde project.\n\nOn 1 October 1956 the Ministry of Supply asked Morgan to form a new study group, the \"Supersonic Transport Aircraft Committee\" (\"STAC\") (sometimes referred to as the \"Supersonic Transport Advisory Committee\"), with the explicit goal of developing a practical SST design and finding industry partners to build it. At the very first meeting, on 5 November 1956, the decision was made to fund the development of a test bed aircraft to examine the low-speed performance of the slender delta, a contract that eventually produced the Handley Page HP.115. This aircraft would ultimately demonstrate safe control at speeds as low as , about that of the F-104 Starfighter.\n\nSTAC stated that an SST would have economic performance similar to existing subsonic types. Although they would burn more fuel in cruise, they would be able to fly more sorties in a given period of time, so fewer aircraft would be needed to service a particular route. This would remain economically advantageous as long as fuel represented a small percentage of operational costs, as it did at the time.\n\nSTAC suggested that two designs naturally fell out of their work, a transatlantic model flying at about Mach 2, and a shorter-range version flying at perhaps Mach 1.2. Morgan suggested that a 150-passenger transatlantic SST would cost about £75 to £90 million to develop, and be in service in 1970. The smaller 100 passenger short-range version would cost perhaps £50 to £80 million, and be ready for service in 1968. To meet this schedule, development would need to begin in 1960, with production contracts let in 1962. Morgan strongly suggested that the US was already involved in a similar project, and that if the UK failed to respond it would be locked out of an airliner market that he believed would be dominated by SST aircraft.\n\nIn 1959, a study contract was awarded to Hawker Siddeley and Bristol for preliminary designs based on the slender delta concept, which developed as the HSA.1000 and Bristol 198. Armstrong Whitworth also responded with an internal design, the M-Wing, for the lower-speed shorter-range category. Even at this early time, both the STAC group and the government were looking for partners to develop the designs. In September 1959, Hawker approached Lockheed, and after the creation of British Aircraft Corporation in 1960, the former Bristol team immediately started talks with Boeing, General Dynamics, Douglas Aircraft and Sud Aviation.\n\nKüchemann and others at the RAE continued their work on the slender delta throughout this period, considering three basic shapes; the classic straight-edge delta, the \"gothic delta\" that was rounded outwards to appear like a gothic arch, and the \"ogival wing\" that was compound-rounded into the shape of an ogee. Each of these planforms had its own advantages and disadvantages in terms of aerodynamics. As they worked with these shapes, a practical concern grew to become so important that it forced selection of one of these designs.\n\nGenerally one wants to have the wing's centre of pressure (CP, or \"lift point\") close to the aircraft's centre of gravity (CG, or \"balance point\") to reduce the amount of control force required to pitch the aircraft. As the aircraft layout changes during the design phase, it is common for the CG to move fore or aft. With a normal wing design this can be addressed by moving the wing slightly fore or aft to account for this. With a delta wing running most of the length of the fuselage, this was no longer easy; moving the wing would leave it in front of the nose or behind the tail. Studying the various layouts in terms of CG changes, both during design and changes due to fuel use during flight, the ogee planform immediately came to the fore.\n\nWhile the wing planform was evolving, so was the basic SST concept. Bristol's original Type 198 was a small design with an almost pure slender delta wing, but evolved into the larger Type 223.\n\nBy this time similar political and economic concerns in France had led to their own SST plans. In the late 1950s the government requested designs from both the government-owned Sud Aviation and Nord Aviation, as well as Dassault. All three returned designs based on Küchemann and Weber's slender delta; Nord suggested a ramjet powered design flying at Mach 3, the other two were jet powered Mach 2 designs that were similar to each other. Of the three, the Sud Aviation Super-Caravelle won the design contest with a medium-range design deliberately sized to avoid competition with transatlantic US designs they assumed were already on the drawing board.\n\nAs soon as the design was complete, in April 1960, Pierre Satre, the company's technical director, was sent to Bristol to discuss a partnership. Bristol was surprised to find that the Sud team had designed a very similar aircraft after considering the SST problem and coming to the very same conclusions as the Bristol and STAC teams in terms of economics. It was later revealed that the original STAC report, marked \"For UK Eyes Only\", had secretly been passed to the French to win political favour. Sud made minor changes to the paper, and presented it as their own work.\n\nUnsurprisingly, the two teams found much to agree on. The French had no modern large jet engines, and had already concluded they would buy a British design anyway (as they had on the earlier subsonic Caravelle). As neither company had experience in the use of high-heat metals for airframes, a maximum speed of around Mach 2 was selected so aluminium could be used – above this speed the friction with the air warms the metal so much that aluminium begins to soften. This lower speed would also speed development and allow their design to fly before the Americans. Finally, everyone involved agreed that Küchemann's ogee shaped wing was the right one.\n\nThe only disagreements were over the size and range. The UK team was still focused on a 150-passenger design serving transatlantic routes, while the French were deliberately avoiding these. However, this proved not to be the barrier it might seem; common components could be used in both designs, with the shorter range version using a clipped fuselage and four engines, the longer one with a stretched fuselage and six engines, leaving only the wing to be extensively re-designed. The teams continued to meet through 1961, and by this time it was clear that the two aircraft would be considerably more similar in spite of different range and seating arrangements. A single design emerged that differed mainly in fuel load. More powerful Bristol Siddeley Olympus engines, being developed for the TSR-2, allowed either design to be powered by only four engines.\n\nWhile the development teams met, French Minister of Public Works and Transport Robert Buron was meeting with the UK Minister of Aviation Peter Thorneycroft, and Thorneycroft soon revealed to the cabinet that the French were much more serious about a partnership than any of the US companies. The various US companies had proved uninterested in such a venture, likely due to the belief that the government would be funding development and would frown on any partnership with a European company, and the risk of \"giving away\" US technological leadership to a European partner.\n\nWhen the STAC plans were presented to the UK cabinet, a very negative reaction resulted. The economic considerations were considered highly questionable, especially as these were based on development costs, now estimated to be £150 million, which were repeatedly overrun in the industry. The Treasury Ministry in particular presented a very negative view, suggesting that there was no way the project would have any positive financial returns for the government, especially in light that \"the industry's past record of over-optimistic estimating (including the recent history of the TSR.2) suggests that it would be prudent to consider the £150 million [cost] to turn out much too low.\"\n\nThis concern led to an independent review of the project by the Committee on Civil Scientific Research and Development, which met on topic between July and September 1962. The Committee ultimately rejected the economic arguments, including considerations of supporting the industry made by Thorneycroft. Their report in October stated that it was unlikely there would be any direct positive economic outcome, but that the project should still be considered for the simple reason that everyone else was going supersonic, and they were concerned they would be locked out of future markets. Conversely, it appeared the project would not be likely to significantly impact other, more important, research efforts.\n\nAfter considerable argument, the decision to proceed ultimately fell to an unlikely political expediency. At the time, the UK was pressing for admission to the European Common Market, which was being controlled by Charles de Gaulle who felt the UK's Special Relationship with the US made them unacceptable in a pan-European group. Cabinet felt that signing a deal with Sud would pave the way for Common Market entry, and this became the main deciding reason for moving ahead with the deal. It was this belief that had led the original STAC documents being leaked to the French. However, De Gaulle spoke of the European origin of the design, and continued to block the UK's entry into the Common Market.\n\nThe development project was negotiated as an international treaty between the two countries rather than a commercial agreement between companies and included a clause, originally asked for by the UK, imposing heavy penalties for cancellation. A draft treaty was signed on 29 November 1962.\n\nReflecting the treaty between the British and French governments that led to Concorde's construction, the name \"Concorde\" is from the French word \"concorde\" (), which has an English equivalent, \"concord\". Both words mean \"agreement\", \"harmony\" or \"union\". The name was officially changed to \"Concord\" by Harold Macmillan in response to a perceived slight by Charles de Gaulle. At the French roll-out in Toulouse in late 1967, the British Government Minister for Technology, Tony Benn, announced that he would change the spelling back to \"Concorde\". This created a nationalist uproar that died down when Benn stated that the suffixed \"e\" represented \"Excellence, England, Europe and Entente (Cordiale)\". In his memoirs, he recounts a tale of a letter from an irate Scotsman claiming: \"[Y]ou talk about 'E' for England, but part of it is made in Scotland.\" Given Scotland's contribution of providing the nose cone for the aircraft, Benn replied, \"[I]t was also 'E' for 'Écosse' (the French name for Scotland) – and I might have added 'e' for extravagance and 'e' for escalation as well!\"\n\nConcorde also acquired an unusual nomenclature for an aircraft. In common usage in the United Kingdom, the type is known as \"Concorde\" without an article, rather than \" Concorde\" or \" Concorde\".\n\nDescribed by \"Flight International\" as an \"aviation icon\" and \"one of aerospace's most ambitious but commercially flawed projects\", Concorde failed to meet its original sales targets, despite initial interest from several airlines.\n\nAt first, the new consortium intended to produce one long-range and one short-range version. However, prospective customers showed no interest in the short-range version and it was dropped.\n\nAn advertisement covering two full pages, promoting Concorde, ran in the 29 May 1967 issue of \"Aviation Week & Space Technology\". The advertisement predicted a market for 350 aircraft by 1980 and boasted of Concorde's head start over the United States' SST project.\n\nConcorde had considerable difficulties that led to its dismal sales performance. Costs had spiralled during development to more than six times the original projections, arriving at a unit cost of £23 million in 1977 (equivalent to £ million in ). Its sonic boom made travelling supersonically over land impossible without causing complaints from citizens. World events had also dampened Concorde sales prospects, the 1973–74 stock market crash and the 1973 oil crisis had made many airlines cautious about aircraft with high fuel consumption rates; and new wide-body aircraft, such as the Boeing 747, had recently made subsonic aircraft significantly more efficient and presented a low-risk option for airlines. While carrying a full load, Concorde achieved 15.8 passenger miles per gallon of fuel, while the Boeing 707 reached 33.3 pm/g, the Boeing 747 46.4 pm/g, and the McDonnell Douglas DC-10 53.6 pm/g. An emerging trend in the industry in favour of cheaper airline tickets had also caused airlines such as Qantas to question Concorde's market suitability.\n\nThe consortium received orders, i.e., non-binding options, for over 100 of the long-range version from the major airlines of the day: Pan Am, BOAC, and Air France were the launch customers, with six Concordes each. Other airlines in the order book included Panair do Brasil, Continental Airlines, Japan Airlines, Lufthansa, American Airlines, United Airlines, Air India, Air Canada, Braniff, Singapore Airlines, Iran Air, Olympic Airways, Qantas, CAAC Airlines, Middle East Airlines, and TWA. At the time of the first flight the options list contained 74 options from 16 airlines:\n! Airline\n! Number\n! Reserved\n! Cancelled\n! Remarks\n\nThe design work was supported by a preceding research programme studying the flight characteristics of low ratio delta wings. A supersonic Fairey Delta 2 was modified to carry the ogee planform, and, renamed as the BAC 221, used for flight tests of the high speed flight envelope, the Handley Page HP.115 also provided valuable information on low speed performance.\n\nConstruction of two prototypes began in February 1965: 001, built by Aérospatiale at Toulouse, and 002, by BAC at Filton, Bristol. Concorde 001 made its first test flight from Toulouse on 2 March 1969, piloted by André Turcat, and first went supersonic on 1 October. The first UK-built Concorde flew from Filton to RAF Fairford on 9 April 1969, piloted by Brian Trubshaw. Both prototypes were presented to the public for the first time on 7–8 June 1969 at the Paris Air Show. As the flight programme progressed, 001 embarked on a sales and demonstration tour on 4 September 1971, which was also the first transatlantic crossing of Concorde. Concorde 002 followed suit on 2 June 1972 with a tour of the Middle and Far East. Concorde 002 made the first visit to the United States in 1973, landing at the new Dallas/Fort Worth Regional Airport to mark that airport's opening.\n\nDuring testing, Concorde F-WTSB attained the highest altitude recorded in sustained level flight of a passenger aircraft of , in June 1973. Concorde G-AXDN attained the highest recorded speed of on 26 March 1974, at an altitude of .\nWhile Concorde had initially held a great deal of customer interest, the project was hit by a large number of order cancellations. The Paris Le Bourget air show crash of the competing Soviet Tupolev Tu-144 had shocked potential buyers, and public concern over the environmental issues presented by a supersonic aircraft—the sonic boom, take-off noise and pollution—had produced a shift in public opinion of SSTs. By 1976 four nations remained as prospective buyers: Britain, France, China, and Iran. Only Air France and British Airways (the successor to BOAC) took up their orders, with the two governments taking a cut of any profits made.\n\nThe United States government cut federal funding for the Boeing 2707, its rival supersonic transport programme, in 1971; Boeing did not complete its two 2707 prototypes. The US, India, and Malaysia all ruled out Concorde supersonic flights over the noise concern, although some of these restrictions were later relaxed. Professor Douglas Ross characterised restrictions placed upon Concorde operations by President Jimmy Carter's administration as having been an act of protectionism of American aircraft manufacturers.\n\nConcorde is an ogival delta winged aircraft with four Olympus engines based on those employed in the RAF's Avro Vulcan strategic bomber. It is one of the few commercial aircraft to employ a tailless design (the Tupolev Tu-144 being another). Concorde was the first airliner to have a (in this case, analogue) fly-by-wire flight-control system; the avionics system Concorde used was unique because it was the first commercial aircraft to employ hybrid circuits. The principal designer for the project was Pierre Satre, with Sir Archibald Russell as his deputy.\n\nConcorde pioneered the following technologies:\n\nFor high speed and optimisation of flight:\nBULLET::::- Double delta (ogee/ogival) shaped wings\nBULLET::::- Variable engine air intake ramp system controlled by digital computers\nBULLET::::- Supercruise capability\nBULLET::::- Thrust-by-wire engines, predecessor of today's FADEC-controlled engines\nBULLET::::- Droop-nose section for better landing visibility\n\nFor weight-saving and enhanced performance:\nBULLET::::- Mach 2.02 (~) cruising speed for optimum fuel consumption (supersonic drag minimum and turbojet engines are more efficient at higher speed) Fuel consumption at and at altitude of was .\nBULLET::::- Mainly aluminium construction using a high temperature alloy similar to that developed for aero-engine pistons. This material gave low weight and allowed conventional manufacture (higher speeds would have ruled out aluminium)\nBULLET::::- Full-regime autopilot and autothrottle allowing \"hands off\" control of the aircraft from climb out to landing\nBULLET::::- Fully electrically controlled analogue fly-by-wire flight controls systems\nBULLET::::- High-pressure hydraulic system using for lighter hydraulic components, tripled independent systems (\"Blue\", \"Green\", and \"Yellow\") for redundancy, with an emergency ram air turbine (RAT) stored in the port-inner elevon jack fairing supplying \"Green\" and \"Yellow\" as backup.\nBULLET::::- Complex Air data computer (ADC) for the automated monitoring and transmission of aerodynamic measurements (total pressure, static pressure, angle of attack, side-slip).\nBULLET::::- Fully electrically controlled analogue brake-by-wire system\nBULLET::::- Pitch trim by shifting fuel fore-and-aft for centre-of-gravity (CoG) control at the approach to Mach 1 and above with no drag penalty. Pitch trimming by fuel transfer had been used since 1958 on the B-58 supersonic bomber.\nBULLET::::- Parts made using \"sculpture milling\", reducing the part count while saving weight and adding strength.\nBULLET::::- No auxiliary power unit, as Concorde would only visit large airports where ground air start carts are available.\n\nA symposium titled \"Supersonic-Transport Implications\" was hosted by the Royal Aeronautical Society on 8 December 1960. Various views were put forward on the likely type of powerplant for a supersonic transport, such as podded or buried installation and turbojet or ducted-fan engines. Boundary layer management in the podded installation was put forward as simpler with only an inlet cone but Dr. Seddon of the RAE saw \"a future in a more sophisticated integration of shapes\" in a buried installation. Another concern highlighted the case with two or more engines situated behind a single intake. An intake failure could lead to a double or triple engine failure. The advantage of the ducted fan over the turbojet was reduced airport noise but with considerable economic penalties with its larger cross-section producing excessive drag. At that time it was considered that the noise from a turbojet optimised for supersonic cruise could be reduced to an acceptable level using noise suppressors as used on subsonic jets.\n\nThe powerplant configuration selected for Concorde, and its development to a certificated design, can be seen in light of the above symposium topics (which highlighted airfield noise, boundary layer management and interactions between adjacent engines) and the requirement that the powerplant, at Mach 2, tolerate combinations of pushovers, sideslips, pull-ups and throttle slamming without surging. Extensive development testing with design changes and changes to intake and engine control laws would address most of the issues except airfield noise and the interaction between adjacent powerplants at speeds above Mach 1.6 which meant Concorde \"had to be certified aerodynamically as a twin-engined aircraft above Mach 1.6\".\n\nRolls-Royce had a design proposal, the RB.169, for the aircraft at the time of Concorde's initial design but \"to develop a brand-new engine for Concorde would have been prohibitively expensive\" so an existing engine, already flying in the TSR-2 prototype, was chosen. It was the Olympus 320 turbojet, a development of the Bristol engine first used for the Avro Vulcan bomber.\n\nGreat confidence was placed in being able to reduce the noise of a turbojet and massive strides by SNECMA in silencer design were reported during the programme. However, by 1974 the spade silencers which projected into the exhaust were reported to be ineffective. The Olympus Mk.622 with reduced jet velocity was proposed to reduce the noise but it was not developed.\n\nSituated behind the leading edge of the wing the engine intake had wing boundary layer ahead of it. Two-thirds was diverted and the remaining third which entered the intake did not adversely affect the intake efficiency except during pushovers when the boundary layer thickened ahead of the intake and caused surging. Extensive wind tunnel testing helped define leading edge modifications ahead of the intakes which solved the problem.\n\nEach engine had its own intake and the engine nacelles were paired with a splitter plate between them to minimise adverse behaviour of one powerplant influencing the other. Only above was an engine surge likely to affect the adjacent engine.\n\nConcorde needed to fly long distances to be economically viable; this required high efficiency from the powerplant. Turbofan engines were rejected due to their larger cross-section producing excessive drag. Olympus turbojet technology was available to be developed to meet the design requirements of the aircraft, although turbofans would be studied for any future SST.\n\nThe aircraft used reheat (afterburners) at take-off and to pass through the upper transonic regime and to supersonic speeds, between Mach 0.95 and 1.7. The afterburners were switched off at all other times. Due to jet engines being highly inefficient at low speeds, Concorde burned of fuel (almost 2% of the maximum fuel load) taxiing to the runway. Fuel used is Jet A-1. Due to the high thrust produced even with the engines at idle, only the two outer engines were run after landing for easier taxiing and less brake pad wear – at low weights after landing, the aircraft would not remain stationary with all four engines idling requiring the brakes to be continuously applied to prevent the aircraft from rolling.\n\nThe intake design for Concorde's engines was especially critical. The intakes had to provide low distortion levels (to prevent engine surge) and high efficiency for all likely ambient temperatures to be met in cruise. They had to provide adequate subsonic performance for diversion cruise and low engine-face distortion at take-off. They also had to provide an alternative path for excess intake air during engine throttling or shutdowns. The variable intake features required to meet all these requirements consisted of front and rear ramps, a dump door, an auxiliary inlet and a ramp bleed to the exhaust nozzle.\n\nAs well as supplying air to the engine, the intake also supplied air through the ramp bleed to the propelling nozzle. The nozzle ejector (or aerodynamic) design, with variable exit area and secondary flow from the intake, contributed to good expansion efficiency from take-off to cruise.\n\nEngine failure causes problems on conventional subsonic aircraft; not only does the aircraft lose thrust on that side but the engine creates drag, causing the aircraft to yaw and bank in the direction of the failed engine. If this had happened to Concorde at supersonic speeds, it theoretically could have caused a catastrophic failure of the airframe. Although computer simulations predicted considerable problems, in practice Concorde could shut down both engines on the same side of the aircraft at Mach 2 without the predicted difficulties. During an engine failure the required air intake is virtually zero. So, on Concorde, engine failure was countered by the opening of the auxiliary spill door and the full extension of the ramps, which deflected the air downwards past the engine, gaining lift and minimising drag. Concorde pilots were routinely trained to handle double engine failure.\n\nConcorde's Air Intake Control Units (AICUs) made use of a digital processor to provide the necessary accuracy for intake control. It was the world's first use of a digital processor to be given full authority control of an essential system in a passenger aircraft. It was developed by the Electronics and Space Systems (ESS) division of the British Aircraft Corporation after it became clear that the analogue AICUs fitted to the prototype aircraft and developed by Ultra Electronics were found to be insufficiently accurate for the tasks in hand.\n\nConcorde's thrust-by-wire engine control system was developed by Ultra Electronics.\n\nAir compression on the outer surfaces caused the cabin to heat up during flight. Every surface, such as windows and panels, was warm to the touch by the end of the flight. Besides engines, the hottest part of the structure of any supersonic aircraft is the nose, due to aerodynamic heating. The engineers used Hiduminium R.R. 58, an aluminium alloy, throughout the aircraft because of its familiarity, cost and ease of construction. The highest temperature that aluminium could sustain over the life of the aircraft was , which limited the top speed to Mach 2.02. Concorde went through two cycles of heating and cooling during a flight, first cooling down as it gained altitude, then heating up after going supersonic. The reverse happened when descending and slowing down. This had to be factored into the metallurgical and fatigue modelling. A test rig was built that repeatedly heated up a full-size section of the wing, and then cooled it, and periodically samples of metal were taken for testing. The Concorde airframe was designed for a life of 45,000 flying hours.\nOwing to air compression in front of the plane as it travelled at supersonic speed, the fuselage heated up and expanded by as much as . The most obvious manifestation of this was a gap that opened up on the flight deck between the flight engineer's console and the bulkhead. On some aircraft that conducted a retiring supersonic flight, the flight engineers placed their caps in this expanded gap, wedging the cap when it shrank again. To keep the cabin cool, Concorde used the fuel as a heat sink for the heat from the air conditioning. The same method also cooled the hydraulics. During supersonic flight the surfaces forward from the cockpit became heated, and a visor was used to deflect much of this heat from directly reaching the cockpit.\n\nConcorde had livery restrictions; the majority of the surface had to be covered with a highly reflective white paint to avoid overheating the aluminium structure due to heating effects from supersonic flight at Mach 2. The white finish reduced the skin temperature by . In 1996, Air France briefly painted F-BTSD in a predominantly blue livery, with the exception of the wings, in a promotional deal with Pepsi. In this paint scheme, Air France was advised to remain at for no more than 20 minutes at a time, but there was no restriction at speeds under Mach 1.7. F-BTSD was used because it was not scheduled for any long flights that required extended Mach 2 operations.\n\nDue to its high speeds, large forces were applied to the aircraft during banks and turns, and caused twisting and distortion of the aircraft's structure. In addition there were concerns over maintaining precise control at supersonic speeds. Both of these issues were resolved by active ratio changes between the inboard and outboard elevons, varying at differing speeds including supersonic. Only the innermost elevons, which are attached to the stiffest area of the wings, were active at high speed. Additionally, the narrow fuselage meant that the aircraft flexed. This was visible from the rear passengers' viewpoints.\n\nWhen any aircraft passes the critical mach of that particular airframe, the centre of pressure shifts rearwards. This causes a pitch down moment on the aircraft if the centre of gravity remains where it was. The engineers designed the wings in a specific manner to reduce this shift, but there was still a shift of about . This could have been countered by the use of trim controls, but at such high speeds this would have dramatically increased drag. Instead, the distribution of fuel along the aircraft was shifted during acceleration and deceleration to move the centre of gravity, effectively acting as an auxiliary trim control.\n\nTo fly non-stop across the Atlantic Ocean, Concorde required the greatest supersonic range of any aircraft. This was achieved by a combination of engines which were highly efficient at supersonic speeds, a slender fuselage with high fineness ratio, and a complex wing shape for a high lift-to-drag ratio. This also required carrying only a modest payload and a high fuel capacity, and the aircraft was trimmed with precision to avoid unnecessary drag.\n\nNevertheless, soon after Concorde began flying, a Concorde \"B\" model was designed with slightly larger fuel capacity and slightly larger wings with leading edge slats to improve aerodynamic performance at all speeds, with the objective of expanding the range to reach markets in new regions. It featured more powerful engines with sound deadening and without the fuel-hungry and noisy afterburner. It was speculated that it was reasonably possible to create an engine with up to 25% gain in efficiency over the Rolls-Royce/Snecma Olympus 593. This would have given additional range and a greater payload, making new commercial routes possible. This was cancelled due in part to poor sales of Concorde, but also to the rising cost of aviation fuel in the 1970s.\n\nConcorde's high cruising altitude meant passengers received almost twice the flux of extraterrestrial ionising radiation as those travelling on a conventional long-haul flight. Upon Concorde's introduction, it was speculated that this exposure during supersonic travels would increase the likelihood of skin cancer. Due to the proportionally reduced flight time, the overall equivalent dose would normally be less than a conventional flight over the same distance. Unusual solar activity might lead to an increase in incident radiation. To prevent incidents of excessive radiation exposure, the flight deck had a radiometer and an instrument to measure the rate of decrease of radiation. If the radiation level became too high, Concorde would descend below .\n\nAirliner cabins were usually maintained at a pressure equivalent to elevation. Concorde's pressurisation was set to an altitude at the lower end of this range, . Concorde's maximum cruising altitude was ; subsonic airliners typically cruise below .\n\nA sudden reduction in cabin pressure is hazardous to all passengers and crew. Above , a sudden cabin depressurisation would leave a \"time of useful consciousness\" up to 10–15 seconds for a conditioned athlete. At Concorde's altitude, the air density is very low; a breach of cabin integrity would result in a loss of pressure severe enough that the plastic emergency oxygen masks installed on other passenger jets would not be effective and passengers would soon suffer from hypoxia despite quickly donning them. Concorde was equipped with smaller windows to reduce the rate of loss in the event of a breach, a reserve air supply system to augment cabin air pressure, and a rapid descent procedure to bring the aircraft to a safe altitude. The FAA enforces minimum emergency descent rates for aircraft and noting Concorde's higher operating altitude, concluded that the best response to pressure loss would be a rapid descent. Continuous positive airway pressure would have delivered pressurised oxygen directly to the pilots through masks.\n\nWhile subsonic commercial jets took eight hours to fly from New York to Paris, the average supersonic flight time on the transatlantic routes was just under 3.5 hours. Concorde had a maximum cruise altitude of and an average cruise speed of , more than twice the speed of conventional aircraft.\n\nWith no other civil traffic operating at its cruising altitude of about , Concorde had exclusive use of dedicated oceanic airways, or \"tracks\", separate from the North Atlantic Tracks, the routes used by other aircraft to cross the Atlantic. Due to the significantly less variable nature of high altitude winds compared to those at standard cruising altitudes, these dedicated SST tracks had fixed co-ordinates, unlike the standard routes at lower altitudes, whose co-ordinates are replotted twice daily based on forecast weather patterns (jetstreams). Concorde would also be cleared in a block, allowing for a slow climb from during the oceanic crossing as the fuel load gradually decreased. In regular service, Concorde employed an efficient \"cruise-climb\" flight profile following take-off.\n\nThe delta-shaped wings required Concorde to adopt a higher angle of attack at low speeds than conventional aircraft, but it allowed the formation of large low pressure vortices over the entire upper wing surface, maintaining lift. The normal landing speed was . Because of this high angle, during a landing approach Concorde was on the \"back side\" of the drag force curve, where raising the nose would increase the rate of descent; the aircraft was thus largely flown on the throttle and was fitted with an autothrottle to reduce the pilot's workload.\n\nBecause of the way Concorde's delta-wing generated lift, the undercarriage had to be unusually strong and tall to allow for the angle of attack at low speed. At rotation, Concorde would rise to a high angle of attack, about 18 degrees. Prior to rotation the wing generated almost no lift, unlike typical aircraft wings. Combined with the high airspeed at rotation ( indicated airspeed), this increased the stresses on the main undercarriage in a way that was initially unexpected during the development and required a major redesign. Due to the high angle needed at rotation, a small set of wheels was added aft to prevent tailstrikes. The main undercarriage units swing towards each other to be stowed but due to their great height also need to contract in length telescopically before swinging to clear each other when stowed. The four main wheel tyres on each bogie unit are inflated to . The twin-wheel nose undercarriage retracts forwards and its tyres are inflated to a pressure of , and the wheel assembly carries a spray deflector to prevent standing water being thrown up into the engine intakes. The tyres are rated to a maximum speed on the runway of . The starboard nose wheel carries a single disc brake to halt wheel rotation during retraction of the undercarriage. The port nose wheel carries speed generators for the anti-skid braking system which prevents brake activation until nose and main wheels rotate at the same rate.\n\nAdditionally, due to the high average take-off speed of , Concorde needed upgraded brakes. Like most airliners, Concorde has anti-skid braking – a system which prevents the tyres from losing traction when the brakes are applied for greater control during roll-out. The brakes, developed by Dunlop, were the first carbon-based brakes used on an airliner. The use of carbon over equivalent steel brakes provided a weight-saving of . Each wheel has multiple discs which are cooled by electric fans. Wheel sensors include brake overload, brake temperature, and tyre deflation. After a typical landing at Heathrow, brake temperatures were around . Landing Concorde required a minimum of runway length, this in fact being considerably less than the shortest runway Concorde ever actually landed on, that of Cardiff Airport.\n\nConcorde's drooping nose, developed by Marshall's of Cambridge at Cambridge Airport, enabled the aircraft to switch between being streamlined to reduce drag and achieve optimal aerodynamic efficiency without obstructing the pilot's view during taxi, take-off, and landing operations. Due to the high angle of attack, the long pointed nose obstructed the view and necessitated the capability to droop. The droop nose was accompanied by a moving visor that retracted into the nose prior to being lowered. When the nose was raised to horizontal, the visor would rise in front of the cockpit windscreen for aerodynamic streamlining.\nA controller in the cockpit allowed the visor to be retracted and the nose to be lowered to 5° below the standard horizontal position for taxiing and take-off. Following take-off and after clearing the airport, the nose and visor were raised. Prior to landing, the visor was again retracted and the nose lowered to 12.5° below horizontal for maximal visibility. Upon landing the nose was raised to the 5° position to avoid the possibility of damage.\n\nThe US Federal Aviation Administration had objected to the restrictive visibility of the visor used on the first two prototype Concordes, which had been designed before a suitable high-temperature window glass had become available, and thus requiring alteration before the FAA would permit Concorde to serve US airports. This led to the redesigned visor used on the production and the four pre-production aircraft (101, 102, 201, and 202). The nose window and visor glass, needed to endure temperatures in excess of at supersonic flight, were developed by Triplex.\n\"Concorde 001\" was modified with rooftop portholes for use on the 1973 Solar Eclipse mission and equipped with observation instruments. It performed the longest observation of a solar eclipse to date, about 74 minutes.\n\nScheduled flights began on 21 January 1976 on the London–Bahrain and Paris–Rio de Janeiro (via Dakar) routes, with BA flights using the \"Speedbird Concorde\" call sign to notify air traffic control of the aircraft's unique abilities and restrictions, but the French using their normal call signs. The Paris-Caracas route (via Azores) began on 10 April. The US Congress had just banned Concorde landings in the US, mainly due to citizen protest over sonic booms, preventing launch on the coveted North Atlantic routes. The US Secretary of Transportation, William Coleman, gave permission for Concorde service to Washington Dulles International Airport, and Air France and British Airways simultaneously began a thrice-weekly service to Dulles on 24 May 1976. Due to low demand, Air France cancelled its Washington service in October 1982, while British Airways cancelled it in November 1994.\n\nWhen the US ban on JFK Concorde operations was lifted in February 1977, New York banned Concorde locally. The ban came to an end on 17 October 1977 when the Supreme Court of the United States declined to overturn a lower court's ruling rejecting efforts by the Port Authority of New York and New Jersey and a grass-roots campaign led by Carol Berman to continue the ban. In spite of complaints about noise, the noise report noted that Air Force One, at the time a Boeing VC-137, was louder than Concorde at subsonic speeds and during take-off and landing. Scheduled service from Paris and London to New York's John F. Kennedy Airport began on 22 November 1977.\n\nIn 1977, British Airways and Singapore Airlines shared a Concorde for flights between London and Singapore International Airport at Paya Lebar via Bahrain. The aircraft, BA's Concorde G-BOAD, was painted in Singapore Airlines livery on the port side and British Airways livery on the starboard side. The service was discontinued after three return flights because of noise complaints from the Malaysian government; it could only be reinstated on a new route bypassing Malaysian airspace in 1979. A dispute with India prevented Concorde from reaching supersonic speeds in Indian airspace, so the route was eventually declared not viable and discontinued in 1980.\n\nDuring the Mexican oil boom, Air France flew Concorde twice weekly to Mexico City's Benito Juárez International Airport via Washington, DC, or New York City, from September 1978 to November 1982. The worldwide economic crisis during that period resulted in this route's cancellation; the last flights were almost empty. The routing between Washington or New York and Mexico City included a deceleration, from Mach 2.02 to Mach 0.95, to cross Florida subsonically and avoid creating a sonic boom over the state; Concorde then re-accelerated back to high speed while crossing the Gulf of Mexico. On 1 April 1989, on an around-the-world luxury tour charter, British Airways implemented changes to this routing that allowed G-BOAF to maintain Mach 2.02 by passing around Florida to the east and south. Periodically Concorde visited the region on similar chartered flights to Mexico City and Acapulco.\n\nFrom December 1978 to May 1980, Braniff International Airways leased 11 Concordes, five from Air France and six from British Airways. These were used on subsonic flights between Dallas-Fort Worth and Washington Dulles International Airport, flown by Braniff flight crews. Air France and British Airways crews then took over for the continuing supersonic flights to London and Paris. The aircraft were registered in both the United States and their home countries; the European registration was covered while being operated by Braniff, retaining full AF/BA liveries. The flights were not profitable and typically less than 50% booked, forcing Braniff to end its tenure as the only US Concorde operator in May 1980.\n\nIn its early years, the British Airways Concorde service had a greater number of \"no shows\" (passengers who booked a flight and then failed to appear at the gate for boarding) than any other aircraft in the fleet.\n\nFollowing the launch of British Airways Concorde services, Britain's other major airline, British Caledonian (BCal), set up a task force headed by Gordon Davidson, BA's former Concorde director, to investigate the possibility of their own Concorde operations. This was seen as particularly viable for the airline's long-haul network as there were two unsold aircraft then available for purchase.\n\nOne important reason for BCal's interest in Concorde was that the British Government's 1976 aviation policy review had opened the possibility of BA setting up supersonic services in competition with BCal's established sphere of influence. To counteract this potential threat, BCal considered their own independent Concorde plans, as well as a partnership with BA. BCal were considered most likely to have set up a Concorde service on the Gatwick–Lagos route, a major source of revenue and profits within BCal's scheduled route network; BCal's Concorde task force did assess the viability of a daily supersonic service complementing the existing subsonic widebody service on this route.\n\nBCal entered into a bid to acquire at least one Concorde. However, BCal eventually arranged for two aircraft to be leased from BA and Aérospatiale respectively, to be maintained by either BA or Air France. BCal's envisaged two-Concorde fleet would have required a high level of aircraft usage to be cost-effective; therefore, BCal had decided to operate the second aircraft on a supersonic service between Gatwick and Atlanta, with a stopover at either Gander or Halifax. Consideration was given to services to Houston and various points on its South American network at a later stage. Both supersonic services were to be launched at some point during 1980; however, steeply rising oil prices caused by the 1979 energy crisis led to BCal shelving their supersonic ambitions.\n\nBy around 1981 in the UK, the future for Concorde looked bleak. The British government had lost money operating Concorde every year, and moves were afoot to cancel the service entirely. A cost projection came back with greatly reduced metallurgical testing costs because the test rig for the wings had built up enough data to last for 30 years and could be shut down. Despite this, the government was not keen to continue. In 1983, BA's managing director, Sir John King, convinced the government to sell the aircraft outright to the then state-owned British Airways for £16.5 million plus the first year's profits. British Airways was subsequently privatised in 1987.\n\nIn 1983, Pan American accused the British Government of subsidising British Airways Concorde air fares, on which a return London–New York was £2,399 (£ in prices), compared to £1,986 (£) with a subsonic first class return, and London–Washington return was £2,426 (£) instead of £2,258 (£) subsonic.\n\nResearch revealed that passengers thought that the fare was higher than it actually was, so the airline raised ticket prices to match these perceptions. It is reported that British Airways then ran Concorde at a profit.\n\nIts estimated operating costs were $3,800 per block hour in 1972, compared to actual 1971 operating costs of $1,835 for a 707 and $3,500 for a 747; for a 3,050 nmi London–New York sector, a 707 cost $13,750 or 3.04c per seat/nmi, a 747 $26,200 or 2.4c per seat/nmi and the Concorde $14,250 or 4.5c per seat/nmi.\n\nBetween March 1984 and March 1991, British Airways flew a thrice-weekly Concorde service between London and Miami, stopping at Washington Dulles International Airport. Until 2003, Air France and British Airways continued to operate the New York services daily. From 1987 to 2003 British Airways flew a Saturday morning Concorde service to Grantley Adams International Airport, Barbados, during the summer and winter holiday season.\n\nPrior to the Air France Paris crash, several UK and French tour operators operated charter flights to European destinations on a regular basis; the charter business was viewed as lucrative by British Airways and Air France.\n\nIn 1997, British Airways held a promotional contest to mark the 10th anniversary of the airline's move into the private sector. The promotion was a lottery to fly to New York held for 190 tickets valued at £5,400 each, to be offered at £10. Contestants had to call a special hotline to compete with up to 20 million people.\n\nOn 10 April 2003, Air France and British Airways simultaneously announced they would retire Concorde later that year. They cited low passenger numbers following the 25 July 2000 crash, the slump in air travel following the September 11 attacks, and rising maintenance costs: Airbus (the company that acquired Aerospatiale in 2000) had made a decision in 2003 to no longer supply replacement parts for the aircraft. Although Concorde was technologically advanced when introduced in the 1970s, 30 years later, its analogue cockpit was outdated. There had been little commercial pressure to upgrade Concorde due to a lack of competing aircraft, unlike other airliners of the same era such as the Boeing 747. By its retirement, it was the last aircraft in the British Airways fleet that had a flight engineer; other aircraft, such as the modernised 747-400, had eliminated the role.\n\nOn 11 April 2003, Virgin Atlantic founder Sir Richard Branson announced that the company was interested in purchasing British Airways' Concorde fleet \"for the same price that they were given them for – one pound\". British Airways dismissed the idea, prompting Virgin to increase their offer to £1 million each. Branson claimed that when BA was privatised, a clause in the agreement required them to allow another British airline to operate Concorde if BA ceased to do so, but the Government denied the existence of such a clause. In October 2003, Branson wrote in \"The Economist\" that his final offer was \"over £5 million\" and that he had intended to operate the fleet \"for many years to come\". The chances for keeping Concorde in service were stifled by Airbus's lack of support for continued maintenance.\n\nIt has been suggested that Concorde was not withdrawn for the reasons usually given but that it became apparent during the grounding of Concorde that the airlines could make more profit carrying first-class passengers subsonically. A lack of commitment to Concorde from Director of Engineering Alan MacDonald was cited as having undermined BA's resolve to continue operating Concorde.\n\nOther reasons why the attempted revival of Concorde never happened relate to the fact that the narrow fuselage did not allow for \"luxury\" features of subsonic air travel such as moving space, reclining seats and overall comfort. In the words of \"The Guardian\"'s Dave Hall, \"Concorde was an outdated notion of prestige that left sheer speed the only luxury of supersonic travel.\"\n\nAir France made its final commercial Concorde landing in the United States in New York City from Paris on 30 May 2003. Air France's final Concorde flight took place on 27 June 2003 when F-BVFC retired to Toulouse.\n\nAn auction of Concorde parts and memorabilia for Air France was held at Christie's in Paris on 15 November 2003; 1,300 people attended, and several lots exceeded their predicted values. French Concorde F-BVFC was retired to Toulouse and kept functional for a short time after the end of service, in case taxi runs were required in support of the French judicial enquiry into the 2000 crash. The aircraft is now fully retired and no longer functional.\n\nFrench Concorde F-BTSD has been retired to the \"Musée de l'Air\" at Paris–Le Bourget Airport near Paris; unlike the other museum Concordes, a few of the systems are being kept functional. For instance, the famous \"droop nose\" can still be lowered and raised. This led to rumours that they could be prepared for future flights for special occasions.\nFrench Concorde F-BVFB is at the Auto & Technik Museum Sinsheim at Sinsheim, Germany, after its last flight from Paris to Baden-Baden, followed by a spectacular transport to Sinsheim via barge and road. The museum also has a Tupolev Tu-144 on display – this is the only place where both supersonic airliners can be seen together.\n\nIn 1989, Air France signed a letter of agreement to donate a Concorde to the National Air and Space Museum in Washington D.C. upon the aircraft's retirement. On 12 June 2003, Air France honoured that agreement, donating Concorde F-BVFA (serial 205) to the Museum upon the completion of its last flight. This aircraft was the first Air France Concorde to open service to Rio de Janeiro, Washington, D.C., and New York and had flown 17,824 hours. It is on display at the Smithsonian's Steven F. Udvar-Hazy Center at Dulles Airport.\n\nBritish Airways conducted a North American farewell tour in October 2003. G-BOAG visited Toronto Pearson International Airport on 1 October, after which it flew to New York's John F. Kennedy International Airport. G-BOAD visited Boston's Logan International Airport on 8 October, and G-BOAG visited Washington Dulles International Airport on 14 October.\n\nIn a week of farewell flights around the United Kingdom, Concorde visited Birmingham on 20 October, Belfast on 21 October, Manchester on 22 October, Cardiff on 23 October, and Edinburgh on 24 October. Each day the aircraft made a return flight out and back into Heathrow to the cities, often overflying them at low altitude. On 22 October, both Concorde flight BA9021C, a special from Manchester, and BA002 from New York landed simultaneously on both of Heathrow's runways. On 23 October 2003, the Queen consented to the illumination of Windsor Castle, an honour reserved for state events and visiting dignitaries, as Concorde's last west-bound commercial flight departed London.\n\nBritish Airways retired its Concorde fleet on 24 October 2003. G-BOAG left New York to a fanfare similar to that given for Air France's F-BTSD, while two more made round trips, G-BOAF over the Bay of Biscay, carrying VIP guests including former Concorde pilots, and G-BOAE to Edinburgh. The three aircraft then circled over London, having received special permission to fly at low altitude, before landing in sequence at Heathrow. The captain of the New York to London flight was Mike Bannister. The final flight of a Concorde in the US occurred on 5 November 2003 when G-BOAG flew from New York's JFK Airport to Seattle's Boeing Field to join the Museum of Flight's permanent collection. The plane was piloted by Mike Bannister and Les Broadie, who claimed a flight time of three hours, 55 minutes and 12 seconds, a record between the two cities. The museum had been pursuing a Concorde for their collection since 1984. The final flight of a Concorde worldwide took place on 26 November 2003 with a landing at Filton, Bristol, UK.\n\nAll of BA's Concorde fleet have been grounded, drained of hydraulic fluid and their airworthiness certificates withdrawn. Jock Lowe, ex-chief Concorde pilot and manager of the fleet estimated in 2004 that it would cost £10–15 million to make G-BOAF airworthy again. BA maintain ownership and have stated that they will not fly again due to a lack of support from Airbus. On 1 December 2003, Bonhams held an auction of British Airways Concorde artefacts, including a nose cone, at Kensington Olympia in London. Proceeds of around £750,000 were raised, with the majority going to charity. G-BOAD is currently on display at the Intrepid Sea, Air & Space Museum in New York. In 2007, BA announced that the advertising spot at Heathrow where a 40% scale model of Concorde was located would not be retained; the model is now on display at the Brooklands Museum, in Surrey, England.\n\nConcorde G-BBDG was used for test flying and trials work. It was retired in 1981 and then only used for spares. It was dismantled and transported by road from Filton to the Brooklands Museum in Surrey where it was restored from essentially a shell. It remains open to visitors to the museum.\n\nConcorde \"G-BOAB\", nicknamed \"Alpha Bravo\", was never modified and returned to service with the rest of British Airways' fleet, and has remained at London Heathrow Airport since its final flight, a ferry flight from JFK in 2000. The aircraft has been moved around the apron at Heathrow several times, and can be regularly seen from other aircraft departing or arriving at the airport. G-BOAB is also occasionally used by BA for apprentice training.\n\nOne of the youngest Concordes (F-BTSD) is on display at Le Bourget Air and Space Museum in Paris. In February 2010, it was announced that the museum and a group of volunteer Air France technicians intend to restore F-BTSD so it can taxi under its own power. In May 2010, it was reported that the British Save Concorde Group and French Olympus 593 groups had begun inspecting the engines of a Concorde at the French museum; their intent was to restore the airliner to a condition where it could fly in demonstrations.\n\nG-BOAF forms the centrepiece of the Aerospace Bristol museum at Filton, which opened to the public in 2017.\n\nOn 15 September 2015, Club Concorde announced it had secured over £160 million to return an aircraft to service. Club Concorde President Paul James said:\nThe organisation aims to buy the Concorde currently on display at Le Bourget airport. A tentative date of 2019 has been put forward for the return to flight—50 years after its maiden journey. However, due to regulatory and technical hurdles, some of the aviation community are highly skeptical of the plan, including former Concorde captain and Club Concorde co-founder William \"Jock\" Lowe, who was quoted in June 2016 saying:\nBULLET::::- Air France\nBULLET::::- British Airways\nBULLET::::- Braniff International Airways (1 on short term lease)\nBULLET::::- Singapore Airlines (1 on short term wet lease)\n\nOn 25 July 2000, Air France Flight 4590, registration F-BTSC, crashed in Gonesse, France, after departing from Charles de Gaulle Airport en route to John F. Kennedy International Airport in New York City, killing all 100 passengers and nine crew members on board and four people on the ground. It was the only fatal accident involving Concorde.\n\nAccording to the official investigation conducted by the \"Bureau d'Enquêtes et d'Analyses pour la Sécurité de l'Aviation Civile\" (BEA), the crash was caused by a metallic strip that had fallen from a Continental Airlines DC-10 that had taken off minutes earlier. This fragment punctured a tyre on Concorde's left main wheel bogie during take-off. The tyre exploded, and a piece of rubber hit the fuel tank, which caused a fuel leak and led to a fire. The crew shut down engine number 2 in response to a fire warning, and with engine number 1 surging and producing little power, the aircraft was unable to gain altitude or speed. The aircraft entered a rapid pitch-up then a sudden descent, rolling left and crashing tail-low into the Hôtelissimo Les Relais Bleus Hotel in Gonesse.\n\nThe claim that a metallic strip caused the crash was disputed during the trial both by witnesses (including the pilot of then French President Jacques Chirac's aircraft that had just landed on an adjacent runway when Flight 4590 caught fire) and by an independent French TV investigation that found a wheel spacer had not been installed in the left-side main gear and that the plane caught fire some 1,000 feet from where the metallic strip lay. British investigators and former French Concorde pilots looked at several other possibilities that the BEA report ignored, including an unbalanced weight distribution in the fuel tanks and loose landing gear. They came to the conclusion that the Concorde veered off course on the runway, which reduced takeoff speed below the crucial minimum. John Hutchinson, who had served as a Concorde captain for 15 years with British Airways, said \"the fire on its own should have been 'eminently survivable; the pilot should have been able to fly his way out of trouble'\", had it not been for a \"lethal combination of operational error and 'negligence' by the maintenance department of Air France\" that \"nobody wants to talk about\".\n\nOn 6 December 2010, Continental Airlines and John Taylor, a mechanic who installed the metal strip, were found guilty of involuntary manslaughter, but on 30 November 2012, a French court overturned the conviction, saying mistakes by Continental and Taylor did not make them criminally responsible. \n\nPrior to the accident, Concorde had been arguably the safest operational passenger airliner in the world in passenger deaths-per-kilometres travelled with zero, but there had been two prior non-fatal accidents and a rate of tyre damage some 30 times higher than subsonic airliners from 1995 to 2000. Safety improvements were made in the wake of the crash, including more secure electrical controls, Kevlar lining on the fuel tanks and specially developed burst-resistant tyres. The first flight with the modifications departed from London Heathrow on 17 July 2001, piloted by BA Chief Concorde Pilot Mike Bannister. During the 3-hour 20-minute flight over the mid-Atlantic towards Iceland, Bannister attained Mach 2.02 and before returning to RAF Brize Norton. The test flight, intended to resemble the London–New York route, was declared a success and was watched on live TV, and by crowds on the ground at both locations.\n\nThe first flight with passengers after the accident took place on 11 September 2001, landing shortly before the World Trade Center attacks in the US. This was not a commercial flight: all the passengers were BA employees. Normal commercial operations resumed on 7 November 2001 by BA and AF (aircraft G-BOAE and F-BTSD), with service to New York JFK, where Mayor Rudy Giuliani greeted the passengers.\n\nConcorde had suffered two previous non-fatal accidents that were similar to each other.\nBULLET::::- 12 April 1989: A Concorde of British registration, G-BOAF, on a chartered flight from Christchurch, New Zealand, to Sydney, suffered a structural failure in-flight at supersonic speed. As the aircraft was climbing and accelerating through Mach 1.7 a \"thud\" was heard. The crew did not notice any handling problems, and they assumed the thud they heard was a minor engine surge. No further difficulty was encountered until descent through 40,000 feet at Mach 1.3, when a vibration was felt throughout the aircraft, lasting two to three minutes. Most of the upper rudder had become separated from the aircraft at this point. Aircraft handling was unaffected, and the aircraft made a safe landing at Sydney. The UK's Air Accidents Investigation Branch (AAIB) concluded that the skin of the rudder had been separating from the rudder structure over a period of time before the accident due to moisture seepage past the rivets in the rudder. Furthermore, production staff had not followed proper procedures during an earlier modification of the rudder, but the procedures were difficult to adhere to. The aircraft was repaired and returned to service.\nBULLET::::- 21 March 1992: A Concorde of British registration, G-BOAB, on a scheduled flight from London to New York, also suffered a structural failure in-flight at supersonic speed. While cruising at Mach 2, at approximately 53,000 feet above mean sea level, the crew heard a \"thump\". No difficulties in handling were noticed, and no instruments gave any irregular indications. This crew also suspected there had been a minor engine surge. One hour later, during descent and while decelerating below Mach 1.4, a sudden \"severe\" vibration began throughout the aircraft. The vibration worsened when power was added to the No 2 engine, and it was attenuated when that engine's power was reduced. The crew shut down the No 2 engine and made a successful landing in New York, noting only that increased rudder control was needed to keep the aircraft on its intended approach course. Again, the skin had become separated from the structure of the rudder, which led to most of the upper rudder becoming separated in-flight. The AAIB concluded that repair materials had leaked into the structure of the rudder during a recent repair, weakening the bond between the skin and the structure of the rudder, leading to it breaking up in-flight. The large size of the repair had made it difficult to keep repair materials out of the structure, and prior to this accident, the severity of the effect of these repair materials on the structure and skin of the rudder was not appreciated.\nBULLET::::- The 2010 trial involving Continental Airlines over the crash of Flight 4590 established that from 1976 until Flight 4590 there had been 57 tyre failures involving Concordes during takeoffs, including a near-crash at Dulles Airport on 14 June 1979 involving Air France Flight 54 where a tyre blowout pierced the plane's fuel tank and damaged the port-side engine and electrical cables, with the loss of two of the craft's hydraulic systems.\n\nOf the 20 aircraft built, 18 remain in good condition. Many are on display at museums in the United Kingdom, France, the United States, Germany and Barbados.\n\nThe only supersonic airliner in direct competition with Concorde was the Soviet Tupolev Tu-144, nicknamed \"Concordski\" by Western European journalists for its outward similarity to Concorde. It had been alleged that Soviet espionage efforts had resulted in the theft of Concorde blueprints, supposedly to assist in the design of the Tu-144. As a result of a rushed development programme, the first Tu-144 prototype was substantially different from the preproduction machines, but both were cruder than Concorde. The Tu-144\"S\" had a significantly shorter range than Concorde. Jean Rech, Sud Aviation, attributed this to two things, a very heavy powerplant with an intake twice as long as that on Concorde, and low-bypass turbofan engines with too-high a bypass ratio which needed afterburning for cruise. The aircraft had poor control at low speeds because of a simpler supersonic wing design; in addition the Tu-144 required braking parachutes to land while Concorde used anti-lock brakes. The Tu-144 had two crashes, one at the 1973 Paris Air Show, and another during a pre-delivery test flight in May 1978.\n\nLater production Tu-144 versions were more refined and competitive. They had retractable canards for better low-speed control, turbojet engines providing nearly the fuel efficiency and range of Concorde and a top speed of Mach 2.35. Passenger service commenced in November 1977, but after the 1978 crash the aircraft was taken out of passenger service after only 55 flights, which carried an average of 58 passengers. The aircraft had an inherently unsafe structural design as a consequence of an automated production method chosen to simplify and speed up manufacturing.\n\nThe American designs, the \"SST\" project (for Supersonic Transport) were the Boeing 2707 and the Lockheed L-2000. These were to have been larger, with seating for up to 300 people. Running a few years behind Concorde, the Boeing 2707 was redesigned to a cropped delta layout; the extra cost of these changes helped to kill the project. The operation of US military aircraft such as the Mach 3+ North American XB-70 Valkyrie prototypes and Convair B-58 Hustler strategic nuclear bomber had shown that sonic booms were quite capable of reaching the ground, and the experience from the Oklahoma City sonic boom tests led to the same environmental concerns that hindered the commercial success of Concorde. The American government cancelled its SST project in 1971, after having spent more than $1 billion.\n\nThe only other large supersonic aircraft comparable to Concorde are strategic bombers, principally the Russian Tu-22, Tu-22M, M-50 (experimental), T-4 (experimental), Tu-160 and the American XB-70 (experimental) and B-1.\n\nBefore Concorde's flight trials, developments in the civil aviation industry were largely accepted by governments and their respective electorates. Opposition to Concorde's noise, particularly on the east coast of the United States, forged a new political agenda on both sides of the Atlantic, with scientists and technology experts across a multitude of industries beginning to take the environmental and social impact more seriously. Although Concorde led directly to the introduction of a general noise abatement programme for aircraft flying out of John F. Kennedy Airport, many found that Concorde was quieter than expected, partly due to the pilots temporarily throttling back their engines to reduce noise during overflight of residential areas. Even before commercial flights started, it had been claimed that Concorde was quieter than many other aircraft. In 1971, BAC's technical director was quoted as saying, \"It is certain on present evidence and calculations that in the airport context, production Concordes will be no worse than aircraft now in service and will in fact be better than many of them.\"\n\nConcorde produced nitrogen oxides in its exhaust, which, despite complicated interactions with other ozone-depleting chemicals, are understood to result in degradation to the ozone layer at the stratospheric altitudes it cruised. It has been pointed out that other, lower-flying, airliners produce ozone during their flights in the troposphere, but vertical transit of gases between the layers is restricted. The small fleet meant overall ozone-layer degradation caused by Concorde was negligible. In 1995, David Fahey, of the National Oceanic and Atmospheric Administration in the United States, warned that a fleet of 500 supersonic aircraft with exhausts similar to Concorde might produce a 2 percent drop in global ozone levels, much higher than previously thought. Each 1 percent drop in ozone is estimated to increase the incidence of non-melanoma skin cancer worldwide by 2 percent. Dr Fahey said if these particles are produced by highly oxidised sulphur in the fuel, as he believed, then removing sulphur in the fuel will reduce the ozone-destroying impact of supersonic transport.\n\nConcorde's technical leap forward boosted the public's understanding of conflicts between technology and the environment as well as awareness of the complex decision analysis processes that surround such conflicts. In France, the use of acoustic fencing alongside TGV tracks might not have been achieved without the 1970s controversy over aircraft noise. In the UK, the CPRE has issued tranquillity maps since 1990.\n\nSome sources say Concorde typically flew per passenger.\n\nConcorde was normally perceived as a privilege of the rich, but special circular or one-way (with return by other flight or ship) charter flights were arranged to bring a trip within the means of moderately well-off enthusiasts.\n\nThe aircraft was usually referred to by the British as simply \"Concorde\". In France it was known as \"le Concorde\" due to \"le\", the definite article, used in French grammar to introduce the name of a ship or aircraft, and the capital being used to distinguish a proper name from a common noun of the same spelling. In French, the common noun \"concorde\" means \"agreement, harmony, or peace\". Concorde's pilots and British Airways in official publications often refer to Concorde both in the singular and plural as \"she\" or \"her\".\n\nAs a symbol of national pride, an example from the BA fleet made occasional flypasts at selected Royal events, major air shows and other special occasions, sometimes in formation with the Red Arrows. On the final day of commercial service, public interest was so great that grandstands were erected at Heathrow Airport. Significant numbers of people attended the final landings; the event received widespread media coverage.\n\nIn 2006, 37 years after its first test flight, Concorde was announced the winner of the Great British Design Quest organised by the BBC and the Design Museum. A total of 212,000 votes were cast with Concorde beating other British design icons such as the Mini, mini skirt, Jaguar E-Type, Tube map, the World Wide Web, K2 telephone box and the Supermarine Spitfire.\n\nThe heads of France and the United Kingdom flew Concorde many times. Presidents Georges Pompidou, Valéry Giscard d'Estaing and François Mitterrand regularly used Concorde as French flagman aircraft in foreign visits. Queen Elizabeth II and Prime Ministers Edward Heath, Jim Callaghan, Margaret Thatcher, John Major and Tony Blair took Concorde in some charter flights such as the Queen's trips to Barbados on her Silver Jubilee in 1977, in 1987 and in 2003, to the Middle East in 1984 and to the United States in 1991. Pope John Paul II flew on Concorde in May 1989.\n\nConcorde sometimes made special flights for demonstrations, air shows (such as the Farnborough, Paris-LeBourget, Oshkosh AirVenture and MAKS air shows) as well as parades and celebrations (for example, of Zurich Airport's anniversary in 1998). The aircraft were also used for private charters (including by the President of Zaire Mobutu Sese Seko on multiple occasions), for advertising companies (including for the firm OKI), for Olympic torch relays (1992 Winter Olympics in Albertville) and for observing solar eclipses, including the solar eclipse of June 30, 1973 and again for the total solar eclipse on August 11, 1999.\n\nThe fastest transatlantic airliner flight was from New York JFK to London Heathrow on 7 February 1996 by the British Airways G-BOAD in 2 hours, 52 minutes, 59 seconds from take-off to touchdown aided by a 175 mph (282 km/h) tailwind. On 13 February 1985, a Concorde charter flight flew from London Heathrow to Sydney—on the opposite side of the world—in a time of 17 hours, 3 minutes and 45 seconds, including refuelling stops.\n\nConcorde also set other records, including the official FAI \"Westbound Around the World\" and \"Eastbound Around the World\" world air speed records. On 12–13 October 1992, in commemoration of the 500th anniversary of Columbus' first New World landing, Concorde Spirit Tours (US) chartered Air France Concorde F-BTSD and circumnavigated the world in 32 hours 49 minutes and 3 seconds, from Lisbon, Portugal, including six refuelling stops at Santo Domingo, Acapulco, Honolulu, Guam, Bangkok, and Bahrain.\n\nThe eastbound record was set by the same Air France Concorde (F-BTSD) under charter to Concorde Spirit Tours in the US on 15–16 August 1995. This promotional flight circumnavigated the world from New York/JFK International Airport in 31 hours 27 minutes 49 seconds, including six refuelling stops at Toulouse, Dubai, Bangkok, Andersen AFB in Guam, Honolulu, and Acapulco. By its 30th flight anniversary on 2 March 1999 Concorde had clocked up 920,000 flight hours, with more than 600,000 supersonic, many more than all of the other supersonic aircraft in the Western world combined.\n\nOn its way to the Museum of Flight in November 2003, G-BOAG set a New York City-to-Seattle speed record of 3 hours, 55 minutes, and 12 seconds. Due to the restrictions on supersonic overflights within the US the flight was granted permission by the Canadian authorities for the majority of the journey to be flown supersonically over sparsely-populated Canadian territory.\n\nBULLET::::- BAC 221 used for ogee delta wing research\nBULLET::::- Barbara Harmer, the first qualified female Concorde pilot.\n\nBULLET::::- .\n\nBULLET::::- Legacy\nBULLET::::- British Airways Concorde page\nBULLET::::- Design Museum (UK) Concorde page\nBULLET::::- Heritage Concorde preservation group site\nBULLET::::- Articles\nBULLET::::- Videos\nBULLET::::- \"Video: Roll-out.\" British Movietone/Associated Press. 14 December 1967, posted online on 21 July 2015.\nBULLET::::- \"This plane could cross the Atlantic in 3.5 hours. Why did it fail?.\" Vox Media. 19 July 2016.\n"}
{"id": "7053", "url": "https://en.wikipedia.org/wiki?curid=7053", "title": "Cannon", "text": "Cannon\n\nA cannon is a type of gun classified as artillery that launches a projectile using propellant. In the past, gunpowder was the primary propellant before the invention of smokeless powder during the 19th century. Cannon vary in caliber, range, mobility, rate of fire, angle of fire, and firepower; different forms of cannon combine and balance these attributes in varying degrees, depending on their intended use on the battlefield. The word \"cannon\" is derived from several languages, in which the original definition can usually be translated as \"tube\", \"cane\", or \"reed\". In the modern era, the term \"cannon\" has fallen into decline, replaced by \"guns\" or \"artillery\" if not a more specific term such as howitzer or mortar, except for high calibre automatic weapons firing bigger rounds than machine guns, called autocannons.\n\nThe earliest known depiction of cannon appeared in Song dynasty China as early as the 12th century; however, solid archaeological and documentary evidence of cannon do not appear until the 13th century. In 1288 Yuan dynasty troops are recorded to have used hand cannons in combat, and the earliest extant cannon bearing a date of production comes from the same period. By the early 14th century, depictions of cannon had appeared in the Middle East and Europe, and almost immediately recorded usage of cannon began appearing. By the end of the 14th century cannon were widespread throughout Eurasia. Cannon were used primarily as anti-infantry weapons until around 1374 when cannon were recorded to have breached walls for the first time in Europe. Cannon featured prominently as siege weapons and ever larger pieces appeared. In 1464 a 16,000 kg (35,000 lbs) cannon known as the Great Turkish Bombard was created in the Ottoman Empire. Cannon as field artillery became more important after 1453 with the introduction of limber, which greatly improved cannon maneuverability and mobility. European cannon reached their longer, lighter, more accurate, and more efficient \"classic form\" around 1480. This classic European cannon design stayed relatively consistent in form with minor changes until the 1750s.\n\n\"Cannon\" is derived from the Old Italian word \"cannone\", meaning \"large tube\", which came from Latin \"canna\", in turn originating from the Greek κάννα (\"kanna\"), \"reed\", and then generalised to mean any hollow tube-like object; cognate with Akkadian \"qanu(m)\" and Hebrew \"qāneh\", \"tube, reed\". The word has been used to refer to a gun since 1326 in Italy, and 1418 in England. Both \"cannons\" and \"cannon\" are correct and in common usage, with one or the other having preference in different parts of the English-speaking world. \"Cannons\" is more common in North America and Australia, while \"cannon\" as plural is more common in the United Kingdom.\n\nThe cannon may have appeared as early as the 12th century in China, and was probably a parallel development or evolution of the fire-lance, a short ranged anti-personnel weapon combining a gunpowder-filled tube and a polearm of some sort. Co-viative projectiles such as iron scraps or porcelain shards were placed in fire lance barrels at some point, and eventually, the paper and bamboo materials of fire lance barrels were replaced by metal.\n\nThe earliest known depiction of a cannon is a sculpture from the Dazu Rock Carvings in Sichuan dated to 1128, however the earliest archaeological samples and textual accounts do not appear until the 13th century. The primary extant specimens of cannon from the 13th century are the Wuwei Bronze Cannon dated to 1227, the Heilongjiang hand cannon dated to 1288, and the Xanadu Gun dated to 1298. However, only the Xanadu gun contains an inscription bearing a date of production, so it is considered the earliest confirmed extant cannon. The Xanadu Gun is 34.7 cm in length and weighs 6.2 kg. The other cannon are dated using contextual evidence.\n\nThe Heilongjiang hand cannon is also often considered by some to be the oldest firearm since it was unearthed near the area where the History of Yuan reports a battle took place involving hand cannon. According to the History of Yuan, in 1288, a Jurchen commander by the name of Li Ting led troops armed with hand cannon into battle against the rebel prince Nayan.\n\nChen Bingying argues there were no guns before 1259 while Dang Shoushan believes the Wuwei gun and other Western Xia era samples point to the appearance of guns by 1220, and Stephen Haw goes even further by stating that guns were developed as early as 1200. Sinologist Joseph Needham and renaissance siege expert Thomas Arnold provide a more conservative estimate of around 1280 for the appearance of the \"true\" cannon. Whether or not any of these are correct, it seems likely that the gun was born sometime during the 13th century.\n\nReferences to cannon proliferated throughout China in the following centuries. Cannon featured in literary pieces. In 1341 Xian Zhang wrote a poem called \"The Iron Cannon Affair\" describing a cannonball fired from an eruptor which could \"pierce the heart or belly when striking a man or horse, and even transfix several persons at once.\"\n\nBy the 1350s the cannon was used extensively in Chinese warfare. In 1358 the Ming army failed to take a city due to its garrisons' usage of cannon, however they themselves would use cannon, in the thousands, later on during the siege of Suzhou in 1366. The Korean kingdom of Joseon started producing gunpowder in 1374 and cannon by 1377. Cannons appeared in Đại Việt by 1390 at the latest.\n\nDuring the Ming dynasty cannon were used in riverine warfare at the Battle of Lake Poyang. One shipwreck in Shandong had a cannon dated to 1377 and an anchor dated to 1372. From the 13th to 15th centuries cannon-armed Chinese ships also travelled throughout Southeast Asia.\n\nThe usage of cannons in the Mongol invasion of Java, led to deployment of cannons (in the form of cetbang breech-loading swivel guns) by Majapahit fleet in 1300s and subsequent near universal use of the swivel-gun and cannons in the Nusantaran archipelago.\n\nThe first of the western cannon to be introduced were breech-loaders in the early 16th century which the Chinese began producing themselves by 1523 and began improving later on.\n\nJapan did not acquire a cannon until 1510 when a monk brought one back from China, and did not produce any in appreciable numbers. During the 1593 Siege of Pyongyang, 40,000 Ming troops deployed a variety of cannon against Japanese troops. Despite their defensive advantage and the use of arquebus by Japanese soldiers, the Japanese were at a severe disadvantage due to their lack of cannon. Throughout the Japanese invasions of Korea (1592–98), the Ming-Joseon coalition used artillery widely in land and naval battles, including on the turtle ships of Yi Sun-sin.\n\nAccording to Ivan Petlin, the first Russian envoy to Beijing, in September 1619, the city was armed with large cannon with cannonballs weighing more than . His general observation was that the Chinese were militarily capable and had firearms:\n\nThere is no clear consensus of when the cannon first appeared in the Islamic world, with dates ranging from 1260 to the mid-14th century. The cannon may have appeared in the Islamic world in the late 13th century, with Ibn Khaldun in the 14th century stating that cannons were used in the Maghreb region of North Africa in 1274, and other Arabic military treatises in the 14th century referring to the use of cannon by Mamluk forces in 1260 and 1303, and by Muslim forces at the 1324 Siege of Huesca in Spain. However, some scholars do not accept these early dates. While the date of its first appearance is not entirely clear, the general consensus among most historians is that there is no doubt the Mamluk forces were using cannon by 1342. \n\nAccording to historian Ahmad Y. al-Hassan, during the Battle of Ain Jalut in 1260, the Mamluks used cannon against the Mongols. He claims that this was \"the first cannon in history\" and used a gunpowder formula almost identical to the ideal composition for explosive gunpowder. He also argues that this was not known in China or Europe until much later. Hassan further claims that the earliest textual evidence of cannon is from the Middle East, based on earlier originals which report hand-held cannon being used by the Mamluks at the Battle of Ain Jalut in 1260. Such an early date is not accepted by some historians, including David Ayalon, Iqtidar Alam Khan, Joseph Needham and Tonio Andrade. Khan argues that it was the Mongols who introduced gunpowder to the Islamic world, and believes cannon only reached Mamluk Egypt in the 1370s. Needham argued that the term \"midfa\", dated to textual sources from 1342 to 1352, did not refer to true hand-guns or bombards, and that contemporary accounts of a metal-barrel cannon in the Islamic world did not occur until 1365. Similarly, Andrade dates the textual appearance of cannon in middle eastern sources to the 1360s. Gabor Ágoston and David Ayalon note that the Mamluks had certainly used siege cannon by 1342 or the 1360s, respectively, but earlier uses of cannon in the Islamic World are vague with a possible appearance in the Emirate of Granada by the 1320s and 1330s, though evidence is inconclusive.\n\nIbn Khaldun reported the use of cannon as siege machines by the Marinid sultan Abu Yaqub Yusuf at the siege of Sijilmasa in 1274. The passage by Ibn Khaldun on the Marinid Siege of Sijilmassa in 1274 occurs as follows: \"[The Sultan] installed siege engines … and gunpowder engines …, which project small balls of iron. These balls are ejected from a chamber … placed in front of a kindling fire of gunpowder; this happens by a strange property which attributes all actions to the power of the Creator.\" The source is not contemporary and was written a century later around 1382. Its interpretation has been rejected as anachronistic by some historians, who urge caution regarding claims of Islamic firearms use in the 1204–1324 period as late medieval Arabic texts used the same word for gunpowder, naft, as they did for an earlier incendiary, naphtha. Ágoston and Peter Purton note that in the 1204–1324 period, late medieval Arabic texts used the same word for gunpowder, \"naft\", that they used for an earlier incendiary naphtha. Needham believes Ibn Khaldun was speaking of fire lances rather than hand cannon.\n\nThe Ottoman Empire made good use of cannon as siege artillery. Sixty-eight super-sized bombards were used by Mehmed the Conqueror to capture Constantinople in 1453. Jim Bradbury argues that Urban, a Hungarian cannon engineer, introduced this cannon from Central Europe to the Ottoman realm; according to Paul Hammer, however, it could have been introduced from other Islamic countries which had earlier used cannon. These cannon could fire heavy stone balls a mile, and the sound of their blast could reportedly be heard from a distance of . Shkodëran historian Marin Barleti discusses Turkish bombards at length in his book \"De obsidione Scodrensi\" (1504), describing the 1478–79 siege of Shkodra in which eleven bombards and two mortars were employed. The Ottomans also used cannon to sink ships which attempted to prevent them from crossing the Bosporus strait. Ottoman cannon also proved effective at stopping crusaders at Varna in 1444 and Kosovo in 1448 despite the presence of European cannon in the former case.\n\nThe similar Dardanelles Guns (for the location) were created by Munir Ali in 1464 and were still in use during the Anglo-Turkish War (1807–09). These were cast in bronze into two parts, the chase (the barrel) and the breech, which combined weighed 18.4 tonnes. The two parts were screwed together using levers to facilitate moving it.\n\nFathullah Shirazi, a Persian inhabitant of India who worked for Akbar in the Mughal Empire, developed a volley gun in the 16th century.\n\nWhile there is evidence of cannon in Iran as early as 1405 they were not widespread. This changed following the increased use of firearms by Shah Isma il I, and the Iranian army used 500 cannon by the 1620s, probably captured from the Ottomans or acquired by allies in Europe. By 1443 Iranians were also making some of their own cannon, as Mir Khawand wrote of a 1200 kg metal piece being made by an Iranian \"rekhtagar\" which was most likely a cannon. Due to the difficulties of transporting cannon in mountainous terrain, their use was less common compared to their use in Europe.\n\nOutside of China, the earliest texts to mention gunpowder are Roger Bacon's \"Opus Majus\" (1267) and \"Opus Tertium\" in what has been interpreted as references to firecrackers. In the early 20th century, a British artillery officer proposed that another work tentatively attributed to Bacon, \"Epistola de Secretis Operibus Artis et Naturae, et de Nullitate Magiae\", also known as \"Opus Minor\", dated to 1247, contained an encrypted formula for gunpowder hidden in the text. These claims have been disputed by science historians. In any case, the formula itself is not useful for firearms or even firecrackers, burning slowly and producing mostly smoke.\n\nThere is a record of a gun in Europe dating to 1322 being discovered in the nineteenth century but the artifact has since been lost. The earliest known European depiction of a gun appeared in 1326 in a manuscript by Walter de Milemete, although not necessarily drawn by him, known as \"De Nobilitatibus, sapientii et prudentiis regum\" (Concerning the Majesty, Wisdom, and Prudence of Kings), which displays a gun with a large arrow emerging from it and its user lowering a long stick to ignite the gun through the touchole In the same year, another similar illustration showed a darker gun being set off by a group of knights, which also featured in another work of de Milemete's, \"De secretis secretorum Aristotelis\". On 11 February of that same year, the Signoria of Florence appointed two officers to obtain \"canones de mettallo\" and ammunition for the town's defense. In the following year a document from the Turin area recorded a certain amount was paid \"for the making of a certain instrument or device made by Friar Marcello for the projection of pellets of lead.\" A reference from 1331 describes an attack mounted by two Germanic knights on Cividale del Friuli, using gunpowder weapons of some sort. The 1320s seem to have been the takeoff point for guns in Europe according to most modern military historians. Scholars suggest that the lack of gunpowder weapons in a well-traveled Venetian's catalogue for a new crusade in 1321 implies that guns were unknown in Europe up until this point, further solidifying the 1320 mark, however more evidence in this area may be forthcoming in the future.\n\nThe oldest extant cannon in Europe is a small bronze example unearthed in Loshult, Scania in southern Sweden. It dates from the early-mid 14th century, and is currently in the Swedish History Museum in Stockholm.\n\nEarly cannon in Europe often shot arrows and were known by an assortment of names such as \"pot-de-fer\", \"tonnoire\", \"ribaldis\", and \"büszenpyle\". The \"ribaldi\"s, which shot large arrows and simplistic grapeshot, were first mentioned in the English Privy Wardrobe accounts during preparations for the Battle of Crécy, between 1345 and 1346. The Florentine Giovanni Villani recounts their destructiveness, indicating that by the end of the battle, \"the whole plain was covered by men struck down by arrows and cannon balls.\" Similar cannon were also used at the Siege of Calais (1346–47), although it was not until the 1380s that the \"ribaudekin\" clearly became mounted on wheels.\n\nThe battle of Crecy which pitted the English against the French in 1346 featured the early use of cannon which helped the long-bowmen repulse a large force of Genoese crossbowmen deployed by the French. The English originally intended to use the cannon against cavalry sent to attack their archers, thinking that the loud noises produced by their cannon would panic the advancing horses along with killing the knights atop them.\n\nEarly cannon could also be used for more than simply killing men and scaring horses. English cannon were used defensively during the siege of the castle Breteuil to launch fire onto an advancing belfry. In this way cannon could be used to burn down siege equipment before it reached the fortifications. The use of cannon to shoot fire could also be used offensively as another battle involved the setting of a castle ablaze with similar methods. The particular incendiary used in these cannon was most likely a gunpowder mixture. This is one area where early Chinese and European cannon share a similarity as both were possibly used to shoot fire.\n\nAnother aspect of early European cannon is that they were rather small, dwarfed by the bombards which would come later. In fact, it is possible that the cannon used at Crecy were capable of being moved rather quickly as there is an anonymous chronicle that notes the guns being used to attack the French camp, indicating that they would have been mobile enough press the attack. These smaller cannon would eventually give way to larger, wall breaching guns by the end of the 1300s.\n\nDocumentary evidence of cannon in Russia does not appear until 1382 and they were used only in sieges, often by the defenders. It was not until 1475 when Ivan III established the first Russian cannon foundry in Moscow that they began to produce cannon natively.\n\nLater on large cannon were known as bombards, ranging from three to five feet in length and were used by Dubrovnik and Kotor in defence during the later 14th century. The first bombards were made of iron, but bronze became more prevalent as it was recognized as more stable and capable of propelling stones weighing as much as . Around the same period, the Byzantine Empire began to accumulate its own cannon to face the Ottoman Empire, starting with medium-sized cannon long and of 10 in calibre. The earliest reliable recorded use of artillery in the region was against the Ottoman siege of Constantinople in 1396, forcing the Ottomans to withdraw. The Ottomans acquired their own cannon and laid siege to the Byzantine capital again in 1422. By 1453, the Ottomans used 68 Hungarian-made cannon for the 55-day bombardment of the walls of Constantinople, \"hurling the pieces everywhere and killing those who happened to be nearby.\" The largest of their cannon was the Great Turkish Bombard, which required an operating crew of 200 men and 70 oxen, and 10,000 men to transport it. Gunpowder made the formerly devastating Greek fire obsolete, and with the final fall of Constantinople—which was protected by what were once the strongest walls in Europe—on 29 May 1453, \"it was the end of an era in more ways than one.\"\n\nWhile previous smaller guns could burn down structures with fire, larger cannon were so effective that engineers were forced to develop stronger castle walls to prevent their keeps from falling. This isn't to say that cannon were only used to batter down walls as fortifications began using cannon as defensive instruments such as an example in India where the fort of Raicher had gun ports built into its walls to accommodate the use of defensive cannon. In \"Art of War\" Niccolò Machiavelli opined that field artillery forced an army to take up a defensive posture and this opposed a more ideal offensive stance. Machiavelli's concerns can be seen in the criticisms of Portuguese mortars being used in India during the sixteenth century as lack of mobility was one of the key problems with the design. In Russia the early cannon were again placed in forts as a defensive tool. Cannon were also difficult to move around in certain types of terrain with mountains providing a great obstacle for them, for these reasons offensives conducted with cannon would be difficult to pull off in places such as Iran.\n\nBy the 16th century, cannon were made in a great variety of lengths and bore diameters, but the general rule was that the longer the barrel, the longer the range. Some cannon made during this time had barrels exceeding in length, and could weigh up to . Consequently, large amounts of gunpowder were needed to allow them to fire stone balls several hundred yards. By mid-century, European monarchs began to classify cannon to reduce the confusion. Henry II of France opted for six sizes of cannon, but others settled for more; the Spanish used twelve sizes, and the English sixteen. Better powder had been developed by this time as well. Instead of the finely ground powder used by the first bombards, powder was replaced by a \"corned\" variety of coarse grains. This coarse powder had pockets of air between grains, allowing fire to travel through and ignite the entire charge quickly and uniformly.\n\nThe end of the Middle Ages saw the construction of larger, more powerful cannon, as well as their spread throughout the world. As they were not effective at breaching the newer fortifications resulting from the development of cannon, siege engines—such as siege towers and trebuchets—became less widely used. However, wooden \"battery-towers\" took on a similar role as siege towers in the gunpowder age—such as that used at Siege of Kazan in 1552, which could hold ten large-calibre cannon, in addition to 50 lighter pieces. Another notable effect of cannon on warfare during this period was the change in conventional fortifications. Niccolò Machiavelli wrote, \"There is no wall, whatever its thickness that artillery will not destroy in only a few days.\" Although castles were not immediately made obsolete by cannon, their use and importance on the battlefield rapidly declined. Instead of majestic towers and merlons, the walls of new fortresses were thick, angled, and sloped, while towers became low and stout; increasing use was also made of earth and brick in breastworks and redoubts. These new defences became known as bastion forts, after their characteristic shape which attempted to force any advance towards it directly into the firing line of the guns. A few of these featured cannon batteries, such as the House of Tudor's Device Forts, in England. Bastion forts soon replaced castles in Europe, and, eventually, those in the Americas, as well.\n\nBy the end of the 15th century, several technological advancements made cannon more mobile. Wheeled gun carriages and trunnions became common, and the invention of the limber further facilitated transportation. As a result, field artillery became more viable, and began to see more widespread use, often alongside the larger cannon intended for sieges. Better gunpowder, cast-iron projectiles (replacing stone), and the standardisation of calibres meant that even relatively light cannon could be deadly. In \"The Art of War\", Niccolò Machiavelli observed that \"It is true that the arquebuses and the small artillery do much more harm than the heavy artillery.\" This was the case at the Battle of Flodden, in 1513: the English field guns outfired the Scottish siege artillery, firing two or three times as many rounds. Despite the increased maneuverability, however, cannon were still the slowest component of the army: a heavy English cannon required 23 horses to transport, while a culverin needed nine. Even with this many animals pulling, they still moved at a walking pace. Due to their relatively slow speed, and lack of organisation, and undeveloped tactics, the combination of pike and shot still dominated the battlefields of Europe.\n\nInnovations continued, notably the German invention of the mortar, a thick-walled, short-barrelled gun that blasted shot upward at a steep angle. Mortars were useful for sieges, as they could hit targets behind walls or other defences. This cannon found more use with the Dutch, who learnt to shoot bombs filled with powder from them. Setting the bomb fuse was a problem. \"Single firing\" was first used to ignite the fuse, where the bomb was placed with the fuse down against the cannon's propellant. This often resulted in the fuse being blown into the bomb, causing it to blow up as it left the mortar. Because of this, \"double firing\" was tried where the gunner lit the fuse and then the touch hole. This, however, required considerable skill and timing, and was especially dangerous if the gun misfired, leaving a lighted bomb in the barrel. Not until 1650 was it accidentally discovered that double-lighting was superfluous as the heat of firing would light the fuse.\n\nGustavus Adolphus of Sweden emphasised the use of light cannon and mobility in his army, and created new formations and tactics that revolutionised artillery. He discontinued using all 12 pounder—or heavier—cannon as field artillery, preferring, instead, to use cannon that could be handled by only a few men. One obsolete type of gun, the \"leatheren\" was replaced by 4 pounder and 9 pounder demi-culverins. These could be operated by three men, and pulled by only two horses. Adolphus's army was also the first to use a cartridge that contained both powder and shot which sped up reloading, increasing the rate of fire. Finally, against infantry he pioneered the use of canister shot – essentially a tin can filled with musket balls. Until then there was no more than one cannon for every thousand infantrymen on the battlefield but Gustavus Adolphus increased the number of cannon sixfold. Each regiment was assigned two pieces, though he often arranged them into batteries instead of distributing them piecemeal. He used these batteries to break his opponent's infantry line, while his cavalry would outflank their heavy guns.\n\nAt the Battle of Breitenfeld, in 1631, Adolphus proved the effectiveness of the changes made to his army, by defeating Johann Tserclaes, Count of Tilly. Although severely outnumbered, the Swedes were able to fire between three and five times as many volleys of artillery, and their infantry's linear formations helped ensure they didn't lose any ground. Battered by cannon fire, and low on morale, Tilly's men broke ranks and fled.\n\nIn England cannon were being used to besiege various fortified buildings during the English Civil War. Nathaniel Nye is recorded as testing a Birmingham cannon in 1643 and experimenting with a saker in 1645. From 1645 he was the master gunner to the Parliamentarian garrison at Evesham and in 1646 he successfully directed the artillery at the Siege of Worcester, detailing his experiences and in his 1647 book \"The Art of Gunnery\". Believing that war was as much a science as an art, his explanations focused on triangulation, arithmetic, theoretical mathematics, and cartography as well as practical considerations such as the ideal specification for gunpowder or slow matches. His book acknowledged mathematicians such as Robert Recorde and Marcus Jordanus as well as earlier military writers on artillery such as Niccolò Fontana Tartaglia and Thomas (or Francis) Malthus (author of \"A Treatise on Artificial Fire-Works\").\n\nAround this time also came the idea of aiming the cannon to hit a target. Gunners controlled the range of their cannon by measuring the angle of elevation, using a \"gunner's quadrant.\" Cannon did not have sights, therefore, even with measuring tools, aiming was still largely guesswork.\n\nIn the latter half of the 17th century, the French engineer Sébastien Le Prestre de Vauban introduced a more systematic and scientific approach to attacking gunpowder fortresses, in a time when many field commanders \"were notorious dunces in siegecraft.\" Careful sapping forward, supported by enfilading ricochets, was a key feature of this system, and it even allowed Vauban to calculate the length of time a siege would take. He was also a prolific builder of bastion forts, and did much to popularize the idea of \"depth in defence\" in the face of cannon. These principles were followed into the mid-19th century, when changes in armaments necessitated greater depth defence than Vauban had provided for. It was only in the years prior to World War I that new works began to break radically away from his designs.\n\nThe lower tier of 17th-century English ships of the line were usually equipped with demi-cannon, guns that fired a solid shot, and could weigh up to . Demi-cannon were capable of firing these heavy metal balls with such force that they could penetrate more than a metre of solid oak, from a distance of , and could dismast even the largest ships at close range. Full cannon fired a shot, but were discontinued by the 18th century, as they were too unwieldy. By the end of the 18th century, principles long adopted in Europe specified the characteristics of the Royal Navy's cannon, as well as the acceptable defects, and their severity. The United States Navy tested guns by measuring them, firing them two or three times—termed \"proof by powder\"—and using pressurized water to detect leaks.\nThe carronade was adopted by the Royal Navy in 1779; the lower muzzle velocity of the round shot when fired from this cannon was intended to create more wooden splinters when hitting the structure of an enemy vessel, as they were believed to be more deadly than the ball by itself. The carronade was much shorter, and weighed between a third to a quarter of the equivalent long gun; for example, a 32-pounder carronade weighed less than a ton, compared with a 32-pounder long gun, which weighed over 3 tons. The guns were, therefore, easier to handle, and also required less than half as much gunpowder, allowing fewer men to crew them. Carronades were manufactured in the usual naval gun calibres, but were not counted in a ship of the line's rated number of guns. As a result, the classification of Royal Navy vessels in this period can be misleading, as they often carried more cannon than were listed.\n\nCannon were crucial in Napoleon's rise to power, and continued to play an important role in his army in later years. During the French Revolution, the unpopularity of the Directory led to riots and rebellions. When over 25,000 royalists led by General Danican assaulted Paris, Paul Barras was appointed to defend the capital; outnumbered five to one and disorganised, the Republicans were desperate. When Napoleon arrived, he reorganised the defences but realised that without cannon the city could not be held. He ordered Joachim Murat to bring the guns from the Sablons artillery park; the Major and his cavalry fought their way to the recently captured cannon, and brought them back to Napoleon. When Danican's poorly trained men attacked, on 13 Vendémiaire, 1795 – 5 October 1795, in the calendar used in France at the time — Napoleon ordered his cannon to fire grapeshot into the mob, an act that became known as the \"whiff of grapeshot\". The slaughter effectively ended the threat to the new government, while, at the same time, made Bonaparte a famous—and popular—public figure. Among the first generals to recognise that artillery was not being used to its full potential, Napoleon often massed his cannon into batteries and introduced several changes into the French artillery, improving it significantly and making it among the finest in Europe. Such tactics were successfully used by the French, for example, at the Battle of Friedland, when sixty-six guns fired a total of 3,000 roundshot and 500 rounds of grapeshot, inflicting severe casualties to the Russian forces, whose losses numbered over 20,000 killed and wounded, in total. At the Battle of Waterloo—Napoleon's final battle—the French army had many more artillery pieces than either the British or Prussians. As the battlefield was muddy, recoil caused cannon to bury themselves into the ground after firing, resulting in slow rates of fire, as more effort was required to move them back into an adequate firing position; also, roundshot did not ricochet with as much force from the wet earth. Despite the drawbacks, sustained artillery fire proved deadly during the engagement, especially during the French cavalry attack. The British infantry, having formed infantry squares, took heavy losses from the French guns, while their own cannon fired at the cuirassiers and lancers, when they fell back to regroup. Eventually, the French ceased their assault, after taking heavy losses from the British cannon and musket fire.\n\nIn the 1810s and 1820s, greater emphasis was placed on the accuracy of long-range gunfire, and less on the weight of a broadside. The carronade, although initially very successful and widely adopted, disappeared from the Royal Navy in the 1850s after the development of wrought-iron-jacketed steel cannon by William Armstrong and Joseph Whitworth. Nevertheless, carronades were used in the American Civil War.\n\nWestern cannon during the 19th century became larger, more destructive, more accurate, and could fire at longer range. One example is the American wrought-iron, muzzle-loading rifle, or Griffen gun (usually called the 3-inch Ordnance Rifle), used during the American Civil War, which had an effective range of over . Another is the smoothbore 12-pounder Napoleon, which originated in France in 1853 and was widely used by both sides in the American Civil War. This cannon was renowned for its sturdiness, reliability, firepower, flexibility, relatively lightweight, and range of .\nThe practice of rifling—casting spiralling lines inside the cannon's barrel—was applied to artillery more frequently by 1855, as it gave cannon projectiles gyroscopic stability, which improved their accuracy. One of the earliest rifled cannon was the breech-loading Armstrong Gun—also invented by William Armstrong—which boasted significantly improved range, accuracy, and power than earlier weapons. The projectile fired from the Armstrong gun could reportedly pierce through a ship's side, and explode inside the enemy vessel, causing increased damage, and casualties. The British military adopted the Armstrong gun, and was impressed; the Duke of Cambridge even declared that it \"could do everything but speak.\" Despite being significantly more advanced than its predecessors, the Armstrong gun was rejected soon after its integration, in favour of the muzzle-loading pieces that had been in use before. While both types of gun were effective against wooden ships, neither had the capability to pierce the armour of ironclads; due to reports of slight problems with the breeches of the Armstrong gun, and their higher cost, the older muzzle-loaders were selected to remain in service instead. Realising that iron was more difficult to pierce with breech-loaded cannon, Armstrong designed rifled muzzle-loading guns, which proved successful; \"The Times\" reported: \"even the fondest believers in the invulnerability of our present ironclads were obliged to confess that against such artillery, at such ranges, their plates and sides were almost as penetrable as wooden ships.\"\n\nThe superior cannon of the Western world brought them tremendous advantages in warfare. For example, in the First Opium War in China, during the 19th century, British battleships bombarded the coastal areas and fortifications from afar, safe from the reach of the Chinese cannon. Similarly, the shortest war in recorded history, the Anglo-Zanzibar War of 1896, was brought to a swift conclusion by shelling from British cruisers. The cynical attitude towards recruited infantry in the face of ever more powerful field artillery is the source of the term \"cannon fodder\", first used by François-René de Chateaubriand, in 1814; however, the concept of regarding soldiers as nothing more than \"food for powder\" was mentioned by William Shakespeare as early as 1598, in Henry IV, Part 1.\n\nCannon in the 20th and 21st centuries are usually divided into sub-categories and given separate names. Some of the most widely used types of modern cannon are howitzers, mortars, guns, and autocannon, although a few very large-calibre cannon, custom-designed, have also been constructed. Nuclear artillery was experimented with, but was abandoned as impractical. Modern artillery is used in a variety of roles, depending on its type. According to NATO, the general role of artillery is to provide fire support, which is defined as \"the application of fire, coordinated with the manoeuvre of forces to destroy, neutralize, or suppress the enemy.\"\n\nWhen referring to cannon, the term \"gun\" is often used incorrectly. In military usage, a gun is a cannon with a high muzzle velocity and a flat trajectory, useful for hitting the sides of targets such as walls, as opposed to howitzers or mortars, which have lower muzzle velocities, and fire indirectly, lobbing shells up and over obstacles to hit the target from above.\n\nBy the early 20th century, infantry weapons had become more powerful, forcing most artillery away from the front lines. Despite the change to indirect fire, cannon proved highly effective during World War I, directly or indirectly causing over 75% of casualties. The onset of trench warfare after the first few months of World War I greatly increased the demand for howitzers, as they were more suited at hitting targets in trenches. Furthermore, their shells carried more explosives than those of guns, and caused considerably less barrel wear. The German army had the advantage here as they began the war with many more howitzers than the French. World War I also saw the use of the Paris Gun, the longest-ranged gun ever fired. This calibre gun was used by the Germans against Paris and could hit targets more than away.\nThe Second World War sparked new developments in cannon technology. Among them were sabot rounds, hollow-charge projectiles, and proximity fuses, all of which increased the effectiveness of cannon against specific target. The proximity fuse emerged on the battlefields of Europe in late December 1944. Used to great effect in anti-aircraft projectiles, proximity fuses were fielded in both the European and Pacific Theatres of Operations; they were particularly useful against V-1 flying bombs and kamikaze planes. Although widely used in naval warfare, and in anti-air guns, both the British and Americans feared unexploded proximity fuses would be reverse engineered leading to them limiting its use in continental battles. During the Battle of the Bulge, however, the fuses became known as the American artillery's \"Christmas present\" for the German army because of their effectiveness against German personnel in the open, when they frequently dispersed attacks. Anti-tank guns were also tremendously improved during the war: in 1939, the British used primarily 2 pounder and 6 pounder guns. By the end of the war, 17 pounders had proven much more effective against German tanks, and 32 pounders had entered development. Meanwhile, German tanks were continuously upgraded with better main guns, in addition to other improvements. For example, the Panzer III was originally designed with a 37 mm gun, but was mass-produced with a 50 mm cannon. To counter the threat of the Russian T-34s, another, more powerful 50 mm gun was introduced, only to give way to a larger 75 mm cannon, which was in a fixed mount as the StuG III, the most-produced German World War II armoured fighting vehicle of any type. Despite the improved guns, production of the Panzer III was ended in 1943, as the tank still could not match the T-34, and was replaced by the Panzer IV and Panther tanks. In 1944, the 8.8 cm KwK 43 and many variations, entered service with the Wehrmacht, and was used as both a tank main gun, and as the PaK 43 anti-tank gun. One of the most powerful guns to see service in World War II, it was capable of destroying any Allied tank at very long ranges.\nDespite being designed to fire at trajectories with a steep angle of descent, howitzers can be fired directly, as was done by the 11th Marine Regiment at the Battle of Chosin Reservoir, during the Korean War. Two field batteries fired directly upon a battalion of Chinese infantry; the Marines were forced to brace themselves against their howitzers, as they had no time to dig them in. The Chinese infantry took heavy casualties, and were forced to retreat.\nThe tendency to create larger calibre cannon during the World Wars has reversed since. The United States Army, for example, sought a lighter, more versatile howitzer, to replace their ageing pieces. As it could be towed, the M198 was selected to be the successor to the World War II–era cannon used at the time, and entered service in 1979. Still in use today, the M198 is, in turn, being slowly replaced by the M777 Ultralightweight howitzer, which weighs nearly half as much and can be more easily moved. Although land-based artillery such as the M198 are powerful, long-ranged, and accurate, naval guns have not been neglected, despite being much smaller than in the past, and, in some cases, having been replaced by cruise missiles. However, the 's planned armament includes the Advanced Gun System (AGS), a pair of 155 mm guns, which fire the Long Range Land-Attack Projectile. The warhead, which weighs , has a circular error of probability of , and will be mounted on a rocket, to increase the effective range to , further than that of the Paris Gun. The AGS's barrels will be water cooled, and will fire 10 rounds per minute, per gun. The combined firepower from both turrets will give a \"Zumwalt\"-class destroyer the firepower equivalent to 18 conventional M198 howitzers. The reason for the re-integration of cannon as a main armament in United States Navy ships is because satellite-guided munitions fired from a gun are less expensive than a cruise missile but have a similar guidance capability.\n\nAutocannons have an automatic firing mode, similar to that of a machine gun. They have mechanisms to automatically load their ammunition, and therefore have a higher rate of fire than artillery, often approaching, or, in the case of rotary autocannons, even surpassing the firing rate of a machine gun. While there is no minimum bore for autocannons, they are generally larger than machine guns, typically 20 mm or greater since World War II and are usually capable of using explosive ammunition even if it isn't always used. Machine guns in contrast are usually too small to use explosive ammunition.\n\nMost nations use rapid-fire cannon on light vehicles, replacing a more powerful, but heavier, tank gun. A typical autocannon is the 25 mm \"Bushmaster\" chain gun, mounted on the LAV-25 and M2 Bradley armoured vehicles. Autocannons may be capable of a very high rate of fire, but ammunition is heavy and bulky, limiting the amount carried. For this reason, both the 25 mm Bushmaster and the 30 mm RARDEN are deliberately designed with relatively low rates of fire. The typical rate of fire for a modern autocannon ranges from 90 to 1,800 rounds per minute. Systems with multiple barrels, such as a rotary autocannon, can have rates of fire of more than several thousand rounds per minute. The fastest of these is the GSh-6-23, which has a rate of fire of over 10,000 rounds per minute.\n\nAutocannons are often found in aircraft, where they replaced machine guns and as shipboard anti-aircraft weapons, as they provide greater destructive power than machine guns.\n\nThe first documented installation of a cannon on an aircraft was on the Voisin Canon in 1911, displayed at the Paris Exposition that year.\nBy World War I, all of the major powers were experimenting with aircraft mounted cannon; however their low rate of fire and great size and weight precluded any of them from being anything other than experimental. The most successful (or least unsuccessful) was the SPAD 12 Ca.1 with a single 37mm Puteaux mounted to fire between the cylinder banks and through the propeller boss of the aircraft's Hispano-Suiza 8C. The pilot (by necessity an ace) had to manually reload each round.\n\nThe first autocannon were developed during World War I as anti-aircraft guns, and one of these – the Coventry Ordnance Works \"COW 37 mm gun\" was installed in an aircraft but the war ended before it could be given a field trial and never became standard equipment in a production aircraft. Later trials had it fixed at a steep angle upwards in both the Vickers Type 161 and the Westland C.O.W. Gun Fighter, an idea that would return later.\n\nDuring this period autocannons became available and several fighters of the German \"Luftwaffe\" and the Imperial Japanese Navy Air Service were fitted with 20mm cannon. They continued to be installed as an adjunct to machine guns rather than as a replacement, as the rate of fire was still too low and the complete installation too heavy. There was a some debate in the RAF as to whether the greater number of possible rounds being fired from a machine gun, or a smaller number of explosive rounds from a cannon was preferable. Improvements during the war in regards to rate of fire allowed the cannon to displace the machine gun almost entirely. The cannon was more effective against armour so they were increasingly used during the course of World War II, and newer fighters such as the Hawker Tempest usually carried two or four versus the six .50 Browning machine guns for US aircraft or eight to twelve M1919 Browning machine guns on earlier British aircraft. The Hispano-Suiza HS.404, Oerlikon 20 mm cannon, MG FF, and their numerous variants became among the most widely used autocannon in the war. Cannon, as with machine guns, were generally fixed to fire forwards (mounted in the wings, in the nose or fuselage, or in a pannier under either); or were mounted in gun turrets on heavier aircraft. Both the Germans and Japanese mounted cannon to fire upwards and forwards for use against heavy bombers, with the Germans calling guns so-installed \"Schräge Musik\". Schräge Musik derives from the German colloquialism for Jazz Music (the German word schräg means slanted or oblique)\nPreceding the Vietnam War the high speeds aircraft were attaining led to a move to remove the cannon due to the mistaken belief that they would be useless in a dogfight, but combat experience during the Vietnam War showed conclusively that despite advances in missiles, there was still a need for them. Nearly all modern fighter aircraft are armed with an autocannon and they are also commonly found on ground-attack aircraft. One of the most powerful examples is the 30mm GAU-8/A Avenger Gatling-type rotary cannon, mounted exclusively on the Fairchild Republic A-10 Thunderbolt II. The Lockheed AC-130 gunship (a converted transport) can carry a 105mm howitzer as well as a variety of autocannons ranging up to 40mm. Both are used in the close air support role.\n\nCannon in general have the form of a truncated cone with an internal cylindrical bore for holding an explosive charge and a projectile. The thickest, strongest, and closed part of the cone is located near the explosive charge. As any explosive charge will dissipate in all directions equally, the thickest portion of the cannon is useful for containing and directing this force. The backward motion of the cannon as its projectile leaves the bore is termed its recoil and the effectiveness of the cannon can be measured in terms of how much this response can be diminished, though obviously diminishing recoil through increasing the overall mass of the cannon means decreased mobility.\n\nField artillery cannon in Europe and the Americas were initially made most often of bronze, though later forms were constructed of cast iron and eventually steel. Bronze has several characteristics that made it preferable as a construction material: although it is relatively expensive, does not always alloy well, and can result in a final product that is \"spongy about the bore\", bronze is more flexible than iron and therefore less prone to bursting when exposed to high pressure; cast iron cannon are less expensive and more durable generally than bronze and withstand being fired more times without deteriorating. However, cast iron cannon have a tendency to burst without having shown any previous weakness or wear, and this makes them more dangerous to operate.\n\nThe older and more-stable forms of cannon were muzzle-loading as opposed to breech-loading— in order to be used they had to have their ordnance packed down the bore through the muzzle rather than inserted through the breech.\n\nThe following terms refer to the components or aspects of a classical western cannon (c. 1850) as illustrated here. In what follows, the words \"near\", \"close\", and \"behind\" will refer to those parts towards the thick, closed end of the piece, and \"far\", \"front\", \"in front of\", and \"before\" to the thinner, open end.\n\nBULLET::::- Bore: The hollow cylinder bored down the centre of the cannon, including the \"base of the bore\" or \"bottom of the bore\", the nearest end of the bore into which the ordnance (wadding, shot, etc.) gets packed. The diameter of the bore represents the cannon's calibre.\nBULLET::::- Chamber: The cylindrical, conical, or spherical recess at the nearest end of the bottom of the bore into which the gunpowder is packed.\nBULLET::::- Vent: A thin tube on the near end of the cannon connecting the explosive charge inside with an ignition source outside and often filled with a length of fuse; always located near the \"breech\". Sometimes called the \"fuse hole\" or the \"touch hole\". On the top of the vent on the outside of the cannon is a flat circular space called the \"vent field\" where the charge is lit. If the cannon is bronze, it will often have a \"vent piece\" made of copper screwed into the length of the vent.\n\nThe main body of a cannon consists of three basic extensions: the foremost and the longest is called the \"chase\", the middle portion is the \"reinforce\", and the closest and briefest portion is the \"cascabel\" or \"cascable\".\nBULLET::::- The chase: Simply the entire conical part of the cannon in front of the \"reinforce\". It is the longest portion of the cannon, and includes the following elements:\nBULLET::::- The \"neck\": the narrowest part of the chase, always located near the foremost end of the piece.\nBULLET::::- The \"muzzle\": the portion of the chase forward of the \"neck\". It includes the following:\nBULLET::::- The \"swell of the muzzle\" refers to the slight swell in the diameter of the piece at the very end of the chase. It is often chamfered on the inside to make loading the cannon easier. In some guns, this element is replaced with a wide ring and is called a \"muzzle band\".\nBULLET::::- The \"face\" is the flat vertical plane at the foremost edge of the muzzle (and of the entire piece).\nBULLET::::- The \"muzzle mouldings\" are the tiered rings which connect the face with the rest of the muzzle, the first of which is called the \"lip\" and the second the \"fillet\"\nBULLET::::- The \"muzzle astragal and fillets\" are a series of three narrow rings running around the outside of the chase just behind the neck. Sometimes also collectively called the \"chase ring\".\nBULLET::::- The \"chase astragal and fillets\": these are a second series of such rings located at the near end of the chase.\nBULLET::::- The \"chase girdle\": this is the brief length of the chase between the chase astragal and fillets and the \"reinforce\".\nBULLET::::- The reinforce: This portion of the piece is frequently divided into a \"first reinforce\" and a \"second reinforce\", but in any case is marked as separate from the chase by the presence of a narrow circular \"reinforce ring\" or \"band\" at its foremost end. The span of the reinforce also includes the following:\nBULLET::::- The \"trunnions\" are located at the foremost end of the reinforce just behind the reinforce ring. They consist of two cylinders perpendicular to the bore and below it which are used to mount the cannon on its carriage.\nBULLET::::- The \"rimbases\" are short broad rings located at the union of the trunnions and the cannon which provide support to the carriage attachment.\nBULLET::::- The \"reinforce band\" is only present if the cannon has two reinforces, and it divides the first reinforce from the second.\nBULLET::::- The \"breech\" refers to the mass of solid metal behind the bottom of the bore extending to the \"base of the breech\" and including the \"base ring\"; it also generally refers to the end of the cannon opposite the \"muzzle\", i.e., the location where the explosion of the gunpowder begins as opposed to the opening through which the pressurized gas escapes.\nBULLET::::- The \"base ring\" forms a ring at the widest part of the entire cannon at the nearest end of the reinforce just before the \"cascabel\".\nBULLET::::- The cascabel: This is that portion of the cannon behind the reinforce(s) and behind the \"base ring\". It includes the following:\nBULLET::::- The \"knob\" which is the small spherical terminus of the piece;\nBULLET::::- The \"neck\", a short, narrow piece of metal holding out the knob; and\nBULLET::::- The \"fillet\", the tiered disk connecting the neck of the cascabel to the \"base of the breech\".\nBULLET::::- The \"base of the breech\" is the metal disk that forms the most forward part of the cascabel and rests against the breech itself, right next to the \"base ring\".\n\nTo pack a muzzle-loading cannon, first gunpowder is poured down the bore. This is followed by a layer of wadding (often nothing more than paper), and then the cannonball itself. A certain amount of windage allows the ball to fit down the bore, though the greater the windage the less efficient the propulsion of the ball when the gunpowder is ignited. To fire the cannon, the fuse located in the vent is lit, quickly burning down to the gunpowder, which then explodes violently, propelling wadding and ball down the bore and out of the muzzle. A small portion of exploding gas also escapes through the vent, but this does not dramatically affect the total force exerted on the ball.\n\nAny large, smoothbore, muzzle-loading gun—used before the advent of breech-loading, rifled guns—may be referred to as a cannon, though once standardised names were assigned to different-sized cannon, the term specifically referred to a gun designed to fire a shot, as distinct from a demi-cannon – , culverin – , or demi-culverin – . \"Gun\" specifically refers to a type of cannon that fires projectiles at high speeds, and usually at relatively low angles; they have been used in warships, and as field artillery. The term \"cannon\" is also used for autocannon, a modern repeating weapon firing explosive projectiles. Cannon have been used extensively in fighter aircraft since World War II.\n\nIn the 1770s, cannon operation worked as follows: each cannon would be manned by two gunners, six soldiers, and four officers of artillery. The right gunner was to prime the piece and load it with powder, and the left gunner would fetch the powder from the magazine and be ready to fire the cannon at the officer's command. On each side of the cannon, three soldiers stood, to ram and sponge the cannon, and hold the ladle. The second soldier on the left tasked with providing 50 bullets.\n\nBefore loading, the cannon would be cleaned with a wet sponge to extinguish any smouldering material from the last shot. Fresh powder could be set off prematurely by lingering ignition sources. The powder was added, followed by wadding of paper or hay, and the ball was placed in and rammed down. After ramming, the cannon would be aimed with the elevation set using a quadrant and a plummet. At 45 degrees, the ball had the utmost range: about ten times the gun's level range. Any angle above a horizontal line was called random-shot. Wet sponges were used to cool the pieces every ten or twelve rounds.\n\nDuring the Napoleonic Wars, a British gun team consisted of five gunners to aim it, clean the bore with a damp sponge to quench any remaining embers before a fresh charge was introduced, and another to load the gun with a bag of powder and then the projectile. The fourth gunner pressed his thumb on the vent hole, to prevent a draught that might fan a flame. The charge loaded, the fourth would prick the bagged charge through the vent hole, and fill the vent with powder. On command, the fifth gunner would fire the piece with a slow match. Friction primers replaced slow match ignition by the mid-19th century.\n\nWhen a cannon had to be abandoned such as in a retreat or surrender, the touch hole of the cannon would be plugged flush with an iron spike, disabling the cannon (at least until metal boring tools could be used to remove the plug). This was called \"spiking the cannon\".\n\nA gun was said to be \"honeycombed\" when the surface of the bore had cavities, or holes in it, caused either by corrosion or casting defects.\n\nHistorically, logs or poles have been used as decoys to mislead the enemy as to the strength of an emplacement. The \"Quaker Gun trick\" was used by Colonel William Washington's Continental Army during the American Revolutionary War; in 1780, approximately 100 Loyalists surrendered to them, rather than face bombardment. During the American Civil War, Quaker guns were also used by the Confederates, to compensate for their shortage of artillery. The decoy cannon were painted black at the \"muzzle\", and positioned behind fortifications to delay Union attacks on those positions. On occasion, real gun carriages were used to complete the deception.\n\nCannon sounds have sometimes been used in classical pieces with a military theme. One of the best known examples of such a piece is Pyotr Ilyich Tchaikovsky's \"1812 Overture\". The overture is to be performed using an artillery section together with the orchestra, resulting in noise levels high enough that musicians are required to wear ear protection. The cannon fire simulates Russian artillery bombardments of the Battle of Borodino, a critical battle in Napoleon's invasion of Russia, whose defeat the piece celebrates. When the overture was first performed, the cannon were fired by an electric current triggered by the conductor. However, the overture was not recorded with real cannon fire until Mercury Records and conductor Antal Doráti's 1958 recording of the Minnesota Orchestra. Cannon fire is also frequently used annually in presentations of the \"1812\" on the American Independence Day, a tradition started by Arthur Fiedler of the Boston Pops in 1974.\n\nThe hard rock band AC/DC also used cannon in their song \"For Those About to Rock (We Salute You)\", and in live shows replica Napoleonic cannon and pyrotechnics were used to perform the piece.\n\nCannons recovered from the sea are often extensively damaged from exposure to salt water; because of this, electrolytic reduction treatment is required to forestall the process of corrosion. The cannon is then washed in deionized water to remove the electrolyte, and is treated in tannic acid, which prevents further rust and gives the metal a bluish-black colour. After this process, cannon on display may be protected from oxygen and moisture by a wax sealant. A coat of polyurethane may also be painted over the wax sealant, to prevent the wax-coated cannon from attracting dust in outdoor displays. In 2011, archaeologists say six cannon recovered from a river in Panama that could have belonged to legendary pirate Henry Morgan are being studied and could eventually be displayed after going through a restoration process.\n\nBULLET::::- .\nBULLET::::- .\nBULLET::::- .\nBULLET::::- .\nBULLET::::- .\nBULLET::::- .\nBULLET::::- Hadden, R. Lee. 2005. \"Confederate Boys and Peter Monkeys.\" Armchair General. January 2005. Adapted from a talk given to the Geological Society of America on March 25, 2004.\nBULLET::::- .\nBULLET::::- .\nBULLET::::- .\nBULLET::::- .\nBULLET::::- .\nBULLET::::- Schmidtchen, Volker (1977a), \"Riesengeschütze des 15. Jahrhunderts. Technische Höchstleistungen ihrer Zeit\", \"Technikgeschichte\" 44 (2): 153–173 (153–157)\nBULLET::::- Schmidtchen, Volker (1977b), \"Riesengeschütze des 15. Jahrhunderts. Technische Höchstleistungen ihrer Zeit\", \"Technikgeschichte\" 44 (3): 213–237 (226–228)\nBULLET::::- .\nBULLET::::- .\n\nBULLET::::- Artillery Tactics and Combat during the Napoleonic Wars\nBULLET::::- Handgonnes and Matchlocks – History of firearms to 1500\nBULLET::::- – \"Patent for a Casting ordnance\"\nBULLET::::- – \"Cannon patent\"\nBULLET::::- – \"Muzzle loading ordnance patent\"\nBULLET::::- Historic Cannons Of San Francisco\n"}
{"id": "7056", "url": "https://en.wikipedia.org/wiki?curid=7056", "title": "Computer mouse", "text": "Computer mouse\n\nA computer mouse (plural mice or mouses) is a hand-held pointing device that detects two-dimensional motion relative to a surface. This motion is typically translated into the motion of a pointer on a display, which allows a smooth control of the graphical user interface. The first public demonstration of a mouse controlling a computer system was in 1968. Originally wired to a computer, many modern mice are cordless, relying on short-range radio communication with the connected system. \n\nMice originally used a ball rolling on a surface to detect motion, but modern mice often have optical sensors that have no moving parts. In addition to moving a cursor, computer mice have one or more buttons to allow operations such as selection of a menu item on a display. Mice often also feature other elements, such as touch surfaces and \"wheels\", which enable additional control and dimensional input.\n\nThe earliest known publication of the term \"mouse\" as referring to a computer pointing device is in Bill English's July 1965 publication, \"Computer-Aided Display Control\" likely originating from its resemblance to the shape and size of a mouse, a rodent, with the cord resembling its tail.\n\nThe plural for the small rodent is always \"mice\" in modern usage. The plural of a computer mouse is either \"mouses\" or \"mice\" according to most dictionaries, with \"mice\" being more common. The first recorded plural usage is \"mice\"; the online \"Oxford Dictionaries\" cites a 1984 use, and earlier uses include J. C. R. Licklider's \"The Computer as a Communication Device\" of 1968. The term computer mouses may be used informally in some cases. Although the plural of a mouse (small rodent) is mice, the two words have undergone a differentiation through usage.\n\nThe trackball, a related pointing device, was invented in 1946 by Ralph Benjamin as part of a post-World War II-era fire-control radar plotting system called Comprehensive Display System (CDS). Benjamin was then working for the British Royal Navy Scientific Service. Benjamin's project used analog computers to calculate the future position of target aircraft based on several initial input points provided by a user with a joystick. Benjamin felt that a more elegant input device was needed and invented what they called a \"roller ball\" for this purpose.\n\nThe device was patented in 1947, but only a prototype using a metal ball rolling on two rubber-coated wheels was ever built, and the device was kept as a military secret.\n\nAnother early trackball was built by Kenyon Taylor, a British electrical engineer working in collaboration with Tom Cranston and Fred Longstaff. Taylor was part of the original Ferranti Canada, working on the Royal Canadian Navy's DATAR (Digital Automated Tracking and Resolving) system in 1952.\n\nDATAR was similar in concept to Benjamin's display. The trackball used four disks to pick up motion, two each for the X and Y directions. Several rollers provided mechanical support. When the ball was rolled, the pickup discs spun and contacts on their outer rim made periodic contact with wires, producing pulses of output with each movement of the ball. By counting the pulses, the physical movement of the ball could be determined. A digital computer calculated the tracks and sent the resulting data to other ships in a task force using pulse-code modulation radio signals. This trackball used a standard Canadian five-pin bowling ball. It was not patented, since it was a secret military project.\n\nDouglas Engelbart of the Stanford Research Institute (now SRI International) has been credited in published books by Thierry Bardini, Paul Ceruzzi, Howard Rheingold, and several others as the inventor of the computer mouse. Engelbart was also recognized as such in various obituary titles after his death in July 2013.\n\nBy 1963, Engelbart had already established a research lab at SRI, the Augmentation Research Center (ARC), to pursue his objective of developing both hardware and software computer technology to \"augment\" human intelligence. That November, while attending a conference on computer graphics in Reno, Nevada, Engelbart began to ponder how to adapt the underlying principles of the planimeter to X-Y coordinate input. On November 14, 1963, he first recorded his thoughts in his personal notebook about something he initially called a \"bug,\" which in a \"3-point\" form could have a \"drop point and 2 orthogonal wheels.\" He wrote that the \"bug\" would be \"easier\" and \"more natural\" to use, and unlike a stylus, it would stay still when let go, which meant it would be \"much better for coordination with the keyboard.\"\n\nIn 1964, Bill English joined ARC, where he helped Engelbart build the first mouse prototype. They christened the device the \"mouse\" as early models had a cord attached to the rear part of the device which looked like a tail, and in turn resembled the common mouse. As noted above, this \"mouse\" was first mentioned in print in a July 1965 report, on which English was the lead author. On 9 December 1968, Engelbart publicly demonstrated the mouse at what would come to be known as The Mother of All Demos. Engelbart never received any royalties for it, as his employer SRI held the patent, which expired before the mouse became widely used in personal computers. In any event, the invention of the mouse was just a small part of Engelbart's much larger project of augmenting human intellect.\n\nSeveral other experimental pointing-devices developed for Engelbart's oN-Line System (NLS) exploited different body movements – for example, head-mounted devices attached to the chin or nose – but ultimately the mouse won out because of its speed and convenience. The first mouse, a bulky device (pictured) used two potentiometers perpendicular to each other and connected to wheels: the rotation of each wheel translated into motion along one axis. At the time of the \"Mother of All Demos\", Engelbart's group had been using their second generation, 3-button mouse for about a year.\n\nOn October 2, 1968, a mouse device named ' (German for \"rolling ball\") was described as an optional device for its SIG-100 terminal was developed by the German company Telefunken. As the name suggests and unlike Engelbart's mouse, the Telefunken model already had a ball. It was based on an earlier trackball-like device (also named ') that was embedded into radar flight control desks. This trackball had been developed by a team led by Rainer Mallebrein at Telefunken for the German \"Bundesanstalt für Flugsicherung (Federal Air Traffic Control)\" as part of their TR 86 process computer system with its SIG 100-86 vector graphics terminal.\n\nWhen the development for the Telefunken main frame began in 1965, Mallebrein and his team came up with the idea of \"reversing\" the existing into a moveable mouse-like device, so that customers did not have to be bothered with mounting holes for the earlier trackball device. Together with light pens and trackballs, it was offered as an optional input device for their system since 1968. Some Rollkugel mouses installed at the in Munich in 1972 are well preserved in a museum. Telefunken considered the invention too unimportant to apply for a patent on it.\n\nThe Xerox Alto was one of the first computers designed for individual use in 1973 and is regarded as the first modern computer to utilize a mouse. Inspired by PARC's Alto, the Lilith, a computer which had been developed by a team around at ETH Zürich between 1978 and 1980, provided a mouse as well. The third marketed version of an integrated mouse shipped as a part of a computer and intended for personal computer navigation came with the Xerox 8010 Star in 1981.\n\nBy 1982, the Xerox 8010 was probably the best-known computer with a mouse. The Sun-1 also came with a mouse, and the forthcoming Apple Lisa was rumored to use one, but the peripheral remained obscure; Jack Hawley of The Mouse House reported that one buyer for a large organization believed at first that his company sold lab mice. Hawley, who manufactured mice for Xerox, stated that \"Practically, I have the market all to myself right now\"; a Hawley mouse cost $415. In 1982, Logitech introduced the P4 Mouse at the Comdex trade show in Las Vegas, its first hardware mouse. That same year Microsoft made the decision to make the MS-DOS program Microsoft Word mouse-compatible, and developed the first PC-compatible mouse. Microsoft's mouse shipped in 1983, thus beginning the Microsoft Hardware division of the company. However, the mouse remained relatively obscure until the appearance of the Macintosh 128K (which included an updated version of the Lisa Mouse) in 1984, and of the Amiga 1000 and the Atari ST in 1985.\n\nA mouse typically controls the motion of a pointer in two dimensions in a graphical user interface (GUI). The mouse turns movements of the hand backward and forward, left and right into equivalent electronic signals that in turn are used to move the pointer.\n\nThe relative movements of the mouse on the surface are applied to the position of the pointer on the screen, which signals the point where actions of the user take place, so hand movements are replicated by the pointer. Clicking or hovering (stopping movement while the cursor is within the bounds of an area) can select files, programs or actions from a list of names, or (in graphical interfaces) through small images called \"icons\" and other elements. For example, a text file might be represented by a picture of a paper notebook and clicking while the cursor hovers this icon might cause a text editing program to open the file in a window.\n\nDifferent ways of operating the mouse cause specific things to happen in the GUI:\nBULLET::::- Click: pressing and releasing a button.\nBULLET::::- (left) Single-click: clicking the main button.\nBULLET::::- (left) Double-click: clicking the button two times in quick succession counts as a different gesture than two separate single clicks.\nBULLET::::- (left) Triple-click: clicking the button three times in quick succession counts as a different gesture than three separate single clicks. Triple clicks are far less common in traditional navigation.\nBULLET::::- Right-click: clicking the secondary button, or clicking with two fingers. (This brings a menu with different options depending on the software)\nBULLET::::- Middle-click: clicking the tertiary button.\nBULLET::::- Drag and drop: pressing and holding a button, then moving the mouse without releasing. (Using the command \"drag with the right mouse button\" instead of just \"drag\" when one instructs a user to drag an object while holding the right mouse button down instead of the more commonly used left mouse button.)\nBULLET::::- Mouse button chording (a.k.a. Rocker navigation).\nBULLET::::- Combination of right-click then left-click.\nBULLET::::- Combination of left-click then right-click or keyboard letter.\nBULLET::::- Combination of left or right-click and the mouse wheel.\nBULLET::::- Clicking while holding down a modifier key.\nBULLET::::- Moving the pointer a long distance: When a practical limit of mouse movement is reached, one lifts up the mouse, brings it to the opposite edge of the working area while it is held above the surface, and then replaces it down onto the working surface. This is often not necessary, because acceleration software detects fast movement, and moves the pointer significantly faster in proportion than for slow mouse motion.\nBULLET::::- Multi-touch: this method is similar to a multi-touch trackpad on a laptop with support for tap input for multiple fingers, the most famous example being the Apple Magic Mouse.\n\nUsers can also employ mice \"gesturally\"; meaning that a stylized motion of the mouse cursor itself, called a \"gesture\", can issue a command or map to a specific action. For example, in a drawing program, moving the mouse in a rapid \"x\" motion over a shape might delete the shape.\n\nGestural interfaces occur more rarely than plain pointing-and-clicking; and people often find them more difficult to use, because they require finer motor control from the user. However, a few gestural conventions have become widespread, including the drag and drop gesture, in which:\n\nBULLET::::1. The user presses the mouse button while the mouse cursor hovers over an interface object\nBULLET::::2. The user moves the cursor to a different location while holding the button down\nBULLET::::3. The user releases the mouse button\n\nFor example, a user might drag-and-drop a picture representing a file onto a picture of a trash can, thus instructing the system to delete the file.\n\nStandard semantic gestures include:\nBULLET::::- Crossing-based goal\nBULLET::::- Drag and drop\nBULLET::::- Menu traversal\nBULLET::::- Pointing\nBULLET::::- Rollover (Mouseover)\nBULLET::::- Selection\n\nOther uses of the mouse's input occur commonly in special application-domains. In interactive three-dimensional graphics, the mouse's motion often translates directly into changes in the virtual objects' or camera's orientation. For example, in the first-person shooter genre of games (see below), players usually employ the mouse to control the direction in which the virtual player's \"head\" faces: moving the mouse up will cause the player to look up, revealing the view above the player's head. A related function makes an image of an object rotate, so that all sides can be examined. 3D design and animation software often modally chords many different combinations to allow objects and cameras to be rotated and moved through space with the few axes of movement mice can detect.\n\nWhen mice have more than one button, the software may assign different functions to each button. Often, the primary (leftmost in a right-handed configuration) button on the mouse will select items, and the secondary (rightmost in a right-handed) button will bring up a menu of alternative actions applicable to that item. For example, on platforms with more than one button, the Mozilla web browser will follow a link in response to a primary button click, will bring up a contextual menu of alternative actions for that link in response to a secondary-button click, and will often open the link in a new tab or window in response to a click with the tertiary (middle) mouse button.\n\nThe German company Telefunken published on their early ball mouse on 2 October 1968. Telefunken's mouse was sold as optional equipment for their computer systems. Bill English, builder of Engelbart's original mouse, created a ball mouse in 1972 while working for Xerox PARC.\n\nThe ball mouse replaced the external wheels with a single ball that could rotate in any direction. It came as part of the hardware package of the Xerox Alto computer. Perpendicular chopper wheels housed inside the mouse's body chopped beams of light on the way to light sensors, thus detecting in their turn the motion of the ball. This variant of the mouse resembled an inverted trackball and became the predominant form used with personal computers throughout the 1980s and 1990s. The Xerox PARC group also settled on the modern technique of using both hands to type on a full-size keyboard and grabbing the mouse when required.\n\nThe ball mouse has two freely rotating rollers. These are located 90 degrees apart. One roller detects the forward–backward motion of the mouse and other the left–right motion. Opposite the two rollers is a third one (white, in the photo, at 45 degrees) that is spring-loaded to push the ball against the other two rollers. Each roller is on the same shaft as an encoder wheel that has slotted edges; the slots interrupt infrared light beams to generate electrical pulses that represent wheel movement. Each wheel's disc has a pair of light beams, located so that a given beam becomes interrupted or again starts to pass light freely when the other beam of the pair is about halfway between changes.\n\nSimple logic circuits interpret the relative timing to indicate which direction the wheel is rotating. This incremental rotary encoder scheme is sometimes called quadrature encoding of the wheel rotation, as the two optical sensors produce signals that are in approximately quadrature phase. The mouse sends these signals to the computer system via the mouse cable, directly as logic signals in very old mice such as the Xerox mice, and via a data-formatting IC in modern mice. The driver software in the system converts the signals into motion of the mouse cursor along X and Y axes on the computer screen.\n\nThe ball is mostly steel, with a precision spherical rubber surface. The weight of the ball, given an appropriate working surface under the mouse, provides a reliable grip so the mouse's movement is transmitted accurately. Ball mice and wheel mice were manufactured for Xerox by Jack Hawley, doing business as The Mouse House in Berkeley, California, starting in 1975. Based on another invention by Jack Hawley, proprietor of the Mouse House, Honeywell produced another type of mechanical mouse. Instead of a ball, it had two wheels rotating at off axes. Key Tronic later produced a similar product.\n\nModern computer mice took form at the École Polytechnique Fédérale de Lausanne (EPFL) under the inspiration of Professor Jean-Daniel Nicoud and at the hands of engineer and watchmaker André Guignard. This new design incorporated a single hard rubber mouseball and three buttons, and remained a common design until the mainstream adoption of the scroll-wheel mouse during the 1990s. In 1985, René Sommer added a microprocessor to Nicoud's and Guignard's design. Through this innovation, Sommer is credited with inventing a significant component of the mouse, which made it more \"intelligent\"; though optical mice from Mouse Systems had incorporated microprocessors by 1984.\n\nAnother type of mechanical mouse, the \"analog mouse\" (now generally regarded as obsolete), uses potentiometers rather than encoder wheels, and is typically designed to be plug compatible with an analog joystick. The \"Color Mouse\", originally marketed by RadioShack for their Color Computer (but also usable on MS-DOS machines equipped with analog joystick ports, provided the software accepted joystick input) was the best-known example.\n\nOptical mice rely entirely on one or more light-emitting diodes (LEDs) and an imaging array of photodiodes to detect movement relative to the underlying surface, eschewing the internal moving parts a mechanical mouse uses in addition to its optics. A laser mouse is an optical mouse that uses coherent (laser) light.\n\nThe earliest optical mice detected movement on pre-printed mousepad surfaces, whereas the modern LED optical mouse works on most opaque diffuse surfaces; it is usually unable to detect movement on specular surfaces like polished stone. Laser diodes are also used for better resolution and precision, improving performance on opaque specular surfaces. Battery powered, wireless optical mice flash the LED intermittently to save power, and only glow steadily when movement is detected.\n\nOften called \"air mice\" since they do not require a surface to operate, inertial mice use a tuning fork or other accelerometer (US Patent 4787051) to detect rotary movement for every axis supported. The most common models (manufactured by Logitech and Gyration) work using 2 degrees of rotational freedom and are insensitive to spatial translation. The user requires only small wrist rotations to move the cursor, reducing user fatigue or \"gorilla arm\".\n\nUsually cordless, they often have a switch to deactivate the movement circuitry between use, allowing the user freedom of movement without affecting the cursor position. A patent for an inertial mouse claims that such mice consume less power than optically based mice, and offer increased sensitivity, reduced weight and increased ease-of-use. In combination with a wireless keyboard an inertial mouse can offer alternative ergonomic arrangements which do not require a flat work surface, potentially alleviating some types of repetitive motion injuries related to workstation posture.\n\nAlso known as bats, flying mice, or wands, these devices generally function through ultrasound and provide at least three degrees of freedom. Probably the best known example would be 3Dconnexion (\"Logitech's SpaceMouse\") from the early 1990s. In the late 1990s Kantek introduced the 3D RingMouse. This wireless mouse was worn on a ring around a finger, which enabled the thumb to access three buttons. The mouse was tracked in three dimensions by a base station. Despite a certain appeal, it was finally discontinued because it did not provide sufficient resolution.\n\nOne example of a 2000s consumer 3D pointing device is the Wii Remote. While primarily a motion-sensing device (that is, it can determine its orientation and direction of movement), Wii Remote can also detect its spatial position by comparing the distance and position of the lights from the IR emitter using its integrated IR camera (since the nunchuk accessory lacks a camera, it can only tell its current heading and orientation). The obvious drawback to this approach is that it can only produce spatial coordinates while its camera can see the sensor bar. More accurate consumer devices have since been released, including the PlayStation Move, the Razer Hydra and the controllers part of the HTC Vive virtual reality system. All of these devices can accurately detect position and orientation in 3D space regardless of angle relative to the sensor station.\n\nA mouse-related controller called the SpaceBall has a ball placed above the work surface that can easily be gripped. With spring-loaded centering, it sends both translational as well as angular displacements on all six axes, in both directions for each. In November 2010 a German Company called Axsotic introduced a new concept of 3D mouse called 3D Spheric Mouse. This new concept of a true six degree-of-freedom input device uses a ball to rotate in 3 axes without any limitations.\nIn 2000, Logitech introduced a \"tactile mouse\" that contained a small actuator to make the mouse vibrate. Such a mouse can augment user-interfaces with haptic feedback, such as giving feedback when crossing a window boundary. To surf by touch requires the user to be able to feel depth or hardness; this ability was realized with the first electrorheological tactile mice but never marketed.\n\nTablet digitizers are sometimes used with accessories called pucks, devices which rely on absolute positioning, but can be configured for sufficiently mouse-like relative tracking that they are sometimes marketed as mice.\n\nAs the name suggests, this type of mouse is intended to provide optimum comfort and avoid injuries such as carpal tunnel syndrome, arthritis and other repetitive strain injuries. It is designed to fit natural hand position and movements, to reduce discomfort.\n\nWhen holding a typical mouse, ulna and radius bones on the arm are crossed. Some designs attempt to place the palm more vertically, so the bones take more natural parallel position. Some limit wrist movement, encouraging arm movement instead, that may be less precise but more optimal from the health point of view. A mouse may be angled from the thumb downward to the opposite side – this is known to reduce wrist pronation. However such optimizations make the mouse right or left hand specific, making more problematic to change the tired hand. Time magazine has criticized manufacturers for offering few or no left-handed ergonomic mice: \"Oftentimes I felt like I was dealing with someone who’d never actually met a left-handed person before.\"\n\nAnother solution is a pointing bar device. The so-called \"roller bar mouse\" is positioned snugly in front of the keyboard, thus allowing bi-manual accessibility.\n\nThese mice are specifically designed for use in computer games. They typically employ a wide array of controls and buttons and have designs that differ radically from traditional mice. It is also common for gaming mice, especially those designed for use in real-time strategy games such as \"StarCraft\", or in multiplayer online battle arena games such as \"Dota 2\" to have a relatively high sensitivity, measured in dots per inch (DPI). Some advanced mice from gaming manufacturers also allow users to customize the weight of the mouse by adding or subtracting weights to allow for easier control. Ergonomic quality is also an important factor in gaming mice, as extended gameplay times may render further use of the mouse to be uncomfortable. Some mice have been designed to have adjustable features such as removable and/or elongated palm rests, horizontally adjustable thumb rests and pinky rests. Some mice may include several different rests with their products to ensure comfort for a wider range of target consumers. Gaming mice are held by gamers in three styles of grip:\nBULLET::::1. Palm Grip: the hand rests on the mouse, with extended fingers.\nBULLET::::2. Claw Grip: palm rests on the mouse, bent fingers.\nBULLET::::3. Finger-Tip Grip: bent fingers, palm doesn't touch the mouse.\n\nTo transmit their input, typical cabled mice use a thin electrical cord terminating in a standard connector, such as RS-232C, PS/2, ADB or USB. Cordless mice instead transmit data via infrared radiation (see IrDA) or radio (including Bluetooth), although many such cordless interfaces are themselves connected through the aforementioned wired serial buses.\n\nWhile the electrical interface and the format of the data transmitted by commonly available mice is currently standardized on USB, in the past it varied between different manufacturers. A bus mouse used a dedicated interface card for connection to an IBM PC or compatible computer.\n\nMouse use in DOS applications became more common after the introduction of the Microsoft Mouse, largely because Microsoft provided an open standard for communication between applications and mouse driver software. Thus, any application written to use the Microsoft standard could use a mouse with a driver that implements the same API, even if the mouse hardware itself was incompatible with Microsoft's. This driver provides the state of the buttons and the distance the mouse has moved in units that its documentation calls \"mickeys\", as does the Allegro library.\n\nIn the 1970s, the Xerox Alto mouse, and in the 1980s the Xerox optical mouse, used a quadrature-encoded X and Y interface. This two-bit encoding per dimension had the property that only one bit of the two would change at a time, like a Gray code or Johnson counter, so that the transitions would not be misinterpreted when asynchronously sampled.\n\nThe earliest mass-market mice, such as on the original Macintosh, Amiga, and Atari ST mice used a D-subminiature 9-pin connector to send the quadrature-encoded X and Y axis signals directly, plus one pin per mouse button. The mouse was a simple optomechanical device, and the decoding circuitry was all in the main computer.\n\nThe DE-9 connectors were designed to be electrically compatible with the joysticks popular on numerous 8-bit systems, such as the Commodore 64 and the Atari 2600. Although the ports could be used for both purposes, the signals must be interpreted differently. As a result, plugging a mouse into a joystick port causes the \"joystick\" to continuously move in some direction, even if the mouse stays still, whereas plugging a joystick into a mouse port causes the \"mouse\" to only be able to move a single pixel in each direction.\n\nBecause the IBM PC did not have a quadrature decoder built in, early PC mice used the RS-232C serial port to communicate encoded mouse movements, as well as provide power to the mouse's circuits. The Mouse Systems Corporation version used a five-byte protocol and supported three buttons. The Microsoft version used a three-byte protocol and supported two buttons. Due to the incompatibility between the two protocols, some manufacturers sold serial mice with a mode switch: \"PC\" for MSC mode, \"MS\" for Microsoft mode.\n\nIn 1986 Apple first implemented the Apple Desktop Bus allowing the daisy-chaining (linking together in series, ie. end to end) of up to 16 devices, including mice and other devices on the same bus with no configuration whatsoever. Featuring only a single data pin, the bus used a purely polled approach to computer/device communications and survived as the standard on mainstream models (including a number of non-Apple workstations) until 1998 when Apple's iMac line of computers joined the industry-wide switch to using USB. Beginning with the Bronze Keyboard PowerBook G3 in May 1999, Apple dropped the external ADB port in favor of USB, but retained an internal ADB connection in the PowerBook G4 for communication with its built-in keyboard and trackpad until early 2005.\n\nWith the arrival of the IBM PS/2 personal-computer series in 1987, IBM introduced the eponymous PS/2 interface for mice and keyboards, which other manufacturers rapidly adopted. The most visible change was the use of a round 6-pin mini-DIN, in lieu of the former 5-pin MIDI style full sized DIN 41524 connector. In default mode (called \"stream mode\") a PS/2 mouse communicates motion, and the state of each button, by means of 3-byte packets. For any motion, button press or button release event, a PS/2 mouse sends, over a bi-directional serial port, a sequence of three bytes, with the following format:\n\n!Bit 7Bit 6Bit 5Bit 4Bit 3Bit 2Bit 1Bit 0\n!Byte 1\nYVXVYSXS1MBRBLB\n!Byte 2\ncolspan=\"8\"X movement\n!Byte 3\ncolspan=\"8\"Y movement\n\nHere, XS and YS represent the sign bits of the movement vectors, XV and YV indicate an overflow in the respective vector component, and LB, MB and RB indicate the status of the left, middle and right mouse buttons (1 = pressed). PS/2 mice also understand several commands for reset and self-test, switching between different operating modes, and changing the resolution of the reported motion vectors.\n\nA Microsoft IntelliMouse relies on an extension of the PS/2 protocol: the ImPS/2 or IMPS/2 protocol (the abbreviation combines the concepts of \"IntelliMouse\" and \"PS/2\"). It initially operates in standard PS/2 format, for backwards compatibility. After the host sends a special command sequence, it switches to an extended format in which a fourth byte carries information about wheel movements. The IntelliMouse Explorer works analogously, with the difference that its 4-byte packets also allow for two additional buttons (for a total of five).\n\nMouse vendors also use other extended formats, often without providing public documentation. The Typhoon mouse uses 6-byte packets which can appear as a sequence of two standard 3-byte packets, such that an ordinary PS/2 driver can handle them. For 3-D (or 6-degree-of-freedom) input, vendors have made many extensions both to the hardware and to software. In the late 1990s, Logitech created ultrasound based tracking which gave 3D input to a few millimeters accuracy, which worked well as an input device but failed as a profitable product. In 2008, Motion4U introduced its \"OptiBurst\" system using IR tracking for use as a Maya (graphics software) plugin.\n\nThe industry-standard USB (Universal Serial Bus) protocol and its connector have become widely used for mice; it is among the most popular types.\n\nCordless or wireless mice transmit data via infrared radiation (see IrDA) or radio (including Bluetooth and Wi-Fi). The receiver is connected to the computer through a serial or USB port, or can be built in (as is sometimes the case with Bluetooth and WiFi).\nModern non-Bluetooth and non-WiFi wireless mice use USB receivers. Some of these can be stored inside the mouse for safe transport while not in use, while other, newer mice use newer \"nano\" receivers, designed to be small enough to remain plugged into a laptop during transport, while still being large enough to easily remove.\n\nSome systems allow two or more mice to be used at once as input devices. Late-1980s era home computers such as the Amiga used this to allow computer games with two players interacting on the same computer (Lemmings and The Settlers for example). The same idea is sometimes used in collaborative software, e.g. to simulate a whiteboard that multiple users can draw on without passing a single mouse around.\n\nMicrosoft Windows, since Windows 98, has supported multiple simultaneous pointing devices. Because Windows only provides a single screen cursor, using more than one device at the same time requires cooperation of users or applications designed for multiple input devices.\n\nMultiple mice are often used in multi-user gaming in addition to specially designed devices that provide several input interfaces.\n\nWindows also has full support for multiple input/mouse configurations for multi-user environments.\n\nStarting with Windows XP, Microsoft introduced a SDK for developing applications that allow multiple input devices to be used at the same time with independent cursors and independent input points.\n\nThe introduction of Vista and Microsoft Surface (now known as Microsoft PixelSense) introduced a new set of input APIs that were adopted into Windows 7, allowing for 50 points/cursors, all controlled by independent users. The new input points provide traditional mouse input; however, they were designed with other input technologies like touch and image in mind. They inherently offer 3D coordinates along with pressure, size, tilt, angle, mask, and even an image bitmap to see and recognize the input point/object on the screen.\n\nAs of 2009, Linux distributions and other operating systems that use X.Org, such as OpenSolaris and FreeBSD, support 255 cursors/input points through Multi-Pointer X. However, currently no window managers support Multi-Pointer X leaving it relegated to custom software usage.\n\nThere have also been propositions of having a single operator use two mice simultaneously as a more sophisticated means of controlling various graphics and multimedia applications.\n\nMouse buttons are microswitches which can be pressed to select or interact with an element of a graphical user interface, producing a distinctive clicking sound.\n\nSince around the late 1990s, the three-button scrollmouse has become the de facto standard. Users most commonly employ the second button to invoke a contextual menu in the computer's software user interface, which contains options specifically tailored to the interface element over which the mouse cursor currently sits. By default, the primary mouse button sits located on the left-hand side of the mouse, for the benefit of right-handed users; left-handed users can usually reverse this configuration via software.\n\nNearly all mice now have an integrated input primarily intended for scrolling on top, usually a single-axis digital wheel or rocker switch which can also be depressed to act as a third button. Though less common, many mice instead have two-axis inputs such as a tiltable wheel, trackball, or touchpad.\n\nMickeys per second is a unit of measurement for the speed and movement direction of a computer mouse, where direction is often expressed as \"horizontal\" versus \"vertical\" mickey count. However, speed can also refer to the ratio between how many pixels the cursor moves on the screen and how far the mouse moves on the mouse pad, which may be expressed as pixels per mickey, pixels per inch, or pixels per centimeter.\n\nThe computer industry often measures mouse sensitivity in terms of counts per inch (CPI), commonly expressed as dots per inch (DPI)the number of steps the mouse will report when it moves one inch. In early mice, this specification was called pulses per inch (ppi). The Mickey originally referred to one of these counts, or one resolvable step of motion. If the default mouse-tracking condition involves moving the cursor by one screen-pixel or dot on-screen per reported step, then the CPI does equate to DPI: dots of cursor motion per inch of mouse motion. The CPI or DPI as reported by manufacturers depends on how they make the mouse; the higher the CPI, the faster the cursor moves with mouse movement. However, software can adjust the mouse sensitivity, making the cursor move faster or slower than its CPI. software can change the speed of the cursor dynamically, taking into account the mouse's absolute speed and the movement from the last stop-point. In most software, an example being the Windows platforms, this setting is named \"speed,\" referring to \"cursor precision\". However, some operating systems name this setting \"acceleration\", the typical Apple OS designation. This term is incorrect. Mouse acceleration in most mouse software refers to the change in speed of the cursor over time while the mouse movement is constant.\n\nFor simple software, when the mouse starts to move, the software will count the number of \"counts\" or \"mickeys\" received from the mouse and will move the cursor across the screen by that number of pixels (or multiplied by a rate factor, typically less than 1). The cursor will move slowly on the screen, with good precision. When the movement of the mouse passes the value set for some threshold, the software will start to move the cursor faster, with a greater rate factor. Usually, the user can set the value of the second rate factor by changing the \"acceleration\" setting.\n\nOperating systems sometimes apply acceleration, referred to as \"ballistics\", to the motion reported by the mouse. For example, versions of Windows prior to Windows XP doubled reported values above a configurable threshold, and then optionally doubled them again above a second configurable threshold. These doublings applied separately in the X and Y directions, resulting in very nonlinear response.\n\nEngelbart's original mouse did not require a mousepad; the mouse had two large wheels which could roll on virtually any surface. However, most subsequent mechanical mice starting with the steel roller ball mouse have required a mousepad for optimal performance.\n\nThe mousepad, the most common mouse accessory, appears most commonly in conjunction with mechanical mice, because to roll smoothly the ball requires more friction than common desk surfaces usually provide. So-called \"hard mousepads\" for gamers or optical/laser mice also exist.\n\nMost optical and laser mice do not require a pad, the notable exception being early optical mice which relied on a grid on the pad to detect movement (e.g. Mouse Systems). Whether to use a hard or soft mousepad with an optical mouse is largely a matter of personal preference. One exception occurs when the desk surface creates problems for the optical or laser tracking, for example, a transparent or reflective surface, such as glass.\n\nAround 1981, Xerox included mice with its Xerox Star, based on the mouse used in the 1970s on the Alto computer at Xerox PARC. Sun Microsystems, Symbolics, Lisp Machines Inc., and Tektronix also shipped workstations with mice, starting in about 1981. Later, inspired by the Star, Apple Computer released the Apple Lisa, which also used a mouse. However, none of these products achieved large-scale success. Only with the release of the Apple Macintosh in 1984 did the mouse see widespread use.\n\nThe Macintosh design, commercially successful and technically influential, led many other vendors to begin producing mice or including them with their other computer products (by 1986, Atari ST, Amiga, Windows 1.0, GEOS for the Commodore 64, and the Apple IIGS).\n\nThe widespread adoption of graphical user interfaces in the software of the 1980s and 1990s made mice all but indispensable for controlling computers. In November 2008, Logitech built their billionth mouse.\n\nThe Classic Mac OS Desk Accessory \"Puzzle\" in 1984 was the first game designed specifically for a mouse. The device often functions as an interface for PC-based computer games and sometimes for video game consoles.\n\nFPSs naturally lend themselves to separate and simultaneous control of the player's movement and aim, and on computers this has traditionally been achieved with a combination of keyboard and mouse. Players use the X-axis of the mouse for looking (or turning) left and right, and the Y-axis for looking up and down; the keyboard is used for movement and supplemental inputs.\n\nMany shooting genre players prefer a mouse over a gamepad analog stick because the mouse is a linear input device, which allows for fast and precise control. Holding a stick in a given position produces a corresponding constant movement or rotation, i.e. the output is an integral of the user's input, and requires that time be spent moving to or from its null position before this input can be given; in contrast, the output of a mouse directly and instantaneously corresponds to how far it is moved in a given direction (often multiplied by an \"acceleration\" factor derived from how quickly the mouse is moved). The effect of this is that a mouse is well suited to small, precise movements; large, quick movements; and immediate, responsive movements; all of which are important in shooter gaming. This advantage also extends in varying degrees to similar game styles such as third-person shooters.\n\nSome incorrectly ported games or game engines have acceleration and interpolation curves which unintentionally produce excessive, irregular, or even negative acceleration when used with a mouse instead of their native platform's non-mouse default input device. Depending on how deeply hardcoded this misbehavior is, internal user patches or external 3rd-party software may be able to fix it.\n\nDue to their similarity to the WIMP desktop metaphor interface for which mice were originally designed, and to their own tabletop game origins, computer strategy games are most commonly played with mice. In particular, real-time strategy and MOBA games usually require the use of a mouse.\n\nThe left button usually controls primary fire. If the game supports multiple fire modes, the right button often provides secondary fire from the selected weapon. Games with only a single fire mode will generally map secondary fire to \"ADS\". In some games, the right button may also invoke accessories for a particular weapon, such as allowing access to the scope of a sniper rifle or allowing the mounting of a bayonet or silencer.\n\nGamers can use a scroll wheel for changing weapons (or for controlling scope-zoom magnification, in older games). On most first person shooter games, programming may also assign more functions to additional buttons on mice with more than three controls. A keyboard usually controls movement (for example, WASD for moving forward, left, backward and right, respectively) and other functions such as changing posture. Since the mouse serves for aiming, a mouse that tracks movement accurately and with less lag (latency) will give a player an advantage over players with less accurate or slower mice. In some cases the right mouse button may be used to move the player forward, either in lieu of, or in conjunction with the typical WASD configuration.\n\nMany games provide players with the option of mapping their own choice of a key or button to a certain control. An early technique of players, circle strafing, saw a player continuously strafing while aiming and shooting at an opponent by walking in circle around the opponent with the opponent at the center of the circle. Players could achieve this by holding down a key for strafing while continuously aiming the mouse towards the opponent.\n\nGames using mice for input are so popular that many manufacturers make mice specifically for gaming. Such mice may feature adjustable weights, high-resolution optical or laser components, additional buttons, ergonomic shape, and other features such as adjustable CPI. Mouse Bungees are typically used with gaming mice because it eliminates the annoyance of the cable.\n\nMany games, such as first- or third-person shooters, have a setting named \"invert mouse\" or similar (not to be confused with \"button inversion\", sometimes performed by left-handed users) which allows the user to look downward by moving the mouse forward and upward by moving the mouse backward (the opposite of non-inverted movement). This control system resembles that of aircraft control sticks, where pulling back causes pitch up and pushing forward causes pitch down; computer joysticks also typically emulate this control-configuration.\n\nAfter id Software's commercial hit of \"Doom\", which did not support vertical aiming, competitor Bungie's \"Marathon\" became the first first-person shooter to support using the mouse to aim up and down. Games using the Build engine had an option to invert the Y-axis. The \"invert\" feature actually made the mouse behave in a manner that users regard as non-inverted (by default, moving mouse forward resulted in looking down). Soon after, id Software released \"Quake\", which introduced the invert feature as users know it.\n\nIn 1988, the VTech Socrates educational video game console featured a wireless mouse with an attached mouse pad as an optional controller used for some games. In the early 1990s, the Super Nintendo Entertainment System video game system featured a mouse in addition to its controllers. The \"Mario Paint\" game in particular used the mouse's capabilities as did its successor on the N64. Sega released official mice for their Genesis/Mega Drive, Saturn and Dreamcast consoles. NEC sold official mice for its PC Engine and PC-FX consoles. Sony released an official mouse product for the PlayStation console, included one along with the Linux for PlayStation 2 kit, as well as allowing owners to use virtually any USB mouse with the PS2, PS3, and PS4. Nintendo's Wii also had this added on in a later software update, retained on the Wii U.\n\nBULLET::::- Computer accessibility\nBULLET::::- Footmouse\nBULLET::::- Graphics tablet\nBULLET::::- Gesture recognition\nBULLET::::- Human–computer interaction (HCI)\nBULLET::::- List of wireless mice with nano receivers\nBULLET::::- Mouse keys\nBULLET::::- Mouse tracking\nBULLET::::- Pointing stick\nBULLET::::- Rotational mouse\nBULLET::::- Computer\n\nBULLET::::- Pang, Alex Soojung-Kim, \"Mighty Mouse: In 1980, Apple Computer asked a group of guys fresh from Stanford's product design program to take a $400 device and make it mass-producible, reliable and cheap. Their work transformed personal computing\", Stanford University Alumni Magazine, March/April 2002.\nBULLET::::- Stanford University MouseSite with stories and annotated archives from Doug Engelbart's work\nBULLET::::- Doug Engelbart Institute mouse resources page includes stories and links\nBULLET::::- Fire-Control and Human-Computer Interaction: Towards a History of the Computer Mouse (1940–1965), by Axel Roch\nBULLET::::- (NB. Contains some historical photos.)\n\nBULLET::::- The video segment of The Mother of All Demos with Doug Engelbart showing the device from 1968\n"}
{"id": "7059", "url": "https://en.wikipedia.org/wiki?curid=7059", "title": "Civil defense", "text": "Civil defense\n\nCivil defence (civil defense in US English) or civil protection is an effort to protect the citizens of a state (generally non-combatants) from military attacks and natural disasters. It uses the principles of emergency operations: prevention, mitigation, preparation, response, or emergency evacuation and recovery. Programs of this sort were initially discussed at least as early as the 1920s and were implemented in some countries during the 1930s as the threat of war and aerial bombardment grew. It became widespread after the threat of nuclear weapons was realized.\n\nSince the end of the Cold War, the focus of civil defence has largely shifted from military attack to emergencies and disasters in general. The new concept is described by a number of terms, each of which has its own specific shade of meaning, such as \"crisis management\", \"emergency management\", \"emergency preparedness\", \"contingency planning\", \"civil contingency\", \"civil aid\" and \"civil protection\".\n\nIn some countries, civil defense is seen as a key part of \"total defense\". For example, in Sweden, the Swedish word \"totalförsvar\" refers to the commitment of a wide range of resources of the nation to its defense—including to civil protection. Respectively, some countries (notably the Soviet Union) may have or have had military-organized civil defense units (Civil Defense Troops) as part of their armed forces or as a paramilitary service.\n\nThe advent of civil defense was stimulated by the experience of the bombing of civilian areas during the First World War. The bombing of the United Kingdom began on 19 January 1915 when German zeppelins dropped bombs on the Great Yarmouth area, killing six people. German bombing operations of the First World War were surprisingly effective, especially after the Gotha bombers surpassed the zeppelins. The most devastating raids inflicted 121 casualties for each ton of bombs dropped; this figure was then used as a basis for predictions.\n\nAfter the war, attention was turned toward civil defense in the event of war, and the Air Raid Precautions Committee (ARP) was established in 1924 to investigate ways for ensuring the protection of civilians from the danger of air-raids.\n\nThe Committee produced figures estimating that in London there would be 9,000 casualties in the first two days and then a continuing rate of 17,500 casualties a week. These rates were thought conservative. It was believed that there would be \"total chaos and panic\" and hysterical neurosis as the people of London would try to flee the city. To control the population harsh measures were proposed: bringing London under almost military control, and physically cordoning off the city with 120,000 troops to force people back to work. A different government department proposed setting up camps for refugees for a few days before sending them back to London.\n\nA special government department, the Civil Defence Service, was established by the Home Office in 1935. Its remit included the pre-existing ARP as well as wardens, firemen (initially the Auxiliary Fire Service (AFS) and latterly the National Fire Service (NFS)), fire watchers, rescue, first aid post, stretcher party and industry. Over 1.9 million people served within the CD; nearly 2,400 lost their lives to enemy action.\nThe organization of civil defense was the responsibility of the local authority. Volunteers were ascribed to different units depending on experience or training. Each local civil defense service was divided into several sections. Wardens were responsible for local reconnaissance and reporting, and leadership, organization, guidance and control of the general public. Wardens would also advise survivors of the locations of rest and food centers, and other welfare facilities.\n\nRescue Parties were required to assess and then access bombed-out buildings and retrieve injured or dead people. In addition they would turn off gas, electricity and water supplies, and repair or pull down unsteady buildings. Medical services, including First Aid Parties, provided on the spot medical assistance.\n\nThe expected stream of information that would be generated during an attack was handled by 'Report and Control' teams. A local headquarters would have an ARP controller who would direct rescue, first aid and decontamination teams to the scenes of reported bombing. If local services were deemed insufficient to deal with the incident then the controller could request assistance from surrounding boroughs.\n\nFire Guards were responsible for a designated area/building and required to monitor the fall of incendiary bombs and pass on news of any fires that had broken out to the NFS. They could deal with an individual magnesium electron incendiary bomb by dousing it with buckets of sand or water or by smothering. Additionally, 'Gas Decontamination Teams' kitted out with gas-tight and waterproof protective clothing were to deal with any gas attacks. They were trained to decontaminate buildings, roads, rail and other material that had been contaminated by liquid or jelly gases.\nLittle progress was made over the issue of air-raid shelters, because of the apparently irreconcilable conflict between the need to send the public underground for shelter and the need to keep them above ground for protection against gas attacks. In February 1936 the Home Secretary appointed a technical Committee on Structural Precautions against Air Attack. During the Munich crisis, local authorities dug trenches to provide shelter. After the crisis, the British Government decided to make these a permanent feature, with a standard design of precast concrete trench lining. They also decided to issue the Anderson shelter free to poorer households and to provide steel props to create shelters in suitable basements.\n\nDuring the Second World War, the ARP was responsible for the issuing of gas masks, pre-fabricated air-raid shelters (such as Anderson shelters, as well as Morrison shelters), the upkeep of local public shelters, and the maintenance of the blackout. The ARP also helped rescue people after air raids and other attacks, and some women became ARP Ambulance Attendants whose job was to help administer first aid to casualties, search for survivors, and in many grim instances, help recover bodies, sometimes those of their own colleagues.\nAs the war progressed, the military effectiveness of Germany's aerial bombardment was very limited. Thanks to the Luftwaffe's shifting aims, the strength of British air defenses, the use of early warning radar and the life-saving actions of local civil defense units, the aerial \"Blitz\" during the Battle of Britain failed to break the morale of the British people, destroy the Royal Air Force or significantly hinder British industrial production. Despite a significant investment in civil and military defense, British civilian losses during the Blitz were higher than in most strategic bombing campaigns throughout the war. For example, there were 14,000-20,0000 UK civilian fatalities during the Battle of Britain, a relatively high number considering that the Luftwaffe dropped only an estimated 30,000 tons of ordinance during the battle. In comparison, Allied strategic bombing of Germany during the war was less lethal, with an estimated 400,000-600,000 German civilian fatalities for approximately 1.35 million tons of bombs dropped on Germany.\n\nIn the United States, the Office of Civil Defense was established in May 1941 to coordinate civilian defense efforts. It coordinated with the Department of the Army and established similar groups to the British ARP. One of these groups that still exists today is the Civil Air Patrol, which was originally created as a civilian auxiliary to the Army. The CAP was created on December 1, 1941, with the main civil defense mission of search and rescue. The CAP also sank two Axis submarines and provided aerial reconnaissance for Allied and neutral merchant ships. In 1946, the Civil Air Patrol was barred from combat by Public Law 79-476. The CAP then received its current mission: search and rescue for downed aircraft. When the Air Force was created, in 1947, the Civil Air Patrol became the auxiliary of the Air Force.\n\nThe Coast Guard Auxiliary performs a similar role in support of the U.S. Coast Guard. Like the Civil Air Patrol, the Coast Guard Auxiliary was established in the run up to World War II. Auxiliarists were sometimes armed during the war, and extensively participated in port security operations. After the war, the Auxiliary shifted its focus to promoting boating safety and assisting the Coast Guard in performing search and rescue and marine safety and environmental protection.\n\nIn the United States a federal civil defense program existed under Public Law 920 of the 81st Congress, as amended, from 1951–1994. That statutory scheme was made so-called all-hazards by Public Law 103-160 in 1993 and largely repealed by Public Law 103-337 in 1994. Parts now appear in Title VI of the Robert T. Stafford Disaster Relief and Emergency Assistance Act, Public Law 100-107 [1988 as amended]. The term EMERGENCY PREPAREDNESS was largely codified by that repeal and amendment. See 42 USC Sections 5101 and following.\n\nIn most of the states of the North Atlantic Treaty Organization, such as the United States, the United Kingdom and West Germany, as well as the Soviet Bloc, and especially in the neutral countries, such as Switzerland and in Sweden during the 1950s and 1960s, many civil defense practices took place to prepare for the aftermath of a nuclear war, which seemed quite likely at that time.\n\nIn the United Kingdom, the Civil Defence Service was disbanded in 1945, followed by the ARP in 1946. With the onset of the growing tensions between East and West, the service was revived in 1949 as the Civil Defence Corps. As a civilian volunteer organization, it was tasked to take control in the aftermath of a major national emergency, principally envisaged as being a Cold War nuclear attack. Although under the authority of the Home Office, with a centralized administrative establishment, the corps was administered locally by Corps Authorities. In general every county was a Corps Authority, as were most county boroughs in England and Wales and large burghs in Scotland.\n\nEach division was divided into several sections, including the Headquarters, Intelligence and Operations, Scientific and Reconnaissance, Warden & Rescue, Ambulance and First Aid and Welfare.\n\nIn 1954 Coventry City Council caused international controversy when it announced plans to disband its Civil Defence committee because the councillors had decided that hydrogen bombs meant that there could be no recovery from a nuclear attack. The British government opposed such a move and held a provocative Civil Defence exercise on the streets of Coventry which Labour council members protested against. The government also decided to implement its own committee at the city's cost until the council reinstituted its committee.\n\nIn the United States, the sheer power of nuclear weapons and the perceived likelihood of such an attack precipitated a greater response than had yet been required of civil defense. Civil defense, previously considered an important and commonsense step, became divisive and controversial in the charged atmosphere of the Cold War. In 1950, the National Security Resources Board created a 162-page document outlining a model civil defense structure for the U.S. Called the \"Blue Book\" by civil defense professionals in reference to its solid blue cover, it was the template for legislation and organization for the next 40 years.\nPerhaps the most memorable aspect of the Cold War civil defense effort was the educational effort made or promoted by the government. In \"Duck and Cover\", Bert the Turtle advocated that children \"duck and cover\" when they \"see the flash.\" Booklets such as \"Survival Under Atomic Attack\", \"Fallout Protection\" and \"Nuclear War Survival Skills\" were also commonplace. The transcribed radio program Stars for Defense combined hit music with civil defense advice. Government institutes created public service announcements including children's songs and distributed them to radio stations to educate the public in case of nuclear attack.\n\nThe US President Kennedy (1961–63) launched an ambitious effort to install fallout shelters throughout the United States. These shelters would not protect against the blast and heat effects of nuclear weapons, but would provide some protection against the radiation effects that would last for weeks and even affect areas distant from a nuclear explosion. In order for most of these preparations to be effective, there had to be some degree of warning. In 1951, CONELRAD (Control of Electromagnetic Radiation) was established. Under the system, a few primary stations would be alerted of an emergency and would broadcast an alert. All broadcast stations throughout the country would be constantly listening to an upstream station and repeat the message, thus passing it from station to station.\n\nIn a once classified US war game analysis, looking at varying levels of war escalation, warning and pre-emptive attacks in the late 1950s early 1960s, it was estimated that approximately 27 million US citizens would have been saved with civil defense education. At the time, however, the cost of a full-scale civil defense program was regarded as less effective in cost-benefit analysis than a ballistic missile defense (Nike Zeus) system, and as the Soviet adversary was increasing their nuclear stockpile, the efficacy of both would follow a diminishing returns trend.\n\nContrary to the largely noncommittal approach taken in NATO, with its stops and starts in civil defense depending on the whims of each newly elected government, the military strategy in the comparatively more ideologically consistent USSR held that, amongst other things, a winnable nuclear war was possible. To this effect the Soviets planned to minimize, as far as possible, the effects of nuclear weapon strikes on its territory, and therefore spent considerably more thought on civil defense preparations than in U.S., with defense plans that have been assessed to be far more effective than those in the U.S.\n\nSoviet Civil Defense Troops played the main role in the massive disaster relief operation following the 1986 Chernobyl nuclear accident. Defense Troop reservists were officially mobilized (as in a case of war) from throughout the USSR to join the Chernobyl task force and formed on the basis of the Kiev Civil Defense Brigade. The task force performed some high-risk tasks including, with the failure of their robotic machinery, the manual removal of highly-radioactive debris. Many of their personnel were later decorated with medals for their work at containing the release of radiation into the environment, with a number of the 56 deaths from the accident being Civil defense troops.\n\nIn Western countries, strong civil defense policies were never properly implemented, because it was fundamentally at odds with the doctrine of \"mutual assured destruction\" (MAD) by making provisions for survivors. It was also considered that a full-fledged total defense would have not been worth the very large expense. For whatever reason, the public saw efforts at civil defense as fundamentally ineffective against the powerful destructive forces of nuclear weapons, and therefore a waste of time and money, although detailed scientific research programs did underlie the much-mocked government civil defense pamphlets of the 1950s and 1960s.\n\nGovernments in most Western countries, with the sole exception of Switzerland, generally sought to underfund Civil Defense due to its perceived pointlessness. Nevertheless, effective but commonly dismissed civil defense measures against nuclear attack were implemented, in the face of popular apathy and skepticism of authority. After the end of the Cold War, the focus moved from defense against nuclear war to defense against a terrorist attack possibly involving chemical or biological weapons.\n\nThe Civil Defence Corps was stood down in Great Britain in 1968 with the tacit realization that nothing practical could be done in the event of an unrestricted nuclear attack. Its neighbors, however, remained committed to Civil Defence, namely the Isle of Man Civil Defence Corps and Civil Defence Ireland (Republic of Ireland).\n\nIn the United States, the various civil defense agencies were replaced with the Federal Emergency Management Agency (FEMA) in 1979. In 2002 this became part of the Department of Homeland Security. The focus was shifted from nuclear war to an \"all-hazards\" approach of Comprehensive Emergency Management. Natural disasters and the emergence of new threats such as terrorism have caused attention to be focused away from traditional civil defense and into new forms of civil protection such as emergency management and homeland security.\n\nMany countries still maintain a national Civil Defence Corps, usually having a wide brief for assisting in large scale civil emergencies such as flood, earthquake, invasion, or civil disorder.\n\nAfter the September 11 attacks in 2001, in the United States the concept of civil defense has been revisited under the umbrella term of homeland security and all-hazards emergency management.\n\nIn Europe, the triangle CD logo continues to be widely used. The old U.S. civil defense logo was used in the FEMA logo until 2006 and is hinted at in the United States Civil Air Patrol logo. Created in 1939 by Charles Coiner of the N. W. Ayer Advertising Agency, it was used throughout World War II and the Cold War era. In 2006, the National Emergency Management Association—a U.S. organization made up of state emergency managers—\"officially\" retired the Civil Defense triangle logo, replacing it with a stylised EM (standing for Emergency management). The name and logo, however, continue to be used by Hawaii State Civil Defense and Guam Homeland Security/Office of Civil Defense.\n\nThe term \"civil protection\" is currently widely used within the European Union to refer to government-approved systems and resources tasked with protecting the non-combat population, primarily in the event of natural and technological disasters. In recent years there has been emphasis on preparedness for technological disasters resulting from terrorist attack. Within EU countries the term \"crisis-management\" emphasizes the political and security dimension rather than measures to satisfy the immediate needs of the population.\n\nIn Australia, civil defense is the responsibility of the volunteer-based State Emergency Service.\n\nIn most former Soviet countries civil defense is the responsibility of governmental ministries, such as Russia's Ministry of Emergency Situations.\n\nRelatively small investments in preparation can speed up recovery by months or years and thereby prevent millions of deaths by hunger, cold and disease. According to human capital theory in economics, a country's population is more valuable than all of the land, factories and other assets that it possesses. People rebuild a country after its destruction, and it is therefore important for the economic security of a country that it protect its people. According to psychology, it is important for people to feel as though they are in control of their own destiny, and preparing for uncertainty via civil defense may help to achieve this.\n\nIn the United States, the federal civil defense program was authorized by statute and ran from 1951 to 1994. Originally authorized by Public Law 920 of the 81st Congress, it was repealed by Public Law 93-337 in 1994. Small portions of that statutory scheme were incorporated into the Robert T. Stafford Disaster Relief and Emergency Assistance Act (Public Law 100-707) which partly superseded in part, partly amended, and partly supplemented the Disaster Relief Act of 1974 (Public Law 93-288). In the portions of the civil defense statute incorporated into the Stafford Act, the primary modification was to use the term \"Emergency Preparedness\" wherever the term \"Civil Defence\" had previously appeared in the statutory language.\n\nAn important concept initiated by President Jimmy Carter was the so-called \"Crisis Relocation Program\" administered as part of the federal civil defense program. That effort largely lapsed under President Ronald Reagan, who discontinued the Carter initiative because of opposition from areas potentially hosting the relocated population.\n\nThreats to civilians and civilian life include NBC (Nuclear, Biological, and Chemical warfare) and others, like the more modern term CBRN (Chemical Biological Radiological and Nuclear). Threat assessment involves studying each threat so that preventative measures can be built into civilian life.\n\nBULLET::::- Conventional\n\nRefers to conventional explosives. A blast shelter designed to protect only from radiation and fallout would be much more vulnerable to conventional explosives. See also fallout shelter.\n\nBULLET::::- Nuclear\n\nShelter intended to protect against nuclear blast effects would include thick concrete and other sturdy elements which are resistant to conventional explosives. The biggest threats from a nuclear attack are effects from the blast, fires and radiation. One of the most prepared countries for a nuclear attack is Switzerland. Almost every building in Switzerland has an \"abri\" (shelter) against the initial nuclear bomb and explosion followed by the fall-out. Because of this, many people use it as a safe to protect valuables, photos, financial information and so on. Switzerland also has air-raid and nuclear-raid sirens in every village.\n\nBULLET::::- Dirty Bomb\nA \"radiologically enhanced weapon\", or \"dirty bomb\", uses an explosive to spread radioactive material. This is a theoretical risk, and such weapons have not been used by terrorists. Depending on the quantity of the radioactive material, the dangers may be mainly psychological. Toxic effects can be managed by standard hazmat techniques.\n\nBULLET::::- Biological\n\nThe threat here is primarily from disease-causing microorganisms such as bacteria and viruses.\n\nBULLET::::- Chemical\n\nVarious chemical agents are a threat, such as nerve gas (VX, Sarin, and so on.).\n\nMitigation is the process of actively preventing the war or the release of nuclear weapons. It includes policy analysis, diplomacy, political measures, nuclear disarmament and more military responses such as a National Missile Defense and air defense artillery. In the case of counter-terrorism, mitigation would include diplomacy, intelligence gathering and direct action against terrorist groups. Mitigation may also be reflected in long-term planning such as the design of the interstate highway system and the placement of military bases further away from populated areas.\n\nPreparation consists of building blast shelters and pre-positioning information, supplies, and emergency infrastructure. For example, most larger cities in the U.S. now have underground emergency operations centers that can perform civil defense coordination. FEMA also has many underground facilities for the same purpose located near major railheads such as the ones in Denton, Texas and Mount Weather, Virginia.\n\nOther measures would include continual government inventories of grain silos, the Strategic National Stockpile, the uncapping of the Strategic Petroleum Reserve, the dispersal of lorry-transportable bridges, water purification, mobile refineries, mobile de-contamination facilities, mobile general and special purpose disaster mortuary facilities such as Disaster Mortuary Operational Response Team (DMORT) and DMORT-WMD, and other aids such as temporary housing to speed civil recovery.\n\nOn an individual scale, one means of preparation for exposure to nuclear fallout is to obtain potassium iodide (KI) tablets as a safety measure to protect the human thyroid gland from the uptake of dangerous radioactive iodine. Another measure is to cover the nose, mouth and eyes with a piece of cloth and sunglasses to protect against alpha particles, which are only an internal hazard.\n\nTo support and supplement efforts at national, regional and local level with regard to disaster prevention, the preparedness of those responsible for civil protection and the intervention in the event of disaster\nBULLET::::- To establish a framework for effective and rapid cooperation between different civil protection services when mutual assistance is needed (police, fire service, healthcare service, public utility provider, voluntary agencies)\nBULLET::::- To set up and implement training programs for intervention and coordination teams as well as assessment experts including joint courses and exchange systems\nBULLET::::- To enhance the coherence of actions undertaken at international level in the field of civil protection, especially in the context of cooperation\n\nPreparing also includes sharing information:\nBULLET::::- To contribute to informing the public, in view of increasing citizens' level of self-protection\nBULLET::::- To collect and disseminate validated emergency information\nBULLET::::- To pool information on national civil protection capabilities, military and medical resources\nBULLET::::- To ensure efficient information sharing between the different authorities\n\nResponse consists first of warning civilians so they can enter fallout shelters and protect assets.\n\nStaffing a response is always full of problems in a civil defense emergency. After an attack, conventional full-time emergency services are dramatically overloaded, with conventional fire fighting response times often exceeding several days. Some capability is maintained by local and state agencies, and an emergency reserve is provided by specialized military units, especially civil affairs, Military Police, Judge Advocates and combat engineers.\n\nHowever, the traditional response to massed attack on civilian population centers is to maintain a mass-trained force of volunteer emergency workers. Studies in World War II showed that lightly trained (40 hours or less) civilians in organised teams can perform up to 95% of emergency activities when trained, liaised and supported by local government. In this plan, the populace rescues itself from most situations, and provides information to a central office to prioritize professional emergency services.\n\nIn the 1990s, this concept was revived by the Los Angeles Fire Department to cope with civil emergencies such as earthquakes. The program was widely adopted, providing standard terms for organization. In the U.S., this is now official federal policy, and it is implemented by community emergency response teams, under the Department of Homeland Security, which certifies training programs by local governments, and registers \"certified disaster service workers\" who complete such training.\n\nRecovery consists of rebuilding damaged infrastructure, buildings and production. The recovery phase is the longest and ultimately most expensive phase. Once the immediate \"crisis\" has passed, cooperation fades away and recovery efforts are often politicized or seen as economic opportunities.\n\nPreparation for recovery can be very helpful. If mitigating resources are dispersed before the attack, cascades of social failures can be prevented. One hedge against bridge damage in riverine cities is to subsidize a \"tourist ferry\" that performs scenic cruises on the river. When a bridge is down, the ferry takes up the load.\n\nCivil Defense is also the name of a number of organizations around the world dedicated to protecting civilians from military attacks, as well as to providing rescue services after natural and human-made disasters alike.\n\nWorldwide protection is managed by the United Nations Office for the Coordination of Humanitarian Affairs (OCHA).\n\nIn a few countries such as Jordan and Singapore (see Singapore Civil Defence Force), civil defense is essentially the same organization as the fire brigade. In most countries, however, civil defense is a government-managed, volunteer-staffed organization, separate from the fire brigade and the ambulance service.\n\nAs the threat of Cold War eased, a number of such civil defense organizations have been disbanded or mothballed (as in the case of the Royal Observer Corps in the United Kingdom and the United States civil defense), while others have changed their focuses into providing rescue services after natural disasters (as for the State Emergency Service in Australia). However, the ideals of Civil Defense have been brought back in the United States under FEMA's Citizen Corps and Community Emergency Response Team (CERT).\n\nIn the United Kingdom Civil Defence work is carried out by Emergency Responders under the Civil Contingencies Act 2004, with assistance from voluntary groups such as RAYNET, Search and Rescue Teams and 4x4 Response. In Ireland, the Civil Defence is still very much an active organization and is occasionally called upon for its Auxiliary Fire Service and ambulance/rescue services when emergencies such as flash flooding occur and require additional manpower. The organization has units of trained firemen and medical responders based in key areas around the country.\n\nBULLET::::- Albanian Civil Protection\nBULLET::::- State Emergency Service – Australia\nBULLET::::- Belgian Civil Protection – Belgium\nBULLET::::- Defesa Civil – Brazil\nBULLET::::- Cyprus Civil Defence\nBULLET::::- Population Protection – Czech Republic\nBULLET::::- Beredskabsstyrelsen, or the Emergency Management Agency – Denmark\nBULLET::::- Protección Civil - El Salvador\nBULLET::::- Civil defense in Finland\nBULLET::::- Sécurité Civile – France\nBULLET::::- General Secretariat for Civil Protection – Greece\nBULLET::::- Civil Aid Service – Hong Kong\nBULLET::::- Civil Defence of India – India\nBULLET::::- Civil Defence Harir in Kurdistan – Iraq\nBULLET::::- Civil Defence Ireland\nBULLET::::- Isle of Man Civil Defence Corps\nBULLET::::- Civil defense in Israel\nBULLET::::- Protezione Civile – Italy\nBULLET::::- Malaysian Civil Defence Department\nBULLET::::- Protección Civil – Mexico\nBULLET::::- Corps des Sapeurs-Pompiers – Monaco\nBULLET::::- Civil Defence (New Zealand)\nBULLET::::- Nigeria security and civil defence corps – Nigeria\nBULLET::::- Norwegian Civil Defence\nBULLET::::- Panama Civil Defense Seismic Network\nBULLET::::- Portugal – Autoridade Nacional de Proteção Civil: http://www.prociv.pt\nBULLET::::- Civil Police – San Marino\nBULLET::::- Singapore Civil Defence Force\n\nUK:\nBULLET::::- Civil Defence Corps\nBULLET::::- 4x4 Response\nBULLET::::- UK's National Attack Warning System\nBULLET::::- Royal Observer Corps\n\nUS:\nBULLET::::- Civil Air Patrol\nBULLET::::- United States civil defense\nBULLET::::- United States civil defense association\nBULLET::::- Comprehensive Emergency Management\nBULLET::::- Federal Emergency Management Agency\nBULLET::::- CONELRAD\nBULLET::::- Duck and cover\n\nGermany:\nBULLET::::- Technisches Hilfswerk\nBULLET::::- DRK\nBULLET::::- ASB\nBULLET::::- DLRG\nBULLET::::- Feuerwehr\nBULLET::::- Johanniter\nBULLET::::- Malteser\nBULLET::::- Bergwacht\nBULLET::::- Verkehrswacht\nBULLET::::- Seenotretter\n\nBULLET::::- The American Civil Defense Association\nBULLET::::- French Civil Protection\nBULLET::::- Blast shelter\nBULLET::::- Civil-defense Geiger counters\nBULLET::::- Civil defense siren\nBULLET::::- Civilian-based defense\nBULLET::::- Collective protection\nBULLET::::- Continuity of government\nBULLET::::- Effects of nuclear explosions on human health\nBULLET::::- Emergency management\nBULLET::::- Fallout shelter\nBULLET::::- Transarmament\n\nGeneral:\nBULLET::::- Nuclear warfare\nBULLET::::- Nuclear holocaust\nBULLET::::- Nuclear terrorism\nBULLET::::- Survivalism\nBULLET::::- Weapon of mass destruction\n\nBULLET::::- Greece\nBULLET::::- Large gallery of Bulgaria's Civil Defense Mechanization\nBULLET::::- The UK Civil Defence Project – History & Photos\nBULLET::::- National Civil Defence College, Nagpur INDIA\nBULLET::::- Special Event Amateur Ham Radio Station operated from Bangalore, INDIA\nBULLET::::- Protezione Civile Italian Civil Defense\nBULLET::::- Dublin Civil Defence Ireland\nBULLET::::- SEBEV Search and Rescue (originally a Civil Defence team in the UK)\nBULLET::::- Civil Protection (Ministry of Interior, Spain).\nBULLET::::- Civil Protection Villena – Spain\nBULLET::::- Civil Defense Logo dies at 67, and Some Mourn its Passing, \"The New York Times\", 1 December 2006 by David Dunlap.\nBULLET::::- Cold War Era Civil Defense Museum – Features much historical information about Civil Defense history, its equipment and methods, and many historical photographs and posters.\nBULLET::::- Annotated bibliography for civil defense from the Alsos Digital Library for Nuclear Issues\nBULLET::::- The American Civil Defense Association\nBULLET::::- Civil Defense Caves – Cold War community getaway in case of nuclear war located in Idaho\nBULLET::::- Comprehensive Emergency Management Reference Material Repository\nBULLET::::- Ready.gov – The official preparedness site of the U.S. Department of Homeland Security\nBULLET::::- \"Civil Defence\" – A site with details of the UK's Civil Defence preparations, including those implemented during the Cold War such as the \"Burlington\" Central Government War HQ., at Corsham, Wiltshire.\nBULLET::::- Emergency Planning in Lincolnshire\nBULLET::::- The official Civil Defence site for the Republic of Ireland\nBULLET::::- The official Civil Defense site of São Paulo State – Brazil\nBULLET::::- Doctors for Disaster Preparedness\nBULLET::::- Physicians for Civil Defense\nBULLET::::- Dutch civil defense instructions in English\nBULLET::::- Emergency Management Portal – online resources for emergency planners and managers\nBULLET::::- The Norwegian Civil Defence\nBULLET::::- German Federal Agency for Technical Relief – THW Technisches Hilfswerk\n"}
{"id": "7060", "url": "https://en.wikipedia.org/wiki?curid=7060", "title": "Chymotrypsin", "text": "Chymotrypsin\n\nChymotrypsin (, chymotrypsins A and B, alpha-chymar ophth, avazyme, chymar, chymotest, enzeon, quimar, quimotrase, alpha-chymar, alpha-chymotrypsin A, alpha-chymotrypsin) is a digestive enzyme component of pancreatic juice acting in the duodenum, where it performs proteolysis, the breakdown of proteins and polypeptides. Chymotrypsin preferentially cleaves peptide amide bonds where the side chain of the amino acid C-terminal to the scissile amide bond (the P position) is a large hydrophobic amino acid (tyrosine, tryptophan, and phenylalanine). These amino acids contain an aromatic ring in their side chain that fits into a hydrophobic pocket (the S position) of the enzyme. It is activated in the presence of trypsin. The hydrophobic and shape complementarity between the peptide substrate P side chain and the enzyme S binding cavity accounts for the substrate specificity of this enzyme. Chymotrypsin also hydrolyzes other amide bonds in peptides at slower rates, particularly those containing leucine and methionine at the P position.\n\nStructurally, it is the archetypal structure for its superfamily, the PA clan of proteases.\n\nChymotrypsin is synthesized in the pancreas by protein biosynthesis as a precursor called chymotrypsinogen that is enzymatically inactive. Trypsin activates chymotrypsinogen by cleaving peptidic bonds in positions Arg15 - Ile16 and produces π-chymotrypsin. In turn, aminic group (-NH3) of the Ile16 residue interacts with the side chain of Glu194, producing the \"oxyanion hole\" and the hydrophobic \"S1 pocket\". Moreover, chymotrypsin induces its own activation by cleaving in positions 14-15, 146-147, and 148-149, producing α-chymotrypsin (which is more active and stable than π-chymotrypsin). The resulting molecule is a three-polypeptide molecule interconnected via disulfide bonds.\n\n\"In vivo\", chymotrypsin is a proteolytic enzyme (serine protease) acting in the digestive systems of many organisms. It facilitates the cleavage of peptide bonds by a hydrolysis reaction, which despite being thermodynamically favorable, occurs extremely slowly in the absence of a catalyst. The main substrates of chymotrypsin are peptide bonds in which the amino acid N-terminal to the bond is a tryptophan, tyrosine, phenylalanine, or leucine. Like many proteases, chymotrypsin also hydrolyses amide bonds \"in vitro\", a virtue that enabled the use of substrate analogs such as N-acetyl-L-phenylalanine p-nitrophenyl amide for enzyme assays.\nChymotrypsin cleaves peptide bonds by attacking the unreactive carbonyl group with a powerful nucleophile, the serine 195 residue located in the active site of the enzyme, which briefly becomes covalently bonded to the substrate, forming an enzyme-substrate intermediate. Along with histidine 57 and aspartic acid 102, this serine residue constitutes the catalytic triad of the active site.\n\nThese findings rely on inhibition assays and the study of the kinetics of cleavage of the aforementioned substrate, exploiting the fact that the enzyme-substrate intermediate \"p\"-nitrophenolate has a yellow colour, enabling measurement of its concentration by measuring light absorbance at 410 nm.\n\nThe reaction of chymotrypsin with its substrate was found to take place in two stages, an initial “burst” phase at the beginning of the reaction and a steady-state phase following Michaelis-Menten kinetics. The mode of action of chymotrypsin explains this as hydrolysis takes place in two steps. First, acylation of the substrate to form an acyl-enzyme intermediate, and then deacylation to return the enzyme to its original state. This occurs via the concerted action of the three-amino-acid residues in the catalytic triad. Aspartate hydrogen bonds to the N-δ hydrogen of histidine, increasing the pKa of its ε nitrogen, thus making it able to deprotonate serine. This deprotonation allows the serine side chain to act as a nucleophile and bind to the electron-deficient carbonyl carbon of the protein main chain. Ionization of the carbonyl oxygen is stabilized by formation of two hydrogen bonds to adjacent main chain N-hydrogens. This occurs in the oxyanion hole. This forms a tetrahedral adduct and breakage of the peptide bond. An acyl-enzyme intermediate, bound to the serine, is formed, and the newly formed amino terminus of the cleaved protein can dissociate. In the second reaction step, a water molecule is activated by the basic histidine, and acts as a nucleophile. The oxygen of water attacks the carbonyl carbon of the serine-bound acyl group, resulting in formation of a second tetrahedral adduct, regeneration of the serine -OH group, and release of a proton, as well as the protein fragment with the newly formed carboxyl terminus \n valign=\"top\"\n\nBULLET::::- Trypsin\nBULLET::::- PA clan of proteases\n\nBULLET::::- The MEROPS online database for peptidases and their inhibitors: S01.001\n"}
{"id": "7061", "url": "https://en.wikipedia.org/wiki?curid=7061", "title": "Community emergency response team", "text": "Community emergency response team\n\nIn the United States, community emergency response team (CERT) can refer to\nBULLET::::- one of five federal programs promoted under Citizen Corps;\nBULLET::::- an implementation of FEMA's National CERT Program, administered by a local sponsoring agency, which provides a standardized training and implementation framework to community members;\nBULLET::::- an organization of volunteer emergency workers who have received specific training in basic disaster response skills, and who agree to supplement existing emergency responders in the event of a major disaster.\n\nSometimes programs and organizations take different names, such as Neighborhood Emergency Response Team (NERT), or Neighborhood Emergency Team (NET).\n\nThe concept of civilian auxiliaries is similar to civil defense, which has a longer history. The CERT concept differs because it includes nonmilitary emergencies, and is coordinated with all levels of emergency authorities, local to national, via an overarching incident command system.\nA local government agency, often a fire department, police department, or emergency management agency, agrees to sponsor CERT within its jurisdiction. The sponsoring agency liaises with, deploys and may train or supervise the training of CERT members. Many sponsoring agencies employ a full-time community-service person as liaison to the CERT members. In some communities, the liaison is a volunteer and CERT member.\n\nAs people are trained and agree to join the community emergency response effort, a CERT is formed. Initial efforts may result in a team with only a few members from across the community. As the number of members grow, a single community-wide team may subdivide. Multiple CERTs are organized into a hierarchy of teams consistent with ICS principles. This follows the Incident Command System (ICS) principle of Span of control until the ideal distribution is achieved: one or more teams are formed at each neighborhood within a community.\n\nA Teen Community Emergency Response Team (TEEN CERT), or Student Emergency Response Team (SERT), can be formed from any group of teens. A Teen CERT can be formed as a school club, service organization, Venturing Crew, Explorer Post, or the training can be added to a school's graduation curriculum. Some CERTs form a club or service corporation, and recruit volunteers to perform training on behalf of the sponsoring agency. This reduces the financial and human resource burden on the sponsoring agency.\n\nWhen not responding to disasters or large emergencies, CERTs may\nBULLET::::- raise funds for emergency response equipment in their community;\nBULLET::::- provide first-aid, crowd control or other services at community events;\nBULLET::::- hold planning, training, or recruitment meetings; and\nBULLET::::- conduct or participate in disaster response exercises.\n\nSome sponsoring agencies use state and federal grants to purchase response tools and equipment for their members and team(s) (subject to Stafford Act limitations). Most CERTs also acquire their own supplies, tools, and equipment. As community members, CERTs are aware of the specific needs of their community and equip the teams accordingly.\n\nThe basic idea is to use CERT to perform the large number of tasks needed in emergencies. This frees highly trained professional responders for more technical tasks. Much of CERT training concerns the Incident Command System and organization, so CERT members fit easily into larger command structures.\n\nA team may self-activate (self-deploy) when their own neighborhood is affected by disaster. An effort is made to report their response status to the sponsoring agency. A self-activated team will size-up the loss in their neighborhood and begin performing the skills they have learned to minimize further loss of life, property, and environment. They will continue to respond safely until redirected or relieved by the sponsoring agency or professional responders on-scene.\n\nTeams in neighborhoods not affected by disaster may be deployed or activated by the sponsoring agency. The sponsoring agency may communicate with neighborhood CERT leaders through an organic communication team. In some areas the communications may be by amateur radio, FRS, GMRS or MURS radio, dedicated telephone or fire-alarm networks. In other areas, relays of bicycle-equipped runners can effectively carry messages between the teams and the local emergency operations center.\n\nThe sponsoring agency may activate and dispatch teams in order to gather or respond to intelligence about an incident. Teams may be dispatched to affected neighborhoods, or organized to support operations. CERT members may augment support staff at an Incident Command Post or Emergency Operations Center. Additional teams may also be created to guard a morgue, locate supplies and food, convey messages to and from other CERTs and local authorities, and other duties on an as-needed basis as identified by the team leader.\n\nIn the short term, CERTs perform data gathering, especially to locate mass-casualties requiring professional response, or situations requiring professional rescues, simple fire-fighting tasks (for example, small fires, turning off gas), light search and rescue, damage evaluation of structures, triage and first aid. In the longer term, CERTs may assist in the evacuation of residents, or assist with setting up a neighborhood shelter.\n\nWhile responding, CERT members are temporary volunteer government workers. In some areas, (such as California, Hawaii and Kansas) registered, activated CERT members are eligible for worker's compensation for on-the-job injuries during declared disasters.\n\nThe Federal Emergency Management Agency (FEMA) recommends that the standard, ten-person team be comprised as follows:\nBULLET::::- CERT Leader. Generally, the first CERT team member arriving on the scene becomes team leader, and is the designated Incident Commander (IC) until the arrival of someone more competent. This person makes the IC initial assessment of the scene and determines the appropriate course of action for team members; assumes role of Safety Officer until assigned to another team member; assigns team member roles if not already assigned; designates triage area, treatment area, morgue, and vehicle traffic routes; coordinates and directs team operations; determines logistical needs (water, food, medical supplies, transportation, equipment, and so on.) and determines ways to meet those needs through team members or citizen volunteers on the scene; collects and writes reports on the operation and victims; and communicates and coordinates with the incident commander, local authorities, and other CERT team leaders. The team leader is identified by two pieces of crossed tape on the hard hat.\nBULLET::::- Safety Officer. Checks team members prior to deployment to ensure they are safe and equipped for the operation; determines safe or unsafe working environments; ensures team accountability; supervises operations (when possible) where team members and victims are at direct physical risk, and alerts team members when unsafe conditions arise.\nBULLET::::- Fire Suppression Team (2 people). Work under the supervision of the Team Leader to suppress small fires in designated work areas or as needed; when not accomplishing their primary mission, assist the search and rescue team or triage team; assist in evacuation and transport as needed; assist in the triage or treatment area as needed, other duties as assigned; communicate with Team Leader.\nBULLET::::- Search and Rescue Team (2). Work under the supervision of the Team Leader, searching for and providing rescue of victims as is prudent under the conditions; when not accomplishing their primary mission, assist the Fire Suppression Team, assist in the triage or treatment area as needed; other duties as assigned; communicate with Team Leader.\nBULLET::::- Medical Triage Team (2). Work under the supervision of the Team Leader, providing START triage for victims found at the scene; marking victims with category of injury per the standard operating procedures; when not accomplishing their primary mission, assist the Fire Suppression Team if needed, assist the Search and Rescue Team if needed, assist in the Medical Triage Area if needed, assist in the Treatment Area if needed, other duties as assigned; communicate with Team Leader.\n\nBULLET::::- Medical Treatment Team (2). Work under the supervision of the Team Leader, providing medical treatment to victims within the scope of their training. This task is normally accomplished in the Treatment Area, however, it may take place in the affected area as well. When not accomplishing their primary mission, assist the Fire Suppression Team as needed, assist the Medical Triage Team as needed; other duties as assigned; communicate with the Team Leader.\n\nBecause every CERT member in a community receives the same core instruction, any team member has the training necessary to assume any of these roles. This is important during a disaster response because not all members of a regular team may be available to respond. Hasty teams may be formed by whichever members are responding at the time. Additionally, members may need to adjust team roles due to stress, fatigue, injury, or other circumstances.\n\nWhile state and local jurisdictions will implement training in the manner that best suits the community, FEMA's National CERT Program has an established curriculum. Jurisdictions may augment the training, but are strongly encouraged to deliver the entire core content. The CERT core curriculum for the basic course is composed of the following nine units (time is instructional hours):\nBULLET::::- Unit 1: Disaster Preparedness (2.5 hrs). Topics include (in part) identifying local disaster threats, disaster impact, mitigation and preparedness concepts, and an overview of Citizen Corps and CERT. Hands on skills include team-building exercises, and shutting off utilities.\nBULLET::::- Unit 2: Fire Safety (2.5 hrs). Students learn about fire chemistry, mitigation practices, hazardous materials identification, suppression options, and are introduced to the concept of size-up. Hands-on skills include using a fire extinguisher to suppress a live flame, and wearing basic protective gear. Firefighting standpipes as well as unconventional firefighting methods are also covered.\nBULLET::::- Unit 3: Disaster Medical Operations part 1 (2.5 hrs). Students learn to identify and treat certain life-threatening conditions in a disaster setting, as well as START triage. Hands-on skills include performing head-tilt/chin-lift, practicing bleeding control techniques, and performing triage as an exercise.\nBULLET::::- Unit 4: Disaster Medical Operations part 2 (2.5 hrs). Topics cover mass casualty operations, public health, assessing patients, and treating injuries. Students practice patient assessment, and various treatment techniques.\nBULLET::::- Unit 5: Light Search and Rescue Operations (2.5 hrs). Size-up is expanded as students learn about assessing structural damage, marking structures that have been searched, search techniques, as well as rescue techniques and cribbing. Hands-on activities include lifting and cribbing an object, and practicing rescue carries.\nBULLET::::- Unit 6: CERT Organization (1.5 hrs). Students are introduced to several concepts from the Incident Command System, and local team organization and communication is explained. Hands-on skills include a table-top exercise focusing on incident command and control.\nBULLET::::- Unit 7: Disaster Psychology (1 hr). Responder well-being and dealing with victim trauma are the topics of this unit.\nBULLET::::- Unit 8: Terrorism and CERT (2.5 hrs). Students learn how terrorists may choose targets, what weapons they may use, and identifying when chemical, biological, radiological, nuclear, or explosive weapons may have been deployed. Students learn about CERT roles in preparing for and responding to terrorist attacks. A table-top exercise highlights topics covered.\nBULLET::::- Unit 9: Course Review and Disaster Simulation (2.5 hrs). Students take a written exam, then participate in a real-time practical disaster simulation where the different skill areas are put to the test. A critique follows the exercise where students and instructors have an opportunity to learn from mistakes and highlight exemplary actions. Students may be given a certificate of completion at the conclusion of the course.\n\nCERT training emphasizes safely \"doing the most good for the most people as quickly as possible\" when responding to a disaster. For this reason, cardiopulmonary resuscitation (CPR) training is not included in the core curriculum, as it is time and responder intensive in a mass-casualty incident. However, many jurisdictions encourage or require CERT members to obtain CPR training. Many CERT programs provide or encourage members to take additional first aid training. Some CERT members may also take training to become a certified first responder or emergency medical technician.\n\nMany CERT programs also provide training in amateur radio operation, shelter operations, flood response, community relations, mass care, the incident command system (ICS), and the National Incident Management System (NIMS).\n\nEach unit of CERT training is ideally delivered by professional responders or other experts in the field addressed by the unit. This is done to help build unity between CERT members and responders, keep the attention of students, and help the professional response organizations be comfortable with the training which CERT members receive.\n\nEach course of instruction is ideally facilitated by one or more instructors certified in the CERT curriculum by the state or sponsoring agency. Facilitating instructors provide continuity between units, and help ensure that the CERT core curriculum is being delivered successfully. Facilitating instructors also perform set-up and tear-down of the classroom, provide instructional materials for the course, record student attendance and other tasks which assist the professional responder in delivering their unit as efficiently as possible.\n\nCERT training is provided free to interested members of the community, and is delivered in a group classroom setting. People may complete the training without obligation to join a CERT. Citizen Corps grant funds can be used to print and provide each student with a printed manual. Some sponsoring agencies use Citizen Corps grant funds to purchase disaster response tool kits. These kits are offered as an incentive to join a CERT, and must be returned to the sponsoring agency when members resign from CERT.\n\nSome sponsoring agencies require a criminal background-check of all trainees before allowing them to participate on a CERT. For example, the city of Albuquerque, New Mexico require all volunteers to pass a background check, while the city of Austin, Texas does not require a background check to take part in training classes but requires members to undergo a background check in order to receive a CERT badge and directly assist first responders during an activation of the Emergency Operations Center. However, most programs do not require a criminal background check in order to participate.\n\nThe CERT curriculum (including the Train-the-Trainer and Program Manager courses) was updated during the last half of 2017 to reflect feedback from instructors across the nation. The update is in final review, and is scheduled for release during 2018.\n\nBULLET::::- Local Emergency Planning Committee\nBULLET::::- Emergency management\nBULLET::::- Incident command system\nBULLET::::- Medical Reserve Corps\nBULLET::::- Disaster Preparedness and Response TeamPakistan based non-governmental organization modeled after CERT.\n\nBULLET::::- Citizen Corps CERT\nBULLET::::- CERT Training Materials (Program Manager, Trainer, Participant...)\n"}
{"id": "7063", "url": "https://en.wikipedia.org/wiki?curid=7063", "title": "Catapult", "text": "Catapult\n\nA catapult is a ballistic device used to launch a projectile a great distance without the aid of gunpowder or other propellants – particularly various types of ancient and medieval siege engines. A catapult uses the sudden release of stored potential energy to propel its payload. Most convert tension or torsion energy that was more slowly and manually built up within the device before release, via springs, bows, twisted rope, elastic, or any of numerous other materials and mechanisms. The counterweight trebuchet is a type of catapult that uses gravity.\n\nIn use since ancient times, the catapult has proven to be one of the most persistently effective mechanisms in warfare. In modern times the term can apply to devices ranging from a simple hand-held implement (also called a \"slingshot\") to a mechanism for launching aircraft from a ship.\n\nThe earliest catapults date to at least the 4th century BC with the advent of the mangonel in ancient China, a type of traction trebuchet and catapult. Early uses were also attributed to Ajatashatru of Magadha in his war against the Licchavis. Early Greek catapults emerged around the 1st century BC.\n\nThe word 'catapult' comes from the Latin 'catapulta', which in turn comes from the Greek (\"katapeltēs\"), itself from κατά (\"kata\"), \"downwards\" and πάλλω (\"pallō\"), \"to toss, to hurl\". Catapults were invented by the ancient Greeks and in ancient India where they were used by the Magadhan Emperor Ajatshatru around the early to mid 5th century BC.\n\nThe catapult and crossbow in Greece are closely intertwined. Primitive catapults were essentially \"the product of relatively straightforward attempts to increase the range and penetrating power of missiles by strengthening the bow which propelled them\". The historian Diodorus Siculus (fl. 1st century BC), described the invention of a mechanical arrow-firing catapult (\"katapeltikon\") by a Greek task force in 399 BC. The weapon was soon after employed against Motya (397 BC), a key Carthaginian stronghold in Sicily. Diodorus is assumed to have drawn his description from the highly rated history of Philistus, a contemporary of the events then. The introduction of crossbows however, can be dated further back: according to the inventor Hero of Alexandria (fl. 1st century AD), who referred to the now lost works of the 3rd-century BC engineer Ctesibius, this weapon was inspired by an earlier foot-held crossbow, called the \"gastraphetes\", which could store more energy than the Greek bows. A detailed description of the \"gastraphetes\", or the \"belly-bow\", along with a watercolor drawing, is found in Heron's technical treatise \"Belopoeica\".\n\nA third Greek author, Biton (fl. 2nd century BC), whose reliability has been positively reevaluated by recent scholarship, described two advanced forms of the \"gastraphetes\", which he credits to Zopyros, an engineer from southern Italy. Zopyrus has been plausibly equated with a Pythagorean of that name who seems to have flourished in the late 5th century BC. He probably designed his bow-machines on the occasion of the sieges of Cumae and Milet between 421 BC and 401 BC. The bows of these machines already featured a winched pull back system and could apparently throw two missiles at once.\n\nPhilo of Byzantium provides probably the most detailed account on the establishment of a theory of belopoietics (\"belos\" = \"projectile\"; \"poietike\" = \"(art) of making\") circa 200 BC. The central principle to this theory was that \"all parts of a catapult, including the weight or length of the projectile, were proportional to the size of the torsion springs\". This kind of innovation is indicative of the increasing rate at which geometry and physics were being assimilated into military enterprises.\n\nFrom the mid-4th century BC onwards, evidence of the Greek use of arrow-shooting machines becomes more dense and varied: arrow firing machines (\"katapaltai\") are briefly mentioned by Aeneas Tacticus in his treatise on siegecraft written around 350 BC. An extant inscription from the Athenian arsenal, dated between 338 and 326 BC, lists a number of stored catapults with shooting bolts of varying size and springs of sinews. The later entry is particularly noteworthy as it constitutes the first clear evidence for the switch to torsion catapults, which are more powerful than the more-flexible crossbows and which came to dominate Greek and Roman artillery design thereafter. This move to torsion springs was likely spurred by the engineers of Philip II of Macedonia. Another Athenian inventory from 330 to 329 BC includes catapult bolts with heads and flights. As the use of catapults became more commonplace, so did the training required to operate them. Many Greek children were instructed in catapult usage, as evidenced by \"a 3rd Century B.C. inscription from the island of Ceos in the Cyclades [regulating] catapult shooting competitions for the young\". Arrow firing machines in action are reported from Philip II's siege of Perinth (Thrace) in 340 BC. At the same time, Greek fortifications began to feature high towers with shuttered windows in the top, which could have been used to house anti-personnel arrow shooters, as in Aigosthena. Projectiles included both arrows and (later) stones that were sometimes lit on fire. Onomarchus of Phocis first used catapults on the battlefield against Philip II of Macedon. Philip's son, Alexander the Great, was the next commander in recorded history to make such use of catapults on the battlefield as well as to use them during sieges.\n\nThe Romans started to use catapults as arms for their wars against Syracuse, Macedon, Sparta and Aetolia (3rd and 2nd centuries BC). The Roman machine known as an arcuballista was similar to a large crossbow. Later the Romans used ballista catapults on their warships.\n\nAjatshatru is recorded in Jaina texts as having used catapults in his campaign against the Licchavis.\n\nKing Uzziah, who reigned in Judah until 750 BC, is documented as having overseen the construction of machines to \"shoot great stones\" in .\n\nThe first recorded use of mangonels was in ancient China. They were probably used by the Mohists as early as 4th century BC, descriptions of which can be found in the \"Mojing\" (compiled in the 4th century BC). In Chapter 14 of the \"Mojing\", the mangonel is described hurling hollowed out logs filled with burning charcoal at enemy troops. The mangonel was carried westward by the Avars and appeared next in the eastern Mediterranean by the late 6th century AD, where it replaced torsion powered siege engines such as the ballista and onager due to its simpler design and faster rate of fire. The Byzantines adopted the mangonel possibly as early as 587, the Persians in the early 7th century, and the Arabs in the second half of the 7th century. The Franks and Saxons adopted the weapon in the 8th century.\n\nCastles and fortified walled cities were common during this period and catapults were used as siege weapons against them. As well as their use in attempts to breach walls, incendiary missiles, or diseased carcasses or garbage could be catapulted over the walls.\n\nDefensive techniques in the Middle Ages progressed to a point that rendered catapults largely ineffective. The Viking siege of Paris (885–6 A.D.) \"saw the employment by both sides of virtually every instrument of siege craft known to the classical world, including a variety of catapults\", to little effect, resulting in failure.\n\nThe most widely used catapults throughout the Middle Ages were as follows:\n\nBULLET::::- Ballista: Ballistae were similar to giant crossbows and were designed to work through torsion. The projectiles were large arrows or darts made from wood with an iron tip. These arrows were then shot \"along a flat trajectory\" at a target. Ballistae were accurate, but lacked firepower compared with that of a mangonel or trebuchet. Because of their immobility, most ballistae were constructed on site following a siege assessment by the commanding military officer.\nBULLET::::- Springald: The springald's design resembles that of the ballista, being a crossbow powered by tension. The springald's frame was more compact, allowing for use inside tighter confines, such as the inside of a castle or tower, but compromising its power.\nBULLET::::- Mangonel: This machine was designed to throw heavy projectiles from a \"bowl-shaped bucket at the end of its arm\". Mangonels were mostly used for “firing various missiles at fortresses, castles, and cities,” with a range of up to 1300 feet. These missiles included anything from stones to excrement to rotting carcasses. Mangonels were relatively simple to construct, and eventually wheels were added to increase mobility.\nBULLET::::- Onager: Mangonels are also sometimes referred to as Onagers. Onager catapults initially launched projectiles from a sling, which was later changed to a \"bowl-shaped bucket\". The word \"Onager\" is derived from the Greek word \"onagros\" for \"wild ass\", referring to the \"kicking motion and force\" that were recreated in the Mangonel's design. Historical records regarding onagers are scarce. The most detailed account of Mangonel use is from “Eric Marsden's translation of a text written by Ammianus Marcellius in the 4th Century AD” describing its construction and combat usage.\nBULLET::::- Trebuchet: Trebuchets were probably the most powerful catapult employed in the Middle Ages. The most commonly used ammunition were stones, but \"darts and sharp wooden poles\" could be substituted if necessary. The most effective kind of ammunition though involved fire, such as \"firebrands, and deadly Greek Fire\". Trebuchets came in two different designs: Traction, which were powered by people, or Counterpoise, where the people were replaced with \"a weight on the short end\". The most famous historical account of trebuchet use dates back to the siege of Stirling Castle in 1304, when the army of Edward I constructed a giant trebuchet known as Warwolf, which then proceeded to \"level a section of [castle] wall, successfully concluding the siege\".\nBULLET::::- Couillard: A simplified trebuchet, where the trebuchet's single counterweight is split, swinging on either side of a central support post.\nBULLET::::- Leonardo da Vinci's catapult: Leonardo da Vinci sought to improve the efficiency and range of earlier designs. His design incorporated a large wooden leaf spring as an accumulator to power the catapult. Both ends of the bow are connected by a rope, similar to the design of a bow and arrow. The leaf spring was not used to pull the catapult armature directly, rather the rope was wound around a drum. The catapult armature was attached to this drum which would be turned until enough potential energy was stored in the deformation of the spring. The drum would then be disengaged from the winding mechanism, and the catapult arm would snap around. Though no records exist of this design being built during Leonardo's lifetime, contemporary enthusiasts have reconstructed it.\n\nThe last large scale military use of catapults was during the trench warfare of World War I. During the early stages of the war, catapults were used to throw hand grenades across no man's land into enemy trenches. They were eventually replaced by small mortars.\n\nIn the 1840s the invention of vulcanized rubber allowed the making of small hand-held catapults, either improvised from Y-shaped sticks or manufactured for sale; both were popular with children and teenagers. These devices were also known as slingshots in the USA.\n\nSpecial variants called aircraft catapults are used to launch planes from land bases and sea carriers when the takeoff runway is too short for a powered takeoff or simply impractical to extend. Ships also use them to launch torpedoes and deploy bombs against submarines. Small catapults, referred to as \"traps\", are still widely used to launch clay targets into the air in the sport of clay pigeon shooting.\n\nIn the 1990s and into the early 2000s, a powerful catapult, a trebuchet, was used by thrill-seekers first on private property and in 2001-2002 at Middlemoor Water Park, Somerset, England to experience being catapulted through the air for . The practice has been discontinued due a fatality at the Water Park. There had been an injury when the trebuchet was in use on private property. Injury and death occurred when those two participants failed to land onto the safety net. The operators of the trebuchet were tried, but found not guilty of manslaughter, though the jury noted that the fatality might have been avoided had the operators \"imposed stricter safety measures.\" Human cannonball circus acts use a catapult launch mechanism, rather than gunpowder, and are risky ventures for the human cannonballs.\n\nEarly launched roller coasters used a catapult system powered by a diesel engine or a dropped weight to acquire their momentum, such as Shuttle Loop installations between 1977-1978. The catapult system for roller coasters has been replaced by flywheels and later linear motors.\n\n\"Pumpkin chunking\" is another widely popularized use, in which people compete to see who can launch a pumpkin the farthest by mechanical means (although the world record is held by a pneumatic air cannon).\n\nIn January 2011, a homemade catapult was discovered that was used to smuggle cannabis into the United States from Mexico. The machine was found 20 feet from the border fence with bales of cannabis ready to launch.\n\nBULLET::::- Aircraft catapult\nBULLET::::- Mangonel\nBULLET::::- Mass driver\nBULLET::::- National Catapult Contest\nBULLET::::- Sling (weapon)\nBULLET::::- Trebuchet\nBULLET::::- .\nBULLET::::- .\n\nBULLET::::- .\nBULLET::::- .\n"}
{"id": "7066", "url": "https://en.wikipedia.org/wiki?curid=7066", "title": "Cinquain", "text": "Cinquain\n\nCinquain is a class of poetic forms that employ a 5-line pattern. Earlier used to describe any five-line form, it now refers to one of several forms that are defined by specific rules and guidelines.\n\nThe modern form, known as American Cinquain inspired by Japanese haiku and tanka, is akin in spirit to that of the Imagists.\nIn her 1915 collection titled \"Verse\", published one year after her death, Adelaide Crapsey included 28 cinquains.\nCrapsey's American Cinquain form developed in two stages. The first, fundamental form is a stanza of five lines of accentual verse, in which the lines comprise, in order, 1, 2, 3, 4, and 1 stresses. Then Crapsey decided to make the criterion a stanza of five lines of accentual-syllabic verse, in which the lines comprise, in order, 1, 2, 3, 4, and 1 stresses and 2, 4, 6, 8, and 2 syllables. Iambic feet were meant to be the standard for the cinquain, which made the dual criteria match perfectly. Some resource materials define classic cinquains as solely iambic, but that is not necessarily so. In contrast to the Eastern forms upon which she based them, Crapsey always titled her cinquains, effectively utilizing the title as a sixth line. Crapsey's cinquain depends on strict structure and intense physical imagery to communicate a mood or feeling.\n\nThe form is illustrated by Crapsey's \"November Night\":\n\nListen... <br>\nWith faint dry sound, <br>\nLike steps of passing ghosts, <br>\nThe leaves, frost-crisp'd, break from the trees <br>\nAnd fall.\n\nThe Scottish poet William Soutar also wrote over one hundred American Cinquains (he labelled them Epigrams) between 1933 and 1940.\n\nThe Crapsey cinquain has subsequently seen a number of variations by modern poets, including:\n\n!Variation\n!Description\nReverse cinquain\na form with one 5-line stanza in a syllabic pattern of two, eight, six, four, two.\nMirror cinquain\na form with two 5-line stanzas consisting of a cinquain followed by a reverse cinquain.\nButterfly cinquain\na nine-line syllabic form with the pattern two, four, six, eight, two, eight, six, four, two.\nCrown cinquain\na sequence of five cinquain stanzas functioning to construct one larger poem.\nGarland cinquain\na series of six cinquains in which the last is formed of lines from the preceding five, typically line one from stanza one, line two from stanza two, and so on.\n\nThe didactic cinquain is closely related to the Crapsey cinquain. It is an informal cinquain widely taught in elementary schools and has been featured in, and popularized by, children's media resources, including Junie B. Jones and PBS Kids. This form is also embraced by young adults and older poets for its expressive simplicity. The prescriptions of this type of cinquain refer to word count, not syllables and stresses. Ordinarily, the first line is a one-word title, the subject of the poem; the second line is a pair of adjectives describing that title; the third line is a three-word phrase that gives more information about the subject (often a list of three gerunds); the fourth line consists of four words describing feelings related to that subject; and the fifth line is a single word synonym or other reference for the subject from line one.\n\nFor example:\n\nSnow\nSilent, white\nDancing, falling, drifting\nCovering everything it touches\nBlanket\n!Form\n!Description\nTanka\nis a five-line form of unrhymed Japanese poetry, totalling 31 moras structured in a 5-7-5-7-7 pattern.\nTetractys\nis a five-line poem of 20 syllables with a title, arranged in the following order: 1, 2, 3, 4, 10, with each line standing as a phrase on its own. It can be inverted, doubled, etc. and was created by English poet Ray Stebbings.\nLanterne\nis an untitled five line quintain verse with a syllabic pattern of 1, 2, 3, 4, 1. Each line is usually able to stand on its own.\n\nBULLET::::- Quintain (poetry)\nBULLET::::- Gogyōshi\nBULLET::::- Poetry\n\nBULLET::::- Essay-Introduction to American cinquains\n"}
{"id": "7067", "url": "https://en.wikipedia.org/wiki?curid=7067", "title": "Cook Islands", "text": "Cook Islands\n\nThe Cook Islands (Cook Islands Māori: \"Kūki 'Āirani\") is a self-governing island country in the South Pacific Ocean in free association with New Zealand. It comprises 15 islands whose total land area is . The Cook Islands' Exclusive Economic Zone (EEZ) covers of ocean.\n\nNew Zealand is responsible for the Cook Islands' defence and foreign affairs, but these responsibilities are exercised in consultation with the Cook Islands. In recent times, the Cook Islands have adopted an increasingly independent foreign policy. Cook Islanders are citizens of New Zealand, but they also have the status of Cook Islands nationals, which is not given to other New Zealand citizens. The Cook Islands has been an active member of the Pacific Community since 1980.\n\nThe Cook Islands' main population centres are on the island of Rarotonga (10,572 in 2011), where there is an international airport. There is also a larger population of Cook Islanders in New Zealand itself: in the 2013 census, 61,839 people said they were Cook Islanders, or of Cook Islands descent.\n\nWith over 168,000 visitors travelling to the islands in 2018, tourism is the country's main industry, and the leading element of the economy, ahead of offshore banking, pearls, and marine and fruit exports.\n\nThe latest carbon-dating evidence reveals that the Cook Islands were first settled by around AD 1000 by Polynesian people who are thought to have migrated from Tahiti, an island to the northeast of the main island of Rarotonga.\n\nSpanish ships visited the islands in the 16th century. The first written record came in 1595 when the island of Pukapuka was sighted by Spanish sailor Álvaro de Mendaña de Neira, who gave it the name \"San Bernardo\" (Saint Bernard). Pedro Fernandes de Queirós, a Portuguese captain working for the Spanish crown, made the first recorded European landing in the islands when he set foot on Rakahanga in 1606, calling the island \"Gente Hermosa\" (Beautiful People).\n\nBritish navigator Captain James Cook arrived in 1773 and again in 1777 giving the island of Manuae the name \"Hervey Island\". The \"Hervey Islands\" later came to be applied to the entire southern group. The name \"Cook Islands\", in honour of Cook, first appeared on a Russian naval chart published in the 1820s.\n\nIn 1813 John Williams, a missionary on the \"Endeavour\" (not the same ship as Cook's) made the first recorded sighting of Rarotonga. The first recorded landing on Rarotonga by Europeans was in 1814 by the \"Cumberland\"; trouble broke out between the sailors and the Islanders and many were killed on both sides. The islands saw no more Europeans until English missionaries arrived in 1821. Christianity quickly took hold in the culture and many islanders are Christians today.\n\nThe islands were a popular stop in the 19th century for whaling ships from the United States, Britain and Australia. They visited, from at least 1826, to obtain water, food, and firewood. Their favourite islands were Rarotonga, Aitutaki, Mangaia and Penrhyn.\nThe Cook Islands became a British protectorate in 1888, largely because of community fears that France might occupy the islands as it already had Tahiti. On 6 September 1900, the islanders' leaders presented a petition asking that the islands (including Niue \"if possible\") be annexed as British territory. On 8 and 9 October 1900, seven instruments of cession of Rarotonga and other islands were signed by their chiefs and people. A British Proclamation was issued, stating that the cessions were accepted and the islands declared parts of Her Britannic Majesty's dominions. However, it did not include Aitutaki. Even though the inhabitants regarded themselves as British subjects, the Crown's title was unclear until the island was formally annexed by that Proclamation. In 1901 the islands were included within the boundaries of the Colony of New Zealand by Order in Council under the Colonial Boundaries Act, 1895 of the United Kingdom. The boundary change became effective on 11 June 1901, and the Cook Islands have had a formal relationship with New Zealand since that time.\n\nWhen the British Nationality and New Zealand Citizenship Act 1948 came into effect on 1 January 1949, Cook Islanders who were British subjects automatically gained New Zealand citizenship. The islands remained a New Zealand dependent territory until the New Zealand Government decided to grant them self-governing status. On 4 August 1965, a constitution was promulgated. The first Monday in August is celebrated each year as Constitution Day. Albert Henry of the Cook Islands Party was elected as the first Premier. Henry led the nation until 1978, when he was accused of vote-rigging and resigned. He was succeeded by Tom Davis of the Democratic Party.\n\nIn March 2019, it was reported that the Cook Islands had plans to change its name and remove the reference to Captain James Cook in favour of \"a title that reflects its 'Polynesian nature'\". It was later reported in May 2019 that the proposed name change had been poorly received by the Cook Islands diaspora. As a compromise, it was decided that the English name of the islands would not be altered, but that a new Cook Islands Māori name would be adopted to replace the current name, a transliteration from English.\n\nThe Cook Islands are in the South Pacific Ocean, northeast of New Zealand, between French Polynesia and American Samoa. There are 15 major islands spread over of ocean, divided into two distinct groups: the Southern Cook Islands and the Northern Cook Islands of coral atolls.\n\nThe islands were formed by volcanic activity; the northern group is older and consists of six atolls, which are sunken volcanoes topped by coral growth. The climate is moderate to tropical. The Cook Islands consist of 15 islands and two reefs.\n\n! Group \n! Island\n! Areakm²\n! Population\n! Density\n! Total \n! Total\n! style='text-align:right;'  237\n! style='text-align:right;'  17,459\n! style='text-align:right;'  73.7\n\nThe table is ordered from north to south. Population figures from the 2016 census.\n\nThe Cook Islands is a representative democracy with a parliamentary system in an associated state relationship with New Zealand. Executive power is exercised by the government, with the Chief Minister as head of government. Legislative power is vested in both the government and the Parliament of the Cook Islands. There is a pluriform multi-party system. The Judiciary is independent of the executive and the legislature. The head of state is the Queen of New Zealand, who is represented in the Cook Islands by the Queen's Representative.\n\nThe islands are self-governing in \"free association\" with New Zealand. New Zealand retains primary responsibility for external affairs, with consultation with the Cook Islands government. Cook Islands nationals are citizens of New Zealand and can receive New Zealand government services, but the reverse is not true; New Zealand citizens are not Cook Islands nationals. Despite this, , the Cook Islands had diplomatic relations in its own name with 43 other countries. The Cook Islands is not a United Nations member state, but, along with Niue, has had their \"full treaty-making capacity\" recognised by United Nations Secretariat, and is a full member of the WHO and UNESCO UN specialised agencies, is an associate member of the Economic and Social Commission for Asia and the Pacific (UNESCAP) and a Member of the Assembly of States of the International Criminal Court.\n\nOn 11 June 1980, the United States signed a treaty with the Cook Islands specifying the maritime border between the Cook Islands and American Samoa and also relinquishing any American claims to Penrhyn, Pukapuka, Manihiki, and Rakahanga. In 1990 the Cook Islands and France signed a treaty that delimited the boundary between the Cook Islands and French Polynesia. In late August 2012, United States Secretary of State Hillary Clinton visited the islands. In 2017, the Cook Islands signed the UN treaty on the Prohibition of Nuclear Weapons.\n\nMale homosexuality is illegal in the Cook Islands and is punishable by a maximum term of seven years imprisonment.\n\nThere are island councils on all of the inhabited outer islands (Outer Islands Local Government Act 1987 with amendments up to 2004, and Palmerston Island Local Government Act 1993) except Nassau, which is governed by Pukapuka (Suwarrow, with only one caretaker living on the island, also governed by Pukapuka, is not counted with the inhabited islands in this context). Each council is headed by a mayor.\n+ The Ten Outer Islands Councils are:\n\nAitutaki (including uninhabited Manuae)\n\nAtiu (including uninhabited Takutea)\n\nMangaia\n\nManihiki\n\nMa'uke\n\nMitiaro\n\nPalmerston\n\nPenrhyn\n\nPukapuka (including Nassau and Suwarrow)\n\nRakahanga\n\nThe three \"Vaka\" councils of Rarotonga established in 1997 (\"Rarotonga Local Government Act 1997\"), also headed by mayors, were abolished in February 2008, despite much controversy.\n\n+The three Vaka councils on Rarotonga were:\nTe-Au-O-Tonga\nPuaikura\nArorangi\nTakitumu\nMatavera, Ngatangiia, Takitumu\n\nOn the lowest level, there are village committees. Nassau, which is governed by Pukapuka, has an island committee (Nassau Island Committee), which advises the Pukapuka Island Council on matters concerning its own island.\n\nBirths and deaths\n! style=\"width:70px;\"Year\n! style=\"width:70px;\"Population\n! style=\"width:70px;\"Live births\n! style=\"width:70px;\"Deaths\n! style=\"width:70px;\"Natural increase\n! style=\"width:70px;\"Crude birth rate\n! style=\"width:70px;\"Crude death rate\n! style=\"width:70px;\"Rate of natural increase\n! style=\"width:70px;\"TFR\n12.6\n3.2\n9.4\n12.1\n3.9\n8.2\n13.6\n3.7\n9.8\n13.3\n5.3\n7.9\n13.8\n6.2\n7.6\n\nThe economy is strongly affected by geography. It is isolated from foreign markets, and has some inadequate infrastructure; it lacks major natural resources, has limited manufacturing and suffers moderately from natural disasters. Tourism provides the economic base that makes up approximately 67.5% of GDP. Additionally, the economy is supported by foreign aid, largely from New Zealand. China has also contributed foreign aid, which has resulted in, among other projects, the Police Headquarters building. The Cook Islands is expanding its agriculture, mining and fishing sectors, with varying success.\n\nSince approximately 1989, the Cook Islands have become a location specialising in so-called asset protection trusts, by which investors shelter assets from the reach of creditors and legal authorities. According to \"The New York Times\", the Cooks have \"laws devised to protect foreigners' assets from legal claims in their home countries\", which were apparently crafted specifically to thwart the long arm of American justice; creditors must travel to the Cook Islands and argue their cases under Cooks law, often at prohibitive expense. Unlike other foreign jurisdictions such as the British Virgin Islands, the Cayman Islands and Switzerland, the Cooks \"generally disregard foreign court orders\" and do not require that bank accounts, real estate, or other assets protected from scrutiny (it is illegal to disclose names or any information about Cooks trusts) be physically located within the archipelago. Taxes on trusts and trust employees account for some 8% of the Cook Islands economy, behind tourism but ahead of fishing.\n\nIn recent years, the Cook Islands has gained a reputation as a debtor paradise, through the enactment of legislation that permits debtors to shield their property from the claims of creditors.\n\nThe languages of the Cook Islands include English, Cook Islands Māori, or \"Rarotongan,\" and Pukapukan. Dialects of Cook Islands Maori include Penrhyn; Rakahanga-Manihiki; the Ngaputoru dialect of Atiu, Mitiaro, and Mauke; the Aitutaki dialect; and the Mangaian dialect. Cook Islands Maori and its dialectic variants are closely related to both Tahitian and to New Zealand Māori. Pukapukan is considered closely related to the Samoan language. English and Cook Islands Māori are official languages of the Cook Islands; per the Te Reo Maori Act. The legal definition of Cook Islands Māori includes Pukapukan.\n\nMusic in the Cook Islands is varied, with Christian songs being quite popular, but traditional dancing and songs in Polynesian languages remain popular.\n\nWoodcarving is a common art form in the Cook Islands. The proximity of islands in the southern group helped produce a homogeneous style of carving but that had special developments in each island. Rarotonga is known for its fisherman's gods and staff-gods, Atiu for its wooden seats, Mitiaro, Mauke and Atiu for mace and slab gods and Mangaia for its ceremonial adzes. Most of the original wood carvings were either spirited away by early European collectors or were burned in large numbers by missionaries. Today, carving is no longer the major art form with the same spiritual and cultural emphasis given to it by the Maori in New Zealand. However, there are continual efforts to interest young people in their heritage and some good work is being turned out under the guidance of older carvers. Atiu, in particular, has a strong tradition of crafts both in carving and local fibre arts such as tapa. Mangaia is the source of many fine adzes carved in a distinctive, idiosyncratic style with the so-called double-k design. Mangaia also produces food pounders carved from the heavy calcite found in its extensive limestone caves.\n\nThe outer islands produce traditional weaving of mats, basketware and hats. Particularly fine examples of rito hats are worn by women to church. They are made from the uncurled immature fibre of the coconut palm and are of very high quality. The Polynesian equivalent of Panama hats, they are highly valued and are keenly sought by Polynesian visitors from Tahiti. Often, they are decorated with hatbands made of minuscule pupu shells that are painted and stitched on by hand. Although pupu are found on other islands the collection and use of them in decorative work has become a speciality of Mangaia. The weaving of rito is a speciality of the northern islands, Manihiki, Rakahanga and Penrhyn.\n\nA major art form in the Cook Islands is tivaevae. This is, in essence, the art of handmade Island scenery patchwork quilts. Introduced by the wives of missionaries in the 19th century, the craft grew into a communal activity, which is probably one of the main reasons for its popularity.\n\nThe Cook Islands has produced internationally recognised contemporary artists, especially in the main island of Rarotonga. Artists include painter (and photographer) Mahiriki Tangaroa, sculptors Eruera (Ted) Nia (originally a film maker) and master carver Mike Tavioni, painter (and Polynesian tattoo enthusiast) Upoko'ina Ian George, Aitutakian-born painter Tim Manavaroa Buchanan, Loretta Reynolds, Judith Kunzlé, Joan Rolls Gragg, Kay George (who is also known for her fabric designs), Apii Rongo, Varu Samuel, and multi-media, installation and community-project artist Ani O'Neill, all of whom currently live on the main island of Rarotonga. Atiuan-based Andrea Eimke is an artist who works in the medium of tapa and other textiles, and also co-authored the book 'Tivaivai – The Social Fabric of the Cook Islands' with British academic Susanne Kuechler. Many of these artists have studied at university art schools in New Zealand and continue to enjoy close links with the New Zealand art scene.\n\nNew Zealand-based Cook Islander artists include Michel Tuffery, print-maker David Teata, Richard Shortland Cooper, Sylvia Marsters and Jim Vivieaere.\n\nOn Rarotonga, the main commercial galleries are Beachcomber Contemporary Art (Taputapuatea, Avarua) run by Ben & Trevon Bergman, and The Art Studio Gallery (Arorangi) run by Ian and Kay George. The Cook Islands National Museum also exhibits art.\n\nBULLET::::- The national flower of the Cook Islands is the \"Tiare māori\" or \"Tiale māoli\" (Penrhyn, Nassau, Pukapuka).\nBULLET::::- The Cook Islands have a large non-native population of Ship rat and \"Kiore toka\" (Polynesian rat). The rats have dramatically reduced the bird population on the islands.\nBULLET::::- In April 2007, 27 Kuhl's lorikeet were re-introduced to Atiu from Rimatara. Fossil and oral traditions indicate that the species was formerly on at least five islands of the southern group. Excessive exploitation for its red feathers is the most likely reason for the species's extinction in the Cook Islands.\n\nRugby league is the most popular sport in the Cook Islands.\n\nBULLET::::- Demographics of the Cook Islands\nBULLET::::- Index of Cook Islands-related articles\nBULLET::::- List of Cook Islanders\nBULLET::::- List of islands\nBULLET::::- Outline of the Cook Islands\nBULLET::::- Gilson, Richard. \"The Cook Islands 1820–1950.\" Wellington, New Zealand: Victoria University Press, 1980.\n\nBULLET::::- Cook Islands Government\nBULLET::::- Chief of State and Cabinet Members\nBULLET::::- Cook Islands from \"UCB Libraries GovPubs\"\n"}
{"id": "7068", "url": "https://en.wikipedia.org/wiki?curid=7068", "title": "History of the Cook Islands", "text": "History of the Cook Islands\n\nThe Cook Islands are named after Captain James Cook, who visited the islands in 1773 and 1777. The Cook Islands became a British protectorate in 1888.\n\nBy 1900, the islands were annexed as British territory. In 1901, the islands were included within the boundaries of the Colony of New Zealand.\n\nThe Cook Islands contain 15 islands in the group spread over a vast area in the South Pacific. The majority of islands are low coral atolls in the Northern Group, with Rarotonga, a volcanic island in the Southern Group, as the main administration and government centre. The main Cook Islands language is Rarotongan Māori. There are some variations in dialect in the 'outer' islands.\n\nIt is thought that the Cook Islands may have been settled between the years 900-1200 AD. Early settlements suggest that the settlers were generally great warriors migrating from Tahiti, to the northeast of the Cooks. The Cook Islands continue to hold important connections with Tahiti, and this is generally found in the two countries' culture, tradition and language. It is also thought that the early settlers were true Tahitians, who landed in Rarotonga (Takitumu city). There are notable historic epics of great warriors who travel between the two nations for a wide variety of reasons. The purpose of these missions is still unclear but recent research indicates that large to small groups often fled their island due to local wars being forced upon them. For each group to travel and to survive, they would normally rely on a warrior to lead them. Outstanding warriors are still mentioned in the countries' traditions and stories.\n\nThese arrivals are evidenced by an older road in Toi, the \"Ara Metua\", which runs around most of Rarotonga, and is believed to be at least 1200 years old. This 29 km long, paved road is a considerable achievement of ancient engineering, possibly unsurpassed elsewhere in Polynesia. The islands of Manihiki and Rakahanga trace their origins to the arrival of Toa, an outcast from Rarotonga, and Tupaeru, a high-ranking woman from the Puaikura tribe of Rarotonga. The remainder of the northern islands were probably settled by expeditions from Samoa.\n\nSpanish ships visited the islands in the 16th century; the first written record of contact between Europeans and the native inhabitants of the Cook Islands came with the sighting of Pukapuka by Spanish sailor Álvaro de Mendaña in 1595, who called it \"San Bernardo\" (Saint Bernard). Portuguese-Spaniard Pedro Fernández de Quirós made the first recorded European landing in the islands when he set foot on Rakahanga in 1606, calling it \"Gente Hermosa\" (Beautiful People).\n\nBritish navigator Captain James Cook arrived in 1773 and 1777. Cook named the islands the 'Hervey Islands' to honour a British Lord of the Admiralty. Half a century later, the Russian Baltic German Admiral Adam Johann von Krusenstern published the \"Atlas de l'Ocean Pacifique\", in which he renamed the islands the Cook Islands to honour Cook. Captain Cook navigated and mapped much of the group. Surprisingly, Cook never sighted the largest island, Rarotonga, and the only island that he personally set foot on was the tiny, uninhabited Palmerston Atoll.\n\nThe first recorded landing by Europeans was in 1814 by the \"Cumberland\"; trouble broke out between the sailors and the Islanders and many were killed on both sides.\n\nThe islands saw no more Europeans until missionaries arrived from England in 1821. Christianity quickly took hold in the culture and remains the predominant religion today.\n\nIn 1823, Captain John Dibbs of the colonial barque \"Endeavour\" made the first official sighting of the island Rarotonga. The \"Endeavour\" was transporting Rev. John Williams on a missionary voyage to the islands.\n\nBrutal Peruvian slave traders, known as blackbirders, took a terrible toll on the islands of the Northern Group in 1862 and 1863. At first, the traders may have genuinely operated as labour recruiters, but they quickly turned to subterfuge and outright kidnapping to round up their human cargo. The Cook Islands was not the only island group visited by the traders, but Penrhyn Atoll was their first port of call and it has been estimated that three-quarters of the population was taken to Callao, Peru. Rakahanga and Pukapuka also suffered tremendous losses.\n\nThe Cook Islands became a British protectorate in 1888, due largely to community fears that France might occupy the territory as it had Tahiti. On 6 September 1900, the leading islanders presented a petition asking that the islands (including Niue \"if possible\") should be annexed as British territory. On 8–9 October 1900, seven instruments of cession of Rarotonga and other islands were signed by their chiefs and people, and a British proclamation issued at the same time accepted the cessions, the islands being declared parts of Her Britannic Majesty's dominions. These instruments did not include Aitutaki. It appears that, though the inhabitants regarded themselves as British subjects, the Crown's title was uncertain, and the island was formally annexed by Proclamation dated 9 October 1900. The islands were included within the boundaries of the Colony of New Zealand in 1901 by Order in Council under the Colonial Boundaries Act, 1895 of the United Kingdom. The boundary change became effective on 11 June 1901, and the Cook Islands have had a formal relationship with New Zealand since that time.\n\nThe islands remained a New Zealand dependent territory until 1965, at which point they became a self-governing territory in free association with New Zealand. The first Prime Minister Albert Henry led the country until 1978 when he was accused of vote-rigging. Since 1965, the Cook Islands have been essentially independent (self-governing in free association with New Zealand), but remained officially placed under New Zealand sovereignty. New Zealand is tasked with overseeing the country's foreign relations and defense. The Cook Islands, Niue, and New Zealand (with its territories: Tokelau and the Ross Dependency) make up the Realm of New Zealand.\n\nAfter achieving autonomy in 1965, the Cook Islands elected Albert Henry of the Cook Islands Party as their first Prime Minister. He was succeeded in 1978 by Tom Davis of the Democratic Party.\n\nOn 11 June 1980, the United States signed a treaty with the Cook Islands specifying the maritime border between the Cook Islands and American Samoa and also relinquishing the US claim to the islands of Penrhyn, Pukapuka, Manihiki, and Rakahanga. In 1990, the Cook Islands signed a treaty with France which delimited the maritime boundary between the Cook Islands and French Polynesia.\n\nOn June 13, 2008, a small majority of members of the House of Ariki attempted a coup, claiming to dissolve the elected government and to take control of the country's leadership. \"Basically we are dissolving the leadership, the prime minister and the deputy prime minister and the ministers,\" chief Makea Vakatini Joseph Ariki explained. The \"Cook Islands Herald\" suggested that the \"ariki\" were attempting thereby to regain some of their traditional prestige or \"mana\". Prime Minister Jim Marurai described the take-over move as \"ill-founded and nonsensical\". By June 23, the situation appeared to have normalised, with members of the House of Ariki accepting to return to their regular duties.\n\n1595 — Spaniard Álvaro de Mendaña de Neira is the first European to sight the islands.\n\n1606 — Portuguese-Spaniard Pedro Fernández de Quirós makes the first recorded European landing in the islands when he sets foot on Rakahanga.\n\n1773 — Captain James Cook explores the islands and names them the Hervey Islands. Fifty years later they are renamed in his honour by Russian Admiral Adam Johann von Krusenstern.\n\n1821 — English and Tahitian missionaries land in Aitutaki, become the first non-Polynesian settlers.\n\n1823 — English missionary John Williams lands in Rarotonga, converting Makea Pori Ariki to Christianity.\n\n1858 — The Cook Islands become united as a state, the Kingdom of Rarotonga.\n\n1862 — Peruvian slave traders take a terrible toll on the islands of Penrhyn, Rakahanga and Pukapuka in 1862 and 1863.\n\n1888 — Cook Islands are proclaimed a British protectorate and a single federal parliament is established.\n\n1900 — The Cook Islands are ceded to the United Kingdom as British territory, except for Aitutaki which was annexed by the United Kingdom at the same time.\n\n1901 — The boundaries of the Colony of New Zealand are extended by the United Kingdom to include the Cook Islands.\n\n1924 — The All Black \"Invincibles\" stop in Rarotonga on their way to the United Kingdom and play a friendly match against a scratch Rarotongan team.\n\n1946 — Legislative Council is established. For the first time since 1912, the territory has direct representation.\n\n1957 — Legislative Council is reorganized as the Legislative Assembly.\n\n1965 — The Cook Islands become a self-governing territory in free association with New Zealand. Albert Henry, leader of the Cook Islands Party, is elected as the territory's first prime minister.\n\n1974 — Albert Henry is knighted by Queen Elizabeth II\n\n1979 — Sir Albert Henry is found guilty of electoral fraud and stripped of his premiership and his knighthood. Tom Davis becomes Premier.\n\n1980 — Cook Islands – United States Maritime Boundary Treaty establishes the Cook Islands – American Samoa boundary\n\n1981 — Constitution is amended. Legislative Assembly is renamed Parliament, which grows from 22 to 24 seats, and the parliamentary term is extended from four to five years. Tom Davis is knighted.\n\n1984 — The country's first coalition government, between Sir Thomas and Geoffrey Henry, is signed in the lead up to hosting regional Mini Games in 1985. Shifting coalitions saw ten years of political instability. At one stage, all but two MPs were in government.\n\n1985 — Rarotonga Treaty is opened for signing in the Cook Islands, creating a nuclear-free zone in the South Pacific.\n\n1986 — In January 1986, following the rift between New Zealand and the USA in respect of the ANZUS security arrangements Prime Minister Tom Davis declared the Cook Islands a neutral country, because he considered that New Zealand (which has control over the islands' defence and foreign policy) was no longer in a position to defend the islands. The proclamation of neutrality meant that the Cook Islands would not enter into a military relationship with any foreign power, and, in particular, would prohibit visits by US warships. Visits by US naval vessels were allowed to resume by Henry's Government.\n\n1990 — Cook Islands – France Maritime Delimitation Agreement establishes the Cook Islands–French Polynesia boundary\n\n1991 — The Cook Islands signed a treaty of friendship and co-operation with France, covering economic development, trade and surveillance of the islands' EEZ. The establishment of closer relations with France was widely regarded as an expression of the Cook Islands' Government's dissatisfaction with existing arrangements with New Zealand which was no longer in a position to defend the Cook Islands.\n\n1995 — The French Government resumed its programme of nuclear-weapons testing at Mururoa Atoll in September 1995 upsetting the Cook Islands. New Prime Minister Geoffrey Henry was fiercely critical of the decision and dispatched a \"vaka\" (traditional voyaging canoe) with a crew of Cook Islands' traditional warriors to protest near the test site. The tests were concluded in January 1996 and a moratorium was placed on future testing by the French government.\n\n1997 — Full diplomatic relations established with the People's Republic of China.\n\n1997 — In November, Cyclone Martin in Manihiki kills at least six people; 80% of buildings are damaged and the black pearl industry suffered severe losses.\n\n1999 — A second era of political instability begins, starting with five different coalitions in less than nine months, and at least as many since then.\n\n2000 — Full diplomatic relations concluded with France.\n\n2002 — Prime Minister Terepai Maoate is ousted from government following second vote of no-confidence in his leadership.\n\n2004 — Prime Minister Robert Woonton visits China; Chinese Premier Wen Jiabao grants $16 million in development aid.\n\n2006 — Parliamentary elections held. The Democratic Party keeps majority of seats in parliament, but is unable to command a majority for confidence, forcing a coalition with breakaway MPs who left, then rejoined the \"Demos\".\n\n2008 — Pacific Island nations imposed a series of measures aimed at halting overfishing.\n\nBULLET::::- Cook Islands mythology\nBULLET::::- Postage stamps and postal history of the Cook Islands\n"}
{"id": "7069", "url": "https://en.wikipedia.org/wiki?curid=7069", "title": "Geography of the Cook Islands", "text": "Geography of the Cook Islands\n\nThe Cook Islands can be divided into two groups: the Southern Cook Islands and the Northern Cook Islands. The country is located in Oceania, in the South Pacific Ocean, about one-half of the way from Hawaii to New Zealand.\n\nBULLET::::- Aitutaki\nBULLET::::- Atiu\nBULLET::::- Mangaia\nBULLET::::- Manuae\nBULLET::::- Mauke\nBULLET::::- Mitiaro\nBULLET::::- Palmerston Island\nBULLET::::- Rarotonga (capital)\nBULLET::::- Takutea\n\nBULLET::::- Manihiki\nBULLET::::- Nassau\nBULLET::::- Penrhyn Island\nBULLET::::- Pukapuka\nBULLET::::- Rakahanga\nBULLET::::- Suwarrow\n\nBULLET::::- Area:\nBULLET::::- Area - comparative:\nBULLET::::- Coastline:\nBULLET::::- Maritime claims:\nBULLET::::- Climate:\nBULLET::::- Terrain:\nBULLET::::- Elevation extremes:\nBULLET::::- Natural resources:\nBULLET::::- Land use:\nBULLET::::- Natural hazards:\nBULLET::::- Environment - international agreements\n"}
{"id": "7070", "url": "https://en.wikipedia.org/wiki?curid=7070", "title": "Demographics of the Cook Islands", "text": "Demographics of the Cook Islands\n\nThis article is about the demographic features of the population of the Cook Islands, including population density, ethnicity, education level, health of the populace, economic status, religious affiliations and other aspects of the population.\n\nA census is carried out every five years in the Cook Islands. The last census was carried out in 2011 and the next census will be carried out in December 2016.\n\nThe following demographic statistics are from the CIA World Factbook, unless otherwise indicated.\n\nBULLET::::- Population\nBULLET::::- 9,290\nBULLET::::- Age structure (2017 est.)\nBULLET::::- 0–14 years: 21.12% (male 1,154/female 1,025)\nBULLET::::- 15–24 years: 16.63% (male 929/female 806)\nBULLET::::- 25–54 years: 38.09% (male 1,876/female 1,867)\nBULLET::::- 55–64 years: 11.99% (male 569/female 494)\nBULLET::::- 65 years and over: 12.16% (male 551/female 567)\nBULLET::::- Population growth rate\nBULLET::::- -2.79%\nBULLET::::- Birth rate\nBULLET::::- 14 births/1,000 population\nBULLET::::- Death rate\nBULLET::::- 8.4 deaths/1,000 population\nBULLET::::- Infant mortality rate\nBULLET::::- Total: 13 deaths/1,000 live births\nBULLET::::- Male: 15.8 deaths/1,000 live births\nBULLET::::- Female: 10.1 deaths/1,000 live births\nBULLET::::- Life expectancy at birth\nBULLET::::- Total population: 76 years\nBULLET::::- Male: 73.2 years\nBULLET::::- Female: 79 years (2017 est.)\nBULLET::::- Total fertility rate\nBULLET::::- 2.19 children born/woman\nBULLET::::- Nationality\nBULLET::::- Cook Islander(s) (Noun)\nBULLET::::- Cook Islander (Adjective)\nBULLET::::- Ethnic groups\nBULLET::::- Cook Island Maori (Polynesian) 81.3%\nBULLET::::- part Cook Island Maori 6.7%\nBULLET::::- Other 11.9%\nBULLET::::- Religions\nBULLET::::- Protestant 62.8%\nBULLET::::- Cook Islands Christian Church 49.1%\nBULLET::::- Seventh-day Adventist 7.9%,\nBULLET::::- Assemblies of God 3.7%\nBULLET::::- Apostolic Church 2.1%),\nBULLET::::- Roman Catholic 17%\nBULLET::::- Mormon 4.4%,\nBULLET::::- Other 8%\nBULLET::::- This \"Other\" group includes smaller Christian denominations, and mostly non-indigenous adherents of Hinduism, Buddhism, and Islam, as well as the irreligious.\nBULLET::::- None 5.6%\nBULLET::::- No response 2.2%\nBULLET::::- Languages\nBULLET::::- English (official) 86.4%\nBULLET::::- Cook Islands Maori (Rarotongan) (official) 76.2%\nBULLET::::- Other 8.3%\n"}
{"id": "7071", "url": "https://en.wikipedia.org/wiki?curid=7071", "title": "Politics of the Cook Islands", "text": "Politics of the Cook Islands\n\nThe politics of the Cook Islands, an associated state, takes place in a framework of a parliamentary representative democracy within a constitutional monarchy. The Queen of New Zealand, represented in the Cook Islands by the Queen's Representative, is the Head of State; the prime minister is the head of government and of a multi-party system. The Islands are self-governing in free association with New Zealand and are fully responsible for internal affairs. New Zealand retains some responsibility for external affairs, in consultation with the Cook Islands. In recent years, the Cook Islands have taken on more of its own external affairs; as of 2005, it has diplomatic relations in its own name with eighteen other countries. Executive power is exercised by the government, while legislative power is vested in both the government and the islands' parliament. The judiciary is independent of the executive and the legislatures.\n\nThe Constitution of the Cook Islands took effect on August 4, 1965, when the Cook Islands became a self-governing territory in free association with New Zealand. The anniversary of these events in 1965 is commemorated annually on Constitution Day, with week long activities known as \"Te Maeva Nui Celebrations\" locally.\n\n Queen  Elizabeth II   6 February 1952</tr>\nThe monarch is hereditary; her representative is appointed by the monarch on the recommendation of the Cook Islands Government. The cabinet is chosen by the prime minister and collectively responsible to Parliament.\n\nTen years of rule by the Cook Islands Party (CIP) came to an end 18 November 1999 with the resignation of Prime Minister Joe Williams. Williams had led a minority government since October 1999 when the New Alliance Party (NAP) left the government coalition and joined the main opposition Democratic Party (DAP). On 18 November 1999, DAP leader Dr. Terepai Maoate was sworn in as prime minister. He was succeeded by his co-partisan Robert Woonton. When Dr Woonton lost his seat in the 2004 elections, Jim Marurai took over. In the 2010 elections, the CIP regained power and Henry Puna was sworn in as prime minister on 30 November 2010.\n\nFollowing uncertainty about the ability of the government to maintain its majority, the Queen's representative dissolved parliament midway through its term and a 'snap' election was held on 26 September 2006. Jim Marurai's Democratic Party retained the Treasury benches with an increased majority.\n\nThe New Zealand High Commissioner is appointed by the New Zealand Government.\n\nThe Parliament of the Cook Islands has 24 members, elected for a five-year term in single-seat constituencies. There is also a House of Ariki, composed of chiefs, which has a purely advisory role. The Koutu Nui is a similar organization consisting of sub-chiefs. It was established by an amendment in 1972 of the 1966 House of Ariki Act. The current president is Te Tika Mataiapo Dorice Reid.\n\nOn June 13, 2008, a small majority of members of the House of Ariki attempted a coup, claiming to dissolve the elected government and to take control of the country's leadership. \"Basically we are dissolving the leadership, the prime minister and the deputy prime minister and the ministers,\" chief Makea Vakatini Joseph Ariki explained. The \"Cook Islands Herald\" suggested that the \"ariki\" were attempting thereby to regain some of their traditional prestige or \"mana\". Prime Minister Jim Marurai described the take-over move as \"ill-founded and nonsensical\". By June 23, the situation appeared to have normalised, with members of the House of Ariki accepting to return to their regular duties.\n\nWith regard to the legal profession, Iaveta Taunga o Te Tini Short was the first Cook Islander to establish a law practice in 1968. He would later become a Cabinet Minister (1978) and High Commissioner for the Cook Islands (1985).\n\nThe 1999 election produced a hung Parliament. Cook Islands Party leader Geoffrey Henry remained prime minister, but was replaced after a month by Joe Williams following a coalition realignment. A further realignment three months later saw Williams replaced by Democratic Party leader Terepai Maoate. A third realignment saw Maoate replaced mid-term by his deputy Robert Woonton in 2002, who ruled with the backing of the CIP.\n\nThe Democratic Party won a majority in the 2004 election, but Woonton lost his seat, and was replaced by Jim Marurai. In 2005 Marurai left the Democrats due to an internal disputes, founding his own Cook Islands First Party. He continued to govern with the support of the CIP, but in 2005 returned to the Democrats. The loss of several by-elections forced a snap-election in 2006, which produced a solid majority for the Democrats and saw Marurai continue as prime minister.\n\nIn December 2009, Marurai sacked his Deputy Prime Minister, Terepai Maoate, sparking a mass-resignation of Democratic Party cabinet members He and new Deputy Prime Minister Robert Wigmore were subsequently expelled from the Democratic Party. Marurai appointed three junior members of the Democratic party to Cabinet, but on 31 December 2009 the party withdrew its support.\n\nIn May 2014 a new party was formed by Teina Bishop of Aitutaki \"One Cook Islands\" Party.\n\nBULLET::::- Foreign relations of the Cook Islands\nBULLET::::- Political status of the Cook Islands and Niue\n\nBULLET::::- Constitution of the Cook Islands and amendments\n"}
{"id": "7072", "url": "https://en.wikipedia.org/wiki?curid=7072", "title": "Economy of the Cook Islands", "text": "Economy of the Cook Islands\n\nThe economy of the Cook Islands is based mainly on tourism, with minor exports made up of tropical and citrus fruit. Manufacturing activities are limited to fruit-processing, clothing and handicrafts.\n\nAs in many other South Pacific nations, the Cook Islands's economy is hindered by the country's isolation from foreign markets, lack of natural resources aside from fish, periodic devastation from natural disasters, and inadequate infrastructure.\n\nTrade deficits are made up for by remittances from emigrants and by foreign aid, overwhelmingly from New Zealand. Efforts to exploit tourism potential, encourage offshore banking, and expand the mining and fishing industries have been partially successful in stimulating investment and growth.\n\nThe Cook Islands has \"Home Rule\" with respect to banking, similar to Guernsey, Jersey and the Isle of Man.\n\nThis \"Home Rule\" banking confuses New Zealanders on vacation in the Cooks. Cook automated teller machines often fail to fully disclose the fact that the Cooks are not part of the New Zealand banking system, thus legally requiring banks to charge the same fees for withdrawing or transferring money as if the person was in Australia or the EU. The New Zealand dollar is the official currency of the Cook Islands, adding to the confusion. Cook Islanders are NZ citizens.\n\nThe banking and incorporation laws of the Cook Islands make it an important centre for setting up companies that are involved in global trade.\n\nBULLET::::- GDP:\nBULLET::::- GDP - real growth rate: -.05% (2005); -1.2% (2014); -1.7% (2013). Growth in the Cook Islands has slowed due to a lack of infrastructure projects and accommodation capacity constraints in the tourism sector. Cook Islands economic activity is expected to be flat in FY2016; to grow by 0.2% in FY2017. Inflation 1.8% (FY2016); 2.0% (FY2017). Statistics Asian Development Bank\nBULLET::::- GDP - per capita: $9 100 (2005 estimate)\nBULLET::::- GDP - composition by sector:\nBULLET::::- Population below poverty line:\nBULLET::::- Household income or consumption by percentage share:\nBULLET::::- Inflation rate (consumer prices):\nBULLET::::- Labor force:\nBULLET::::- Labor force - by occupation:\nBULLET::::- Unemployment rate: 13.1% (2005)\nBULLET::::- Budget:\nBULLET::::- Industries:\nBULLET::::- Industrial production growth rate:\nBULLET::::- Electricity - production:\nBULLET::::- Electricity - production by source:\nBULLET::::- Electricity - consumption:\nBULLET::::- Electricity - exports:\nBULLET::::- Electricity - imports:\nBULLET::::- Oil consumption:\nBULLET::::- Agriculture - products:\nBULLET::::- Exports:\nBULLET::::- Exports - commodities:\nBULLET::::- Exports - partners:\nBULLET::::- Imports:\nBULLET::::- Imports - commodities:\nBULLET::::- Imports - partners:\nBULLET::::- Debt - external:\nBULLET::::- Economic aid - recipient:\nBULLET::::- Currency:\nBULLET::::- Exchange rates:\nBULLET::::- Fiscal year:\n\nTelecom Cook Islands Ltd (TCI) is the sole provider of telecommunications in the Cook Islands. TCI is a private company owned by Spark New Zealand Ltd (60%) and the Cook Islands Government (40%). In operation since July 1991, TCI provides local, national and international telecommunications as well as internet access on all islands except Suwarrow. Communications to Suwarrow is via HF radio.\n\nBULLET::::- CIA World Factbook, 2006\nBULLET::::- Cook Islands banking explained from TVNZ program Fair Go .\n"}
{"id": "7073", "url": "https://en.wikipedia.org/wiki?curid=7073", "title": "Telecommunications in the Cook Islands", "text": "Telecommunications in the Cook Islands\n\nLike most countries and territories in Oceania, telecommunications in the Cook Islands is limited by its isolation and low population, with only one major television broadcasting station and six radio stations. However, most residents have a main line or mobile phone. Its telecommunications are mainly provided by Telecom Cook Islands, who is currently working with O3b Networks, Ltd. for faster Internet connection.\n\nIn July 2012, there were about 7,500 main line telephones, which covers about 98% of the country's population. There were approximately 7,800 mobile phones in 2009. Telecom Cook Islands, owned by Spark New Zealand, is the islands' main telephone system and offers international direct dialling, Internet, email, fax, and Telex. The individual islands are connected by a combination of satellite earth stations, microwave systems, and very high frequency and high frequency radiotelephone; within the islands, service is provided by small exchanges connected to subscribers by open wire, cable, and fibre optic cable. For international communication, they rely on the satellite earth station Intelsat.\n\nIn 2003, the largest island of Rarotonga started using a GSM/GPRS mobile data service system with GSM 900 by 2013 3G UMTS 900 was introduce covering 98% of Rarotonga with HSPA+. In March 2017 4G+ launch in Rarotonga with LTE700 (B28A) and LTE1800 (B3) .\n\nMobile service covers Aitutaki GSM/GPRS mobile data service system in GSM 900 from 2006 to 2013 while in 2014 3G UMTS 900 was introduce with HSPA+ stand system. In March 2017 4G+ also launch in Aitutaki with LTE700 (B28A).\nThe rest of the Outer Islands (Pa Enua) mobile was well establish in 2007 with mobile coverage at GSM 900 from Mangaia 3 villages (Oneroa, Ivirua, Tamarua), Atiu, Mauke, Mitiaro, Palmerston in the Southern Group (Pa Enua Tonga) and the Northern Group (Pa Enua Tokerau) Nassau, Pukapuka, Rakahanga, Manihiki 2 Village (Tukao, Tauhunu) and Penrhyn 2 villages (Omoka Tetautua).\n\nThe Cook Islands uses the country calling code +682.\n\nThere are six radio stations in the Cook Islands, with one reaching all islands. there were 14,000 radios.\n\nCook Islands Television broadcasts from Rarotonga, providing a mix of local news and overseas-sourced programs. there were 4,000 television sets.\n\nThere were 6,000 Internet users in 2009 and 3,562 Internet hosts as of 2012. The country code top-level domain for the Cook Islands is .ck.\n\nIn June 2010, Telecom Cook Islands partnered with O3b Networks, Ltd. to provide faster Internet connection to the Cook Islands. On 25 June 2013 the O3b satellite constellation was launched from an Arianespace Soyuz ST-B rocket in French Guiana. The medium Earth orbit satellite orbits at and uses the K band. It has a latency of about 100 milliseconds because it is much closer to Earth than standard geostationary satellites, whose latencies can be over 600 milliseconds. Although the initial launch consisted of 4 satellites, as many as 20 may be launched eventually to serve various areas with little or no optical fibre service, the first of which is the Cook Islands.\n\nIn December 2015, Alcatel-Lucent and Bluesky Pacific Group announced that they would build the Moana Cable system connecting New Zealand to Hawaii with a single fibre pair branching off to the Cook Islands. The Moana Cable is expected to be completed in 2018.\n\nBULLET::::- Telecom Cook Islands\nBULLET::::- Bluesky Cook Islands\n"}
{"id": "7074", "url": "https://en.wikipedia.org/wiki?curid=7074", "title": "Transport in the Cook Islands", "text": "Transport in the Cook Islands\n\nThis article lists transport in the Cook Islands.\n\nBULLET::::- Railways:\nBULLET::::- Highways:\nBULLET::::- Ports and harbours:\nBULLET::::- Merchant marine:\nBULLET::::- Airports:\nBULLET::::- Airports - with paved runways:\nBULLET::::- Airports - with unpaved runways:\n\nBULLET::::- Cook Islands\n\nBULLET::::- Airports and Ports in the Cook Islands\n"}
{"id": "7077", "url": "https://en.wikipedia.org/wiki?curid=7077", "title": "Computer file", "text": "Computer file\n\nA computer file is a computer resource for recording data discretely in a computer storage device. Just as words can be written to paper, so can information be written to a computer file. Files can be edited and transferred through the internet on that particular computer system.\n\nThere are different types of computer files, designed for different purposes. A file may be designed to store a picture, a written message, a video, a computer program, or a wide variety of other kinds of data. Some types of files can store several types of information at once.\n\nBy using computer programs, a person can open, read, change, save, and close a computer file. Computer files may be reopened, modified, and copied an arbitrary number of times.\n\nTypically, files are organised in a file system, which keeps track of where the files are located on disk and enables user access.\n\nThe word \"file\" derives from the Latin \"filum\" (\"a thread\").\n\n\"File\" was used in the context of computer storage as early as January 1940. In \"Punched Card Methods in Scientific Computation\", W. J. Eckert stated, \"The first extensive use of the early Hollerith Tabulator in astronomy was made by Comrie. He used it for building a table from successive differences, and for adding large numbers of harmonic terms\". \"Tables of functions are constructed from their differences with great efficiency, either as printed tables or as a \"file of punched cards\".\"\n\nIn February 1950, in a Radio Corporation of America (RCA) advertisement in \"Popular Science\" magazine describing a new \"memory\" vacuum tube it had developed, RCA stated: \"the results of countless computations can be kept 'on file' and taken out again. Such a 'file' now exists in a 'memory' tube developed at RCA Laboratories. Electronically it retains figures fed into calculating machines, holds them in storage while it memorizes new ones – speeds intelligent solutions through mazes of mathematics.\"\n\nIn 1952, \"file\" denoted, among other things, information stored on punched cards.\n\nIn early use, the underlying hardware, rather than the contents stored on it, was denominated a \"file\". For example, the IBM 350 disk drives were denominated \"disk files\". The introduction, circa 1961, by the Burroughs MCP and the MIT Compatible Time-Sharing System of the concept of a \"file system\" that managed several virtual \"files\" on one storage device is the origin of the contemporary denotation of the word. Although the contemporary \"register file\" demonstrates the early concept of files, its use has greatly decreased.\n\nOn most modern operating systems, files are organized into one-dimensional arrays of bytes. The format of a file is defined by its content since a file is solely a container for data, although on some platforms the format is usually indicated by its filename extension, specifying the rules for how the bytes must be organized and interpreted meaningfully. For example, the bytes of a plain text file ( in Windows) are associated with either ASCII or UTF-8 characters, while the bytes of image, video, and audio files are interpreted otherwise. Most file types also allocate a few bytes for metadata, which allows a file to carry some basic information about itself.\n\nSome file systems can store arbitrary (not interpreted by the file system) file-specific data outside of the file format, but linked to the file, for example extended attributes or forks. On other file systems this can be done via sidecar files or software-specific databases. All those methods, however, are more susceptible to loss of metadata than are container and archive file formats.\n\nAt any instant in time, a file might have a size, normally expressed as number of bytes, that indicates how much storage is associated with the file. In most modern operating systems the size can be any non-negative whole number of bytes up to a system limit. Many older operating systems kept track only of the number of blocks or tracks occupied by a file on a physical storage device. In such systems, software employed other methods to track the exact byte count (e.g., CP/M used a special control character, Ctrl-Z, to signal the end of text files).\n\nThe general definition of a file does not require that its size have any real meaning, however, unless the data within the file happens to correspond to data within a pool of persistent storage. A special case is a zero byte file; these files can be newly created files that have not yet had any data written to them, or may serve as some kind of flag in the file system, or are accidents (the results of aborted disk operations). For example, the file to which the link points in a typical Unix-like system probably has a defined size that seldom changes. Compare this with which is also a file, but as a character special file, its size is not meaningful.\n\nInformation in a computer file can consist of smaller packets of information (often called \"records\" or \"lines\") that are individually different but share some common traits. For example, a payroll file might contain information concerning all the employees in a company and their payroll details; each record in the payroll file concerns just one employee, and all the records have the common trait of being related to payroll—this is very similar to placing all payroll information into a specific filing cabinet in an office that does not have a computer. A text file may contain lines of text, corresponding to printed lines on a piece of paper. Alternatively, a file may contain an arbitrary binary image (a blob) or it may contain an executable.\n\nThe way information is grouped into a file is entirely up to how it is designed. This has led to a plethora of more or less standardized file structures for all imaginable purposes, from the simplest to the most complex. Most computer files are used by computer programs which create, modify or delete the files for their own use on an as-needed basis. The programmers who create the programs decide what files are needed, how they are to be used and (often) their names.\n\nIn some cases, computer programs manipulate files that are made visible to the computer user. For example, in a word-processing program, the user manipulates document files that the user personally names. Although the content of the document file is arranged in a format that the word-processing program understands, the user is able to choose the name and location of the file and provide the bulk of the information (such as words and text) that will be stored in the file.\n\nMany applications pack all their data files into a single file called an archive file, using internal markers to discern the different types of information contained within. The benefits of the archive file are to lower the number of files for easier transfer, to reduce storage usage, or just to organize outdated files. The archive file must often be unpacked before next using.\n\nThe most basic operations that programs can perform on a file are:\nBULLET::::- Create a new file\nBULLET::::- Change the access permissions and attributes of a file\nBULLET::::- Open a file, which makes the file contents available to the program\nBULLET::::- Read data from a file\nBULLET::::- Write data to a file\nBULLET::::- Close a file, terminating the association between it and the program\n\nFiles on a computer can be created, moved, modified, grown, shrunk, and deleted. In most cases, computer programs that are executed on the computer handle these operations, but the user of a computer can also manipulate files if necessary. For instance, Microsoft Word files are normally created and modified by the Microsoft Word program in response to user commands, but the user can also move, rename, or delete these files directly by using a file manager program such as Windows Explorer (on Windows computers) or by command lines (CLI).\n\nIn Unix-like systems, user space programs do not operate directly, at a low level, on a file. Only the kernel deals with files, and it handles all user-space interaction with files in a manner that is transparent to the user-space programs. The operating system provides a level of abstraction, which means that interaction with a file from user-space is simply through its filename (instead of its inode). For example, rm \"filename\" will not delete the file itself, but only a link to the file. There can be many links to a file, but when they are all removed, the kernel considers that file's memory space free to be reallocated. This free space is commonly considered a security risk (due to the existence of file recovery software). Any secure-deletion program uses kernel-space (system) functions to wipe the file's data.\n\nIn modern computer systems, files are typically accessed using names (filenames). In some operating systems, the name is associated with the file itself. In others, the file is anonymous, and is pointed to by links that have names. In the latter case, a user can identify the name of the link with the file itself, but this is a false analogue, especially where there exists more than one link to the same file.\n\nFiles (or links to files) can be located in directories. However, more generally, a directory can contain either a list of files or a list of links to files. Within this definition, it is of paramount importance that the term \"file\" includes directories. This permits the existence of directory hierarchies, i.e., directories containing sub-directories. A name that refers to a file within a directory must be typically unique. In other words, there must be no identical names within a directory. However, in some operating systems, a name may include a specification of type that means a directory can contain an identical name for more than one type of object such as a directory and a file.\n\nIn environments in which a file is named, a file's name and the path to the file's directory must uniquely identify it among all other files in the computer system—no two files can have the same name and path. Where a file is anonymous, named references to it will exist within a namespace. In most cases, any name within the namespace will refer to exactly zero or one file. However, any file may be represented within any namespace by zero, one or more names.\n\nAny string of characters may be a well-formed name for a file or a link depending upon the context of application. Whether or not a name is well-formed depends on the type of computer system being used. Early computers permitted only a few letters or digits in the name of a file, but modern computers allow long names (some up to 255 characters) containing almost any combination of unicode letters or unicode digits, making it easier to understand the purpose of a file at a glance. Some computer systems allow file names to contain spaces; others do not. Case-sensitivity of file names is determined by the file system. Unix file systems are usually case sensitive and allow user-level applications to create files whose names differ only in the case of characters. Microsoft Windows supports multiple file systems, each with different policies regarding case-sensitivity. The common FAT file system can have multiple files whose names differ only in case if the user uses a disk editor to edit the file names in the directory entries. User applications, however, will usually not allow the user to create multiple files with the same name but differing in case.\n\nMost computers organize files into hierarchies using folders, directories, or catalogs. The concept is the same irrespective of the terminology used. Each folder can contain an arbitrary number of files, and it can also contain other folders. These other folders are referred to as subfolders. Subfolders can contain still more files and folders and so on, thus building a tree-like structure in which one \"master folder\" (or \"root folder\" — the name varies from one operating system to another) can contain any number of levels of other folders and files. Folders can be named just as files can (except for the root folder, which often does not have a name). The use of folders makes it easier to organize files in a logical way.\n\nWhen a computer allows the use of folders, each file and folder has not only a name of its own, but also a path, which identifies the folder or folders in which a file or folder resides. In the path, some sort of special character—such as a slash—is used to separate the file and folder names. For example, in the illustration shown in this article, the path uniquely identifies a file called in a folder called , which in turn is contained in a folder called . The folder and file names are separated by slashes in this example; the topmost or root folder has no name, and so the path begins with a slash (if the root folder had a name, it would precede this first slash).\n\nMany computer systems use extensions in file names to help identify what they contain, also known as the file type. On Windows computers, extensions consist of a dot (period) at the end of a file name, followed by a few letters to identify the type of file. An extension of identifies a text file; a extension identifies any type of document or documentation, commonly in the Microsoft Word file format; and so on. Even when extensions are used in a computer system, the degree to which the computer system recognizes and heeds them can vary; in some systems, they are required, while in other systems, they are completely ignored if they are presented.\n\nMany modern computer systems provide methods for protecting files against accidental and deliberate damage. Computers that allow for multiple users implement file permissions to control who may or may not modify, delete, or create files and folders. For example, a given user may be granted only permission to read a file or folder, but not to modify or delete it; or a user may be given permission to read and modify files or folders, but not to execute them. Permissions may also be used to allow only certain users to see the contents of a file or folder. Permissions protect against unauthorized tampering or destruction of information in files, and keep private information confidential from unauthorized users.\n\nAnother protection mechanism implemented in many computers is a \"read-only flag.\" When this flag is turned on for a file (which can be accomplished by a computer program or by a human user), the file can be examined, but it cannot be modified. This flag is useful for critical information that must not be modified or erased, such as special files that are used only by internal parts of the computer system. Some systems also include a \"hidden flag\" to make certain files invisible; this flag is used by the computer system to hide essential system files that users should not alter.\n\nAny file that has any useful purpose, must have some physical manifestation. That is, a file (an abstract concept) in a real computer system must have a real physical analogue if it is to exist at all.\n\nIn physical terms, most computer files are stored on some type of data storage device. For example, most operating systems store files on a hard disk. Hard disks have been the ubiquitous form of non-volatile storage since the early 1960s. Where files contain only temporary information, they may be stored in RAM. Computer files can be also stored on other media in some cases, such as magnetic tapes, compact discs, Digital Versatile Discs, Zip drives, USB flash drives, etc. The use of solid state drives is also beginning to rival the hard disk drive.\n\nIn Unix-like operating systems, many files have no associated physical storage device. Examples are and most files under directories , and . These are virtual files: they exist as objects within the operating system kernel.\n\nAs seen by a running user program, files are usually represented either by a file control block or by a file handle. A file control block (FCB) is an area of memory which is manipulated to establish a filename etc. and then passed to the operating system as a parameter; it was used by older IBM operating systems and early PC operating systems including CP/M and early versions of MS-DOS. A file handle is generally either an opaque data type or an integer; it was introduced in around 1961 by the ALGOL-based Burroughs MCP running on the Burroughs B5000 but is now ubiquitous.\n\nWhen a file is said to be corrupted, it is because its contents have been saved to the computer in such a way that they can't be properly read, either by a human or by software. Depending on the extension of the damage, the original file can sometimes be recovered, or at least partially understood. A file may be created corrupt, or it may be corrupted at a later point through overwriting.\n\nThere are many ways by which a file can become corrupted. Most commonly, the issue happens in the process of writing the file to a disk. For example, if an image-editing program unexpectedly crashes while saving an image, that file may be corrupted because the program couldn't save its entirety. The program itself might warn the user that there was an error, allowing for another attempt at saving the file. Some other examples of reasons for which files become corrupted include:\nBULLET::::- The computer itself shutting down unexpectedly (for example, due to a power loss) with open files, or files in the process of being saved;\nBULLET::::- A download being interrupted before it was completed;\nBULLET::::- Due to a bad sector on the hard drive;\nBULLET::::- The user removing a flash drive (such as a USB stick) without properly unmounting (commonly referred to as \"safely removing\");\nBULLET::::- Malicious software, such as a computer virus;\nBULLET::::- A flash drive becoming too old.\n\nAlthough file corruption usually happens accidentally, it may also be done on purpose, as to fool someone else into thinking an assignment was ready at an earlier date, potentially gaining time to finish said assignment. There are services that provide on demand file corruption, which essentially fill a given file with random data so that it can't be opened or read, yet still seems legitimate.\n\nOne of the most effective countermeasures for unintentional file corruption is backing up important files. In the event of an important file becoming corrupted, the user can simply replace it with the backed up version.\n\nWhen computer files contain information that is extremely important, a \"back-up\" process is used to protect against disasters that might destroy the files. Backing up files simply means making copies of the files in a separate location so that they can be restored if something happens to the computer, or if they are deleted accidentally.\n\nThere are many ways to back up files. Most computer systems provide utility programs to assist in the back-up process, which can become very time-consuming if there are many files to safeguard. Files are often copied to removable media such as writable CDs or cartridge tapes. Copying files to another hard disk in the same computer protects against failure of one disk, but if it is necessary to protect against failure or destruction of the entire computer, then copies of the files must be made on other media that can be taken away from the computer and stored in a safe, distant location.\n\nThe grandfather-father-son backup method automatically makes three back-ups; the grandfather file is the oldest copy of the file and the son is the current copy.\n\nThe way a computer organizes, names, stores and manipulates files is globally referred to as its \"file system.\" Most computers have at least one file system. Some computers allow the use of several different file systems. For instance, on newer MS Windows computers, the older FAT-type file systems of MS-DOS and old versions of Windows are supported, in addition to the NTFS file system that is the normal file system for recent versions of Windows. Each system has its own advantages and disadvantages. Standard FAT allows only eight-character file names (plus a three-character extension) with no spaces, for example, whereas NTFS allows much longer names that can contain spaces. You can call a file \"\" in NTFS, but in FAT you would be restricted to something like (unless you were using VFAT, a FAT extension allowing long file names).\n\nFile manager programs are utility programs that allow users to manipulate files directly. They allow you to move, create, delete and rename files and folders, although they do not actually allow you to read the contents of a file or store information in it. Every computer system provides at least one file-manager program for its native file system. For example, File Explorer (formerly Windows Explorer) is commonly used in Microsoft Windows operating systems, and Nautilus is common under several distributions of Linux.\n\nBULLET::::- Block (data storage)\nBULLET::::- Computer file management\nBULLET::::- Data hierarchy\nBULLET::::- File camouflage\nBULLET::::- File copying\nBULLET::::- File conversion\nBULLET::::- File deletion\nBULLET::::- File directory\nBULLET::::- File manager\nBULLET::::- File system\nBULLET::::- Filename\nBULLET::::- Flat file database\nBULLET::::- Object composition\nBULLET::::- Soft copy\n\n"}
{"id": "7079", "url": "https://en.wikipedia.org/wiki?curid=7079", "title": "CID", "text": "CID\n\nCID may refer to:\nBULLET::::- \"C.I.D.\" (1955 film), an Indian Malayalam film\nBULLET::::- \"C.I.D.\" (1956 film), an Indian Hindi film\nBULLET::::- \"C. I. D.\" (1965 film), an Indian Telugu film\nBULLET::::- \"C.I.D.\" (1990 film), an Indian Hindi film\n\nBULLET::::- \"CID\" (Indian TV series)\nBULLET::::- \"C.I.D.\" (Singaporean TV series)\n\nBULLET::::- Criminal Investigation Department (disambiguation)\nBULLET::::- Criminal Investigation Division (disambiguation)\nBULLET::::- United States Army Criminal Investigation Command\n\nBULLET::::- Center for International Development, at Harvard University\nBULLET::::- Central Institute for the Deaf\nBULLET::::- Committee of Imperial Defence, a former part of the government of Great Britain and the British Empire\nBULLET::::- Conseil International de la Danse, an umbrella organization for all forms of dance in the world\nBULLET::::- Council of Industrial Design, a UK body renamed the Design Council\nBULLET::::- University of Colombo, Centre for Instrument Development, in Sri Lanka\n\nBULLET::::- \"Clinical Infectious Diseases\", a medical journal\nBULLET::::- Cytomegalic inclusion disease\n\nBULLET::::- Collision-induced dissociation, a mass spectrometry mechanism\nBULLET::::- Compound identification number, a field in the PubChem database\nBULLET::::- Configuration interaction doubles, in quantum chemistry\n\nBULLET::::- Caller ID, a telephone service that transmits the caller's telephone number to the called party\nBULLET::::- Card Identification Number, a security feature on credit cards\nBULLET::::- Cell ID, used to identify cell phone towers\nBULLET::::- Certified Interconnect Designer, a certification for printed circuit-board designers\nBULLET::::- CID fonts, a font file format\n\nBULLET::::- Channel-iron deposits, one of the major sources of saleable iron ore\nBULLET::::- Controlled Impact Demonstration, a project to improve aircraft crash survivability\nBULLET::::- Cubic inch displacement, a measurement in internal combustion engines\n\nBULLET::::- Cid (DJ) (Carlos Cid), American DJ and record producer\nBULLET::::- Centro Insular de Deportes, an indoor sports arena in Spain\nBULLET::::- Combat Identification, the accurate characterization of detected objects for military action\nBULLET::::- Community improvement district, an area within which businesses are required to pay an additional tax\nBULLET::::- The Eastern Iowa Airport (IATA code CID), serving Cedar Rapids, Iowa\nBULLET::::- \"Corpus des inscriptions de Delphes\", a compendium of ancient Greek inscriptions from Delphi\n\nBULLET::::- Cid (disambiguation)\nBULLET::::- SID (disambiguation)\n"}
{"id": "7080", "url": "https://en.wikipedia.org/wiki?curid=7080", "title": "Christian Doppler", "text": "Christian Doppler\n\nChristian Andreas Doppler (; ; 29 November 1803 – 17 March 1853) was an Austrian mathematician and physicist. He is celebrated for his principle — known as the Doppler effect — that the observed frequency of a wave depends on the relative speed of the source and the observer. He used this concept to explain the color of binary stars.\n\nDoppler was born in Salzburg (today Austria) in 1803. After completing high school, Doppler studied philosophy in Salzburg and mathematics and physics at the Imperial–Royal Polytechnic Institute (now TU Wien), where he became an assistant in 1829. In 1835 he began work at the Prague Polytechnic (now Czech Technical University in Prague), where he received an appointment in 1841.\n\nOne year later, at the age of 38, Doppler gave a lecture to the Royal Bohemian Society of Sciences and subsequently published his most notable work, \"Über das farbige Licht der Doppelsterne und einiger anderer Gestirne des Himmels\" (\"On the coloured light of the binary stars and some other stars of the heavens\"). There is a facsimile edition with an English translation by Alec Eden. In this work, Doppler postulated his principle (later coined the Doppler effect) that the observed frequency of a wave depends on the relative speed of the source and the observer, and he later tried to use this concept for explaining the colour of binary stars.\n\nThis was independently at the same time when physicist Armand Hippolyte Louis Fizeau () became also involved in aspects of the discovery of the Doppler effect, which is known by the French as the \"Doppler-Fizeau Effect\". Fizeau contributed towards understanding its effect with light rather than sound, and in doing so, corrected many of Doppler's persistent errors. He also developed the formal mathematical theorem underlying the principles of this effect. In 1848, he discovered the frequency shift of a wave when the source and receiver are moving relative to each other, therefore being the first to predict blue shifts and red shifts of light waves.\n\nDoppler continued working as a professor at the Prague Polytechnic, publishing over 50 articles on mathematics, physics and astronomy, but in 1847 he left Prague for the professorship of mathematics, physics, and mechanics at the Academy of Mines and Forests (its successor is the University of Miskolc) in Selmecbánya (then Kingdom of Hungary, now Banská Štiavnica, Slovakia),\n\nDoppler's research was interrupted by the revolutionary incidents of 1848. During the Hungarian Revolution, he fled to Vienna. There he was appointed head of the Institute for Experimental Physics at the University of Vienna in 1850. While there, Doppler, along with Franz Unger, influenced the development of young Gregor Mendel, the founding father of genetics, who was a student at the University of Vienna from 1851 to 1853.\n\nDoppler died on 17 March 1853 at age 49 from a pulmonary disease in Venice (at that time part of the Austrian Empire). His tomb, found by Dr. Peter M. Schuster, is just inside the entrance of the Venetian island cemetery of San Michele.\n\nSome confusion exists about Doppler's full name. Doppler referred to himself as Christian Doppler. The records of his birth and baptism stated Christian \"Andreas\" Doppler. Forty years after Doppler's death the misnomer \"Johann\" Christian Doppler was introduced by the astronomer Julius Scheiner. Scheiner's mistake has since been copied by many.\n\nOn November 29, 2017, Google celebrated his 214th birthday with a Google Doodle.\n\nBULLET::::- \"Christian Doppler (1803–1853)\". Wien: Böhlau, 1992.\nBULLET::::- Bd. 1:\nBULLET::::- 1. Teil: Helmuth Grössing (unter Mitarbeit von B. Reischl): \"Wissenschaft, Leben, Umwelt, Gesellschaft\";\nBULLET::::- 2. Teil: Karl Kadletz (unter Mitarbeit von Peter Schuster und Ildikó Cazan-Simányi) \"Quellenanhang\".\nBULLET::::- Bd. 2:\nBULLET::::- 3. Teil: Peter Schuster: \"Das Werk\".\n\nBULLET::::- List of Austrian scientists\nBULLET::::- List of Austrians\nBULLET::::- List of minor planets named after people\n\nBULLET::::- Alec Eden: \"Christian Doppler: Leben und Werk.\" Salzburg: Landespressebureau, 1988.\nBULLET::::- Hoffmann, Robert (2007). \"The Life of an (almost) Unknown Person\". Christian Doppler's Youth in Salzburg and Vienna. In: Ewald Hiebl, Maurizio Musso (Eds.), \"Christian Doppler – Life and Work. Principle an Applications\". Proceedings of the Commemorative Symposia in Salzburg, Salzburg, Prague, Vienna, Venice. Pöllauberg/Austria, Hainault/UK, Atascadero/US, pages 33 – 46.\n\n"}
{"id": "7081", "url": "https://en.wikipedia.org/wiki?curid=7081", "title": "Clerihew", "text": "Clerihew\n\nA clerihew () is a whimsical, four-line biographical poem invented by Edmund Clerihew Bentley. The first line is the name of the poem's subject, usually a famous person put in an absurd light, or revealing something unknown or spurious about them. The rhyme scheme is AABB, and the rhymes are often forced. The line length and metre are irregular. Bentley invented the clerihew in school and then popularized it in books. One of his best known is this (1905):\n\nA clerihew has the following properties:\nBULLET::::- It is biographical and usually whimsical, showing the subject from an unusual point of view; it mostly pokes fun at famous people\nBULLET::::- It has four lines of irregular length and metre for comic effect\nBULLET::::- The rhyme structure is AABB; the subject matter and wording are often humorously contrived in order to achieve a rhyme, including the use of phrases in Latin, French and other non-English languages\nBULLET::::- The first line contains, and may consist solely of, the subject's name. According to a letter in \"The Spectator\" in the 1960s, Bentley said that a true clerihew has to have the name \"at the end of the first line\", as the whole point was the skill in rhyming awkward names.\n\nClerihews are not satirical or abusive, but they target famous individuals and reposition them in an absurd, anachronistic or commonplace setting, often giving them an over-simplified and slightly garbled description.\n\nThe form was invented by and is named after Edmund Clerihew Bentley. When he was a 16-year-old pupil at St Paul's School in London, the lines of his first clerihew, about Humphry Davy, came into his head during a science class. Together with his schoolfriends, he filled a notebook with examples. The first use of the word in print was in 1928. Bentley published three volumes of his own clerihews: \"Biography for Beginners\" (1905), published as \"edited by E. Clerihew\"; \"More Biography\" (1929); and \"Baseless Biography\" (1939), a compilation of clerihews originally published in \"Punch\" illustrated by the author's son Nicolas Bentley.\n\nG. K. Chesterton, a friend of Bentley, was also a practitioner of the clerihew and one of the sources of its popularity. Chesterton provided verses and illustrations for the original schoolboy notebook and illustrated \"Biography for Beginners\". Other serious authors also produced clerihews, including W. H. Auden, and it remains a popular humorous form among other writers and the general public. Among contemporary writers, the satirist Craig Brown has made considerable use of the clerihew in his columns for \"The Daily Telegraph\".\n\nThere has been newfound popularity of the form on Twitter.\n\nBentley's first clerihew, published in 1905, was written about Sir Humphry Davy:\nThe original poem had the second line \"Was not fond of gravy\"; but the published version has \"Abominated gravy\".\n\nOther clerihews by Bentley include:\n\nand\nW. H. Auden's \"Academic Graffiti\" (1971) includes:\nSatirical magazine \"Private Eye\" noted Auden's work and responded:\n\nA second stanza aimed a jibe at Auden's publisher, Faber and Faber.\n\nAlan Turing, one of the founders of computing, was the subject of a clerihew written by the pupils of his \"alma mater\", Sherborne School in England:\nA clerihew appreciated by chemists is cited in \"Dark Sun\" by Richard Rhodes, and regards the inventor of the thermos bottle (or Dewar flask): \n\"Dark Sun\" also features a clerihew about the German-British physicist and Soviet nuclear spy Klaus Fuchs:\nIn 1983, \"Games\" magazine ran a contest titled \"Do You Clerihew?\" The winning entry was:\nThe clerihew form has also occasionally been used for non-biographical verses. Bentley opened his 1905 \"Biography for Beginners\" with an example, entitled \"Introductory Remarks\", on the theme of biography itself:\n\nThe third edition of the same work, published in 1925, included a \"Preface to the New Edition\" in 11 stanzas, each in clerihew form. One stanza ran:\n\nBULLET::::- Balliol rhyme\nBULLET::::- Double dactyl\nBULLET::::- Light verse\n\nBULLET::::- Teague, Frances (1993). \"Clerihew\". Preminger, Alex; Brogan, T. V. F. (ed.), \"The New Princeton Encyclopedia of Poetry and Poetics\". Princeton University Press. pp. 219–220.\n"}
